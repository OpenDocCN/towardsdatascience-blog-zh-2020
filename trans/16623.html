<html>
<head>
<title>Quantize Your Deep Learning Model to Run on an NPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量化你的深度学习模型在NPU上运行</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5?source=collection_archive---------24-----------------------#2020-11-16">https://towardsdatascience.com/quantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5?source=collection_archive---------24-----------------------#2020-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="185d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">准备TensorFlow模型，以在集成在<a class="ae kf" href="https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/" rel="noopener ugc nofollow" target="_blank"> phyBOARD-Pollux </a>中的<a class="ae kf" href="https://www.nxp.com/products/processors-and-microcontrollers/arm-processors/i-mx-applications-processors/i-mx-8-processors/i-mx-8m-plus-arm-cortex-a53-machine-learning-vision-multimedia-and-industrial-iot:IMX8MPLUS" rel="noopener ugc nofollow" target="_blank"> i.MX 8M Plus </a> NPU上运行推理</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/2393d2ae1940062afbdbd9ebe0b905b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2aBhPs9UXIvr_Bdd2fEWUg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片由加拿大CC Liam Huang和mikemacmarketing提供</p></figure><h1 id="2fc1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">目录</h1><ul class=""><li id="f075" class="lo lp iq lq b lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="lq ir">简介</strong><br/>——为什么大多数人工神经网络都是在float32中训练的，NPU却使用int8？<br/> -先决条件</li><li id="c35c" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><strong class="lq ir">使用TensorFlow版本2.x </strong> <br/>进行训练后量化——第一种方法——直接量化已训练的模型<br/>——第二种和第三种方法——量化来自*.h5或*的已保存模型。pb文件</li><li id="db3c" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><strong class="lq ir">使用TensorFlow以下版本转换</strong></li></ul><h1 id="58d1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="31e1" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">在本文中，我们将在本文中解释使用不同的TensorFlow版本对您的模型进行变换和量化需要采取哪些步骤。我们只关注培训后的量化。</p><p id="793d" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">我们使用<a class="ae kf" href="https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/" rel="noopener ugc nofollow" target="_blank"> phyBOARD-Pollux </a>来运行我们的模型。phyBOARD-Pollux集成了一个<a class="ae kf" href="https://www.phytec.de/produkte/system-on-modules/phycore-imx-8m-plus/" rel="noopener ugc nofollow" target="_blank"> i.MX 8M Plus </a>，它具有来自VeriSilicon (Vivante VIP8000)的专用神经网络加速器IP。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nd"><img src="../Images/e778b1d3705b5f09cce06e5ad0dbcb9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40lL1qSawrcZFWNkY0A7qA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kf" href="https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/" rel="noopener ugc nofollow" target="_blank"> phyBOARD pollux </a> [ <em class="ne">图片经由phytec.de授权给Jan Werth】</em></p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nf"><img src="../Images/ea025d6c1bd573579cf4ac0b7d8d0043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGGm3advp3keABHQig0SPw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">恩智浦i.mx8MPlus框图[图片来自phytec.de，经恩智浦许可]</p></figure><p id="aedf" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">由于恩智浦的神经处理单元(NPU)需要完全int8量化模型，我们必须研究TensorFlow lite或PyTorch模型的完全int8量化。恩智浦的eIQ库支持这两个库。这里我们只研究张量流变体。</p><p id="34c1" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">关于如何进行训练后量化的概述可以在<a class="ae kf" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank"> TensorFlow网站</a>上找到。</p><h2 id="3876" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lv nl nm li lx nn no lk lz np nq lm nr bi translated">为什么NPU使用int8，而大多数人工神经网络都是用float32训练的？</h2><p id="f54e" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">浮点运算比整数运算更复杂(算术运算，避免溢出)。这导致只能使用更简单和更小的算术单元，而不是更大的浮点单元。​</p><p id="8d6f" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">float32操作所需的物理空间比int8大得多。这导致:</p><ul class=""><li id="9289" class="lo lp iq lq b lr my lt mz lv ns lx nt lz nu mb mc md me mf bi translated">更低的功耗，</li><li id="5856" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">较少的热量产生，</li><li id="6766" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">加入更多计算单元的能力减少了推理时间。</li></ul><h2 id="1cae" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lv nl nm li lx nn no lk lz np nq lm nr bi translated">先决条件</h2><p id="57fc" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">要首先在您的PC上创建代码，我们建议在运行Python 3.6、TensorFlow 2.x、numpy、opencv-python和pandas的虚拟环境中使用Anaconda。</p><p id="29ed" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">克隆环境的环境文件可以在<a class="ae kf" href="https://github.com/JanderHungrige/tf.keras-vggface/tree/main/Anaconda" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="c2bc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">tensor flow 2 . x版训练后量化</h1><p id="3366" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">如果您通过tf.keras创建并训练了一个模型，那么有三种类似的方法来量化这个模型。</p><h2 id="eafc" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lv nl nm li lx nn no lk lz np nq lm nr bi translated">第一种方法——直接量化训练好的模型</h2><p id="d9c5" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">经过训练的TensorFlow模型必须转换为TFlite模型，并且可以直接量化，如以下代码块中所述。对于训练的模型，我们示例性地使用基于<a class="ae kf" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank"> rcmalli </a>的工作的更新的<a class="ae kf" href="https://github.com/JanderHungrige/tf.keras-vggface" rel="noopener ugc nofollow" target="_blank"> tf.keras_vggface </a>模型。转换从第28行开始。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="25b6" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">在加载/训练您的模型后，您首先必须创建一个代表性数据集。转换器使用代表性数据集来获取最大值和最小值，以便估算比例因子。这限制了从float32到intX的量化所引入的误差。误差来源于浮点数和整数的不同的数空间。从float到int8的转换将数字空间限制为-128到127之间的整数值。根据输入的动态范围校准模型可以限制这种误差。</p><p id="a1bd" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">在这里，你可以像我们的例子一样循环遍历你的图像或者创建一个生成器。我们使用<a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank"><em class="nx">TF . keras . preprocessing . image . imagedata generator()</em></a>生成图像，并对图像进行必要的预处理。作为发电机，你当然也可以使用<a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors" rel="noopener ugc nofollow" target="_blank"> tf。<em class="nx">数据。dataset . from _ tensors()</em></a><em class="nx"/>或<em class="nx">…</em><a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices" rel="noopener ugc nofollow" target="_blank"><em class="nx">from . tensor _ slices()</em></a><em class="nx">。请记住，在这里对您的数据进行与您训练网络所用的数据相同的预处理(标准化、调整大小、去噪等)。这些都可以打包到生成器的<em class="nx">预处理_函数</em>调用中(第19行)。</em></p><p id="8a5a" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">转换从第28行开始。简单的TensorFlow lite转换如下所示:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="aed6" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">量化部分介于两者之间:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><ul class=""><li id="ad03" class="lo lp iq lq b lr my lt mz lv ns lx nt lz nu mb mc md me mf bi translated">第3行:不推荐使用默认优化以外的优化。目前(2020年)没有其他选择。</li><li id="b531" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">第4行:这里我们设置了代表性的数据集。</li><li id="e008" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">第5行:在这里，我们确保完全转换为int8。如果没有此选项，只有权重和偏差会被转换，而不是激活。当我们只想减小模型尺寸时，使用这种方法。然而，我们的NPU需要完整的int8量化。激活仍处于浮点状态会导致整体浮点，并且无法在NPU上运行。</li><li id="d656" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">第6行:启用基于MLIR的转换，而不是TOCO转换，这使得RNN支持，更容易的错误跟踪，<a class="ae kf" href="https://groups.google.com/a/tensorflow.org/g/tflite/c/C7Ag0sUrLYg?pli=1" rel="noopener ugc nofollow" target="_blank">和更多的</a>。</li><li id="2381" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">第7行:将内部常量值设置为int8。target_spec对应于第5行的TFLITE_BUILTINS。</li><li id="7c6e" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">第9行和第10行:也将输入设置为int8。这在TF 2.3中是完全可用的</li></ul><p id="726e" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">现在，如果我们使用TF2.3转换模型</p><ul class=""><li id="4620" class="lo lp iq lq b lr my lt mz lv ns lx nt lz nu mb mc md me mf bi translated"><em class="nx">experimental _ new _ converter = True</em></li><li id="d9ad" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><em class="nx">推论_输入_类型=tf.int8 </em></li><li id="de03" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><em class="nx">推论_输出_类型=tf.int8 </em></li></ul><p id="fd4a" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">我们收到以下模型:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ny"><img src="../Images/83e39c213400dddfe7e73b332ac27388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*praIgT3_W4kz7MeH6NiSRA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="ne">作者图片</em></p></figure><p id="ac97" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">然而，如果我们不设置<em class="nx">推理_输入_类型</em>和<em class="nx">推理_输出_类型</em>，我们将收到以下模型:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nz"><img src="../Images/49793963edae78aed0569f00068f47a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2O6xO4D0pm13BPJvvJCMQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="ne">作者图片</em></p></figure><p id="6e40" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">因此，效果是您可以确定模型接受和返回哪种输入数据类型。如果您使用嵌入式摄像机，这一点可能很重要，如<a class="ae kf" href="https://www.phytec.de/produkte/development-kits/phyboard-pollux-ki-kit/" rel="noopener ugc nofollow" target="_blank"> pyhBOARD-Pollux </a>所包含的。mipi摄像头返回8位值，所以如果你想转换成float32 int8输入会很方便。但是请注意，如果您使用没有预测层的模型来获得(例如嵌入)，int8输出将导致非常差的性能。这里推荐使用float32输出。这说明每个问题都需要特定的解决方案。</p><h2 id="c262" class="ng kx iq bd ky nh ni dn lc nj nk dp lg lv nl nm li lx nn no lk lz np nq lm nr bi translated">第二种和第三种方法—从*.h5或*量化保存的模型。pb文件</h2><p id="8c3a" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">如果你已经有了你的模型，你最有可能把它保存在某个地方，作为一个Keras h5文件或者一个TensorFlow协议缓冲pb。我们将使用TF2.3快速保存我们的模型:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="180e" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">下面的转换和量化与方法一非常相似。唯一的区别是我们如何用转换器加载模型。要么加载模型，然后像方法一那样继续。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="3e90" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">或者直接加载h5模型。使用TensorFlow版本2及更高版本时，您必须使用兼容的转换器:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="7240" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">如果从TensorFlow pb文件加载，请使用:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="9ba8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用低于2.0的TensorFlow版本进行转换</h1><p id="20c8" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">如果你想转换一个用TensorFlow版本编写的模型&lt; 1.15.3 using Keras, not all options are available for TFlite conversion and quantization. The best way is to save the model with the TensorFlow version it was created in (e.g., rcmalli keras-vggface was trained in TF 1.13.2). I would suggest not using the “<a class="ae kf" href="https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125" rel="noopener ugc nofollow" target="_blank">保存</a>和<a class="ae kf" href="https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc" rel="noopener ugc nofollow" target="_blank">冻结图形</a>的方法来创建一个pb文件，因为TF1和TF2的pb文件不同。<em class="nx">tflite converter . from _ saved _ model</em>不工作，为实现量化制造了相当大的麻烦。我建议使用Keras的上述方法:</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="b466" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">然后，从1.15.3开始，使用TensorFlow版本转换和量化您的模型。在这个版本的基础上，增加了很多功能，为TF2做准备。我建议使用最新版本。这将导致前面介绍的相同模型。</p><p id="535c" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">祝你好运，玩得开心。</p></div></div>    
</body>
</html>