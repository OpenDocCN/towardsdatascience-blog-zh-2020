<html>
<head>
<title>‘Drawing’ the inner world of a story using GauGAN in a real environment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在真实环境中使用GauGAN绘制故事的内心世界</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/drawing-the-inner-world-of-a-story-using-gaugan-in-a-real-environment-d8e303aaa2f9?source=collection_archive---------41-----------------------#2020-09-23">https://towardsdatascience.com/drawing-the-inner-world-of-a-story-using-gaugan-in-a-real-environment-d8e303aaa2f9?source=collection_archive---------41-----------------------#2020-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ab6d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><a class="ae kf" href="https://avner.js.org/" rel="noopener ugc nofollow" target="_blank"> <em class="kg">艾夫纳·佩莱德</em> </a> <em class="kg">和我正在分享</em>骨髓<a class="ae kf" href="https://shirin.works/Marrow-dev-phase-Machine-learning-immersive-theater-WIP" rel="noopener ugc nofollow" target="_blank"/><em class="kg">发育阶段的经验教训。这篇文章着眼于我们如何在360°环境中实时使用</em><a class="ae kf" href="https://blogs.nvidia.com/blog/2019/07/30/gaugan-ai-painting/" rel="noopener ugc nofollow" target="_blank"><em class="kg">GauGan</em></a><em class="kg">。</em></h2></div><p id="7c41" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">人工智能和交互式讲故事都是复杂和不可预测的系统。随着我们深入骨髓的设计过程，将这两个系统结合成一个连贯的体验的挑战变得明显。一方面，作为作者，我们开发了人工智能系统和实时交互来引导体验的流动。另一方面，我们希望讲述一个能激发参与者想象力和情感的故事。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/f823229e7d0774d1030789f618f47d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*UIWX8IP9Gz4PREc7IRKoBw.gif"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Avner Peled在测试我们的GauGan系统时拍摄的截图；一瓶水换成了花</p></figure><p id="6f31" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">《骨髓》是一个关于机器学习模型中精神疾病可能性的故事，主要关注<a class="ae kf" href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="noopener ugc nofollow" target="_blank">生成性对抗网络</a>(甘)。我们质疑在高级人工智能中会出现什么样的精神障碍，并邀请参与者在由甘操作的互动剧场场景中进行表演。他们一起扮演一个功能失调的人工智能家庭。由于我们正在处理非常抽象和复杂的概念，我们想探索多种方式来传达这个故事，而不仅仅是通过家庭成员之间的对话。我们的策略是让房间更加“生动”，反映出模特的精神状态。我们想消除参与者和环境之间的障碍；让他们慢慢沉浸在房间里不熟悉的魔法体验中。房间和晚餐的场景是一个邀请，让我们放手，沉迷于与其他三个陌生人的情感事件。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lp"><img src="../Images/a8658beffb9f77e0556c222b09c426f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7cz4cPA-BA5DvlXmo_4sw.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Andre Bendahan在NFB实验室拍摄的照片2020版权所有</p></figure><p id="7312" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">在实践中，这意味着我们必须实施经常与环境和参与者交互的GAN网络。由于GAN的训练过程不是实时发生的，这就变成了一个挑战，即操纵预训练GAN网络的输出以响应环境的实时变化。为了解释我们的解决方案，我们首先需要看看标准GANs和条件GANs之间的区别。</p><h2 id="1543" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kq md me mf ku mg mh mi ky mj mk ml mm bi translated"><strong class="ak">标准与有条件的GANs </strong></h2><p id="fbf6" class="pw-post-body-paragraph kh ki iq kj b kk mn jr km kn mo ju kp kq mp ks kt ku mq kw kx ky mr la lb lc ij bi translated">在其基本形式中，GAN被训练产生视觉上类似于训练集的新图像。如果我们使用人脸数据集，它会生成新的人脸。如果我们把它用在猫身上，它会产生新的猫。它可以通过采用输入“噪声”向量(本质上是一系列随机数)并使用它们作为输出图像的基础来保持可变性(不是每次都产生相同的图像)。因此，如果我们想要将GAN的输出与环境变化联系起来，我们需要根据这些变化来控制噪声矢量。然而，正如我们在<a class="ae kf" rel="noopener" target="_blank" href="/a-tool-for-collaborating-over-gans-latent-space-b7ea92ad63d8">之前的文章</a>中所展示的，对于改变噪声向量会产生什么样的变化，几乎没有任何控制。</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/a-tool-for-collaborating-over-gans-latent-space-b7ea92ad63d8"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd ir gy z fp na fr fs nb fu fw ip bi translated">甘潜在空间的合作工具</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">2020年1月，我们完成了骨髓的开发阶段。我和Shirin Anlen正在分享在…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj lj mv"/></div></div></a></div><p id="e440" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们可以将物理房间中的不同变量(如参与者的位置、物体的位置和参与者的情绪分析)与生成的输出联系起来，但缺乏对输出的精确控制导致了与环境的脆弱联系。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nk"><img src="../Images/4f4b2359bf6d5e42c62e171f4191ac91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MbMHZT8E6MDpV4hl9Aaqg.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">作者演示了标准GAN和条件GAN之间的差异</p></figure><p id="8885" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">这就是<em class="nl">有条件的GANs </em>进入画面的地方。我们不是在一组图像上训练，而是在由一个图像和一个标签(数字输入)组成的<em class="nl">对</em>上训练网络，当呈现特定种类的标签时，该网络被调整为生成一种类型的图像。这使得用户可以完全控制GAN如何针对特定输入产生输出。与原始GAN一样，结果仍会随着噪声矢量的变化而变化。然而，现在作者可以创造与环境有意义的互动。最著名的条件gan之一是<a class="ae kf" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank"> Pix2Pix </a>。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nm"><img src="../Images/8f73ee60853fcd3252aaa2d79b67f5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S95ATxksrPcxhOMz"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图片f<em class="kg">rom</em><a class="ae kf" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank"><em class="kg">https://phillipi.github.io/pix2pix/</em></a></p></figure><p id="8119" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">这是一个通用的图像到图像转换器。它可以根据任何类型的图像生成另一个图像。它分析两幅图像中的像素，学习如何从一种颜色转换到另一种颜色。Pix2Pix的使用方式多种多样，比如将草图转化为画作，将色彩映射表转化为照片。我们还在我们的原型中使用它来将一个人的彩色姿势分析转换成一个从家庭照片中生成的人。</p><div class="ms mt gp gr mu mv"><a href="https://immerse.news/family2family-first-steps-5f085dc75666" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd ir gy z fp na fr fs nb fu fw ip bi translated">家庭2家庭:第一步</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">2018年11月，我们呈现了《骨髓》的序幕:我一直在IDFA嫉妒别人的家庭…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">沉浸.新闻</p></div></div><div class="ne l"><div class="nn l ng nh ni ne nj lj mv"/></div></div></a></div><h2 id="3462" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kq md me mf ku mg mh mi ky mj mk ml mm bi translated">高根</h2><p id="460e" class="pw-post-body-paragraph kh ki iq kj b kk mn jr km kn mo ju kp kq mp ks kt ku mq kw kx ky mr la lb lc ij bi translated">Pix2Pix找到了自己的优势，作为一个从任何图像到任何图像的通用翻译器，它也有自己的弱点。仅仅依靠颜色会错过可以输入网络的元数据。该算法只查看形状和颜色。如果餐盘和飞碟在照片上看起来相似，它就无法区分它们。这就是NVIDIA的研究人员在创造<a class="ae kf" href="https://arxiv.org/abs/1903.07291" rel="noopener ugc nofollow" target="_blank"> <em class="nl"> GauGAN </em> </a> <em class="nl">时所解决的问题。以后印象派画家保罗·高更的名字命名，GauGAN也用彩色地图创作现实主义的图像。但是，它不是学习像素值，而是学习图像的<em class="nl">语义</em>数据。该项目也被称为<a class="ae kf" href="https://nvlabs.github.io/SPADE/" rel="noopener ugc nofollow" target="_blank"> SPADE </a> : <em class="nl">具有空间自适应归一化的语义图像合成</em>。GauGAN学习哪里有草地和天空，而不是学习图片中绿色和蓝色的位置。</em>这是可能的，因为训练集中使用的图像，如通用数据库<a class="ae kf" href="https://github.com/nightrome/cocostuff" rel="noopener ugc nofollow" target="_blank"> COCO-Stuff </a>，包含图片中不同元素的语义分类。然后，研究人员能够通过制作一种交互式绘画工具来展示GauGAN的能力，在这种工具中，颜色不仅仅是颜色，而且还有意义。当你在源草图上画绿色时，你是在告诉GauGAN这里有草。在这里自己试试。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6b2e63f395fc597bc0f580864f5da5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/1*WYwux5_2mddVeQmU0lvs4w.gif"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">图片来自<em class="kg"/><a class="ae kf" href="https://www.nvidia.com/en-us/research/ai-playground/" rel="noopener ugc nofollow" target="_blank"><em class="kg">https://www.nvidia.com/en-us/research/ai-playground/</em></a></p></figure><h2 id="2595" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kq md me mf ku mg mh mi ky mj mk ml mm bi translated"><strong class="ak">将GauGAN连接到实时360°环境</strong></h2><p id="2d4a" class="pw-post-body-paragraph kh ki iq kj b kk mn jr km kn mo ju kp kq mp ks kt ku mq kw kx ky mr la lb lc ij bi translated">GauGAN可以从手绘草图中生成真实感图像。我们的目标是让它与实时物理环境互动。解决这个问题就像拼拼图一样:</p><ol class=""><li id="0b51" class="np nq iq kj b kk kl kn ko kq nr ku ns ky nt lc nu nv nw nx bi translated">我们知道NVIDIA对GauGAN进行了语义数据方面的培训:他们使用<a class="ae kf" href="https://github.com/kazuto1011/deeplab-pytorch" rel="noopener ugc nofollow" target="_blank"> DeepLab v2网络</a>来分析<a class="ae kf" href="https://github.com/nightrome/cocostuff" rel="noopener ugc nofollow" target="_blank"> COCO-Stuff </a>数据库并产生标签。</li><li id="b2bb" class="np nq iq kj b kk ny kn nz kq oa ku ob ky oc lc nu nv nw nx bi translated">我们知道DeepLab V2可以实时分割相机流。</li><li id="95d4" class="np nq iq kj b kk ny kn nz kq oa ku ob ky oc lc nu nv nw nx bi translated">1+2:如果我们将DeepLab的相机流输出直接馈送给GauGAN，我们应该会得到它的真实镜像状态。</li></ol><p id="e54d" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">代码本身相对简单，主要与两个网络之间的格式转换有关。我们还升级了DeepLab的网络摄像头代码，以从我们的360°摄像头传输:<a class="ae kf" href="https://theta360.com/en/about/theta/z1.html" rel="noopener ugc nofollow" target="_blank">理光THETA Z1 </a>。分割网络是如此强大，以至于我们可以将加宽的拼接图像直接馈送给分割和生成。结果出奇的准确。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi od"><img src="../Images/9a2520451fd675ed6fd311879fe0bf77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0I5vtnOrOb3ch1lVqYTCKw.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">GauGAN系统流程的作者插图</p></figure><h2 id="0c3a" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kq md me mf ku mg mh mi ky mj mk ml mm bi translated"><strong class="ak">操纵甘的现实</strong></h2><p id="d090" class="pw-post-body-paragraph kh ki iq kj b kk mn jr km kn mo ju kp kq mp ks kt ku mq kw kx ky mr la lb lc ij bi translated">我们现在有一个生成的镜像，描绘了甘(可可的东西)的版本，无论相机在房间里看到什么。但是我们想要更多；我们想要一个能根据故事变化的空间，并能模拟角色的精神状态。我们寻找产生与故事世界相联系的视觉效果的方法。为了找到单词之间的含义并吸引用户继续行动，四处移动物体，看到倒影，并想知道这是怎么回事。</p><p id="7d98" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们意识到我们可以干预感知和生成的过程。就在DeepLab分析了相机流中的标签之后，为什么不用其他东西替换它们呢？例如，让我们将任何公认的碗映射到海洋。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/ca4fb1359e4dcddf4f0a3fc4cfa7d21c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*ygLXeyUiDMSRxic2ZcC9fQ.gif"/></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Avner Peled在测试我们的GauGan系统时拍摄的截图；一个碗被替换为海洋纹理</p></figure><p id="d215" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们开始寻找我们的角色的故事可以浮出水面的模式，以及物理空间可以支持整个视觉形式的模式:一张脸，一幅风景，一个物体，一朵花。故事是可识别的模式，在这些模式中，我们找到了意义。它们是噪音中的信号。</p><p id="a718" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">当我们最终到达实验室进行测试时，我们发现了物理环境的影响。我们通过排列(和重新排列)奇怪的元素和探索我们能达到的结果开始玩。我们开发了一个脚本平台，让我们可以轻松地将对象映射到其他对象。我们可以从场景中屏蔽某些对象，一次选择多个对象，或者反转选择来映射除指定对象之外的所有对象。例如:“餐桌”、“桌子”、“书桌”、“书桌用品”、“地板”、“床”、“汽车”——突然变成了同一个项目，并被映射成一片海洋，而其他所有东西都被丢弃了。虽然我们没有汽车，没有塑料，也没有床。或者“飞盘”、“纸”、“老鼠”、“金属”、“岩石”、“碗”、“酒杯”、“瓶子”——都映射到“岩石”。同样，有趣的是，我们在真实场景中没有鼠标、飞盘、金属、石头或纸张，但是网络检测到了它们。因此，我们也需要考虑它们。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oe"><img src="../Images/84cd20306d21290012d1b715cbdc7acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFvZKUKDAcG8_3IV6z5N2Q.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">来自髓的GitHub的截图</p></figure><p id="c51e" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">如果这还不够，我们发现灯光、阴影和相机角度的变化每次都会产生不同的标签，这打乱了我们的贴图。在一个互动的讲故事框架中，这感觉既不可思议又可怕。在开幕之前，我们有不到十天的时间来完善空间和调试技术，同时了解我们刚刚开发的产品可以创造的各种可能性。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi of"><img src="../Images/6beb1f0021b97845ef14d044d7a42127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vTS4pgtbUtBPYjd8Sd24IQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">作者照片。第一个实验——左侧:我们如何排列物品。右侧:高根人是如何绘制和投射图像的。</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nm"><img src="../Images/360d0eb9a7996bf1bf95753ab4e719b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BTdX94GIDuLZhX4o"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">作者的照片展示了布景的原型:碗和瓶子变成了船，桌子变成了海洋。</p></figure><p id="a3da" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">我们和我们的网络一起玩，很少控制视觉效果，我们希望形象化我们角色的内心世界。</p><p id="4dae" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">慢慢地，我们开始学习这个系统——什么可行，什么不可行，如何清理场景，如何稳定灯光。我们还决定投影这个过程的两个阶段，DeepLab的彩色分段分析和GAN生成的输出。渐渐地，物理环境变得更加身临其境，可以与故事的文字联系起来。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lp"><img src="../Images/df55e07a81b66b32b4c8b01e7a42b5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MfmXTdXCoRr5cWwe0tetA.jpeg"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Andre Bendahan在NFB实验室拍摄的照片2020版权所有</p></figure><h2 id="0ddf" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kq md me mf ku mg mh mi ky mj mk ml mm bi translated"><strong class="ak">倒影</strong></h2><ul class=""><li id="5308" class="np nq iq kj b kk mn kn mo kq og ku oh ky oi lc oj nv nw nx bi translated">预训练的SPADE/GauGan网络的分辨率以较低的256x256分辨率生成图像。很难让人们参与到这种视觉中，并让他们理解他们所看到的。实现更高的分辨率需要我们在培训中投入更多的资源，这在当时是不可能的。</li><li id="196c" class="np nq iq kj b kk ny kn nz kq oa ku ob ky oc lc oj nv nw nx bi translated">因为GauGAN具有语义意识，所以图像的上下文非常重要。例如，将一张桌子映射到大海，而将混凝土墙留在背景中，会生成一个昏暗的湖泊或池塘。但是把墙映射成蓝天，现在海看起来更像海洋了。</li><li id="878a" class="np nq iq kj b kk ny kn nz kq oa ku ob ky oc lc oj nv nw nx bi translated">由于这种上下文感知，也很难用孤立的对象来传达意思。当我们完整地展示这些图像时，它们通常看起来是最好的。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ok"><img src="../Images/466a5cf1f3a318ab22e5b0a44585721d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkOeK-eGG-Vx9WcrG3hoKQ.png"/></div></div><p class="ll lm gj gh gi ln lo bd b be z dk translated">Andre Bendahan在NFB实验室拍摄的照片2020版权所有</p></figure><p id="69c5" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">虽然我们仍然觉得围绕我们的故事还有很多实验和润色的空间，但这些结果让我们第一次看到了甘的“意识”，作为一个产生其内心世界的感知实体。这样的过程与人类意识的哲学产生共鸣。</p><p id="8ad9" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">伊曼纽尔·康德的<a class="ae kf" href="https://plato.stanford.edu/entries/kant-mind/#3.2" rel="noopener ugc nofollow" target="_blank">先验哲学</a>谈到了<em class="nl">综合的行为:</em>我们的表象一起行动，塑造一个统一的意识。在现代神经科学中，我们谈到了意识的<a class="ae kf" href="https://en.wikipedia.org/wiki/Neural_correlates_of_consciousness" rel="noopener ugc nofollow" target="_blank">神经关联</a>，它描述了意识所需的神经活动，不是作为物体识别的离散前馈机制，而是统一体验的长期持续反馈波。这也是我们希望在髓室设计的体验类型，最终的“编辑”发生在参与者的头脑中。</p><p id="b606" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">有一件事我们确信不会给这项创造性的工作带来伤害——那就是让更多的人使用它。除非你做了很多次，否则你不会知道你在做什么，尤其是在这种复杂的项目中。只是制造制造制造。</p><p id="609c" class="pw-post-body-paragraph kh ki iq kj b kk kl jr km kn ko ju kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kj ir">这里是项目的</strong> <a class="ae kf" href="https://github.com/Marrow-AI/gan-deeplab-spade?fbclid=IwAR2yixniZbSSwbZY97zadd7wlMZ85OaEMFFpRMJn0AiiKpfQtTEqSHPmxcY" rel="noopener ugc nofollow" target="_blank"> <strong class="kj ir">开源GitHub库</strong> </a> <strong class="kj ir">。</strong>请和我们分享你的制作和想法！</p><blockquote class="ol om on"><p id="729c" class="kh ki nl kj b kk kl jr km kn ko ju kp oo kr ks kt op kv kw kx oq kz la lb lc ij bi translated"><em class="iq">开发阶段由音效师菲利普·兰伯特</em>  <em class="iq">和动画师帕洛玛·道金斯</em>  <em class="iq">合作完成。合拍中的</em> <a class="ae kf" href="https://www.nfb.ca/interactive/marrow/" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> NFB互动</em></a><em class="iq"/><a class="ae kf" href="https://atlasv.io/ive-always-been" rel="noopener ugc nofollow" target="_blank"><em class="iq">图集五</em> </a></p></blockquote></div></div>    
</body>
</html>