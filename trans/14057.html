<html>
<head>
<title>Neural Network through scratch coding in R; A novice guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过R中的划痕编码的神经网络；新手向导</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-a-neural-network-through-scratch-coding-in-r-a-novice-guide-a81e5777274b?source=collection_archive---------47-----------------------#2020-09-27">https://towardsdatascience.com/understanding-a-neural-network-through-scratch-coding-in-r-a-novice-guide-a81e5777274b?source=collection_archive---------47-----------------------#2020-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="24b3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">动手香草建模第二部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3e5b654b9811e3fb35ad32cffc48368f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6wse9D4fOWR0esqISqmPw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="814c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你曾经深入研究过数据科学的世界，那么我认为你现在肯定在探索机器学习和人工智能或一般数据科学的旅程中的某个时候，在某个地方遇到过术语<strong class="la iu"> <em class="lu">【神经网络</em> </strong>，这并不荒谬。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/5c7455dae83e4ae44e2fb60b7cfd69d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9aGX80WLR-XUiAanEebiEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。只有一个隐藏层的神经网络(图片由作者提供)</p></figure><p id="457a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最被认可的神经网络(NN)定义是<em class="lu">它是一种受大脑启发的计算机架构，包含各种功能的网络拓扑，其中节点以特定的方式相互连接，通过迭代地遵循一系列算法来揭示数据集中的潜在模式(图1)。</em>也许，只要有一点数学上的成熟和一点最优化理论的知识，把神经网络称为函数逼近器或回归器就简单得犯罪了。从我以前的文章中，我们通过一个实际的例子看到了Keras和Tensorflow等高级API如何使构建和训练神经网络变得非常简单。然而，Keras和Tesnsorflow所提供的简单性困扰着一个具有顽强依赖性的初学者，并使新手真正难以理解<strong class="la iu"> <em class="lu">学习</em> </strong> <em class="lu"> </em>(前馈-反向传播)过程背后的实际底层数学和推理。从我的文章中，你可能会发现我是一个非常视觉化的人，当谈到学习时，我认为引人入胜的视觉效果有助于更深层次的理解。在这里，我将尝试稍微揭开这个黑匣子，我们将温和地深入到包含<strong class="la iu">梯度下降优化器</strong>的<strong class="la iu">神经网络</strong>的数学形式中，并全面尝试构建我们自己的网络。如果你对初等线性代数和微分方程不太熟悉，不要担心，因为视觉效果足以让这个概念在你的大脑控制台中滚动。同样，如果你对这些数学前提感到满意，那么你一定会意识到这个模型是多么的夸张。</p><p id="3f4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们开始吧！！</p><h2 id="e083" class="lw lx it bd ly lz ma dn mb mc md dp me lh mf mg mh ll mi mj mk lp ml mm mn mo bi translated">神经网络<strong class="ak"> <em class="mp">架构</em> </strong></h2><p id="e39c" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">带有一个隐藏层的简单神经网络架构如图2所示。第一层叫做<strong class="la iu"> <em class="lu">输入层</em> </strong>。数据集的每个特征充当单个<strong class="la iu"> <em class="lu">神经元/节点</em> </strong>(红色)。给定一组<strong class="la iu"> <em class="lu">权重</em> </strong>，这些节点的线性组合连同<strong class="la iu"><em class="lu"/></strong>(蓝色)生成下一个顺向层的单个节点，称为<strong class="la iu"> <em class="lu">隐藏层</em> </strong>(黄色)。注意，输入层的<strong class="la iu"> iᵗʰ </strong>节点在形成隐藏层值<strong class="la iu"> hⱼ </strong>的<strong class="la iu"> jᵗʰ </strong>节点中的贡献是iᵗʰ节点值<strong class="la iu"> Xᵢ </strong>和jᵗʰ权重元组即wⱼᵢ.的iᵗʰ元素的乘积然后对隐藏层和输出层重复类似的方案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/afd5391d4d389150002ab5e967ff6e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*M9NCQF9mLB_WgCGwMg7qmw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。阐释神经网络(图片由作者提供)</p></figure><p id="6164" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这一点上，我建议您尝试将权重元组可视化为列向量，组合形成一个矩阵<strong class="la iu"> W </strong>行等于当前层中的节点数，列等于后续层中的节点数。因此，对于动画中的给定网络，对应于输入层和隐藏层的权重矩阵<strong class="la iu"> W </strong>将为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/42605fda3afce596b248e503a412dafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*PVhffT4Q-RF4rzTK3YyxBg.png"/></div></figure><p id="58db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种对权重的矩阵理解对于理解即将到来的基础数学很重要。</p><p id="0ecf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们都知道神经网络的概念化是受大脑功能的启发，在这一点上，提到脑细胞/神经元的突触末端及其与神经网络的类比是很重要的。由上述操作产生的每个神经元值都类似于脑细胞携带的刺激，并且应该将其传递给下一个连接的神经元。轴突的突触末端有一种机制，决定刺激是否足够强大，可以传递到下一个神经元的树突。类似地，该激活由NN中的<strong class="la iu">激活功能</strong>执行。因此，准确地说，每个生成的节点值都通过一个激活函数传递，该函数决定是否传递激励以及传递多少激励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/009c4213848dd1b6fcbc058964fbaa52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3QFvxDlIOjW1FlGCdToUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。范围从-inf到+inf的sigmoid函数(图片由作者提供)</p></figure><p id="c427" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最常用的激活函数有<strong class="la iu"> Sigmoid </strong>、<strong class="la iu"> ReLU、Tanh、</strong>和<strong class="la iu"> Softmax </strong>(广义的Sigmoid)。虽然ReLU函数只允许传递正值，但是sigmoid激活函数，<strong class="la iu"> <em class="lu"> S(x) </em> </strong>唯一地将任意实数<em class="lu"> x </em>映射到范围(0，1)中图3。数学上，<strong class="la iu"><em class="lu">s(x)=1/(1+eˣ)</em></strong>。有趣的是，<strong class="la iu"><em class="lu"/></strong>的微分即<strong class="la iu"> <em class="lu"> dS(x)/dx </em> </strong>等于<strong class="la iu"> <em class="lu"> S(x)(1-S(x)) </em> </strong>。这在计算梯度时带来了极大的数学便利，也是为什么sigmoid是最著名的激活函数的原因。这个从一层到另一层的信息转换过程叫做<strong class="la iu">前馈</strong>。</p><p id="1bb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦估计了最终输出层，则使用误差指数函数<strong class="la iu"> <em class="lu"> P </em> </strong>来评估性能。数学上，<strong class="la iu"><em class="lu">P =</em>ll<em class="lu">d—z</em>ll<em class="lu">/2，</em> </strong>其中<strong class="la iu"> <em class="lu"> d </em> </strong>是实际/期望输出，<strong class="la iu"> <em class="lu"> z </em> </strong>是估计输出。优化目标是最小化<strong class="la iu"> <em class="lu"> P. </em> </strong>实践中有许多优化器，如牛顿下降法、ADAM等。这里我们将重点介绍最流行、应用最广泛的<strong class="la iu">梯度下降</strong>法。</p><p id="6988" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然<strong class="la iu"> <em class="lu"> P </em> </strong>是<strong class="la iu"> <em class="lu"> z </em> </strong>的函数，而后者又是权重<strong class="la iu"><em class="lu"/></strong>W<strong class="la iu"><em class="lu"/></strong>b .所以要尽量减少<strong class="la iu"> <em class="lu"> P、</em> </strong>权重、<strong class="la iu"><em class="lu">的变化</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/39e6d5bb6693410c865e29da94c19666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhH4kScIvc548Xnuu11N-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4(作者图片)</p></figure><p id="fa81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们看看如何计算这些所需的梯度。因此，如果我们用图2中的实例映射这个前奏，这个过程看起来就像下图(图5)。注意，为了计算性能指数相对于权重的梯度，即<strong class="la iu"> <em class="lu"> dP/dW </em> </strong> ₁和<strong class="la iu"> <em class="lu"> dP/dW </em> </strong> ₂，我们从过程的右端开始应用链式法则，向后传播。(回想一下，性能指标是<strong class="la iu"><em class="lu">P =</em>ll<em class="lu">d—z</em>ll<em class="lu">/2。</em> </strong>)。这个过程被称为<strong class="la iu">反向传播。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/afd2e45be36888399b3adca9edd650b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*03YdXYNDovf3Sqc7CchVcA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。反向传播中的链式法则(图片由作者提供)</p></figure><p id="efff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度<strong class="la iu">▽<em class="lu">p =σdpᵢ</em></strong><strong class="la iu"><em class="lu">/dwᵢ</em></strong>对于i = 1，2，3…，k+1其中k为建筑深度。一旦<strong class="la iu"> ▽ <em class="lu"> P </em> </strong>被评估，权重和偏差分别升级为:</p><p id="b16f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"><em class="lu">W _ new = W _ current</em></strong><em class="lu">—</em><strong class="la iu"><em class="lu">【η*(T38】<em class="lu">【P】</em></em></strong><strong class="la iu"><em class="lu">b _ new = b _ current</em></strong><em class="lu">—</em><strong class="la iu"><em class="lu">【η*(T50】<em class="lu">【P)</em></em></strong></p><p id="42ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中值<strong class="la iu"> <em class="lu"> η </em> </strong>为学习率。这两个方程非常重要，我们会经常用到它们。有了这些笔记，我们现在将一步一步地制作我们的模型</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="1245" class="nh lx it bd ly ni nj nk mb nl nm nn me jz no ka mh kc np kd mk kf nq kg mn nr bi translated"><strong class="ak">逐步建模</strong></h1><p id="cf61" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">为了建模一个简单的基于神经网络的分类模型，我将使用Iris数据集，该数据集具有四个特征和一个包含三个不同类别的标签向量。我们的模型将包含两个大小为4和3神经元的隐藏层。该模型如图6所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/42dbe5b9fed4b2b11853978adff67c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5bxjSbhiHrOVzvUCDyzJQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。我们的模型(图片由作者提供)</p></figure><h2 id="00c1" class="lw lx it bd ly lz ma dn mb mc md dp me lh mf mg mh ll mi mj mk lp ml mm mn mo bi translated">一个热编码</h2><p id="15ed" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">一个热编码，这是类<strong class="la iu">编码的</strong>标签被移除并且为每个唯一标签值添加新的二进制变量{0，1}的地方。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="702b" class="lw lx it nu b gy ny nz l oa ob">set.seed(123)<br/>data&lt;-as.matrix(iris) #iris dataset</span><span id="86a8" class="lw lx it nu b gy oc nz l oa ob">classes&lt;-unique(data[,5])# retreive unique classes</span><span id="ed4f" class="lw lx it nu b gy oc nz l oa ob">###########ONE HOT ENCODING###########<br/>#Initializing an empty matrix<br/>hot_encode&lt;-matrix(0, ncol=length(unique(classes)), byrow = T,<br/>                   nrow= nrow(data), <br/>                   dimnames = list(NULL, c(unique(classes))))</span><span id="bf95" class="lw lx it nu b gy oc nz l oa ob">#Imputing 1 at respective class</span><span id="4009" class="lw lx it nu b gy oc nz l oa ob">for(i in 1:nrow(data)){<br/>  for(j in 1:3){<br/>    if(data[i,5] == classes[j]){hot_encode[i,j]&lt;-1} <br/>    else next<br/>  }<br/>}</span><span id="9800" class="lw lx it nu b gy oc nz l oa ob"># Combining the data and encoded labels<br/>data&lt;-as.matrix(cbind(iris[,1:4], hot_encode))<br/>data&lt;-data[sample(1:nrow(data)), ]</span><span id="8667" class="lw lx it nu b gy oc nz l oa ob">set.seed(123)<br/>seq&lt;-sample(1:nrow(data)) # preserving the shuffle order</span><span id="d9ae" class="lw lx it nu b gy oc nz l oa ob">head(data)</span></pre><h2 id="cae3" class="lw lx it bd ly lz ma dn mb mc md dp me lh mf mg mh ll mi mj mk lp ml mm mn mo bi translated"><strong class="ak">定义功能</strong></h2><p id="a132" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">我们将需要一个Sigmoid激活函数<strong class="la iu"><em class="lu">【S(x)】</em></strong>及其导数函数<strong class="la iu"><em class="lu">【S(x)【1-S(x)】</em></strong>。此外，我们需要规范化的输入数据，即我们的虹膜数据集的特征列。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="a83d" class="lw lx it nu b gy ny nz l oa ob">############ Sigmoid Activation Function######<br/>activation&lt;-function(x){ <br/> y&lt;- 1/(1+exp(-x)) #Sigmaod function<br/> return(y)<br/>}</span><span id="da42" class="lw lx it nu b gy oc nz l oa ob">###########Derivative of Sigmoid Function#####<br/>derivative_activation&lt;-function(x){<br/> y&lt;-x*(1-x) #derivative of sigmoid function<br/> return(y)<br/>}</span><span id="a2f3" class="lw lx it nu b gy oc nz l oa ob">###########Normalization Function############<br/>normalize&lt;-function(x){<br/> for(i in 1:ncol(x)){<br/> x[,i]&lt;-(max(x[,i])-x[,i])/(max(x[,i])-min(x[,i]))<br/> }<br/> return(x)<br/>}</span><span id="8c7f" class="lw lx it nu b gy oc nz l oa ob">#Normalizing the input before feeding to an NN is a good practice###<br/>data[, 1:4]&lt;-normalize(data[, 1:4])</span></pre><h2 id="cecb" class="lw lx it bd ly lz ma dn mb mc md dp me lh mf mg mh ll mi mj mk lp ml mm mn mo bi translated">定义模型的结构并初始化权重矩阵</h2><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="6586" class="lw lx it nu b gy ny nz l oa ob">neuron_input&lt;- 4 #Define number of neurons in Input layer<br/>neuron_layer1&lt;- 4 #Define number of neurons in first hidden layer<br/>neuron_layer2&lt;- 3 #Define number of neurons in second hidden layer<br/>neuron_output&lt;- 3 #Define number of neurons in the output layer</span><span id="3a18" class="lw lx it nu b gy oc nz l oa ob">#initalizing weight W1 and bias b1<br/>set_weight_1&lt;-matrix(runif(4*4, 0,1), <br/> ncol= neuron_layer1, nrow= neuron_input)<br/>bias_1&lt;-runif(neuron_layer1, 0,1)</span><span id="58ca" class="lw lx it nu b gy oc nz l oa ob">#initalizing weight W2 and bias b2<br/>set_weight_2&lt;-matrix(runif(4*3, 0,1), <br/> ncol= neuron_layer2, nrow= neuron_layer1)<br/>bias_2&lt;-runif(neuron_layer2, 0,1)</span><span id="d9e6" class="lw lx it nu b gy oc nz l oa ob">#initalizing weight W3 and bias b3<br/>set_weight_3&lt;-matrix(runif(3*3, 0,1), <br/> ncol= neuron_output, nrow= neuron_layer2)<br/>bias_3&lt;-runif(neuron_output, 0,1)</span><span id="0b0c" class="lw lx it nu b gy oc nz l oa ob">################# TRAINING SET #################<br/>input_layer&lt;-data[1:120, 1:4]<br/>label&lt;-data[1:120, 5:7]</span><span id="3a23" class="lw lx it nu b gy oc nz l oa ob">################# TEST SET #####################<br/>test&lt;-data[121:150, 1:4]<br/>test_label&lt;-as.integer(iris$Species[seq[121:150]])</span><span id="84f2" class="lw lx it nu b gy oc nz l oa ob">#--------------------------------------------------------#<br/>lr=0.1 # Learning Rate<br/>er&lt;-NA  # The performance function value<br/>itr&lt;-1  # Iteration/epoch<br/>accuracy&lt;-NA #Training Accuracy<br/>t.accuracy&lt;-NA #Test Accuracy<br/>loss&lt;-NA #loss vector containing the error value at current epoch</span></pre><h1 id="1890" class="nh lx it bd ly ni od nk mb nl oe nn me jz of ka mh kc og kd mk kf oh kg mn nr bi translated">模型拟合</h1><p id="734b" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">从这里开始，逐步实现模型拟合。参考图7，它通过仅显示前10个实例来说明梯度是如何计算的。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="8f6b" class="lw lx it nu b gy ny nz l oa ob">while(itr &lt;= 5000){<br/>  <br/>  <br/>  print(paste("epoch =", itr)) #print the current epoch<br/>  itr&lt;-itr+1 #Update the iteration number<br/>  ###############FORWARD FEED##################################<br/>  #-----------------------STEP 1-----------------------------#<br/>  hidden_layer_1&lt;-t(t(input_layer %*% set_weight_1) + bias_1) <br/>  activated_hidden_layer_1&lt;-activation(hidden_layer_1)<br/>  <br/>  #-----------------------STEP 2-----------------------------#<br/>  hidden_layer_2&lt;-t(t(activated_hidden_layer_1 %*% set_weight_2) + bias_2)<br/>  activated_hidden_layer_2&lt;-activation(hidden_layer_2)<br/>  #-----------------------STEP3------------------------------#<br/>  final_layer&lt;-activation(t(t(activated_hidden_layer_2 %*% set_weight_3) + bias_3))<br/>  #-----------------------STEP4------------------------------#<br/>  er&lt;-sum(((label-final_layer)^2)/2)/120 <br/>  error&lt;- -(label-final_layer)<br/>  loss[itr]&lt;-er<br/>  ###################BACKPROPOGATION#################################<br/>  #-------------------------STEP5-----------------------------#<br/>  derivation_final_layer&lt;-derivative_activation(final_layer)<br/>  delta_final_layer&lt;- derivation_final_layer * error<br/>  #-------------------------STEP6-----------------------------#<br/>  derivative_hidden_layer_2&lt;-derivative_activation(activated_hidden_layer_2)<br/>  error_layer_2&lt;-delta_final_layer%*%t(set_weight_3)<br/>  delta_layer_2&lt;- derivative_hidden_layer_2 * error_layer_2<br/>  #-------------------------STEP7------------------------------#<br/>  derivative_hidden_layer_1&lt;-derivative_activation(activated_hidden_layer_1) <br/>  error_layer_1&lt;- delta_layer_2 %*% t(set_weight_2)<br/>  delta_layer_1&lt;- derivative_hidden_layer_1 * error_layer_1<br/>  <br/>  #####################UPDATE##################################<br/>  #-------------------------STEP8-----------------------------#<br/>  set_weight_3 &lt;-set_weight_3 -<br/>    lr*t(activated_hidden_layer_2)%*%delta_final_layer<br/>  <br/>  #---------------------------STEP9--------------------------#<br/>  set_weight_2 &lt;-set_weight_2 -<br/>    lr*t(activated_hidden_layer_1)%*%delta_layer_2<br/>  <br/>  #--------------------------STEP10--------------------------#<br/>  set_weight_1 &lt;-set_weight_1 -<br/>    lr*t(input_layer)%*%delta_layer_1<br/>  <br/>  #--------------------------STEP11--------------------------#<br/>  bias_3 &lt;- bias_3 - lr* colSums(delta_final_layer)<br/>  bias_2 &lt;- bias_2 - lr* colSums(delta_layer_2)<br/>  bias_1 &lt;- bias_1 - lr* colSums(delta_layer_1)<br/>  <br/>  #---Storing the accuracy acheived at current epoch-------------#<br/>  <br/>  prediction&lt;-NA<br/>  for(i in 1:nrow(final_layer)){<br/>    # Among the three values the maximum value indicates the     <br/>    # instance's corrsponding class <br/>    prediction[i]&lt;-(which(final_layer[i,]== max(final_layer[i,])))<br/>  }<br/>  actual&lt;-as.integer(iris$Species[seq[1:120]])<br/>  <br/>  result&lt;-table(prediction, actual) #Confusion Matrix<br/>  accuracy[itr]&lt;- sum(diag(result))/sum(result)<br/>}</span><span id="2c9d" class="lw lx it nu b gy oc nz l oa ob">#--------------------------------------------------------------#<br/>#--------------------------------------------------------------#<br/>#--Prediction function to classify the future test sets--------#</span><span id="03fe" class="lw lx it nu b gy oc nz l oa ob">predict&lt;-function(test, label){<br/>#Dont Worry, this is just a single forwardfeed using the final #learned weights</span><span id="ff30" class="lw lx it nu b gy oc nz l oa ob">  t.hidden_layer_1&lt;-t(t(test %*% set_weight_1) + bias_1)<br/>  t.activated_hidden_layer_1&lt;-activation(t.hidden_layer_1)</span><span id="beee" class="lw lx it nu b gy oc nz l oa ob">  t.hidden_layer_2&lt;-t(t(t.activated_hidden_layer_1 %*% set_weight_2) + bias_2)<br/>  t.activated_hidden_layer_2&lt;-activation(t.hidden_layer_2)<br/>  <br/>  t.final_layer&lt;-activation(t(t(t.activated_hidden_layer_2 %*% set_weight_3) + bias_3))</span><span id="e12d" class="lw lx it nu b gy oc nz l oa ob">  t.prediction&lt;-NA<br/>  for(i in 1:nrow(t.final_layer)){<br/>    t.prediction[i]&lt;-(which(t.final_layer[i,]== max(t.final_layer[i,])))<br/>  }<br/>  t.actual&lt;-label<br/>  t.result&lt;-table(t.prediction, t.actual)<br/>  colnames(t.result)&lt;-unique(iris$Species)<br/>  t.accuracy&lt;- sum(diag(t.result))/sum(t.result)<br/>  result&lt;-list(t.result, t.accuracy)<br/>  names(result)&lt;- c("Confusion Matrix", "Result")<br/>  <br/>  return(result)<br/>}<br/></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/8de59edd0ab9ff88f2e00f7a3d73bfb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YANhCyPnQRwq51n4iXnilQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。前馈-反馈传播(图片由作者提供)</p></figure><h1 id="d21f" class="nh lx it bd ly ni od nk mb nl oe nn me jz of ka mh kc og kd mk kf oh kg mn nr bi translated">是时候测试我们的模型了</h1><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="9ef7" class="lw lx it nu b gy ny nz l oa ob">predict(test,test_label)</span></pre><p id="1e4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于我们的玩具模型来说，性能相当不错:)。这个模型只给出了一个错误的预测。测试集上的准确率为0.966。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/04f2bad0255c6a6a0e6cde49eec2416a.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*cFJhFUHWB8ytekS_XT-k-w.png"/></div></figure><p id="faa9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以绘制损耗曲线和精度曲线。你可以在这里得到完整的代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1b35eef7a1f44a2cfd03a262f2374827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*vMOjnlIvq0w7UGoeP2b0Fg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。训练损失vs我们模型的训练精度<strong class="bd om">(作者图片)</strong></p></figure></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="02f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">唷…</p><p id="ef05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这个神经网络的快速概述能帮助你理解这个非常革命性的算法。我总是乐于接受任何有助于提高本文质量的建议，所以如果你遇到任何错误，请告诉我。谢谢你的阅读和快乐的R-ing。</p><h1 id="b963" class="nh lx it bd ly ni od nk mb nl oe nn me jz of ka mh kc og kd mk kf oh kg mn nr bi translated">其他著名读物</h1><p id="c99b" class="pw-post-body-paragraph ky kz it la b lb mq ju ld le mr jx lg lh ms lj lk ll mt ln lo lp mu lr ls lt im bi translated">虽然互联网上充斥着关于神经网络的信息，但我仍然会参考一些非常简明扼要的文章。</p><blockquote class="on oo op"><p id="c5d2" class="ky kz lu la b lb lc ju ld le lf jx lg oq li lj lk or lm ln lo os lq lr ls lt im bi translated"><a class="ae ok" href="https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network" rel="noopener ugc nofollow" target="_blank">训练神经网络的5种算法</a></p><p id="2b0b" class="ky kz lu la b lb lc ju ld le lf jx lg oq li lj lk or lm ln lo os lq lr ls lt im bi translated"><a class="ae ok" rel="noopener" target="_blank" href="/introduction-to-neural-networks-ead8ec1dc4dd">神经网络简介</a></p><p id="45c7" class="ky kz lu la b lb lc ju ld le lf jx lg oq li lj lk or lm ln lo os lq lr ls lt im bi translated"><a class="ae ok" href="https://medium.com/@purnasaigudikandula/a-beginner-intro-to-neural-networks-543267bda3c8" rel="noopener">神经网络入门</a></p></blockquote></div></div>    
</body>
</html>