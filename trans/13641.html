<html>
<head>
<title>NLP: Detecting Spam Messages with TensorFlow (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:用TensorFlow检测垃圾邮件(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-detecting-spam-messages-with-tensorflow-b12195b8cf0e?source=collection_archive---------9-----------------------#2020-09-19">https://towardsdatascience.com/nlp-detecting-spam-messages-with-tensorflow-b12195b8cf0e?source=collection_archive---------9-----------------------#2020-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="352d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">训练递归神经网络进行文本分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a4b913883e8c6a125b5228e4aed6fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aE0djWPn_APWxHpDrvd98Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自<a class="ae ky" href="https://pixabay.com/photos/cell-phone-phone-cell-mobile-690192/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/free-photos-242387/" rel="noopener ugc nofollow" target="_blank">免费照片</a></p></figure><p id="9dd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一个如何使用递归神经网络来检测垃圾邮件的示例。本例中使用的数据集来自<a class="ae ky" href="https://www.kaggle.com/team-ai/spam-text-message-classification" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>(原作者<a class="ae ky" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/" rel="noopener ugc nofollow" target="_blank"> Almeida和Hidalgo，2011 </a>)。</p><p id="9bb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练集中，某些邮件被标记为“垃圾邮件”(为此已用1替换)。非垃圾邮件被标记为“ham”(为此用0替换)。</p><p id="d16c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">递归神经网络是使用TensorFlow作者的原始单词嵌入和情感笔记本构建的——原始笔记本可在<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>获得。</p><p id="d8ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该分析通过以下步骤进行:</p><ol class=""><li id="e54a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">数据被加载，句子被分成训练集和测试集。</li></ol><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="5ac7" class="mj mk it mf b gy ml mm l mn mo">dataset = pd.read_csv('spam.csv')<br/>dataset</span><span id="49b6" class="mj mk it mf b gy mp mm l mn mo">sentences = dataset['Message'].tolist()<br/>labels = dataset['Category'].tolist()</span><span id="125d" class="mj mk it mf b gy mp mm l mn mo"># Separate out the sentences and labels into training and test sets<br/>training_size = int(len(sentences) * 0.8)</span><span id="4e96" class="mj mk it mf b gy mp mm l mn mo">training_sentences = sentences[0:training_size]<br/>testing_sentences = sentences[training_size:]<br/>training_labels = labels[0:training_size]<br/>testing_labels = labels[training_size:]</span><span id="da1e" class="mj mk it mf b gy mp mm l mn mo"># Make labels into numpy arrays for use with the network later<br/>training_labels_final = np.array(training_labels)<br/>testing_labels_final = np.array(testing_labels)</span></pre><p id="2694" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.数据集被标记化。换句话说，每个单词都被分配了一个唯一的数字——这是神经网络解释输入所必需的。</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="224f" class="mj mk it mf b gy ml mm l mn mo">vocab_size = 1000<br/>embedding_dim = 16<br/>max_length = 100<br/>trunc_type='post'<br/>padding_type='post'<br/>oov_tok = "&lt;OOV&gt;"</span><span id="abad" class="mj mk it mf b gy mp mm l mn mo">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="d9f9" class="mj mk it mf b gy mp mm l mn mo">tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)<br/>tokenizer.fit_on_texts(training_sentences)<br/>word_index = tokenizer.word_index</span></pre><p id="ddc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.然后将这些记号分类成序列，以确保每个单词的记号遵循每个句子所规定的正确顺序。</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="0767" class="mj mk it mf b gy ml mm l mn mo">sequences = tokenizer.texts_to_sequences(training_sentences)</span></pre><p id="c12f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.然后引入填充——在每句话尾引入0。当一个句子比另一个句子长时，这是必要的，因为出于RNN分析的目的，每个句子必须具有相同的长度。</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="1933" class="mj mk it mf b gy ml mm l mn mo">padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, <br/>                       truncating=trunc_type)</span><span id="e53c" class="mj mk it mf b gy mp mm l mn mo">testing_sequences = tokenizer.texts_to_sequences(testing_sentences)<br/>testing_padded = pad_sequences(testing_sequences,maxlen=max_length, <br/>                               padding=padding_type, truncating=trunc_type)</span></pre><p id="0acf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.递归神经网络在20个时期内建立和训练——输入层由一个<strong class="lb iu">嵌入层</strong>组成，该层用<a class="ae ky" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">密集向量表示法</a>表示句子。</p><h1 id="88bf" class="mq mk it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">递归神经网络(RNN)</h1><p id="e6f9" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">以下是递归神经网络配置:</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="c84e" class="mj mk it mf b gy ml mm l mn mo">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),<br/>    tf.keras.layers.Flatten(),<br/>    tf.keras.layers.Dense(6, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])<br/>model.summary()</span></pre><p id="3444" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是对模型参数的详细介绍:</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="e5a2" class="mj mk it mf b gy ml mm l mn mo">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (None, 100, 16)           16000     <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 1600)              0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 6)                 9606      <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 7         <br/>=================================================================<br/>Total params: 25,613<br/>Trainable params: 25,613<br/>Non-trainable params: 0</span></pre><p id="e9f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型产生以下训练和验证损失:</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="5d4b" class="mj mk it mf b gy ml mm l mn mo">num_epochs = 20<br/>history=model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/738b00a8f2e6b568b467764033056bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*ALd31kFzM5HbIa-cz6qpvw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="7007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们看到验证损失在5个时期后达到最低点。在这点上，选择5个时期再次运行模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4ea5c4d8c5930d01206d448819718d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*kXjfI3SBZUysaVp2ZI1CRw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><h1 id="95cf" class="mq mk it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">对看不见的数据进行测试</h1><p id="b9e9" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">现在模型已经建立，让我们看看分类器如何识别以下邮件中的垃圾邮件(我随机发明的):</p><ul class=""><li id="3931" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nn mb mc md bi translated">格雷格，你听到留言后能给我回个电话吗？(意图为正版)</li><li id="0790" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">恭喜你有了新的iPhone！点击此处领取您的奖品……'(意为垃圾邮件)</li><li id="f919" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">真的很喜欢你的新照片(有意为之)</li><li id="ed29" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">你今天听到新闻了吗？‘发生了什么可怕的事情……’(本意是真诚的)</li><li id="c6f9" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">立即参加免费的COVID网络研讨会:立即预订您的会议…’(意为垃圾邮件)</li></ul><p id="4c05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是生成的分数(分数越接近1，该句子是垃圾邮件的概率越高:</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="1b85" class="mj mk it mf b gy ml mm l mn mo">['Greg, can you call me back once you get this?', 'Congrats on your new iPhone! Click here to claim your prize...', 'Really like that new photo of you', 'Did you hear the news today? Terrible what has happened...', 'Attend this free COVID webinar today: Book your session now...']<br/>Greg, can you call me back once you get this?<br/>[0.0735679]<br/><br/><br/>Congrats on your new iPhone! Click here to claim your prize...<br/>[0.91035014]<br/><br/><br/>Really like that new photo of you<br/>[0.01672107]<br/><br/><br/>Did you hear the news today? Terrible what has happened...<br/>[0.02904579]<br/><br/><br/>Attend this free COVID webinar today: Book your session now...<br/>[0.54472804]</span></pre><p id="ea5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，对于两个旨在作为垃圾邮件的消息，分类器显示两者都有很大的可能性。在句子的情况下，<strong class="lb iu">“今天参加这个免费的COVID网络研讨会:现在就预订您的会议……”</strong>-分类器在标记高于50%的垃圾邮件概率方面表现相当好-即使在构建训练集时COVID不是一个术语。</p><p id="6043" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这方面，很明显，分类器依赖于使用单词的上下文，而不是简单地将单个单词标记为垃圾邮件。</p><p id="629e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于递归神经网络在模拟序列数据和识别单词之间的模式方面是有效的，这个使用TensorFlow构建的简单垃圾邮件检测器已经被证明在我们用于测试目的的有限数据上是非常有效的。</p><h1 id="9a15" class="mq mk it bd mr ms mt mu mv mw mx my mz jz na ka nb kc nc kd nd kf ne kg nf ng bi translated">结论</h1><p id="120e" class="pw-post-body-paragraph kz la it lb b lc nh ju le lf ni jx lh li nj lk ll lm nk lo lp lq nl ls lt lu im bi translated">在本例中，我们看到:</p><ul class=""><li id="7d13" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nn mb mc md bi translated">递归神经网络如何用于文本分类</li><li id="59dd" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">使用标记化、序列和填充为分析准备文本数据</li><li id="9f85" class="lv lw it lb b lc no lf np li nq lm nr lq ns lu nn mb mc md bi translated">用于分析文本数据的神经网络模型的配置</li></ul><p id="49be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们还看到了如何使用该模型来预测看不见的数据(或本例中的消息),以确定该模型在真实场景中的潜在工作方式。</p><p id="04d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢阅读，这个例子的相关GitHub库可以在<a class="ae ky" href="https://github.com/MGCodesandStats/tensorflow-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="52f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新:</strong>您还可以在本文的第二部分中找到对这个模型的更新，<a class="ae ky" href="https://medium.com/@mgcodesandstats/nlp-detecting-spam-messages-with-tensorflow-part-ii-77826c8f1abf" rel="noopener">可以在这里找到</a>。</p><p id="0bb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">免责声明:本文是在“原样”的基础上编写的，没有任何担保。本文旨在提供数据科学概念的概述，不应被解释为任何形式的专业建议。作者与本文中提到的任何一方都没有关系，本文或其调查结果也没有得到同样的认可。</em></p></div></div>    
</body>
</html>