<html>
<head>
<title>One Hot Encoding, Standardization, PCA: Data preparation for segmentation in python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一次热编码，标准化，PCA:python中分段的数据准备</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b?source=collection_archive---------3-----------------------#2020-10-10">https://towardsdatascience.com/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b?source=collection_archive---------3-----------------------#2020-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c07bb14c13e6dc2bf8852f8789ccd4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i9oglcC2QOD_Ct5E"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@mael_bld" rel="noopener ugc nofollow" target="_blank">梅尔·巴兰德</a>在<a class="ae jd" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="0720" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">获得正确的数据，实现完美的细分！</h2></div><p id="07b5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据驱动的客户定位或产品捆绑对于企业在激烈的竞争中保持相关性至关重要。消费者现在有太多的选择，更喜欢个性化的产品。随着以人工智能和大数据技术的巨大增长为形式的第四次工业革命的到来，现在是利用细分模型进行此类分析的最佳时机。但是在我们深入研究这些模型之前，我们应该知道这些模型需要什么样的数据。这是我博客的重点，因为我们将经历将原始数据集转换为训练和测试分割算法所需格式的所有必要步骤。</p><p id="6761" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">数据</strong></p><p id="1bfb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个练习中，我们将使用来自一家为孕妇提供服装的在线商店的点击流数据。它有从2008年4月到2008年8月的数据，包括产品类别、图片在网页上的位置、IP地址的来源国以及产品的美元价格等变量。我选择这个数据集的原因是点击流数据正在成为提供关于客户行为的精细信息的一个非常重要的来源。它还为我们提供了一个具有典型挑战的数据集，如高维度、对特征工程的需求、分类变量的存在和不同规模的字段。</p><p id="e442" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将尝试通过执行以下步骤为产品细分准备数据:</p><ol class=""><li id="ae4e" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">探索性数据分析</li><li id="96c9" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">特征工程</li><li id="11ca" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">一个热编码</li><li id="3aee" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">标准化</li><li id="877f" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">主成分分析</li></ol><p id="9b58" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">探索性数据分析(EDA) </strong></p><p id="2129" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将首先尝试读取数据集(使用<code class="fe mf mg mh mi b">read_csv</code>函数)并查看前5行(使用<code class="fe mf mg mh mi b">head</code>函数):</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="cdaf" class="mr ms jg mi b gy mt mu l mv mw"># Read dataset and look at top records<br/>import pandas as pd<br/>df = pd.read_csv('e-shop clothing 2008.csv', delimiter=";")<br/>df.head(5)</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/d2dd8eca3d801314bf5ca7cb95f06316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dU_bngWNy9GlUkpuR1rYdQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:前5条记录</p></figure><p id="f022" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们有多个服装模特每天的数据(字段名:“第2页(服装模特)”)。接下来，让我们检查行数和列数及其类型(使用<code class="fe mf mg mh mi b">info</code>函数)</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="d944" class="mr ms jg mi b gy mt mu l mv mw">#Check the number of rows and columns and their types<br/>df.info()</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div class="gh gi my"><img src="../Images/4fa67945a7af3cff5d2df9fa1cf63c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*9bC5Tx1NNhRu0v3XvpRveg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:检查字段类型</p></figure><p id="db7f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们有165474条记录和14个字段。需要注意的一点是，很多字段都是数值型的，但理想情况下应该是字符串。让我们使用<code class="fe mf mg mh mi b">as.type(str)</code>函数将字段转换为字符串:</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="1378" class="mr ms jg mi b gy mt mu l mv mw"># Convert categorical variables to string<br/>cat_vars = ['year', 'month', 'day', 'country', 'session ID',<br/>               'page 1 (main category)', 'page 2 (clothing model)',   'colour',<br/>               'location', 'model photography', 'price 2', 'page']<br/>df[cat_vars] = df[cat_vars].astype(str)<br/>df.info()</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4c8f480c3eeb5ad775670dc578788243.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*kKhGfe7-hf8CXQU4WPCvEQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:转换后检查字段类型</p></figure><p id="f751" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来让我们检查数字字段的属性:</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="a82a" class="mr ms jg mi b gy mt mu l mv mw"># Check properties of numeric fields<br/>df.describe()</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e07a7b1c2fdbbc9206daa943e641866e.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*46ZqPCbFNRR9SjvFlb3jjQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4:检查数值字段的属性</p></figure><p id="27f2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如图4所示，产品价格(字段名:“价格”)比一个会话期间的点击序列(字段名:“订单”)要大得多。这意味着我们必须将这些场标准化，使它们具有相同的比例，因为基于距离的模型(如K-means)会受到场的比例的影响。</p><p id="d75c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">特征工程</strong></p><p id="74b7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，我们的数据集是每日级别的，我们需要在产品级别聚合数据，因为我们想要执行产品细分。我们在产品级别聚合时创建了以下功能:</p><ol class=""><li id="b0b9" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">最常出现的产品颜色、浏览日期、国家、照片类型(个人资料、正面)、价格类型(高于或低于类别平均值)、网站内的页码以及产品照片在页面上的位置(使用<code class="fe mf mg mh mi b">mode</code>功能)</li><li id="18e4" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">唯一会话id的总数(使用<code class="fe mf mg mh mi b">nununique</code>函数)</li><li id="90e6" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">单次点击序列的中值、最小值和最大值以及产品价格(使用<code class="fe mf mg mh mi b">median</code>、<code class="fe mf mg mh mi b">min</code>和<code class="fe mf mg mh mi b">max</code>功能)</li></ol><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="3711" class="mr ms jg mi b gy mt mu l mv mw"># Feature Engineering<br/>from scipy.stats import mode <br/>df2 = df.groupby(['country','page 1 (main category)','page 2 (clothing model)']).agg(<br/>                                                                             median_no_of_clicks_per_session=('order', 'median'),<br/>                                                                             min_no_of_clicks_per_session=('order', 'max'),<br/>                                                                             max_no_of_clicks_per_session=('order', 'min'),<br/>                                                                             median_price=('price', 'median'),<br/>                                                                             min_price=('price', 'max'),<br/>                                                                             max_price=('price', 'min'),<br/>                                                                             total_number_of_sessions =('session ID', pd.Series.nunique),<br/>                                                                             most_frequent_day=('day', lambda x: mode(x)[0][0]),<br/>                                                                             most_frequent_colour=('colour', lambda x: mode(x)[0][0]),<br/>                                                                             most_frequent_location=('location', lambda x: mode(x)[0][0]),<br/>                                                                             most_frequent_photo_type=('model photography', lambda x: mode(x)[0][0]),<br/>                                                                             most_frequent_price_type =('price 2', lambda x: mode(x)[0][0]),<br/>                                                                             most_frequent_page_number =('page', lambda x: mode(x)[0][0])<br/>                                                                            )</span><span id="2f3d" class="mr ms jg mi b gy nb mu l mv mw">df2</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/21b2750d48b2557afd1dd8fc930273ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-F0wH18-Mmhkt5HaGqzZw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5:特征工程</p></figure><p id="089a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">一个热编码</strong></p><p id="705b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一种热编码创建虚拟变量，虚拟变量是代表分类变量的一个级别的重复变量。水平的存在由1表示，不存在由0表示。如果分类变量是有序的(即变量的类别是有序的)，那么我们可以使用<code class="fe mf mg mh mi b">OrdinalEncoder</code>函数将变量转换为数值变量。在我们的例子中，分类变量没有任何顺序性，因此，我们使用<code class="fe mf mg mh mi b">get_dummies</code>函数来创建虚拟变量。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="7cce" class="mr ms jg mi b gy mt mu l mv mw"># One hot encoding - to convert categorical data to continuous</span><span id="e8e9" class="mr ms jg mi b gy nb mu l mv mw">cat_vars = ['most_frequent_day',<br/>           'most_frequent_colour', 'most_frequent_location',<br/>           'most_frequent_photo_type', 'most_frequent_price_type',<br/>           'most_frequent_page_number']<br/>df2[cat_vars] = df2[cat_vars].astype(str)<br/>df3 = pd.get_dummies(df2)</span><span id="9cb7" class="mr ms jg mi b gy nb mu l mv mw">df3.head(5)</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/aa825aee9c5c8140cd71fa62a4edfa0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lcVwE0m1LkdK8NzKSBji4g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6:一个热编码</p></figure><p id="6050" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们的标称特征是整数，我们也可以使用<code class="fe mf mg mh mi b">OneHotEncoder</code>函数代替<code class="fe mf mg mh mi b">get_dummies</code>函数。</p><p id="1dbd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">标准化</strong></p><p id="90f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如图4所示，我们的数字特征有不同的标度。缩放有助于将不同范围或单位的独立要素转换为可比值后进行比较。有两种主要的缩放方法:</p><figure class="mj mk ml mm gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e7b4140e5172e8282aa64de197db1437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*u5j0ED2cF6I-7WMHvZ24Ag.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="9a52" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们知道数据的分布不遵循高斯分布时，或者对于不假设任何数据分布的算法(如K-最近邻和神经网络),归一化是理想的选择。另一方面，当数据遵循高斯分布时，可以使用标准化。但是这些并不是严格的规则，理想情况下，我们可以两者都尝试，并选择给出最佳集群验证结果的选项。在这个例子中，我们将使用<code class="fe mf mg mh mi b">StandardScaler</code>函数来标准化我们的数字字段。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="d91f" class="mr ms jg mi b gy mt mu l mv mw"># Standardizing</span><span id="326a" class="mr ms jg mi b gy nb mu l mv mw">from sklearn.preprocessing import StandardScaler<br/>con_vars = ['median_no_of_clicks_per_session', 'min_no_of_clicks_per_session',<br/>           'max_no_of_clicks_per_session', 'median_price', 'min_price',<br/>           'max_price', 'total_number_of_sessions']</span><span id="5951" class="mr ms jg mi b gy nb mu l mv mw">scaler = StandardScaler()<br/>df3[con_vars]=scaler.fit_transform(df3[con_vars])<br/>df3.head(5)</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/f64195ee4106cf571f7255a4180ee2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQTM_8J5xIq5ND0FWF7pPg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图7:标准化</p></figure><p id="f1cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">主成分分析</strong></p><p id="3130" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">主成分分析以特定的方式结合我们当前的特征来创建新的特征，然后我们可以丢弃“最不重要的”，同时仍然保留所有原始变量中最有价值的部分。当我们有很多特性要处理时，这是一个有用的方法。它计算所有特征的协方差矩阵，然后从矩阵中生成特征向量和特征值。然后，协方差矩阵乘以特征向量以创建主分量。这些主成分是基于我们的原始特征的新特征，并且它们在解释数据集中的可变性方面的重要性由特征值给出。我们可以保留解释原始数据集中最小方差的排名最高的主成分。</p><p id="01a3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以使用来自<code class="fe mf mg mh mi b">sklearn.decomposition</code>模块的<code class="fe mf mg mh mi b">pca</code>函数实现PCA分析。我设置了一个循环函数来识别能够解释数据集中至少85%方差的主成分的数量。</p><pre class="mj mk ml mm gt mn mi mo mp aw mq bi"><span id="ace6" class="mr ms jg mi b gy mt mu l mv mw"># PCA</span><span id="eff4" class="mr ms jg mi b gy nb mu l mv mw">from sklearn.decomposition import PCA</span><span id="200b" class="mr ms jg mi b gy nb mu l mv mw"># Loop Function to identify number of principal components that explain at least 85% of the variance<br/>for comp in range(3, df3.shape[1]):<br/>    pca = PCA(n_components= comp, random_state=42)<br/>    pca.fit(df3)<br/>    comp_check = pca.explained_variance_ratio_<br/>    final_comp = comp<br/>    if comp_check.sum() &gt; 0.85:<br/>        break<br/>        <br/>Final_PCA = PCA(n_components= final_comp,random_state=42)<br/>Final_PCA.fit(df3)<br/>cluster_df=Final_PCA.transform(df3)</span><span id="9210" class="mr ms jg mi b gy nb mu l mv mw">num_comps = comp_check.shape[0]<br/>print("Using {} components, we can explain {}% of the variability in the original data.".format(final_comp,comp_check.sum()))</span></pre><figure class="mj mk ml mm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/7068cc07ce50e3e6bd2a1164fd125ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8F-z1O5ehOP0VGGi3vQOQg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图8: PCA输出</p></figure><p id="f031" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如图8所示，15个成分能够解释我们数据集中85%的差异。我们现在可以在我们的无监督模型中使用这些特征，如K均值、DBSCAN、分层聚类等来细分我们的产品。</p><p id="87c6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">结论</strong></p><p id="1394" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们了解了为细分分析准备数据所需的步骤。</p><p id="c57a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">具体来说，我们了解到:</p><ul class=""><li id="ce47" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq nh lx ly lz bi translated">我们应该如何通过查看数据、字段类型和数值字段的属性来执行探索性数据分析。</li><li id="889d" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nh lx ly lz bi translated">我们可以从原始分类字段和连续字段中创建何种特征的示例。</li><li id="e305" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nh lx ly lz bi translated">如何在python中实现一个热编码以及顺序编码</li><li id="2ee2" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nh lx ly lz bi translated">各种类型的缩放技术以及如何在它们之间进行选择</li><li id="da5e" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq nh lx ly lz bi translated">什么是PCA以及如何在python中使用它进行特征约简</li></ul><p id="af96" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><p id="47a5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ni">最后，我鼓励您阅读下面的文章，以深入了解选择最佳分类数的不同方法:</em></p><p id="6314" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://indraneeldb1993ds.medium.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad" rel="noopener"> <em class="ni">在Python中实现7种选择最佳聚类数方法的备忘单</em> </a></p><h1 id="cfa5" class="nj ms jg bd nk nl nm nn no np nq nr ns km nt kn nu kp nv kq nw ks nx kt ny nz bi translated">感谢您的阅读！</h1><p id="8d41" class="pw-post-body-paragraph kv kw jg kx b ky oa kh la lb ob kk ld le oc lg lh li od lk ll lm oe lo lp lq ij bi translated">如果你像我一样，对人工智能、数据科学或经济学充满热情，请随时在<a class="ae jd" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上添加我。</p></div></div>    
</body>
</html>