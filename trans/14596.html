<html>
<head>
<title>Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-3c39fbf5cb9d?source=collection_archive---------23-----------------------#2020-10-08">https://towardsdatascience.com/principal-component-analysis-3c39fbf5cb9d?source=collection_archive---------23-----------------------#2020-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a4ca" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">(2021年9月更新)无监督学习中最重要的算法之一背后的逐步直觉、数学原理和python代码片段</h2></div><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">这个动画展示了当旋转方向到达两个特殊方向时，投影点(A…E)的协方差矩阵如何对角化(是下面的特征分解方程的解)。为了验证这一点，当方向与橙色虚线(第一主成分)或第二粉红色虚线(第二主成分)重叠时，请减慢视频速度或尝试暂停视频。您还会注意到矩阵的对角线元素达到了它们的最大值(最大方差)。(视频由作者使用Geogebra6 sw制作)</p></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kq"><img src="../Images/c31901e6811ccf249dc11ed19deee2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*iaBIRPF7leWOcPh_PysrNg.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">这个公式叫做特征分解方程。最初的工作始于1800年底，但由于(个人)计算能力的进步，直到最近才在大数据分析中得到实际应用。</p></figure><p id="81a2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">大家好，我是意大利米兰的Andrea Grianti。在阅读了许多关于这个主题的书籍和论文后，我写了这篇文章来分享我的想法。这不是一本教科书，而是进一步理解该主题的起点。因为后面的数学/代数很难，我把它分成了四部分:</p><ol class=""><li id="e79d" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls ly lz ma mb bi translated">PCA直觉</li><li id="13bf" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">数学/代数(简单)</li><li id="a7ce" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">数学/代数(难)</li><li id="9dfa" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls ly lz ma mb bi translated">Python片段</li></ol><h1 id="b770" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> 1。PCA直觉</strong></h1><p id="dcde" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">如果您有一个包含数千/数百万个观察值(行)和数百个不同变量(列)的大型数据集，首要目标之一是验证是否有可能简化和缩减数据集，以便于对原始数据的一个小得多的子集进行分析。</p><p id="8688" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">直接消除变量是显而易见的方法，但它显然会影响数据集的<em class="ne">信息</em>内容。过多或错误的剔除会使你的数据集变得无用，过少的剔除会使数据集变得庞大和难以分析。</p><p id="00d7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">术语'<em class="ne">信息</em>'是一个非常通用的主题，很难定义它。这取决于数据集。对我来说，一个数据集可能包含<em class="ne">信息</em>,而对其他人来说，可能什么也不包含，反之亦然。</p><p id="a42c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以尝试使用“信息内容”这样的概念来定义数据集中的<em class="ne">信息量</em>，这是一个与特定值发生的概率相关的概念，在数据集变量的所有可能值中。</p><p id="60c1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">根据这一概念，变量<em class="ne"> x </em>的可能结果越多，预测其值的概率越低，因此信息含量越高。</p><p id="6814" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是我们<strong class="kz ir">要牢记的第一个假设</strong>:<strong class="kz ir"><em class="ne">更高的方差= &gt;更高的信息含量</em>。只有上下文和我们的数据能告诉我们这个假设是否成立。</strong></p><p id="1744" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在关注高方差数据概念之前我们必须处理一个<strong class="kz ir">第二个假设:</strong> <strong class="kz ir"> <em class="ne">变量之间的相关性是数据冗余的一种形式</em> </strong>。如果两个变量之间有明确定义的关系，例如以度为单位的角度和以弧度为单位的角度，或者以厘格或法拉为单位的温度，那么这两个变量中的一个是无用的，可以直接从数据集中消除。相反，当相关性不太明确时，不推荐直接排除，可以通过第三个假设尝试管理相关性证据的缺乏。</p><p id="448a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">第三个假设</strong>是:<strong class="kz ir"> <em class="ne">我们假设无论变量之间的相关性是什么，这都是线性的</em> </strong>。</p><p id="a2d3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">概括一下，我们假设:</p><ul class=""><li id="a22f" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">方差与信息内容有关，应该最大化</li><li id="6d87" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">冗余变量和变量之间的高度相关性是一种应该最小化的噪声形式</li><li id="18ae" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">变量之间的相关性是线性的</li></ul><p id="41cb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi ng translated">这两个假设为什么重要？因为要降低数据集的维度，我们应该评估每个变量对数据集总体方差(=信息)的贡献，以便选择贡献最大的变量，丢弃贡献最小的变量。</p><p id="32d4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这种运算的战场是协方差矩阵或它的兄弟相关矩阵。因为相关矩阵通过绑定两个变量之间的简单协方差和相关性的相同关系严格绑定到协方差矩阵，所以我将使用协方差矩阵，因为选择与后面的数学原理的理解无关。</p><p id="0fba" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在任何情况下，对于那些喜欢简单刷新协方差和相关性概念以便理解这里的关系的人来说，它是:</p><p id="83cb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们知道，对于两个变量，协方差公式为(如果我们的数据集是从总体中抽取的数据样本):</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9f95a4c626d1d58a1503120613bc7ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*aVR1j1MP6H3dbWvwa3OxEQ.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">不集中在平均样本数据的协方差(来源:作者)</p></figure><p id="a0eb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">协方差代表一种离差度量，它包括两个变量之间的线性“<strong class="kz ir">同步性</strong>的概念，这两个变量与它们各自的均值相关。</p><p id="5236" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">也就是说，对于每个点，测量一个点的x坐标&gt;与所有x点的平均值&gt;、<strong class="kz ir">之间的差值<strong class="kz ir">如何与<strong class="kz ir">同步的</strong>、<strong class="kz ir">、&lt;与同一点的y坐标&gt;和&lt;之间的差值</strong>，然后对所有y点的平均值&gt;求平均值。</strong></strong></p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/708bb63533dc4bc9a063a35e01ad6d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*HrbyH5HkAl0sNFLmfuDnpw.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">画在笛卡尔坐标系上的点。注意红色的平均点。还要注意，当重新定义轴时，将(0，0)点设置在平均值上，离散度不受影响。这实际上简化了理解协方差和相关性之间关系的推理。(来源:作者)</p></figure><p id="3f77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有趣的是，在上面的示例图片中，平均值的左右两边有相同数量的点(对于维度x)，平均值的上下两边也有相同数量的点(对于维度y)。</p><p id="eed6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个简单的例子让你明白了<strong class="kz ir">协方差对于</strong> <strong class="kz ir">符号</strong>的重要性。当结果给你一个正号或负号时，它给你一个关于<strong class="kz ir">象限</strong>的概念，其中<strong class="kz ir">同步</strong>的<strong class="kz ir">方向</strong>在哪里。还要注意，具有相同的符号并不能告诉你任何关于方向的<strong class="kz ir">斜率</strong>的信息，只能告诉你方向所在的象限。</p><p id="774b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果点不是从左下到右上，而是从左上到右下(沿平均值的水平线反映)，协方差将是相同的值，但前面有一个负号。</p><h2 id="4ed2" class="nr mi iq bd mj ns nt dn mn nu nv dp mr lg nw nx mt lk ny nz mv lo oa ob mx oc bi translated">协方差如何绑定到相关性</h2><p id="5bb1" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">如果你暂时不考虑上述协方差公式中的常数，等式的其余部分就是<strong class="kz ir">乘积</strong>与<strong class="kz ir">差值</strong>的<strong class="kz ir">和</strong>，这应该让你想起(至少对于<strong class="kz ir">乘积</strong>的和)代数中的<strong class="kz ir">点积</strong>概念。</p><p id="a1f3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了使它看起来像两个向量之间的点积，我们可以<strong class="kz ir">将数据点居中</strong>(从每个变量值中减去该变量的平均值)，并且我们两个的平均值都= 0，而<strong class="kz ir">方差保持不变</strong>(因为移动所有点不会改变点之间的距离，并且离差保持不变)。</p><p id="a499" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这种情况下，中心数据的协方差公式简化为:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/0c42607c89b66ee8106755c4055cf10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*-qOeqaIBZHV7Q1rFoyyqtg.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">以平均数据为中心的的<strong class="bd oe">协方差公式(来源:作者)</strong></p></figure><p id="6269" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中包含点积的定义我们知道是:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi of"><img src="../Images/b0397070315be0e52b003647322e5c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rVH5o7GsWmBqjdSE0QsVg.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">点积(来源:作者)</p></figure><p id="726a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是我们从几何学中知道，点积也可以写成:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1ac13fb4e5b953ca98d3522ec5c7b4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*2g4qdpYQv3kgac_RFv3j5w.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">点积:几何版(来源:作者)</p></figure><p id="df7a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">考虑到当数据居中时，方差公式也简化为:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b877cd0ae9ae96b242b76293304d5edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*rBtNOYLd5G7uNnD2Ifo10w.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">居中变量X和y的方差公式。</p></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a26d246ea737c4aa9ff00e3a611f32ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*hI662L0J3QKAiQ7x_4Zfbg.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">中心数据相对于向量X和Y长度的标准偏差</p></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oj"><img src="../Images/d3cee1eb49f18a3b369d4df96efb1e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KiYTNgQAZl7mLZKuVtTSzQ.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">对于中心数据，协方差公式用标准差和向量X和Y之间的夹角余弦表示，即相关系数。(来源:作者)</p></figure><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ok"><img src="../Images/01a89a4ef07c09c6986d4c34cefd183a.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*lo4rPpHGJYQIFUVucvXylA.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">两个中心变量情况下协方差和相关性(rho)之间的关系。当变量为n时，相同概念可以根据矩阵进行转置(来源:作者)</p></figure><p id="d198" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">两个中心变量的简单协方差和相关性之间的关系:σ是它们的标准偏差，ρ是相关系数。在相关性为零的极端情况下，协方差为零。另一方面，当相关性最大时，ρ= 1或ρ=-1，协方差是每个变量的标准偏差的乘积。</p><h1 id="4b91" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">从协方差到协方差矩阵</h1><p id="316e" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">考虑到上面的协方差公式仅适用于2个变量，我们可以将协方差矩阵视为数据集中所有变量之间所有协方差的“大图”。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ol"><img src="../Images/33a35ff3075b02ecd30bc4eded5a1f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J26kRMJj3j56CmawjL99eQ.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">协方差矩阵(在文献中也称为方差/协方差矩阵)(来源:作者)</p></figure><p id="d9db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当我们计算原始数据集(我们称原始数据集X为许多列向量X1，X2…Xn的集合)的方差/协方差矩阵时，我们会看到沿对角线的方差，但我们也会看到非对角线元素中的联合协方差，这是一个变量与其他变量之间的<strong class="kz ir">‘同步性’</strong>大小的(难以解释)度量。</p><p id="37a3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因为我们当然不能为了消除变量之间的相关性而修改我们的原始数据X(除非我们完全删除一个变量，这总是可能的，但有风险，因为我们可能不情愿地删除重要信息)， 我们可以尝试找到一种方法，将X“转换”成不同的数据集Y，该数据集Y具有与X相关联的<strong class="kz ir"> special </strong>(德语中special =<strong class="kz ir">eigen</strong>……)但是以这样的方式构建，Y(Y1，Y2…Yn)的新变量将具有不同的协方差矩阵Cy，其中这些变量的方差将与其他变量的区间(相关性)隔离开来(= &gt;协方差= 0)。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi om"><img src="../Images/a5b71b16411de7386607ba28a30e1012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acPhRGRPer8De5z9B0FEPA.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">这是我们想降落的地方。具有对角化的新协方差矩阵Cy。意味着我们要解决一个问题，找到一个新的数据集Y，绑定到原始数据集X(我们将看到绑定是如何工作的)，其中Y的所有新变量在它们之间是不相关的。这样，Cy矩阵中的协方差将为零，方差被隔离在对角线中。这将清楚地允许我们将数据集的总方差定义为对角线元素的总和(Trace)(来源:作者)。</p></figure><p id="243b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个操作就是<strong class="kz ir">特征分解方程</strong>的魔力。通过求解该方程，我们将找到一种方法来转换原始X数据集，从而将协方差矩阵Cx转换为新的数据集Y，并使用一种称为B的特殊转换矩阵将<strong class="kz ir">对角化</strong>协方差矩阵Cy转换为新的数据集Y。B将是我们用来从X到Y来回转换的方式，反之亦然。</p><p id="3c38" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi ng translated">假设我们能够解出那个<strong class="kz ir">特征分解方程</strong> …那又怎样？如果使用原始数据X，我不得不使用Cx来解决无法根据相关性隔离单个变量对总体方差的贡献的问题，现在，通过求解该方程，我可以使用一个新的Y数据集，其协方差矩阵为Cy，并且是对角的，因此Y的每个变量的方差贡献都有明确的定义，并且不受联合交互/相关性的影响。这样，我们就可以根据它们的方差值来决定在哪里切割Y数据集，以根据它们相对于Cy中的方差值总和的贡献权重来保留最重要的Y变量的子集。</p><p id="602a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi ng translated">当我们用Y而不是X工作时，要付出什么样的代价？因为新的Y数据集的变量(我们将会看到)是原始X数据的线性组合，它们的含义是不确定的。因此，要给y变量赋予上下文意义，就需要一定的敏锐度，让它们从实用的角度“说话”。当然，事情有点复杂，关于如何命名新发现的变量，我们在这里跳过了许多细节，但要框定问题和解决方案，就这样。</p><p id="ffb8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在任何情况下，要从X到Y(以及从Cx到Cy)，我们需要理解我们可以对原始数据进行的转换的逻辑(通过一个仍待定义的B矩阵)。所以我们需要讨论(研究)一下<strong class="kz ir">投影</strong>和<strong class="kz ir">线性变换</strong>，因为它们是严格联系的。</p><h1 id="371a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> 2。PCA后面的数学/代数(更容易)</strong></h1><p id="a4ad" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated"><strong class="kz ir">投影</strong>概念:简而言之，我们在图表中绘制的内容取决于我们用来表示数据的坐标系统。艺术中的<em class="ne">透视</em>思考:它是现实生活中的<em class="ne">投影</em>按照特定的规则变换。</p><p id="69b9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">例如，在三维中，我们使用笛卡尔系统，其由正交向量(长度=1的正交/垂直向量)的矩阵(E)表示。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/511d3e66762fd77107c29d3d87c6f8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*eRnWL6rfPsLKPMV4do0bFA.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">卡茨安基地(来源:作者)</p></figure><p id="7a7d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">例如，当我们有一个数据集X(测量值是三维的:列向量x1、x2、x3)时，为了计算变量x1的方差，我们应用通常的方差公式。<strong class="kz ir">但是</strong>我们实际做的是通过<strong class="kz ir">点积</strong>将X的每个数据点的坐标投影到E的3个方向上。</p><p id="839e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因此，一般来说，我们有<strong class="kz ir">X \u E = X。</strong>这个投影操作在笛卡尔坐标系中是“透明的”,我们甚至没有意识到要进行投影，因为它在我们的头脑中是“自动的”。</p><p id="0154" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是，如果我们决定离开笛卡尔系统，我们可以建立一组类似的<strong class="kz ir">正交</strong> <strong class="kz ir">向量</strong>，它们定义了由b1、b2、b3(而不是由e1、e2、e3组成的E)组成的新基B，如下所示:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6e6f8e89a920d58124c100162caeaaf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*w1UHrBt9I_cgbCw3C3qS4g.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">(来源:作者)</p></figure><p id="a350" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果我们<strong class="kz ir">点</strong>乘(例如)数据集X的一个通用行向量，该向量包含单个数据点的坐标(如果您喜欢，可以是单个观察值):</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3b0e6e287b952b32a33cea74e060a80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*k0fbIU7iyTC5KYSPjTsVRQ.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">通用数据行，(1，3)表示1行3列</p></figure><p id="0863" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于B (=3行，3列)，我们获得新的行向量(=1行，3列):</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/86926726d2c937ea7d949a5e53cebb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*QFtB5Y1Unx_EivRo8Uhlcg.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">Y中相应的新行，使用X的一般行，用B转换</p></figure><p id="970e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中该向量的每个元素具有新的“度量”/“坐标”，由下式给出:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi or"><img src="../Images/b8ac4a00fedffaf2a0904cd7a927d84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acpYlBXNW_nuaSrskdhF0Q.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">这里的三个方程是上图易方程的展开。因此，在维度上，形状仍然是(1行，3列)，但行中的每个元素都是xi和B的线性组合。</p></figure><p id="730e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在代数术语中，一般来说，在我们的例子中，我们简单地用以下公式改变基:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8ca55ee393482957b4ed180f3b35d475.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*-x0Ihn7sUruGDcOP7l61jw.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">首先是矩阵形式。在我们的例子中，Y由3列向量(y1，y2，y3，m行)组成，B由3列向量(b1，b2，b3，3行)组成。所以Y是(m，n) = X (m，n)。B (n，n)。当找到B的解时，这3个方程将代表所谓的“主分量”。</p></figure><p id="db82" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其中X是基E中的原始数据矩阵(m行乘n列)，B是新的标准正交基(n乘n)，Y (m乘n)是在新基B 中测量值被转置<strong class="kz ir">的结果新数据矩阵。</strong></p><p id="139d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">注意，y1、y2、y3是<strong class="kz ir">新变量</strong>，其含义与x1、x2、x3无关，但是它们是新的，并且新含义<em class="ne">必须在语义上定义</em>，因为它们是分别由B的b1、b2、b3向量的相应分量加权的<strong class="kz ir">所有</strong>X变量<em class="ne">的线性组合。那就是:</em></p><ul class=""><li id="8081" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">y1由向量b1=[b11，b12，b13]加权的X的每个变量组成</li><li id="f177" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">y2由向量b2=[b21，b22，b23]加权的X的每个变量组成</li><li id="c746" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">y3由向量b3=[b31，b32，b33]加权的X的每个变量组成。</li></ul><p id="8147" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">所以如果我们找到B的解，我们可以用y的新变量。但是，我们应该使用什么B来达到我们的目标，找到Y与Cy对角矩阵？</p><p id="3b84" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">b可以逐步建立<strong class="kz ir">如果我们知道它存在一个唯一的方向，使沿该方向投影的X的方差最大化。我们可以沿着笛卡尔坐标系的X轴投影数据集X，但在这种情况下，方差可能没有最大化。因此，我们需要找到那个特定的方向。一旦我们找到了最大方差的方向和值，我们就知道找到了第一个本征向量和第一个本征值。一般来说，我们可以说已经找到了第一主分量PC1: y1=X.b1，其强度是特征值1(λ1)。</strong></p><p id="bc47" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了根据方差解释了多少来评估y1，在这个阶段，可以将λ1除以协方差矩阵Cx的对角元素之和，因为总方差之和不会从Cx变为Cy。</p><p id="7dc9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，利用相同的原理，我们可以找到第二方向<strong class="kz ir">B2(第二<em class="ne">特征向量</em>)作为最大化X沿着单位长度且正交于b1的第二方向的所有可能投影之间的方差(第二<em class="ne">特征向量</em>)的方向。当发现这是第二个主成分时:PC2: y2=X.b2</strong></p><p id="b44a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后<strong class="kz ir">第三方向</strong> b3使X沿第三方向的方差最大化，该第三方向再次由单位向量定义，该单位向量也必须与b2和b1都正交。当发现这是第三个主成分:PC3: y3=X.b3</p><p id="db1f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这些迭代结束时，我们将建立一个特殊的B=[b1*，b2*，b3*](特征向量矩阵)，一个新的数据集Y =[y1=X.b1，y2=X.b2，y3=X.b3]由3个“主分量”组成，按方差强度排序，一个特殊的特征值向量λs =(λ1，λ2，λ3)，其中每个λ是每个y1，y2，y3的方差。</p><p id="4744" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当然，迭代可以进行到n维。最后，根据总方差和(λ1，λ2，λ3)上的每个方差的相应权重，我们有所有元素来减少Y。希望用Y的几个变量，我们可以“解释”总方差的大部分，我们可以减少变量的数量，但不能减少它包含的信息量..</p><h1 id="17b3" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">3.PCA后面的数学/代数(难)</h1><p id="24f1" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">以上是解释手动迭代程序以求解特征分解方程的冗长部分。现在我们来看看从开始到本征分解方程的解的整个数学过程。</p><p id="b4cc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们称X为原始数据集(m行x n列),其中列中的每个变量已经围绕它们各自的平均值“居中”, Cx是X的协方差矩阵，b1是n个元素的未知向量和未来B变换矩阵的第一列，y1是X沿未知向量b1的投影:y1=X.b1</p><ul class=""><li id="5974" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">Y的第一个新变量的方差我们称之为y1:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ot"><img src="../Images/a67624e9ef2bd780669ac5ab10947601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21qCSATVmJWZqMhpLX_85g.png"/></div></div></figure><ul class=""><li id="dc16" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">为了最大化Var(y1)并找到相应的第一方向，我们使用带有拉格朗日乘数的拉普拉斯方程。符号是梯度，f是最大化的函数，在我们的情况下是y1的方差X.b1，g是我们设置约束的函数，b向量的长度必须是1:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/89d2e8e5bd4ecab02b031f06f0fa1a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*sFMeD2E8QHrDqdAd_R1M_Q.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图一</p></figure><ul class=""><li id="d18c" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">f对B1(n分量的未知向量)的偏导数为:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/87be9e896e4ecffb0ffdc140376104d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*1z-dj4o5I0JlTMjuhmAzrw.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图2如果你想知道为什么你可以用一个简单的未知向量B1(b11，b12)，一个给定的simmetric Cx like ([4，2][2，1])，以及未知项B1(b11，b12)。你将有一个二次多项式和两个偏导数(b11和b12)组成一个矩阵，它是Cx乘以b1的2倍</p></figure><ul class=""><li id="9e72" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">g对b1的偏导数是:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/f43b10b01e0fcbe734633e96cc62256a.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*HUhxn1DMoPWCzi4TCGOFBw.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图3</p></figure><ul class=""><li id="d62d" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">本征分解方程变为:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/691fef987c3bca531e268b85c3e0cbea.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*t1TGP3BHIXuwdbdRTFKUvQ.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图4</p></figure><ul class=""><li id="5ded" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">为了找到某个b1向量&lt;&gt; 0，我们必须找到括号中的项的行列式，并将其设置为0。只有在这种情况下，b1的n个未知分量中的n个方程组才有非平凡解:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6e6f845ea8e3eeb2073795716b97d4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*kMWyhVSk5sFvpqnBJqvurQ.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图5</p></figure><ul class=""><li id="17e6" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">这个含有n个未知数的n方程系统的解可能会给出(我保持简单，因为可能有例外)n个不同的λ。这些λ代表X的投影点沿待定方向的n个不同方差。</li><li id="86f0" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">现在我们应该指定找到的lambdas(姑且称之为lambda1)的最大值，并通过求解由下式给出的系统来找到b1:</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1afade8ada8046a3cc395e9f823d53d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*uE_X-JojEweQ9zNi4A2Rfw.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">图6</p></figure><ul class=""><li id="8ca5" class="lt lu iq kz b la lb ld le lg lv lk lw lo lx ls nf lz ma mb bi translated">在我们找到b1之后，我们取λ2，并且我们在上面的等式中再次求解该系统以找到b2，等等，直到对于所有的λ，我们有了所有的b向量。</li><li id="75b4" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">正如你所想象的，计算对于3个变量来说是非常长的，这就是为什么我们需要Python和数字算法来完成这些脏活。</li><li id="cf95" class="lt lu iq kz b la mc ld md lg me lk mf lo mg ls nf lz ma mb bi translated">最后我们将(例外情况除外)得到:作为b1，b2，b3 … bn列向量序列的特征向量B矩阵；特征值向量L(ambda)表示Y(作为主分量)的每个Y的方差或特征向量的幅度。</li></ul><h1 id="a26f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">4.Python片段:</h1><p id="8608" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">这里有很多例子和库，但在这里我想用Python来展示，只需用<strong class="kz ir"> numpy </strong>代码片段，你就可以快速尝试你的样本小数据集，并通过查看结果来理解发生了什么。我跳过了print语句，因为您可以在控制台上工作并自己检查变量的内容。甚至图表都被省略了，但是你可以用matplotlib或者类似的工具做你想做的事情。这里的重点不是构建一个应用程序，而是展示对逻辑的理解。</p><pre class="kf kg kh ki gt pa pb pc pd aw pe bi"><span id="f28d" class="nr mi iq pb b gy pf pg l ph pi">import numpy as np<br/>import pandas as pd</span><span id="9c9f" class="nr mi iq pb b gy pj pg l ph pi">pd.options.display.max_columns = 200<br/>pd.options.display.width=200<br/>pd.options.display.max_rows=200</span><span id="d5d3" class="nr mi iq pb b gy pj pg l ph pi">def fcenter(X): #function to center data<br/>    data_mean=np.mean(X,axis=0) #calc mean by column <br/>    X=X-data_mean #centered data<br/>    return X</span><span id="677c" class="nr mi iq pb b gy pj pg l ph pi">def fcov(X): #function to find the covariance matrix<br/>    covx=(X.T.dot(X))/X.shape[0] #calc cov matrix of X<br/>    #alternative to: covx=np.cov(X,rowvar=False,ddof=0)<br/>    return covx</span><span id="396a" class="nr mi iq pb b gy pj pg l ph pi">def feigen_decomp(Cx): #this is the core to solve the eigen decomposition equation<br/>    eigval,eigvec=np.linalg.eig(Cx) #solve eigen equation<br/>    return eigval,eigvec</span><span id="8fd7" class="nr mi iq pb b gy pj pg l ph pi">X=np.array([[1,1,6],[4,2,9],[2,-2,3],[-3,3,1],[-5,1,7]]) #some data<br/>Xc=fcenter(X) #centered data<br/>#----------------------<br/>#this shows that variance does not change when centering data<br/>vX=np.var(X,axis=0)   #variance of columns of X<br/>vXc=np.var(Xc,axis=0) #variance of columns of Xc<br/>#----------------------<br/>Cx=fcov(Xc)           #Cx=covariance of Xc<br/>L,B=feigen_decomp(Cx) #L=eigenvalues vector, B=eigenvectors matrix<br/>Lw=L/L.sum()          #weight in % of every PC (not cumulative)<br/>Y=Xc.dot(B)           #Y=Principal Components matrix (in columns = y  <br/>                       scores) <br/>Cy=fcov(Y)            #shows that diagonal of covariance matrix of Y<br/>                       coincides con L<br/>#----------------------<br/>#Loadings analysis<br/>Loadings=np.sqrt(L)*B   #see comments<br/>Loadings_sq=Loadings**2 #see comments<br/>Loadings_sq.sum(axis=0) #see comments<br/>Loadings_sq.sum(axis=1) #see comments</span></pre><p id="f38b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">关于加载的一个注意事项:当您想要了解结果时，加载是有用的。回想一下，Y的每个新变量都是所有X变量的线性组合。负载矩阵垂直表示每个PC的方差有多少是由X的每个变量X解释的:事实上，每列的总和等于L，水平表示每个PC解释了每个X的方差有多少:事实上，行的总和等于X的方差。如果你想的话，可以去看看。最后比这个还好玩:-)。</p><p id="6cfd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于试图定义与对主成分的值(分数)有贡献的最相关的x的名称相关的PCs的名称，加载是重要的。</p><h1 id="71fd" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">…跟我来</h1><p id="a250" class="pw-post-body-paragraph kx ky iq kz b la mz jr lc ld na ju lf lg nb li lj lk nc lm ln lo nd lq lr ls ij bi translated">大家好，我叫Andrea Grianti，我的职业生涯是在IT和数据仓库方面度过的，但后来我对数据科学和分析主题越来越有热情。</p><p id="e96a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">请考虑跟随我，以使我达到追随者数量的阈值，以便Medium platform将我纳入他们的合作伙伴计划。</p></div></div>    
</body>
</html>