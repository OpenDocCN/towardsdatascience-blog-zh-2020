<html>
<head>
<title>Understanding Error Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解误差反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/error-backpropagation-5394d33ff49b?source=collection_archive---------17-----------------------#2020-10-23">https://towardsdatascience.com/error-backpropagation-5394d33ff49b?source=collection_archive---------17-----------------------#2020-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6935" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的神经网络解释误差反向传播</h2></div><h2 id="f2ef" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h2><p id="682b" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">深度学习所基于的技术神经网络(NN)在机器学习中非常流行。我记得早在2015年，在阅读了由<a class="ae lx" href="https://twitter.com/iamtrask" rel="noopener ugc nofollow" target="_blank"> Andrew Trask </a>撰写的文章<a class="ae lx" href="https://iamtrask.github.io/2015/07/12/basic-python-network/" rel="noopener ugc nofollow" target="_blank">11行python代码的神经网络</a>后，我立即迷上了人工智能领域。但是尝试从零开始构建一个神经网络，我相信大多数人会同意我的观点，即<strong class="lg iu">误差反向传播</strong>或简单的反向传播(BP)将是完成这项任务的早期障碍之一，至少取决于你愿意钻研的深度。对于那些不熟悉的人来说，BP是与优化算法(如梯度下降(GD ))一起使用的算法，用于学习NN模型的参数。BP产生梯度，然后用于优化。在本文中，我将尝试解释这种算法是如何工作的，然后从头开始构建一个简单的神经网络，在我之前的<a class="ae lx" rel="noopener" target="_blank" href="/linear-regression-from-math-to-code-9659132383ec">帖子</a>中使用的一个回归问题上测试这个网络。对于那些正在与这种算法作斗争的人来说，我希望这篇文章能作为一个直观的指南。</p><p id="3650" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">请注意，为了完全理解这个算法，很好地掌握矩阵代数和多元微积分是非常必要的</p><h2 id="ee76" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">设置</h2><p id="96f9" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">为了对BP有一个全面的了解，我将从给出我们将要建立的神经网络的大图开始。由此你将有望对神经网络的设计决策以及BP中使用的矩阵运算有一个直观的理解。请注意，这只是我的设计，我觉得很直观，其他作者可能有其他设计。我将使用一个简单的前馈神经网络，它只是一个相互堆叠的函数/层的组合，如下图所示。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/c51191f34735493917cba8c9a485ce23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49fUvSpyAnHCR82alSd68w.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><h2 id="501b" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">该算法</h2><p id="7b60" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">神经网络的每次训练迭代有两个主要阶段</p><ol class=""><li id="6b90" class="mt mu it lg b lh ly lk lz kr mv kv mw kz mx lw my mz na nb bi translated"><em class="nc">正向传递/传播</em></li><li id="ce09" class="mt mu it lg b lh nd lk ne kr nf kv ng kz nh lw my mz na nb bi translated"><em class="nc"> BP </em></li></ol><p id="9fab" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">BP阶段有以下步骤</p><ul class=""><li id="ad53" class="mt mu it lg b lh ly lk lz kr mv kv mw kz mx lw ni mz na nb bi translated"><em class="nc">评估每层的误差信号</em></li><li id="01ec" class="mt mu it lg b lh nd lk ne kr nf kv ng kz nh lw ni mz na nb bi translated"><em class="nc">使用误差信号计算误差梯度</em></li><li id="febc" class="mt mu it lg b lh nd lk ne kr nf kv ng kz nh lw ni mz na nb bi translated"><em class="nc">使用带有优化算法(如GD)的误差梯度更新层参数。</em></li></ul><p id="54e5" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">这里的想法是，网络在正向传递期间估计目标值。然后，我们计算我们的估计与最后一层的实际目标有多远(误差信号<strong class="lg iu"> <em class="nc"> δ_k </em> </strong>)。最后，我们递归地计算每个先前层的误差信号。</p><p id="f537" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">给定诸如均方根的误差函数，可以使用偏导数找到最后一层的误差梯度。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nj"><img src="../Images/a904cd621e661fad31a1dd1382cee8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wLSHX3FX6jPaTzBdB12Q6A.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="8dc0" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">请注意，<strong class="lg iu"><em class="nc">h’(a _ k)= 1</em></strong>为线性激活，因为这个<strong class="lg iu"><em class="nc">【∂e_n/∂y_k】</em></strong><strong class="lg iu">=<em class="nc">【∂e_n/∂a_k】</em></strong>。为了保持等式的整洁，忽略了索引n。最后一层的量<strong class="lg iu"> <em class="nc"> (y_nk - t_nk) </em> </strong>称为误差信号<strong class="lg iu"><em class="nc">【δ_ k】</em></strong>。因此，链接特定误差信号和输入信号的参数的<strong class="lg iu">梯度是输入信号和误差信号</strong>的乘积。使用链式法则，可以使用当前层的误差信号来计算前一层的误差信号。从上图来看，</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nk"><img src="../Images/33e8d84bc70b9a53d4b6cc59d9b40a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_otHHVhpYK1dai85QjPVg.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="8cf2" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">注意，如何通过对来自当前层节点的所有误差信号进行加权求和来获得前一层中的节点的误差信号，前一层节点向当前层节点发送其信号，即在索引<em class="nc"> k </em>上求和。这就是为什么它被称为误差反向传播。同样，为了了解k 上的这个和在数学上来自哪里，请注意<strong class="lg iu"><em class="nc"/></strong>∂e_n/∂a_k是雅可比向量，而<strong class="lg iu"> <em class="nc"> ∂a_k/∂a_j </em> </strong>是雅可比矩阵。</p><p id="6e52" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如前所述，一般而言，链接一层中特定误差信号和输入信号的参数的梯度是该层的输入信号和误差信号的乘积。在前一种情况下，</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/13dfe13c6ada3cd524f4d95ec37cf725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WqDPO5SXjBVyf5zCX_l5w.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="cc2b" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">类似地，对于偏置参数，</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nm"><img src="../Images/a015e294274c347596660f59c4f057d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvwx0tf45t04RxOEcwkvuQ.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="cc35" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">注意，这是递归的，来自当前层的误差信号用于评估前一层中的误差信号。这是拼图中非常重要的一块，因此让我们看看我们如何向量化，然后在代码中实现它。我们将假设批量训练，但是通过将批量大小设置为1，相同的设计可以用于在线训练。</p><p id="4222" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">对于一个输出<strong class="lg iu"> <em class="nc"> K </em> </strong>和输入<strong class="lg iu"> <em class="nc"> P </em> </strong>的层，层权重会初始化为<strong class="lg iu"> <em class="nc"> (PxK) </em> </strong>。因此，对于尺寸为<strong class="lg iu"> <em class="nc"> N </em> </strong>输入和尺寸为<strong class="lg iu"> <em class="nc"> P </em> </strong>的输入，我们得到尺寸为<strong class="lg iu"><em class="nc"/></strong>N<strong class="lg iu"><em class="nc">K</em></strong>的输出。如图所示，这是层向前传播步骤。偏置初始化为<strong class="lg iu"> <em class="nc"> (K，)</em> </strong>，此处未显示，因为它通过<strong class="lg iu"> <em class="nc"> N </em> </strong>广播，不会影响输出尺寸。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nn"><img src="../Images/fdec4c44a83ba7113c477a42a2dacc68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2s_TCJ-KxvBgXpd9GbL2g.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="0ecc" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">误差信号，<strong class="lg iu"><em class="nc">【δ_ k】</em></strong>，因此有形状<strong class="lg iu"> <em class="nc"> (NxK) </em> </strong>。出于一个我们很快就会看到的原因，我将这个误差信号转置并作为<strong class="lg iu"> <em class="nc"> (KxN) </em> </strong>馈入反向传播函数。现在，将层参数与层误差信号相乘，对所有的<strong class="lg iu"><em class="nc"/></strong>图案在k上执行加权求和，因此是误差信号首先被转置的原因。我们把这一步得到的矩阵叫做<strong class="lg iu"> <em class="nc"> DM </em> </strong>。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi nl"><img src="../Images/7a1de90ec4ece17d7af0189116bff84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0c0P4rLGBqbUiqqyi4J9g.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="01ba" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在是棘手的部分，要完成前一层反向传播误差信号的计算，每个节点加权和，必须乘以<strong class="lg iu"><em class="nc">【h’(a _ j)</em></strong>，其维数为<strong class="lg iu"> <em class="nc"> (NxP) </em> </strong>。我就把这个导数矩阵<strong class="lg iu"> <em class="nc"> D </em> </strong>。在矩阵代数中，这是通过将特定的输入模式作为对角矩阵，然后将该矩阵乘以相应的<strong class="lg iu"> DM </strong>列来实现的。我就把这个对角矩阵叫做，<strong class="lg iu"> <em class="nc"> S_n </em> </strong>。我将对一个大小为<strong class="lg iu"><em class="nc">【PxN】</em></strong>的矩阵<strong class="lg iu"><em class="nc"/></strong>进行零初始化，以迭代累加前一层信号</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi no"><img src="../Images/82e544533313436e2b49d117ccb74aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_DCuzaGgBoTX84H94lwVQ.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="a6b8" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如何构建矩阵<strong class="lg iu"> <em class="nc"> S_n </em> </strong>和<strong class="lg iu"> <em class="nc"> DM_n </em> </strong>由你决定。您将在代码部分看到实现这一点的一种方法。也许有更有效的不涉及循环的方法，如果你知道的话请告诉我:)。</p><p id="64f6" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">现在这些误差信号被传递到上一层，<strong class="lg iu"> <em class="nc"> L_k-1，</em> </strong>来更新它的参数。当前层<strong class="lg iu"> <em class="nc"> L_k </em> </strong>的参数由误差信号更新，该误差信号通过其“反向传播”功能传递到该层。</p><p id="8641" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">需要注意的一点是，在更新层<strong class="lg iu"> <em class="nc"> L_k </em> </strong>参数之前，计算<strong class="lg iu"> <em class="nc">层L_k-1 </em> </strong>误差信号。</p><p id="3e24" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">如前所述，为了计算层参数梯度，我们将误差信号乘以该层的输入信号。我将称之为<strong class="lg iu"> <em class="nc"> G_w </em> </strong>权重渐变和<strong class="lg iu"> <em class="nc"> G_b </em> </strong>偏移渐变。对于层<strong class="lg iu"> K </strong>，矩阵A为<strong class="lg iu"> <em class="nc"> (KxN) </em> </strong>，输入信号<strong class="lg iu"> <em class="nc"> I </em> </strong>为<strong class="lg iu"> <em class="nc">(恩智浦)</em> </strong></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi np"><img src="../Images/b72db7d735e79bed6d093f50f4ca7092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KV9nMZR7dM1xywhMbjqEXg.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="3259" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">注意，上面的操作对<strong class="lg iu"> <em class="nc"> N </em> </strong>求和，其效果是累加一批大小为<strong class="lg iu"> <em class="nc"> N </em> </strong>的梯度。</p><p id="9a30" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">好了，说够了，让我们编码吧！</p><h2 id="4763" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">密码</h2><p id="c583" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">首先，导入所有需要的东西</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="bdbe" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，我将创建一个图层类。当调用该层时，它使用__call__执行正向传播。通过将前一层实例传递到当前层的实例中，可以将多个层堆叠在一起。正向传播从最早的层进行到最新的层。并且只有在前一层的输出大小/尺寸与当前层的输入尺寸匹配时，两层才能被附着。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="16c3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，模特班。这个类处理神经网络的训练过程和预测。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d232" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">最后是一个生成器类，用于在训练时向网络提供数据</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><h2 id="c901" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">测试</h2><p id="3562" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">首先，让我们生成一些数据</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="e2a9" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">接下来，让我们创建我们的网络并开始训练。在本例中，我将创建一个4层网络。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="43bf" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">最后，测试和绘图</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="c6a3" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">图表</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e83639361df8b529f53f3b74e6b98a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*HlCa96Xj_3V90KI4s44nuQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><h2 id="158e" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h2><p id="7e30" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">在这篇文章中，我试图涵盖反向传播的具体细节，并在此过程中展示了如何从头开始创建一个神经网络。测试结果表明，该神经网络比我以前文章中的线性回归模型更加强大和灵活。这篇文章的笔记本可以在我的<a class="ae lx" href="https://github.com/hollan86/neural_network/blob/main/NN.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。对于感兴趣的读者，我鼓励你尝试修改网络来测试它的分类任务。</p><p id="cb7b" class="pw-post-body-paragraph le lf it lg b lh ly ju lj lk lz jx lm kr ma lo lp kv mb lr ls kz mc lu lv lw im bi translated">谢谢你看我的文章，下次再见！与此同时，在这段艰难的时期要小心！</p><h2 id="e88f" class="ki kj it bd kk kl km dn kn ko kp dp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">参考</h2><p id="bc22" class="pw-post-body-paragraph le lf it lg b lh li ju lj lk ll jx lm kr ln lo lp kv lq lr ls kz lt lu lv lw im bi translated">[1]:主教，C. M. (2006年)。<em class="nc">模式识别与机器学习</em>。斯普林格。</p></div></div>    
</body>
</html>