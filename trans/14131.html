<html>
<head>
<title>Deep Deterministic and Twin Delayed Deep Deterministic Policy Gradient With TensorFlow 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">采用TensorFlow 2.x的深度确定性和双延迟深度确定性策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-deterministic-and-twin-delayed-deep-deterministic-policy-gradient-with-tensorflow-2-x-43517b0e0185?source=collection_archive---------23-----------------------#2020-09-29">https://towardsdatascience.com/deep-deterministic-and-twin-delayed-deep-deterministic-policy-gradient-with-tensorflow-2-x-43517b0e0185?source=collection_archive---------23-----------------------#2020-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4e24" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">DDPG和TD3的TensorFlow 2.x实施</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/04e7614a6382e8414428e3ca8c470c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7K3yKJYQ1xx6LmlSC1hGPA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">摄影爱好在<a class="ae kv" href="https://unsplash.com/s/photos/robot?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上</p></figure><p id="72b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将使用TensorFlow 2.x实现深度确定性策略梯度和双延迟深度确定性策略梯度方法。我们不会深入研究理论，只会涵盖基本内容。在你继续学习之前，建议你先熟悉一下DQN和双DQN。对于这篇文章，我将菲尔博士的Td3 PyTorch代码转换为TensorFlow，对于DDPG，我采用了他的目标网络更新方法。请参考他的youtube频道<a class="ae kv" href="https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw" rel="noopener ugc nofollow" target="_blank">这里</a>(编码RL的最佳youtube频道)。</p><h1 id="4f67" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">DDPG:</h1><p id="3469" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">DDPG用于有连续活动空间的环境。DDPG结合了DQN和演员评论家的方法。让我们试着用代码来理解。</p><h2 id="49d4" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">网络:</h2><ol class=""><li id="cc2a" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">我们的批评家网络将状态和行为作为输入，这些输入被连接在一起。</li><li id="d6e2" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">批评家网络输出特定状态下的行动值。</li><li id="35e6" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">我们正在使用一个连续的环境，这就是我们使用tanh激活(输出值b/w -1和1)的原因，输出是动作的长度，即在“LunarLanderContinuous-v2”中，一个动作被表示为一个数组[-1 1]，所以这里的长度是2。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="2a6f" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">代理类别和操作选择:</h2><ol class=""><li id="b5ec" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">在DDPG，我们有演员和评论家的目标网络，就像在DQN我们有目标网络一样。</li><li id="6815" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">请注意，我们已经编译了目标网络，因为我们不想在将权重从主网络复制到目标网络时出错。</li><li id="6dbe" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">我们还使用了重放缓冲区来存储体验。</li><li id="09c5" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">对于动作选择，首先，我们将状态转换为张量，然后将其传递给actor-network。</li><li id="f1b8" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">为了训练，我们在动作中添加了噪音，为了测试，我们不会添加任何噪音。</li><li id="0c1a" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">我们将剪切最大和最小动作值的动作b/w范围。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="f4f0" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">更新目标网络:</h2><ol class=""><li id="d320" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">更新ddpg和td3中的目标网络使用软更新，即我们每次都会稍微更新权重。</li><li id="afb1" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">网络可以按照研究报告中的解释进行更新。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f267d572f2ff49319eb0a5e5138b047f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*qcyVV_FSvjk4R7dwDhPq0A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片经由<a class="ae kv" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a></p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="e0d8" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">训练功能:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a9d5979edf3781011d0871b899670608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*HbqrrwEFpOmfeEOYRal6zw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a></p></figure><ol class=""><li id="87d6" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">首先，我们从重放缓冲区中抽取经验样本，并将它们转换成张量。</li><li id="cc3c" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">评论家损失的目标值是通过使用行动者的目标网络预测下一个状态的行动来计算的，然后使用这些行动，我们使用评论家的目标网络来获得下一个状态的值。</li><li id="5897" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">然后，我们应用贝尔曼方程计算目标值(target _ values = rewards+selfγ* target _ next _ state _ values * done)。请注意，在训练时，此处的“完成”存储为(1-完成)。</li><li id="e71d" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">我们的预测值是从缓冲样本中提取状态和动作的主评价网络的输出。</li><li id="7965" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">然后根据目标值和预测值的MSE计算临界损失。</li><li id="af56" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">参与者损失被计算为批评家主要值的负值，输入作为主要参与者预测的行动。</li><li id="2586" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">然后，我们用0.005的tau更新我们的目标网络。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="158c" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">训练循环:</h2><ol class=""><li id="5118" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">我们的训练循环很简单，即它在每个行动步骤中相互作用并储存经验和学习。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="8ee4" class="ls lt iq bd lu lv od lx ly lz oe mb mc jw of jx me jz og ka mg kc oh kd mi mj bi translated">TD3PG:</h1><p id="4a42" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">TD3受到双DQN的启发，解决了评论家值被高估的问题，与DDPG相比有以下变化。</p><ol class=""><li id="b1b4" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">除了他们的目标网络之外，使用两个主要的评论家网络。</li><li id="034b" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">延迟演员网络的更新。</li><li id="8767" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">动作噪音调节。</li></ol><p id="531f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将只讨论DDPG代码的不同之处。</p><h2 id="eb59" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">代理类别:</h2><ol class=""><li id="8783" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">在代理课程中，我们有两个主要的评论家网络和他们各自的目标网络。</li><li id="76ce" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">在动作选择中，我们在一些步骤之后停止向动作添加噪声。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><h2 id="d155" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">训练功能:</h2><ol class=""><li id="daa7" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated">td3列车功能与DDPG列车功能只有3处不同。</li><li id="af65" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">首先，通过添加噪声，然后在最大和最小动作的范围内裁剪动作，来自演员的目标网络的动作被规则化。</li><li id="cabf" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">第二，下一状态值和当前状态值都是目标评论家和两个主要评论家网络。并且对于下一个状态值和当前状态值都考虑最少两个网络。</li><li id="b5f2" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">第三，在每2步之后训练演员网络。</li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4ba3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，这都是关于实现上的差异。现在让我们看看训练结果。</p><h2 id="da21" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">DDPG vs TD3PG:</h2><p id="6f78" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这两个图显示了在过去100集内，两种算法在“LunarLanderContinuous-v2”中达到平均分数200所拍摄的集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/90dbfaff0e5a13a7273aae625ca11214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*AAbrXSYxIXAvxTEMKjZppg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者制作并通过matplotlib生成的图像</p></figure><p id="5a9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，TD3算法在400集的最后100集中得到了平均200英镑的奖励。而DDPG在560集之后只得了100分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e38838643e7d1361fb30b21d2764a753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*LagkcqEZFhpMx4IazkoLDg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者制作并通过matplotlib生成的图像</p></figure></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="07c1" class="ls lt iq bd lu lv od lx ly lz oe mb mc jw of jx me jz og ka mg kc oh kd mi mj bi translated">实施时需要注意的事项:</h1><p id="02d5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在编写RL代码时，要记住以下几点。</p><ol class=""><li id="40fd" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">神经元的数量、隐藏层数、学习速率对学习有巨大的影响。</li><li id="57c7" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">张量和NumPy数组的形状应该是正确的。很多时候，实现是正确的，代码也是有效的，但是代理没有学到任何东西，只是因为张量的形状不正确，并且当对这些张量进行运算时，会给出错误的结果。</li></ol></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="7b40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这里找到这篇文章的完整代码<a class="ae kv" href="https://github.com/abhisheksuran/Reinforcement_Learning/blob/master/DDPGwithtau.ipynb" rel="noopener ugc nofollow" target="_blank"/>和<a class="ae kv" href="https://github.com/abhisheksuran/Reinforcement_Learning/blob/master/td3withtau.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。敬请关注即将发布的文章，我们将在TensorFlow 2中实现更多RL算法和深度学习算法。</p><p id="eb23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，本文到此结束。谢谢你的阅读，希望你喜欢并且能够理解我想要解释的内容。希望你阅读我即将发表的文章。哈里奥姆…🙏</p><h1 id="faee" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考资料:</h1><div class="ok ol gp gr om on"><a href="https://arxiv.org/abs/1509.02971" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">具有深度强化学习的连续控制</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">我们将深度Q学习成功背后的思想应用于持续行动领域。我们提出一个…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/td3-learning-to-run-with-ai-40dfc512f93"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">TD3:学习用人工智能跑步</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">学习构建强化学习中最强大和最先进的算法之一，TD3</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb kp on"/></div></div></a></div><div class="ok ol gp gr om on"><a href="https://www.youtube.com/channel/UC58v9cLitc8VaCjrcKyAbrw" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">菲尔的机器学习</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">你好。在Neuralnet.ai，我们涵盖了各种主题的人工智能教程，从强化…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">www.youtube.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb kp on"/></div></div></a></div></div></div>    
</body>
</html>