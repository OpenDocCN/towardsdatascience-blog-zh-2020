<html>
<head>
<title>Building an Event-Driven Data Pipeline to Copy Data from Amazon S3 to Azure Storage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建事件驱动的数据管道，将数据从亚马逊S3复制到Azure Storage</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb?source=collection_archive---------18-----------------------#2020-10-22">https://towardsdatascience.com/building-an-automated-data-pipeline-to-copy-data-from-amazon-s3-to-azure-storage-2a1b8d1ac6eb?source=collection_archive---------18-----------------------#2020-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6890" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="6956" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用AWS Data Wrangler、亚马逊S3库存、亚马逊S3批处理操作、Athena、Fargate和AzCopy进行多云批量文件传输</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/04308ddaa954bf5dc88cfe72c6e3a79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDf3W4XtgQjYPECwEjBSoA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@jsweissphoto" rel="noopener ugc nofollow" target="_blank"> @jsweissphoto </a>在<a class="ae lh" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a3fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您的企业可能需要定期将大量数据从一个公共云转移到另一个公共云。更具体地说，您可能会面临需要多云解决方案的要求。本文介绍了一种使用亚马逊S3库存、亚马逊S3批处理操作、Fargate和AzCopy实现从AWS S3桶到Microsoft Azure Blob存储容器的自动化数据复制的方法。</p><h1 id="d7a9" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">方案</h1><p id="c9a1" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">您的公司每天在内部生成新的CSV文件，压缩后总大小约为100GB。所有文件的大小为1-2gb，需要在每晚凌晨3点到5点之间的固定时间窗口上传到亚马逊S3。在所有文件上传到S3后，您的企业决定将这些CSV文件从S3复制到Microsoft Azure存储。您必须找到一种简单快捷的方法来自动化数据复制工作流。</p><p id="baeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了完成这项任务，我们可以建立一个数据管道，使用AWS Data Wrangler、亚马逊S3库存、亚马逊S3批处理操作、Athena、Fargate和AzCopy定期将数据从S3复制到Azure存储。</p><p id="c258" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb">下图展示了管道解决方案的高级架构:</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/367dfc82cb88942cadee819550cfb8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UqxgjsB__qcf2uLPZfmVg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">艾毅的建筑图</p></figure><h2 id="6185" class="nd mf it bd mg ne nf dn mk ng nh dp mo lr ni nj mq lv nk nl ms lz nm nn mu iz bi translated">我们将涵盖的内容:</h2><ul class=""><li id="750d" class="no np it lk b ll mw lo mx lr nq lv nr lz ns md nt nu nv nw bi translated">创建一个包含私有和公共子网、S3端点和NAT网关的VPC。</li><li id="6c33" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">创建Azure存储帐户和blob容器，生成SAS令牌，然后添加防火墙规则以允许从AWS VPC到Azure存储的流量。</li><li id="1a1a" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">在S3存储桶上配置每日S3库存报告。</li><li id="c428" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">使用Athena仅过滤S3库存报告中的新对象，并将这些对象的存储桶名称和对象关键字导出到CSV清单文件中。</li><li id="da57" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">使用导出的CSV清单文件创建S3批处理操作PUT复制作业，该作业将对象复制到配置了生命周期策略到期规则的目标S3存储桶。</li><li id="f7c0" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">设置一个Eventbridge规则，调用lambda函数来运行Fargate任务，该任务将目标桶中具有相同前缀的所有对象复制到Azure存储容器。</li></ul><h1 id="03ce" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">先决条件</h1><ul class=""><li id="6bc1" class="no np it lk b ll mw lo mx lr nq lv nr lz ns md nt nu nv nw bi translated">设置AWS帐户</li><li id="4abc" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">设置Azure帐户</li><li id="c29a" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">安装最新的<a class="ae lh" href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html" rel="noopener ugc nofollow" target="_blank"> AWS-CLI </a></li><li id="c069" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">安装自动气象站<a class="ae lh" href="https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html" rel="noopener ugc nofollow" target="_blank"> CDK-CLI </a></li><li id="5a98" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">对<a class="ae lh" href="https://aws.amazon.com/cdk/" rel="noopener ugc nofollow" target="_blank"> AWS CDK </a>的基本了解</li><li id="3395" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">对Docker的基本了解</li></ul><h1 id="8a85" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">我们开始吧！</h1><h1 id="260d" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">创建来源和目标S3时段</h1><p id="e973" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们使用CDK在AWS上构建我们的基础设施。首先，让我们创建一个源桶来接收来自外部提供者或内部的文件，并设置每日清单报告，提供对象和元数据的平面文件列表。</p><p id="51a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，创建一个目标存储桶作为临时存储，并在前缀<code class="fe oc od oe of b">/tmp_transition</code>上配置生命周期策略到期规则。所有带有前缀的文件(如<code class="fe oc od oe of b">/tmp_transition/file1.csv</code>)将被复制到Azure，并在24小时后被生命周期策略删除。</p><p id="904a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用以下代码创建S3存储桶。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><h1 id="c3df" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">创建AWS VPC</h1><p id="2dcc" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">接下来，我们需要创建具有公共和私有子网的VPC、NAT网关、S3端点，并附加一个端点策略，该策略允许访问Fargate容器，我们将数据复制到Azure的S3桶就位于该容器中。</p><p id="8444" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在使用下面的代码定义您的VPC和相关资源。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="bef2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">创建NAT网关时，将在AWS中创建一个弹性IP地址。我们将需要IP地址来在步骤3中设置Azure存储防火墙规则。</p><h1 id="d7e2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">正在部署Azure存储帐户</h1><p id="4bc2" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了简化资源管理，我们可以使用Azure资源管理器模板(ARM模板)在我们的Azure订阅级别部署资源。</p><p id="0f5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我会假设你已经有一个Azure订阅设置。我们将使用云外壳来部署资源组、Azure存储帐户、容器和防火墙规则，以允许来自特定IP地址的流量。</p><p id="bd15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">点击Azure门户标题栏的云壳图标，就会打开云壳。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/7ead1bbcf92e32212a56883e8c5de313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPjcMRzlMfIqF18L7iZLPA.jpeg"/></div></div></figure><p id="fdc5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行以下命令进行部署:</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="9466" class="nd mf it of b gy on oo l op oq">az group create --name <strong class="of jd">examplegroup</strong> --location australiaeast</span><span id="22b1" class="nd mf it of b gy or oo l op oq">az deployment group create --resource-group <strong class="of jd">examplegroup</strong> --template-uri <a class="ae lh" href="https://raw.githubusercontent.com/yai333/DataPipelineS32Blob/master/Azure-Template-DemoRG/template.json" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/yai333/DataPipelineS32Blob/master/Azure-Template-DemoRG/template.json</a>  --parameters storageAccounts_mydemostroageaccount_name=<strong class="of jd">mydemostorageaccountaiyi</strong> --debug</span></pre><p id="7bec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦部署了模板，我们就可以通过探索Azure门户的资源组来验证部署。所有部署的资源都将显示在资源组的概述部分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/2f3b37dbc1a2e8d43924cbda34b51a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9IJ3zNDrkXq-TPiH0ZDGQ.png"/></div></div></figure><p id="149a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们为我们的存储帐户创建一个防火墙规则:</p><ul class=""><li id="5114" class="no np it lk b ll lm lo lp lr ot lv ou lz ov md nt nu nv nw bi translated">首先，转到我们刚刚部署的存储帐户。</li><li id="7af0" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">其次，点击设置菜单中的防火墙和虚拟网络。</li><li id="4842" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">第三，检查您是否选择了允许来自选定网络的访问。</li><li id="7a6e" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">然后，要授予对互联网IP范围的访问权限，请输入AWS VPC的公共IP地址(步骤2)并保存。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/08d5fc3fec7bd5f5532e2a79b19e369d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6XVFYwzhplykJiMTkvFIA.png"/></div></div></figure><p id="2a0e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们将生成共享访问签名(SAS)来授予对Azure存储资源的有限访问权限。</p><p id="5d72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在Cloudshell中运行以下命令:</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="0cc4" class="nd mf it of b gy on oo l op oq"><strong class="of jd">RG_NAME</strong>='<strong class="of jd">examplegroup</strong>'<br/><strong class="of jd">ACCOUNT_NAME</strong>='<strong class="of jd">mydemostorageaccountaiyi</strong>' <br/><strong class="of jd">ACCOUNT_KEY</strong>=`az storage account keys list --account-name=$ACCOUNT_NAME --query [0].value -o tsv`<br/><strong class="of jd">BLOB_CONTAINER</strong>=democontainer</span><span id="d518" class="nd mf it of b gy or oo l op oq"><strong class="of jd">STORAGE_CONN_STRING</strong>=`az storage account show-connection-string --name $ACCOUNT_NAME --resource-group $RG_NAME --output tsv`</span><span id="8946" class="nd mf it of b gy or oo l op oq">SAS=`az storage container generate-sas --connection-string $STORAGE_CONN_STRING -n $BLOB_CONTAINER --expiry '<strong class="of jd">2021-06-30</strong>' --permissions <strong class="of jd">aclrw</strong> --output tsv`</span><span id="4ca9" class="nd mf it of b gy or oo l op oq">echo <strong class="of jd">$SAS</strong></span></pre><p id="a2c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将获得授权<code class="fe oc od oe of b"> (a)dd (d)elete (r)ead (w)rite </code>访问blob容器<code class="fe oc od oe of b">democontainer</code>所需的sa和URL。</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="4e79" class="nd mf it of b gy on oo l op oq">se=2021-06-30&amp;sp=racwl&amp;sv=2018-11-09&amp;sr=c&amp;sig=xxxxbBfqfEppPpBZPOTRiwvkh69xxxx/xxxxQA0YtKo%3D</span></pre><p id="d040" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们回到AWS，将sa放到AWS SSM参数存储中。</p><p id="c49d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本地终止符中运行以下命令。</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="9338" class="nd mf it of b gy on oo l op oq">aws ssm put-parameter --cli-input-json '{<br/>  "Name": "<strong class="of jd">/s3toblob/azure/storage/sas</strong>",<br/>  "Value": "<strong class="of jd">se=2021-06-30&amp;sp=racwl&amp;sv=2018-11-09&amp;sr=c&amp;sig=xxxxbBfqfEppPpBZPOTRiwvkh69xxxx/xxxxQA0YtKo%3D</strong>",<br/>  "Type": "<strong class="of jd">SecureString</strong>"<br/>}'</span></pre><h1 id="35c3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">定义Lambda函数和AWS数据管理器层</h1><p id="e91a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在，让我们来看看λ函数。我们将创建三个lambda函数和一个lambda层:</p><ul class=""><li id="4aa3" class="no np it lk b ll lm lo lp lr ot lv ou lz ov md nt nu nv nw bi translated">fn_create_s3batch_manifest和DataWranglerLayer</li><li id="419f" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">fn _创建_批处理_作业</li><li id="522e" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">fn _流程_转移_任务</li></ul><h2 id="5252" class="nd mf it bd mg ne nf dn mk ng nh dp mo lr ni nj mq lv nk nl ms lz nm nn mu iz bi translated">fn_create_s3batch_manifest和AWS Data Wrangler层</h2><p id="f283" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这个lambda函数使用AWS Data Wrangler的Athena模块来过滤过去UTC日期的新文件，并将文件列表保存到CSV清单文件中。</p><p id="4065" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将下面的代码复制到CDK stack.py，从<a class="ae lh" href="https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob/layers" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">这里</strong> </a>下载<code class="fe oc od oe of b">awswranger-layer</code> zip文件。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="6f5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后用下面的代码创建<code class="fe oc od oe of b">./src/lambda_create_s3batch_manifest.py</code>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="f93d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的代码中，我们使用Athena query创建Glue数据库、表，并每天向该表添加一个分区。然后lambda执行except query来返回两个日期分区之间的差异。</p><p id="5cb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意<code class="fe oc od oe of b">start_query_execution</code>是异步的，因此不需要等待Lambda中的结果。一旦查询被执行，结果将作为CSV文件保存到<code class="fe oc od oe of b">s3_output=f"s3://{os.getenv('DESTINATION_BUCKET_NAME')}/csv_manifest/dt={partition_dt}"</code>。</p><h2 id="3d6d" class="nd mf it bd mg ne nf dn mk ng nh dp mo lr ni nj mq lv nk nl ms lz nm nn mu iz bi translated">fn_create_batch_job和S3通知</h2><p id="6b6b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本节中，我们将创建一个lambda函数<code class="fe oc od oe of b">fn_create_batch_job</code>，并使亚马逊S3能够在一个CSV文件被添加到亚马逊S3 Bucket <code class="fe oc od oe of b">/csv_manifest</code>前缀时发送一个通知来触发<code class="fe oc od oe of b">fn_create_batch_job</code>。将下面的代码放到CDK的stack.py中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="c928" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用下面的代码创建<code class="fe oc od oe of b">./src/lambda_create_batch_job.py</code>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="347c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Lambda <code class="fe oc od oe of b">fn_create_batch_job</code>函数创建S3批处理操作作业，将CSV清单中列出的所有文件复制到S3目的地桶<code class="fe oc od oe of b">/tmp_transition prefix</code>。</p><p id="0c98" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">S3批处理操作是亚马逊S3的一项数据管理功能，可以让您大规模管理数十亿个对象。要启动一个S3批处理操作作业，我们还需要设置一个IAM角色<code class="fe oc od oe of b">S3BatchRole</code>和相应的策略:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><h2 id="3e99" class="nd mf it bd mg ne nf dn mk ng nh dp mo lr ni nj mq lv nk nl ms lz nm nn mu iz bi translated">fn_process_transfer_task和Eventbridge自定义规则</h2><p id="0605" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们将创建一个Eventbridge定制规则，通过AWS CloudTrail跟踪Amazon EventBridge中的S3批处理操作作业，并将处于完成状态的事件发送到目标通知资源<code class="fe oc od oe of b">fn_process_transfer_task</code>。</p><p id="ad13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Lambda <code class="fe oc od oe of b">fn_process_transfer_task</code>然后会以编程方式启动一个Fargate任务，将前缀为<code class="fe oc od oe of b">/tmp_transition</code>的文件复制到Azure存储容器<code class="fe oc od oe of b">democontainer</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="2cdb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用以下代码创建<code class="fe oc od oe of b">./src/lambda_process_s3transfer_task.py</code>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="7cb8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们已经设置了无服务器部分。让我们转到Fargate任务并处理数据复制。</p><h1 id="b467" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">创建AWS Fargate任务</h1><p id="723d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们将创建:</p><ul class=""><li id="66da" class="no np it lk b ll lm lo lp lr ot lv ou lz ov md nt nu nv nw bi translated">安装了带有AzCopy的ECR映像。<a class="ae lh" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?toc=/azure/storage/blobs/toc.json" rel="noopener ugc nofollow" target="_blank"> AzCopy </a>是一个命令行实用程序，可以用来将blobs或文件复制到存储帐户或从存储帐户复制。</li><li id="98e7" class="no np it lk b ll nx lo ny lr nz lv oa lz ob md nt nu nv nw bi translated">具有Fargte任务的ECS集群。</li></ul><p id="625e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们开始吧。</p><ol class=""><li id="588d" class="no np it lk b ll lm lo lp lr ot lv ou lz ov md ox nu nv nw bi translated">构建ECS、ECR和Fargate堆栈。</li></ol><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="a32f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2.构建一个Docker镜像并在那里安装Azcopy。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="b54a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，要使用来自AWS的AzCopy传输文件，我们需要在容器中设置AWS凭证。我们可以使用以下方法检索AWS凭据:</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="8291" class="nd mf it of b gy on oo l op oq">curl <a class="ae lh" href="http://169.254.170.2/$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI" rel="noopener ugc nofollow" target="_blank">http://169.254.170.2/$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</a></span></pre><p id="9009" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">3.将Docker图像推送到ECR</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="17aa" class="nd mf it of b gy on oo l op oq">eval $(aws ecr get-login --region ap-southeast-2 --no-include-email)</span><span id="d74f" class="nd mf it of b gy or oo l op oq">docker build . -t YOUR_ACCOUNT_ID.dkr.ecr.ap-southeast-2.amazonaws.com/YOUR_ECR_NAME</span><span id="83c6" class="nd mf it of b gy or oo l op oq">docker push YOUR_ACCOUNT_ID.dkr.ecr.ap-southeast-2.amazonaws.com/YOUR_ECR_NAME</span></pre><p id="d2a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">太好了！我们有我们需要的！你可以在我的<a class="ae lh" href="https://github.com/yai333/DataPipelineS32Blob" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>中找到CDK项目的完整解决方案。克隆存储库并部署堆栈:</p><pre class="ks kt ku kv gt oj of ok ol aw om bi"><span id="1c3d" class="nd mf it of b gy on oo l op oq">cd CDK-S3toblob <br/>pip install -r requirements.txt<br/>cdk deploy</span></pre><p id="e1ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦成功创建了堆栈，导航到AWS CloudFormation <a class="ae lh" href="https://console.aws.amazon.com/cloudformation" rel="noopener ugc nofollow" target="_blank">控制台</a>，找到我们刚刚创建的堆栈，并转到Resources选项卡以找到部署的资源。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/7fd8be5c0404b615b4521dc97fdd9954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yus8PjCPr2Fhw88aJEmaSA.png"/></div></div></figure><p id="f957" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在是时候测试我们的工作流程了；转到S3源斗<code class="fe oc od oe of b">demo-databucket-source</code>。在不同的文件夹中上传尽可能多的文件(前缀)。等待24小时，等待下一份库存报告生成；然后，你会看到整个管道开始运行，文件最终会被复制到Azure <code class="fe oc od oe of b">democontainer</code>。</p><p id="aa49" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们应该可以看到Fargate任务的日志，如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/87eede4cf6fef05e07a177553315c91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*w25k25eWtieWcU7ezYwGgg.png"/></div></figure><p id="82cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还可以使用CloudWatch Container Insights对ECS资源进行监控、故障排除和设置警报。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/8d35ae26c0cad4067272bf6f233e786d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Es8AFrnzxCzn8MoRjO7DNQ.png"/></div></div></figure><h1 id="ba1b" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="0aa3" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在本文中，我介绍了从AWS S3到Microsoft Azure Storage的自动化数据复制方法。我带你了解了如何使用CDK部署VPC、AWS S3、Lambda、Cloudtrail、Fargte资源，向你展示了如何使用ARM模板部署Azure服务。我向您展示了如何使用AWS Wrangler库和Athena query来创建表和查询表。</p><p id="89a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望这篇文章对你有用。你可以在我的<a class="ae lh" href="https://github.com/yai333/DataPipelineS32Blob/tree/master/CDK-S3toblob" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> GitHub repo </strong> </a>中找到完整的项目。</p></div></div>    
</body>
</html>