<html>
<head>
<title>Building Autoencoders on Sparse, One Hot Encoded Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在稀疏的热编码数据上构建自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7?source=collection_archive---------14-----------------------#2020-09-28">https://towardsdatascience.com/building-autoencoders-on-sparse-one-hot-encoded-data-53eefdfdbcc7?source=collection_archive---------14-----------------------#2020-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/fdc026d11e1014616fb032b4c4a0627b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jr9An7w2pp7WRpTh"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">维罗妮卡·贝纳维德斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="40f6" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">适用于在PyTorch中嵌入稀疏独热编码数据的损失函数的实践回顾</h2></div><p id="41e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自1986年引入以来[1]，通用自动编码器神经网络已经渗透到现代机器学习的大多数主要部门的研究中。自动编码器已经被证明在嵌入复杂数据方面非常有效，它提供了简单的方法将复杂的非线性依赖关系编码成简单的矢量表示。但是，尽管它们的有效性已经在许多方面得到了证明，但它们通常不能再现稀疏数据，尤其是当列像热编码一样相关时。</p><p id="4256" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将简要讨论一个热编码(OHE)数据和通用自动编码器。然后，我将介绍使用自动编码器对一个热编码数据进行训练所带来的问题的用例。最后，我将深入讨论重建稀疏OHE数据的问题，然后讨论我发现在这些条件下工作良好的3个损失函数:</p><ol class=""><li id="1e4b" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">共同嵌入损失</li><li id="1f17" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">索伦森-戴斯系数损失</li><li id="4d57" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">个体OHE成分的多任务学习损失</li></ol><p id="f45c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">—解决上述挑战，包括在PyTorch中实现它们的代码。</p><h2 id="6a3b" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">一个热编码数据</h2><p id="3790" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">One hot编码数据是一般机器学习场景中最简单但经常被误解的数据预处理技术之一。该过程将具有‘N’个不同类别的分类数据二进制化为N列二进制0和1，其中在第‘N’个类别中出现1表示观察值属于该类别。使用<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank">sci kit-Learn onehotencode模块</a>，这个过程在Python中很简单:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="15bd" class="mi mj jj nl b gy np nq l nr ns"><strong class="nl jk">from</strong> sklearn.preprocessing <strong class="nl jk">import</strong> OneHotEncoder<br/><strong class="nl jk">import</strong> numpy <strong class="nl jk">as</strong> np</span><span id="a551" class="mi mj jj nl b gy nt nq l nr ns"># Instantiate a column of 10 random integers from 5 classes<br/>x = <strong class="nl jk">np.random.randint</strong>(5, size=10).reshape(-1,1)</span><span id="0e57" class="mi mj jj nl b gy nt nq l nr ns"><strong class="nl jk">print</strong>(x)<br/>&gt;&gt;&gt; [[2][3][2][2][1][1][4][1][0][4]]</span><span id="f0da" class="mi mj jj nl b gy nt nq l nr ns"># Instantiate OHE() + Fit/Transform the data<br/>ohe_encoder = <strong class="nl jk">OneHotEncoder</strong>(categories="auto")<br/>encoded = ohe_encoder.<strong class="nl jk">fit_transform</strong>(x).<strong class="nl jk">todense</strong>()</span><span id="1980" class="mi mj jj nl b gy nt nq l nr ns"><strong class="nl jk">print</strong>(encoded)<br/>&gt;&gt;&gt; matrix([[0., 1., 0., 0., 0.],<br/>           [0., 0., 0., 1., 0.],<br/>           [0., 0., 1., 0., 0.],<br/>           [0., 0., 0., 1., 0.],<br/>           [0., 0., 1., 0., 0.],<br/>           [1., 0., 0., 0., 0.],<br/>           [0., 0., 1., 0., 0.],<br/>           [0., 0., 1., 0., 0.],<br/>           [0., 0., 0., 1., 0.],<br/>           [0., 0., 0., 0., 1.]])</span><span id="ecad" class="mi mj jj nl b gy nt nq l nr ns"><strong class="nl jk">print</strong>(<strong class="nl jk">list</strong>(ohe_encoder.<strong class="nl jk">get_feature_names()))<br/></strong>&gt;&gt;&gt; ["x0_0", "x0_1", "x0_2", "x0_3", "x0_4"]</span></pre><p id="0386" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然简单，但是如果你不小心的话，这种技术会很快变质。它很容易给数据增加额外的复杂性，并改变某些数据分类方法的有效性。例如，转换为OHE向量的列现在是相互依赖的，这种交互使得在某些类型的分类器中很难有效地表示数据的各个方面。例如，如果您有一个包含15个不同类别的列，则需要一个深度为15的决策树来处理这个热编码列中的if-then模式。如果你感兴趣，可以在这里找到这些问题的一个很好的例子。类似地，由于列是相互依赖的，如果您使用带有bagging(<a class="ae jg" rel="noopener" target="_blank" href="/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">Bootstrap Aggregating</a>)的分类策略并执行<a class="ae jg" rel="noopener" target="_blank" href="/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">特征采样</a>，您可能会完全错过独热编码列，或者只考虑其部分组件类。</p><h2 id="d411" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">自动编码器</h2><p id="f066" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">自动编码器是无监督的神经网络，用于将数据嵌入到有效的压缩格式中。它通过利用编码和解码过程将数据编码成更小的格式，然后将该更小的格式解码回原始输入表示来实现这一点。通过获取模型重建(解码)和原始数据之间的损失来训练模型。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/c997fd3da6a76af9964374e7fd6a1074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ6hJYbAzwpZ0agQfHfFfw@2x.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">A. Dertat在他的TDS文章中提供:<a class="ae jg" rel="noopener" target="_blank" href="/applied-deep-learning-part-3-autoencoders-1c083af4d798">应用深度学习——第3部分:自动编码器</a></p></figure><p id="d35f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上用代码表示这个网络也很容易做到。我们从两个函数开始:<strong class="la jk"> <em class="nv">编码器</em> </strong>模型，和<strong class="la jk"> <em class="nv">解码器</em> </strong>模型。这两个“模型”都被打包到一个名为Network的类中，该类将包含我们培训和评估的整个系统。最后，我们定义了一个函数<strong class="la jk"> <em class="nv"> Forward </em> </strong>，PyTorch使用它作为进入网络的入口，该网络包装了数据的编码和解码。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="a03f" class="mi mj jj nl b gy np nq l nr ns"><strong class="nl jk">import</strong> torch<br/><strong class="nl jk">import</strong> torch.nn <strong class="nl jk">as</strong> nn<br/><strong class="nl jk">import</strong> torch.nn.functional <strong class="nl jk">as</strong> F<br/><strong class="nl jk">import</strong> torch.optim <strong class="nl jk">as</strong> optim</span><span id="2882" class="mi mj jj nl b gy nt nq l nr ns"><strong class="nl jk">class</strong> <strong class="nl jk">Network</strong>(nn.Module):<br/>   <strong class="nl jk">def</strong> <strong class="nl jk">__init__</strong>(self, input_shape: int):<br/>      <strong class="nl jk">super</strong>().<strong class="nl jk">__init__</strong>()<br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">encode1</strong> = nn.<strong class="nl jk">Linear</strong>(input_shape, 500)<br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">encode2</strong> = nn.<strong class="nl jk">Linear</strong>(500, 250)<br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">encode3</strong> = nn.<strong class="nl jk">Linear</strong>(250, 50)<br/>      <br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">decode1</strong> = nn.<strong class="nl jk">Linear</strong>(50, 250)<br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">decode2</strong> = nn.<strong class="nl jk">Linear</strong>(250, 500)<br/>      <strong class="nl jk">self</strong>.<strong class="nl jk">decode3</strong> = nn.<strong class="nl jk">Linear</strong>(500, input_shape)</span><span id="bebd" class="mi mj jj nl b gy nt nq l nr ns">   <strong class="nl jk">def</strong> <strong class="nl jk">encode</strong>(<strong class="nl jk">self</strong>, x: torch.Tensor):<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">encode1</strong>(x))<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">encode2</strong>(x))<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">encode3</strong>(x))<br/>      <strong class="nl jk">return</strong> x</span><span id="ead9" class="mi mj jj nl b gy nt nq l nr ns">   <strong class="nl jk">def decode</strong>(<strong class="nl jk">self</strong>, x: torch.Tensor):<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">decode1</strong>(x))<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">decode2</strong>(x))<br/>      x = F.<strong class="nl jk">relu</strong>(<strong class="nl jk">self</strong>.<strong class="nl jk">decode3</strong>(x))<br/>      <strong class="nl jk">return</strong> x</span><span id="2708" class="mi mj jj nl b gy nt nq l nr ns">   <strong class="nl jk">def</strong> <strong class="nl jk">forward</strong>(self, x: torch.Tensor):<br/>      x = <strong class="nl jk">encode</strong>(x)<br/>      x = <strong class="nl jk">decode</strong>(x)<br/>      <strong class="nl jk">return</strong> x<br/></span><span id="b2fd" class="mi mj jj nl b gy nt nq l nr ns"><strong class="nl jk">def</strong> <strong class="nl jk">train_model</strong>(data: pd.DataFrame):<br/>   net = <strong class="nl jk">Network</strong>()<br/>   optimizer = optim.<strong class="nl jk">Adagrad</strong>(net.parameters(), lr=1e-3, weight_decay=1e-4)<br/>   losses = []</span><span id="7ef6" class="mi mj jj nl b gy nt nq l nr ns">   <strong class="nl jk">for</strong> epoch <strong class="nl jk">in</strong> <strong class="nl jk">range</strong>(250):<br/>     <strong class="nl jk">for</strong> batch <strong class="nl jk">in</strong> <strong class="nl jk">get_batches</strong>(data)<br/>        net.<strong class="nl jk">zero_grad</strong>()<br/>        <br/>        # Pass batch through <br/>        output = <strong class="nl jk">net</strong>(batch)<br/>        <br/>        # Get Loss + Backprop<br/>        loss = <strong class="nl jk">loss_fn</strong>(output, batch).sum() # <br/>        losses.<strong class="nl jk">append</strong>(loss)<br/>        loss.<strong class="nl jk">backward</strong>()<br/>        optimizer.<strong class="nl jk">step</strong>()<br/>     <strong class="nl jk">return</strong> net, losses</span></pre><p id="6575" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们在上面看到的，我们有一个编码函数，它从输入数据的形状开始，然后随着它向下传播到50的形状，减少它的维数。从那里，解码层获得嵌入，然后将其扩展回原始形状。在训练中，我们从解码器<strong class="la jk">获取重构，并获取重构相对于原始输入的损失。</strong></p><h2 id="5cdd" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">损失函数的问题</h2><p id="a84e" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">现在我们已经介绍了自动编码器的结构和One Hot编码过程，我们终于可以谈谈在自动编码器中使用One Hot编码的相关问题，以及如何解决这个问题了。当自动编码器将重构与原始输入数据进行比较时，必须对建议的重构与真实值之间的距离进行一些评估。通常，在输出的值被认为彼此不相交的情况下，将使用交叉熵损失或MSE损失。但是在我们的One Hot编码的情况下，有几个问题使得系统更加复杂:</p><ol class=""><li id="ac86" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">一列中出现一意味着其对应的OHE列中必须有一个零。<strong class="la jk"> <em class="nv">即列不相交</em> </strong></li><li id="a265" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">OHE向量输入的稀疏性会导致系统<strong class="la jk"> <em class="nv">选择简单地为大多数列返回0的</em> </strong>以减少误差</li></ol><p id="2cb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些问题结合起来导致上述两个损失(MSE，交叉熵)在重建稀疏OHE数据时无效。下面我将介绍三个损失，它们提供了上述一个或两个问题的解决方案，以及在PyTorch中实现它们的代码:</p><h2 id="9677" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">余弦嵌入损失</h2><p id="e638" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">余弦距离是一种经典的向量距离度量，通常在比较NLP问题中的单词包表示时使用。距离的计算方法是找出两个向量之间的余弦角，计算公式如下:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6251b60542fa69cf5aa2051ceb955114.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*8R_oUz4-k_sHzD9Ui4mBcg.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="107a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法被证明在量化稀疏OHE嵌入的重构中的误差方面是很好的，因为它能够考虑各个列中二进制值的偏差来评估两个向量的距离。这个loss是PyTorch中最容易实现的，因为它在<a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html" rel="noopener ugc nofollow" target="_blank">torch . nn . cosinembeddingloss</a>中有一个预构建的解决方案</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="44ee" class="mi mj jj nl b gy np nq l nr ns">loss_function = torch.nn.<strong class="nl jk">CosineEmbeddingLoss</strong>(reduction='none')</span><span id="5423" class="mi mj jj nl b gy nt nq l nr ns"># . . . Then during training . . . </span><span id="75a3" class="mi mj jj nl b gy nt nq l nr ns">loss = <strong class="nl jk">loss_function</strong>(reconstructed, input_data).sum()<br/>loss.<strong class="nl jk">backward</strong>()</span></pre><h2 id="053f" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">骰子损失</h2><p id="3e77" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">骰子损失是<strong class="la jk">索伦森</strong>–<strong class="la jk">骰子</strong> <strong class="la jk">系数</strong>【2】的一种实现，在分割任务的计算机视觉领域非常流行。简单地说，它是两个集合之间重叠的度量，并且与两个向量之间的Jaccard距离有关。dice系数对向量中列值的差异高度敏感，并且在图像分割中很流行，因为它利用这种敏感性来有效地区分图像中的像素边缘。骰子损失遵循以下等式:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1a1fdb40487fbed3c2494e6089cfeb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*5NoINzA6Rnt5E31plVBjLQ.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><em class="ny">作者图片</em></p></figure><p id="2f15" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想了解更多关于索伦森骰子系数的信息，你可以看看杜的这篇博文</p><p id="039d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="nv"> PyTorch没有内部实现的骰子系数</em> </strong>。但是可以在Kaggle的<a class="ae jg" href="https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch" rel="noopener ugc nofollow" target="_blank">损失函数库——Keras&amp;py torch</a>[3]中找到一个很好的实现:</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="7cb5" class="mi mj jj nl b gy np nq l nr ns">class <strong class="nl jk">DiceLoss</strong>(nn.Module):<br/>    def __init__(self, weight=None, size_average=True):<br/>        super(DiceLoss, self).__init__()<br/><br/>    def <strong class="nl jk">forward</strong>(self, inputs, targets, smooth=1):<br/>        <br/>        <em class="nv">#comment out if your model contains a sigmoid acitvation</em><br/>        inputs = F.<strong class="nl jk">sigmoid</strong>(inputs)       <br/>        <br/>        <em class="nv">#flatten label and prediction tensors</em><br/>        inputs = inputs.view(-1)<br/>        targets = targets.view(-1)<br/>        <br/>        intersection = (inputs * targets).sum()                            <br/>        dice = (2.*intersection + smooth)/<br/>               (inputs.sum() + targets.sum() + smooth)  <br/>        <br/>        return 1 - dice</span></pre><h2 id="5b44" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">不同OHE列的个体损失函数</h2><p id="d677" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">最后，您可以将每个热编码列视为其自己的分类问题，并计算每个分类的损失。这是一个多任务学习问题的用例，其中自动编码器正在解决重建输入向量的各个分量。当您的输入数据中有几个/全部OHE列时，这种方法最有效。例如，如果您有一个编码列，前七列是7个类别:您可以将其视为多类别分类问题，并将损失视为子问题的交叉熵损失。然后，您可以将子问题的损失组合在一起，并将其作为整个批次的损失向后传递。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/30db9cdfd29bc6b2963c962f9c312a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*kflPPsfovUcKl_469MxI_w.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="6d0e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面你会看到这个过程的一个例子，有三个One Hot编码列，每个列有50个类别。</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="1e29" class="mi mj jj nl b gy np nq l nr ns">from torch.nn.modules import _Loss<br/>from torch import argmax</span><span id="e393" class="mi mj jj nl b gy nt nq l nr ns">class <strong class="nl jk">CustomLoss</strong>(_Loss):<br/>  <strong class="nl jk">def</strong> <strong class="nl jk">__init__</strong>(self):<br/>    <strong class="nl jk">super</strong>(CustomLoss, self).__init__()</span><span id="685e" class="mi mj jj nl b gy nt nq l nr ns">  def <strong class="nl jk">forward</strong>(self, input, target):<br/>    """ loss function called at runtime """<br/>   <br/>    # Class 1 - Indices [0:50]<br/>    class_1_loss = F.<strong class="nl jk">nll_loss</strong>(<br/>        F.<strong class="nl jk">log_softmax</strong>(input[:, 0:50], <strong class="nl jk">dim</strong>=1), <br/>        <strong class="nl jk">argmax</strong>(target[:, 0:50])<br/>    )</span><span id="81c9" class="mi mj jj nl b gy nt nq l nr ns">    # Class 2 - Indices [50:100]<br/>    class_2_loss = F.<strong class="nl jk">nll_loss</strong>(<br/>        F.<strong class="nl jk">log_softmax</strong>(input[:, 50:100], <strong class="nl jk">dim</strong>=1), <br/>        <strong class="nl jk">argmax</strong>(target[:, 50:100])<br/>    )</span><span id="bf1a" class="mi mj jj nl b gy nt nq l nr ns">    # Class 3 - Indices [100:150]<br/>    class_3_loss = F.<strong class="nl jk">nll_loss</strong>(<br/>        F.<strong class="nl jk">log_softmax</strong>(input[:, 100:150], <strong class="nl jk">dim</strong>=1), <br/>        <strong class="nl jk">argmax</strong>(target[:, 100:150])<br/>    )</span><span id="653a" class="mi mj jj nl b gy nt nq l nr ns">    <strong class="nl jk">return</strong> class_1_loss <strong class="nl jk">+</strong> class_2_loss <strong class="nl jk">+</strong> class_3_loss</span></pre><p id="a33f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的代码中，您可以看到如何对重构输出的子集进行单个损失，然后在最后合并为一个总和。这里我们使用一个<strong class="la jk"> <em class="nv">负对数似然损失</em> </strong> ( <strong class="la jk"> nll_loss </strong>)，它对于多类分类方案是一个很好的损失函数，并且与<strong class="la jk"> <em class="nv">交叉熵损失相关。</em> </strong></p><h2 id="8f6b" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">谢谢大家！</h2><p id="8c0c" class="pw-post-body-paragraph ky kz jj la b lb nb kk ld le nc kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在本文中，我们浏览了One Hot Encoding分类变量的概念以及自动编码器的一般结构和目标。我们讨论了一个热编码向量的缺点，以及尝试在稀疏的一个热编码数据上训练自动编码器模型时的主要问题。最后，我们讨论了解决稀疏热编码问题的3个损失函数。尝试训练这些网络没有更好或更坏的损失，在我介绍的功能中，没有办法知道哪一个适合您的用例，直到您尝试它们！ </p><p id="2ba6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面我已经包括了一堆深入到我上面讨论的主题的资源，以及一些我提出的损失函数的资源。</p><h2 id="ef32" class="mi mj jj bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">来源</h2><ol class=""><li id="be0d" class="lu lv jj la b lb nb le nc lh oa ll ob lp oc lt lz ma mb mc bi translated">D.E. Rumelhart，G.E. Hinton和R.J. Williams，“通过错误传播学习内部表征”并行分布式处理。第一卷:基础。麻省理工学院出版社，剑桥，麻省，1986年。</li><li id="8b25" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">特伦森(1948年)。“在植物社会学中基于物种相似性建立等幅群的方法及其在丹麦公共地植被分析中的应用”。<a class="ae jg" href="https://en.wikipedia.org/wiki/Kongelige_Danske_Videnskabernes_Selskab" rel="noopener ugc nofollow" target="_blank"> <em class="nv">孔格里格</em>丹斯克</a>。<strong class="la jk">5</strong>(4):1–34。<strong class="la jk"> <em class="nv">和</em> </strong> <em class="nv"> </em>骰子，李R. (1945)。“物种间生态关联数量的度量”。<em class="nv">生态学</em>。<strong class="la jk">26</strong>(3):297–302。</li><li id="52fc" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Kaggle的损失函数库:<a class="ae jg" href="https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/bigiron sphere/Loss-Function-Library-keras-py torch</a></li></ol><p id="b6d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">提到的其他有用资源</strong></p><ol class=""><li id="4a62" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">OHE数据的问题:<a class="ae jg" rel="noopener" target="_blank" href="/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769">https://towards Data science . com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b 282 b 5769</a></li><li id="bbda" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">装袋背景:<a class="ae jg" rel="noopener" target="_blank" href="/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">https://towards data science . com/ensemble-methods-Bagging-boosting-and-stacking-c 9214 a 10 a 205</a></li><li id="78c9" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">关于骰子系数的一篇大文章:<a class="ae jg" href="https://medium.com/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b" rel="noopener">https://medium . com/ai-salon/understanding-Dice-loss-for-crisp-boundary-detection-bb 30 C2 e 5 f 62 b</a></li></ol></div></div>    
</body>
</html>