<html>
<head>
<title>15 Minutes to Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">15分钟后点火</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/15-minutes-to-spark-89cca49993f0?source=collection_archive---------30-----------------------#2020-09-21">https://towardsdatascience.com/15-minutes-to-spark-89cca49993f0?source=collection_archive---------30-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d30c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从配置到UDF，像老板一样在900秒内开始工作</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f2c0ce505a88bd39480d0200a127aca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyEEAx1t-pdzP-HEl9cXVQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Jukan Tateisi 在<a class="ae ky" href="https://unsplash.com/s/photos/stairs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fc8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">正如</span>我在<a class="ae ky" href="https://medium.com/@andrea.ialenti" rel="noopener">写的几乎所有关于这个工具的文章</a>一样，Spark和SQL一样超级好用。但是不管我花多少时间写代码，<strong class="lb iu">我就是不能在我的大脑中永久存储Spark APIs】(有人会说我的内存就像RAM，小而易失(:)。</strong></p><p id="d780" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论您是想要Spark SQL 的<strong class="lb iu">快速入门，还是不耐烦编写您的第一个查询，或者您和我一样需要一个<strong class="lb iu">备忘单</strong>，我相信您会发现这篇文章很有用。</strong></p><p id="426b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的想法是涵盖Spark SQL的所有主要功能/特性，并且在片段中，您总是可以看到原始的SQL查询及其在PySpark中的翻译。</p><p id="df9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在<a class="ae ky" href="https://drive.google.com/file/d/1kCXnIeoPT6p9kS_ANJ0mmpxlfDwK1yio/view" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">上执行我的代码。这个数据集</strong> </a> : <strong class="lb iu"> </strong>是我几个月前为另一篇中型文章创建的，名为<strong class="lb iu"> </strong> <a class="ae ky" rel="noopener" target="_blank" href="/six-spark-exercises-to-rule-them-all-242445b24565?utm_source=tr.im&amp;utm_medium=no_referer&amp;utm_campaign=tr.im%2F1Triu&amp;utm_content=direct_input"> <strong class="lb iu">六个Spark练习，用来管理所有这些练习</strong> </a> <strong class="lb iu"> — </strong>，它由三个简单的表格组成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/f33e64b52ddf506fa661a7c03b3923cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ui4283kZRcyr607W.png"/></div></div></figure><h1 id="1f0f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">基础知识</h1><p id="9b86" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">Apache Spark是一个用于大规模并行数据处理的<strong class="lb iu">引擎</strong>。这个框架的一个惊人的特点是<strong class="lb iu">它用多种语言</strong>公开API:我通常用<strong class="lb iu"> Scala </strong>与它交互，但其他很多人用<strong class="lb iu"> SQL </strong>、<strong class="lb iu"> Python </strong>，甚至<strong class="lb iu"> Java </strong>和<strong class="lb iu"> R </strong>。</p><p id="95f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们编写一个Spark程序时，首先要知道的是，当我们执行<em class="nc">代码</em>时，我们不一定对数据执行任何操作。事实上，该工具有两种类型的API调用:<em class="nc">转换</em>和<em class="nc">动作</em>。Spark <em class="nc">转换</em>背后的范例被称为<strong class="lb iu">惰性评估</strong>，这意味着<strong class="lb iu">实际的数据处理不会开始，直到我们调用<em class="nc">动作</em> </strong>。<br/>为了理解这个概念，想象一下你需要做一个<code class="fe nd ne nf ng b">SELECT</code>和一个列的重命名:<strong class="lb iu">而没有调用一个<em class="nc">动作</em> </strong>(比如一个<code class="fe nd ne nf ng b">collect</code>或者一个<code class="fe nd ne nf ng b">count</code>)，你的代码只是定义了所谓的<strong class="lb iu"> <em class="nc"> Spark执行计划</em> </strong>，也就是一旦一个<em class="nc">动作</em>被触发，将要执行的一组操作。</p><p id="1b9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark在一个<strong class="lb iu">有向无环图</strong>(非常著名的<strong class="lb iu"> DAG </strong>)中组织执行计划。该结构描述了将要执行的确切操作，并使调度器能够决定在给定时间执行哪个任务。</p><p id="eae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如宫城先生教导我们的:</p><ol class=""><li id="4624" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated"><strong class="lb iu">蜡上</strong>:定义DAG ( <em class="nc">变换</em>)</li><li id="710b" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">打蜡</strong>:执行DAG ( <em class="nc">动作</em>)</li></ol><h1 id="8c1e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">与火花互动</h1><p id="4a44" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">太好了，我们从哪里开始？使用Spark有多种方式:</p><ul class=""><li id="a90b" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nv nn no np bi translated"><strong class="lb iu">使用IDE </strong>:我会推荐IntelliJ或者PyCharm，但是我想你可以选择你想要的。<strong class="lb iu">查看附录中的PyCharm快速入门(在本地运行查询)</strong>。我认为可以从您的本地环境中使用远程Spark executor(使用<a class="ae ky" href="https://livy.apache.org/" rel="noopener ugc nofollow" target="_blank"> Livy </a>肯定可以，不确定没有它是否容易)，但说实话，我从未经历过那种配置。</li><li id="9f58" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><strong class="lb iu"> Jupyter笔记本+Spark magic</strong>:<em class="nc">Spark magic是一套通过</em><a class="ae ky" href="https://livy.incubator.apache.org/" rel="noopener ugc nofollow" target="_blank"><em class="nc">Livy</em></a><em class="nc">、Spark REST服务器</em> [ <a class="ae ky" href="https://github.com/jupyter-incubator/sparkmagic" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]与远程Spark clusters进行交互工作的工具。<strong class="lb iu">这是你在AWS、Azure或谷歌云等云系统上工作时使用Spark的主要方式。大多数云提供商都有在大约10分钟内配置集群和笔记本电脑的服务。</strong></li><li id="8b07" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><strong class="lb iu">通过使用</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">spark-shell</strong></code>的终端:有时你不想在你和你的数据之间有任何东西(例如，在一个表上做一个超级快速的检查)；在这些情况下，您可以只打开一个终端并启动<code class="fe nd ne nf ng b">spark-shell</code>。</li></ul><p id="2c97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码很大程度上是针对IDE的。</p><p id="7339" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在编写任何查询之前，我们需要导入一些库并启动一个Spark会话(使用<strong class="lb iu">数据集和DataFrame API </strong>编程Spark的入口点)。以下PySpark和Scala代码片段将加载您需要的所有内容(假设您已经配置了您的系统)。之后，为了简单起见，我们只看PySpark代码。除了一些细微差别之外，Scala APIs非常相似。</p><h2 id="dc30" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">PySpark</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="e8ac" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">斯卡拉</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="a894" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解释数据集、数据帧和rdd之间的差异在我承诺的15分钟内无法完成，所以我将跳过这一部分，假装它不存在。</p><h1 id="0009" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">基本操作</h1><p id="b7c0" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">您可以编写的最简单的查询可能是您曾经使用过的最重要的查询。让我们看看如何使用<code class="fe nd ne nf ng b">Sales</code>表进行基本操作。</p><h2 id="98ba" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">简单的Select语句和显示数据</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="8294" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在代码片段中做的第一件事是定义执行计划；<strong class="lb iu">只有当我们输入了显示<em class="nc">动作</em> </strong>时才会执行。</p><p id="6140" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以对Spark计划采取的其他行动包括:</p><ul class=""><li id="3475" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nv nn no np bi translated"><code class="fe nd ne nf ng b">collect()</code> —返回我们的整个数据集</li><li id="58b5" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">count()</code> —返回行数</li><li id="9aee" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">take(n)</code> —从数据集中返回<code class="fe nd ne nf ng b">n</code>行</li><li id="a8e0" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">show(n, truncate=False)</code> —显示<code class="fe nd ne nf ng b">n</code>行。您可以决定截断结果或显示所有长度的字段</li></ul><p id="7efe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个有趣的注意事项是<strong class="lb iu">列由</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">col</strong></code> <strong class="lb iu">对象</strong>标识。在本例中，<strong class="lb iu">我们让Spark推断这些列属于哪个数据帧。</strong></p><p id="bb91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用语法<code class="fe nd ne nf ng b">execution_plan_variable["column_name"]</code>到<strong class="lb iu">来指定列来自哪个执行计划</strong>。使用这种替代语法，我们得到:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="5afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">当我们处理连接</strong>时，限定字段的源表尤为重要(例如，两个表可能有两个同名的字段，因此仅使用<code class="fe nd ne nf ng b">col</code>对象不足以消除歧义)。Scala中的语法略有不同:</p><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="45fd" class="nw mg it ng b gy oo op l oq or">// Qualify the source execution plan in Scala<br/>sales_table.col("order_id")</span></pre><h2 id="68b2" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">重命名和添加列</h2><p id="9ea3" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">有时我们只是想重命名一个列，或者我们想添加一个新的列进行一些计算(例如一个<code class="fe nd ne nf ng b">CASE WHEN</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="2059" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">简单聚合</h2><p id="7d7a" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">Spark支持<strong class="lb iu">所有主要的聚合功能</strong>。下面的例子仅指“简单”的(如<em class="nc">平均值</em>、<em class="nc">总和</em>、<em class="nc">计数</em>等)。).稍后将描述数组的聚合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="c187" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">显示表模式</h2><p id="b8e8" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">显示“<em class="nc">表</em>”模式是一种误导性的措辞；更精确的定义应该是“<em class="nc">显示执行计划的输出模式</em>”。<strong class="lb iu">有了Spark APIs，我们可以一个接一个地用管道传输多个操作</strong>；使用<code class="fe nd ne nf ng b">printSchema</code> API，<strong class="lb iu">我们输出如果我们将执行计划的结果写到磁盘</strong>上，最终的表会是什么样子。</p><p id="1fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的示例中，我们重命名了几列，进行了聚合，并添加了另一列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="3268" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nd ne nf ng b">printSchema</code>的输出是:</p><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="60c3" class="nw mg it ng b gy oo op l oq or">root<br/> |-- product_id: string (nullable = true)<br/> |-- total_pieces: double (nullable = true)<br/> |-- fake_column: integer (nullable = false)</span></pre><p id="2bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意<code class="fe nd ne nf ng b"><strong class="lb iu">printSchema</strong></code> <strong class="lb iu">不会触发<em class="nc">动作</em></strong><em class="nc">；</em>相反，Spark会评估执行计划，以了解DAG在输出列方面的走向。<strong class="lb iu">由于这个原因，这个操作比</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">show</strong></code> <strong class="lb iu">要快得多，后者反而触发DAG </strong>的执行。</p><h2 id="9200" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">解释执行计划</h2><p id="829b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">当<em class="nc">动作</em>被触发时，引擎将做什么的更详细解释可以通过<code class="fe nd ne nf ng b">explain</code> API获得。在这种情况下，我们将不会获得最终DAG节点的简单模式作为输出，但是<strong class="lb iu">我们将获得Spark </strong>将执行的操作的详细说明。让我们调用前面查询中的<code class="fe nd ne nf ng b">explain</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="e0a9" class="nw mg it ng b gy oo op l oq or">== Physical Plan ==<br/>*(2) HashAggregate(keys=[product_id#361], functions=[sum(cast(pieces#379 as double))])<br/>+- Exchange hashpartitioning(product_id#361, 200)<br/>   +- *(1) HashAggregate(keys=[product_id#361], functions=[partial_sum(cast(pieces#379 as double))])<br/>      +- *(1) Project [product_id#361, num_pieces_sold#364 AS pieces#379]<br/>         +- *(1) FileScan parquet [product_id#361,num_pieces_sold#364] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:&lt;PATH_TO_FILE&gt;/sales_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;product_id:string,num_pieces_sold:string&gt;</span></pre><p id="1a7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">老实说，我从来没有发现<code class="fe nd ne nf ng b">explain</code> API太有用，尤其是当DAG开始变得庞大和复杂的时候。<strong class="lb iu">一个更好的视图可以在</strong><a class="ae ky" href="https://spark.apache.org/docs/3.0.0-preview/web-ui.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Spark UI</strong></a><strong class="lb iu">中找到，它展示了相同信息的图形表示</strong>。</p><h2 id="967e" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">选择不同</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="e772" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">什么情况下</h2><p id="a734" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">Spark非常好地实现了<code class="fe nd ne nf ng b">CASE WHEN</code>操作(不需要专门的UDFs让我们简单地用<code class="fe nd ne nf ng b">sales_table</code>将每一行插入到不同的桶中，这取决于<code class="fe nd ne nf ng b">num_pieces_sold</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="4dbd" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">联合所有</h2><p id="ef12" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">有时我们需要将我们的流分成多个部分，然后<strong class="lb iu">将所有内容合并到一个表中</strong>；在SQL中，这用<code class="fe nd ne nf ng b">UNION ALL</code>表示。在Spark 2.1中，在执行<code class="fe nd ne nf ng b">UNION ALL</code>操作之前，必须对列进行排序。<strong class="lb iu">幸运的是，Spark 2.3使用列名</strong>来对齐正在被合并的执行计划。在下面的例子中，我们首先将我们的表一分为二，然后将各部分合并在一起(完全没有必要，但它将显示如何使用API):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="050b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看<code class="fe nd ne nf ng b">explain</code>的幕后发生了什么:</p><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="a695" class="nw mg it ng b gy oo op l oq or">Union<br/><strong class="ng iu">:- *(1) Project [order_id#483, product_id#484, seller_id#485, date#486, num_pieces_sold#487, bill_raw_text#488]</strong><br/>:  +- *(1) Filter (isnotnull(num_pieces_sold#487) &amp;&amp; (cast(num_pieces_sold#487 as int) &gt; 50))<br/>:     +- *(1) FileScan parquet [order_id#483,product_id#484,seller_id#485,date#486,num_pieces_sold#487,bill_raw_text#488] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:&lt;FILE_PATH&gt;/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(num_pieces_sold)], ReadSchema: struct&lt;order_id:string,product_id:string,seller_id:string,date:string,num_pieces_sold:string,bill...<br/><strong class="ng iu">+- *(2) Project [order_id#483, product_id#484, seller_id#485, date#486, num_pieces_sold#487, bill_raw_text#488]</strong><br/>   +- *(2) Filter (isnotnull(num_pieces_sold#487) &amp;&amp; (cast(num_pieces_sold#487 as int) &lt;= 50))<br/>      +- *(2) FileScan parquet [order_id#483,product_id#484,seller_id#485,date#486,num_pieces_sold#487,bill_raw_text#488] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:&lt;FILE_PATH&gt;/sales_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(num_pieces_sold)], ReadSchema: struct&lt;order_id:string,product_id:string,seller_id:string,date:string,num_pieces_sold:string,bill...</span></pre><p id="e015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">粗体的两行是被合并在一起的表。</p><h1 id="cd93" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">加入火花</h1><p id="855f" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">当我们的代码出现性能问题时，连接通常是我们想查看的第一个地方。Spark引擎在并行化非连接操作方面非常出色，但在处理<code class="fe nd ne nf ng b">join</code>任务时可能需要调整。<a class="ae ky" rel="noopener" target="_blank" href="/the-art-of-joining-in-spark-dcbd33d693c"> <strong class="lb iu">关于这个话题</strong> </a> <strong class="lb iu">我写了一整篇文章，所以就不深究这个了</strong>:如果你想了解更多，或者你正遇到一些关于joins性能的问题，我绝对建议看看！</p><p id="1408" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时，这里是加入的语法。在示例中，我们将连接<code class="fe nd ne nf ng b">Sales</code>和<code class="fe nd ne nf ng b">Sellers</code>表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="f8a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了传统的加入类型(<code class="fe nd ne nf ng b">left</code>、<code class="fe nd ne nf ng b">right</code>、<code class="fe nd ne nf ng b">inner</code>、<code class="fe nd ne nf ng b">cross</code>等)。)，Spark也支持<code class="fe nd ne nf ng b">semi</code>和<code class="fe nd ne nf ng b">anti</code>加入；<strong class="lb iu">这两种基本上是一种方式来表达</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">IN</strong></code> <strong class="lb iu">和</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">NOT IN</strong></code> <strong class="lb iu">在Spark </strong>中的操作:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="9fb9" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">窗口功能</h1><p id="e931" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu">窗口函数对特定的行子集执行计算，定义为<em class="nc">帧</em>或<em class="nc">窗口</em> </strong>。经典的例子是子群的排序。在我们的玩具数据集中，假设我们想知道，对于每个卖家，卖得最多的产品是什么。为了提取这些信息，我们需要:</p><ol class=""><li id="d592" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated"><strong class="lb iu">定义我们将应用排名函数</strong>的“分区”——我们需要对每个<code class="fe nd ne nf ng b">seller_id</code>销售的产品执行一次排名操作</li><li id="b73f" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">应用我们的首选排序函数</strong> n — <code class="fe nd ne nf ng b">dense_rank</code>、<code class="fe nd ne nf ng b">rank</code>、<code class="fe nd ne nf ng b">row_number</code>。这里是Spark中的窗口函数列表。</li></ol><p id="02de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图是我们希望如何对数据进行分区的示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/7f8013e51fdbca1d2ee031b395c0e9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LPtXgiVEH469A1CnYLROA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="27cd" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">用线串</h1><p id="0f7e" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">数据科学家在处理数据时面临的另一组非常常见的操作，包括从字符串中提取信息。当然，有很多Spark APIs可以对文本数据进行几乎任何(基本)操作。让我们从简单的<code class="fe nd ne nf ng b">LIKE</code>操作符开始，然后逐步了解正则表达式的用法。对于API的完整列表，<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" rel="noopener ugc nofollow" target="_blank">我会参考文档</a>；下面，大概是用的最多的那些。</p><h2 id="be08" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">喜欢</h2><p id="f0cc" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在下面的例子中，我们想要使用<code class="fe nd ne nf ng b">sales</code>表来选择所有的字符串，其中<code class="fe nd ne nf ng b">bill_raw_text</code>是<code class="fe nd ne nf ng b">LIKE 'ab%cd%'</code>(即从字符串<code class="fe nd ne nf ng b">ab</code>开始，中间有一个字符串<code class="fe nd ne nf ng b">cd</code>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="14e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时候我们想要寻找的模式更复杂，不能用简单的通配符来表达。<strong class="lb iu">在这种情况下，我们需要使用正则表达式</strong>。让我们深入研究几个函数。在下面的例子中，我们总是要应用相同的正则表达式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/320f6f0595748097ddf5b0ee7e701eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_8tJCXosNiKtswah53x5A.png"/></div></div></figure><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="2c70" class="nw mg it ng b gy oo op l oq or">(ab[cd]{2,4})|(aa[abcde]{1,2})</span></pre><h2 id="087a" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">像正则表达式(Regex)</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="22ea" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">用正则表达式提取模式</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="30a6" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">操纵数组</h1><p id="c8cf" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在大数据领域定义表模式/数据集市时，数组是一种可能会更多使用的数据类型。Spark实现了很多操作数组的函数(准确的说是<strong class="lb iu">从2.4版</strong>开始就是这样)。让我们深入研究基础知识。</p><h2 id="972d" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">数组聚合</h2><p id="1945" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu">将列转换成数组就像调用聚合函数</strong>一样简单。Spark 2.3有两种主要类型的数组聚合函数<code class="fe nd ne nf ng b">collect_set</code>和<code class="fe nd ne nf ng b">collect_list</code>:第一种只包含唯一元素，而后者只是一个组到一个列表的转换。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="20c0" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">分解数组</h2><p id="4478" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">聚合的逆操作是“数组爆炸”，<strong class="lb iu">意味着从一个水平数组中，我们要生成一个“垂直”列</strong>。为此，我们可以使用<code class="fe nd ne nf ng b">explode</code>功能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="5635" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">其他数组操作(从Spark 2.4开始)</h2><p id="a97f" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu">不幸的是，Spark 2.3不支持太多的数组操作</strong>。很幸运，<strong class="lb iu"> Spark 2.4确实</strong>！从Spark 2.4开始提供的一些功能包括:</p><ul class=""><li id="b027" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_except(array1, array2)</code> —返回array1中而不是array2中的元素的数组，没有重复。</li><li id="2140" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_intersect(array1, array2)</code> — <em class="nc">返回</em> <code class="fe nd ne nf ng b"><em class="nc">array1</em></code> <em class="nc">和</em> <code class="fe nd ne nf ng b"><em class="nc">array2</em></code> <em class="nc">交集中元素的数组，不重复。</em></li><li id="1277" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_join(array, delimiter[, nullReplacement]) </code> — <em class="nc">使用分隔符和替换空值的可选字符串连接给定数组的元素。如果</em> <code class="fe nd ne nf ng b"><em class="nc">nullReplacement</em></code> <em class="nc">没有设置值，则过滤任何空值。</em></li><li id="9cc3" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_max(array)</code> — <em class="nc">返回数组中的最大值。</em> <code class="fe nd ne nf ng b"><em class="nc">NULL</em></code> <em class="nc">元素被跳过。</em></li><li id="bd13" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_min(array)</code> — <em class="nc">返回数组中的最小值。</em> <code class="fe nd ne nf ng b"><em class="nc">NULL</em></code> <em class="nc">元素被跳过。</em></li><li id="1b62" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated"><code class="fe nd ne nf ng b">array_sort(array) </code> — <em class="nc">按升序对输入数组进行排序。输入数组的元素必须是可排序的。</em> <code class="fe nd ne nf ng b"><em class="nc">NULL</em></code> <em class="nc">元素将被放在返回数组的末尾。</em></li></ul><p id="de45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如此类。以上定义直接取自<a class="ae ky" href="https://spark.apache.org/docs/2.4.0/api/sql/index.html#array_except" rel="noopener ugc nofollow" target="_blank">参考文献</a>。我建议你去了解更多的细节！</p><h1 id="9f19" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">UDF</h1><p id="c793" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">最后，用户定义的函数。<strong class="lb iu">当我们在默认的API</strong>中找不到<em class="nc">转换</em>时，UDF就是我们要走的路。UDF是一个自定义函数，程序员可以定义它并将其应用于列，就像我们到目前为止看到的所有API一样。它们允许最大的灵活性(我们可以在其中编写几乎任何代码)；<strong class="lb iu">缺点是Spark将它们视为黑盒，因此内部Spark引擎优化器(</strong><a class="ae ky" href="https://databricks.com/glossary/catalyst-optimizer" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Catalyst</strong></a><strong class="lb iu">)无法进行任何优化:</strong>UDF可能会降低我们代码的速度。</p><p id="1fa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，让我们实现一个模拟函数<code class="fe nd ne nf ng b">array_repeat(element, count)</code>的UDF，该函数返回一个包含<code class="fe nd ne nf ng b">element</code> <code class="fe nd ne nf ng b">count</code>次的数组。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="f873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了UDF的语法，我建议关注上面使用的<code class="fe nd ne nf ng b">lit</code>函数。<strong class="lb iu">一些Spark函数只接受列作为输入:如果我们需要使用一个常量，我们可能需要将这个常量转换成一个“</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">Column</strong></code> <strong class="lb iu">”。</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">lit</strong></code> <strong class="lb iu">创建一个</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">Column</strong></code> <strong class="lb iu">的文字值</strong>。</p><h1 id="980c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">接下来呢？</h1><p id="0183" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">太棒了。我希望我能够展示出Spark并不比SQL更难，它基本上是一样的东西，只是用了类固醇。</p><p id="663b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以想象，这篇文章的标题有点言过其实:<strong class="lb iu">要精通这个工具</strong>需要的时间远远超过15分钟；我相信以上是一个很好的快速入门，但Spark可以提供更多！</p><p id="a5c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里去哪里？<strong class="lb iu">首先，我的建议是开始使用上面的API，因为它们将覆盖70%的用例</strong>。当你对基础知识有信心的时候，<strong class="lb iu">我会推荐下面两篇文章，是一位值得信赖的作者(<em class="nc"> lol </em>)几个月前写的</strong>。第一个将向您挑战使用该工具开发时遇到的一些经典问题，而第二个将深入探讨Spark Joins。</p><p id="e9d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更多关于你的文章可以</strong> <a class="ae ky" href="https://medium.com/@andrea.ialenti" rel="noopener"> <strong class="lb iu">关注我中的</strong> </a>！</p><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/the-art-of-joining-in-spark-dcbd33d693c"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">火花中加入的艺术</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">Spark中加速连接的实用技巧</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ks ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/six-spark-exercises-to-rule-them-all-242445b24565"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">六个星火练习来统治他们</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">一些具有挑战性的Spark SQL问题，易于在许多现实世界的问题上提升和转移(带解决方案)</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pm l pi pj pk pg pl ks ox"/></div></div></a></div><h1 id="2c8e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">附录—配置PyCharm</h1><p id="0e94" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在本地(非分布式)环境中安装Spark是一个<strong class="lb iu">非常简单的任务</strong>。在本附录中，我将向您展示PyCharm Community Edition的<strong class="lb iu">基本配置，以便使用Python </strong>运行Spark。有五个简单的步骤:</p><ol class=""><li id="4cde" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated"><strong class="lb iu">下载PyCharm社区版</strong></li><li id="d2c0" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">下载星火</strong></li><li id="73b7" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">安装PySpark </strong></li><li id="ca47" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><strong class="lb iu">配置PyCharm执行正确的火花执行器</strong></li><li id="306c" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">测试是否一切正常</li></ol><p id="c962" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">细节之前的两个注意事项:</p><ul class=""><li id="ad07" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nv nn no np bi translated">我假设你已经在你的系统中正确安装了Java。</li><li id="610e" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nv nn no np bi translated">在<strong class="lb iu"> Windows </strong>上，你需要安装<strong class="lb iu"> Winutils </strong>，这是运行Hadoop所需的一组二进制文件。<a class="ae ky" href="https://github.com/steveloughran/winutils" rel="noopener ugc nofollow" target="_blank">查看Git repo了解更多信息</a>。</li></ul><h2 id="c0a0" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">下载PyCharm社区版</h2><p id="3680" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">幸运的是，JetBrains有PyCharm的开源版本。我们可以从他们的网站下载最新版本。安装很简单。</p><h2 id="b389" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">下载Spark</h2><p id="1233" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们只需要从Spark官方网站下载一个zip文件<a class="ae ky" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">。我在写的时候有两个主要版本可用:<code class="fe nd ne nf ng b">3.0.1</code>和<code class="fe nd ne nf ng b">2.4.7</code>。对于文章的范围，我们可以选择其中之一。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/b190c52384cde122eab814c94dabf10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_mxRgazb3PcnYDtOgOUxzQ.png"/></div></div></figure><p id="2e41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦下载完成，<strong class="lb iu">我们只需要在合适的位置</strong>解压软件包。</p><h2 id="7619" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">安装PySpark</h2><p id="c539" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">是时候运行<strong class="lb iu"> PyCharm并安装所有需要的包了</strong>。首先，让我们<strong class="lb iu">打开PyCharm，创建一个新项目和一个新的虚拟环境</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/0d4b86a525269a9f444f08f5b958b11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rxUio8RG1fKDllOlaK5uw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/9a23fc895a730dcb671fe45eb1683bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_z-1XN0_4WDd5qcBKTJmaQ.png"/></div></div></figure><p id="345a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，直接从PyCharm，我们可以简单地安装PySpark:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/a8f2bf0c861f5e564ffdfb3b3b2b7c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssTNqEEJNGkxNX0h6GdPiw.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/3a9801b6871dfe6bb3d1426c8f310aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkvgmRn5cIgu9orD92AMUg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/5c4024285de15e7656c65cffb60cc94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxMC33Wjv1Y52Iv-5CdHhg.png"/></div></div></figure><p id="004a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，为了<strong class="lb iu">启用提示</strong>，我们还应该<strong class="lb iu">安装</strong> <code class="fe nd ne nf ng b"><strong class="lb iu">pyspark-stubs</strong></code> <strong class="lb iu">包</strong>。</p><h2 id="df07" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">配置PyCharm来执行正确的Spark执行器</h2><p id="85d8" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">希望我们没有出错，所以<strong class="lb iu">我们只需要指示PyCharm运行正确的Spark执行器</strong>。这位于我们解压Spark本身的文件夹中。让我们为PyCharm项目创建一个<code class="fe nd ne nf ng b">Run Configuration</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/e0a5a1921ce0fc87d1fb7aff6745b542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRW7LS6GJoISWq2IJj42ag.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/b6f2a2131a69399af8ec3d3c26fa7021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGfCnh50GzaKPCe-GXJ8tA.png"/></div></div></figure><h2 id="754c" class="nw mg it bd mh nx ny dn ml nz oa dp mp li ob oc mr lm od oe mt lq of og mv oh bi translated">测试是否一切正常</h2><p id="1789" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">要测试Spark是否工作，只需运行下面的代码片段</p><pre class="kj kk kl km gt ok ng ol om aw on bi"><span id="f144" class="nw mg it ng b gy oo op l oq or"># Import Libraries<br/>import pyspark<br/>from pyspark.sql import SparkSession<br/>from pyspark.sql.functions import *<br/><br/>#   Initialize the Spark session<br/>spark = SparkSession.builder \<br/>    .master("local") \<br/>    .appName("SparkLikeABoss") \<br/>    .getOrCreate()<br/><br/>print(spark.version)</span></pre></div></div>    
</body>
</html>