<html>
<head>
<title>Calculating Document Similarities using BERT, word2vec, and other models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT、word2vec和其他模型计算文档相似度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630?source=collection_archive---------1-----------------------#2020-09-26">https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630?source=collection_archive---------1-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="06ad" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/c8b4cc747d8712f1cf9e1b2abaaca621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4yppLHBTu9uBBMOr"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@viktortalashuk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">维克多·塔拉舒克</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e9ab" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">简介</strong></p><p id="4fe9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">文档相似性是自然语言处理中最关键的问题之一。寻找文档间的相似性被用于几个领域，例如推荐相似的书籍和文章、识别剽窃的文档、法律文档等。</p><p id="1f17" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果两个文档在语义上相似，并且定义了相同的概念，或者它们是重复的，我们可以称之为相似的。</p><p id="95a2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了让机器找出文档之间的相似性，我们需要定义一种方法来数学地测量相似性，并且它应该是可比较的，以便机器可以告诉我们哪些文档最相似，哪些最不相似。我们还需要以可量化的形式(或数学对象，通常是向量形式)表示文档中的文本，以便我们可以在此基础上执行相似性计算。</p><p id="ad71" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，将文档转换成数学对象和定义相似性度量主要是让机器执行这项工作所需的两个步骤。我们将研究做这件事的不同方法。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="7239" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">相似度函数</strong></p><p id="3601" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">计算相似性的一些最常见和最有效的方法是，</p><p id="3ec1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lu">余弦距离/相似度</em>——是两个向量之间角度的余弦，它给了我们向量之间的角距离。计算两个向量A和B之间余弦相似性公式为:</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/2f66d85811f5effb4cf8d3fdedc2f0d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*YInqm5R0ZgokYXjNjE3MlQ.png"/></div></figure><p id="21c9" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在二维空间中，它看起来像这样，</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/cec5f2ffce1d882b52d5ac9c49ae1d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*mRjgETrg-mPt8jMBu1VtDg.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">二维空间中两个向量A和B之间的角度(图片由作者提供)</p></figure><p id="ae2d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可以很容易地计算出数学公式，并使用余弦定律来证明这个公式。</p><p id="2833" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">余弦在θ= 0°时为1，在θ= 180°时为-1，这意味着对于两个重叠的矢量，余弦在两个完全相反的矢量中最高和最低。正因如此，所以称之为相似。你可以把1余弦当作距离。</p><p id="e1a6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lu">欧氏距离</em>——这是p=2时闵可夫斯基距离的形式之一。其定义如下:</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/6b34e99808a6acc7871e72af8058ee76.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*55jbZL3qTdeEI5gL.png"/></div></figure><p id="d908" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在二维空间中，欧几里德距离会是这样的，</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/042ce0ae8afda6e496ac31769b89f289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*aUFcVBD_dBAAayDFfAmo_A.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">二维空间中两个向量A和B之间的欧氏距离(图片由作者提供)</p></figure><p id="520a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lu"> Jaccard距离- </em> Jaccard指数用于计算两个有限集之间的相似度。Jaccard距离可以认为是1 - Jaccard指数。</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mb"><img src="../Images/9e6dd334e56f2ae3a0a7f3d56cbc118d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZyyreB3IE90pHLWO.png"/></div></div></figure><p id="0d8d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们可以在向量空间中表示文档，我们可以使用余弦或欧几里德距离。如果我们认为我们的文档只是没有任何语义意义的单词集或集合，则可以使用Jaccard距离。</p><p id="4d5b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">余弦和欧几里德距离是最广泛使用的度量，我们将在下面的例子中使用这两个。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="a190" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd">嵌入</strong></p><p id="c996" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">嵌入是文本的向量表示，其中具有相似含义或上下文的单词或句子具有相似的表示。</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mc"><img src="../Images/f8d11bd1b43979c122995ba0e7e164fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-9s1B8LTONza48LTeVxUQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">单词的三维矢量表示(图片由作者提供)</p></figure><p id="fa06" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下面是一些计算文档嵌入的算法和例子，</p><p id="569f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> <em class="lu"> Tf-idf - </em> </strong> Tf-idf是词频和逆文档频的组合。它为文档中的每个单词分配一个权重，该权重是使用该单词在文档中的频率以及该单词在整个文档语料库中的文档频率来计算的。关于tf-idf的更多细节请参考这个<a class="ae ko" href="https://medium.com/analytics-vidhya/the-quantitative-value-of-text-tf-idf-and-more-e3c7883f1df3" rel="noopener">故事</a>。</p><p id="65dc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们将以下定义为我们想要计算相似性的文档的语料库(集合),</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi md"><img src="../Images/4115cedd7552f193442f7503fb50ed17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmhAh50gcb3ZAQJjuE5IzA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">文档语料库(图片由作者提供)</p></figure><p id="9d85" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将执行基本的文本清理，删除特殊字符，删除停用词，并将所有内容转换为小写。然后，我们将把文档转换成它们的tf-idf向量，并使用余弦和欧几里德距离计算成对的相似度。</p><p id="f6fe" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">成对余弦相似度将只是tf-idf向量的点积，因为来自sklearn的tf-idf向量已经被归一化，并且这些向量的L2范数是1。所以在这种情况下，余弦相似性公式的分母是1。</p><figure class="lw lx ly lz gt kd"><div class="bz fp l di"><div class="me mf l"/></div></figure><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="ddb7" class="ml mm it mh b gy mn mo l mp mq">print (tfidf_vectors[0].toarray())</span><span id="2799" class="ml mm it mh b gy mr mo l mp mq">print (pairwise_similarities.shape)</span><span id="aebc" class="ml mm it mh b gy mr mo l mp mq">print (pairwise_similarities[0][:])</span></pre><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ms"><img src="../Images/8bee4220cffae8c4f8d45eb785bf89a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_f4aA0b_uRv83Nakk2RuA.png"/></div></div></figure><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="e9f2" class="ml mm it mh b gy mn mo l mp mq"># documents similar to the first document in the corpus<br/>most_similar(0,pairwise_similarities,'Cosine Similarity')</span></pre><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mt"><img src="../Images/090523fd25caaef7185984593cccb76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_hv4N8ixoY4HeRHl0A5WQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">基于余弦相似性和欧几里德距离与第一个文档相似的文档(图片由作者提供)</p></figure></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="621d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> <em class="lu"> Word2vec - </em> </strong>顾名思义Word2vec将单词嵌入向量空间。Word2vec将文本语料库作为输入，并将单词嵌入作为输出。word2vec主要有两种学习算法:连续单词包和连续跳格。</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mu"><img src="../Images/f6684d8d668fe77b9986f05614ea770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sW3mNscCRw_K20pR.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">连续单词袋(CBOW)和跳格模型(图片由<a class="ae ko" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a>提供)</p></figure><p id="a86f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果有足够的数据和计算可用，我们可以训练我们自己的嵌入，或者我们可以使用预训练的嵌入。我们将使用由<a class="ae ko" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> Google </a>提供的预训练嵌入。</p><p id="126f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将从标记和填充每个文档开始，使它们都具有相同的大小。</p><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="0611" class="ml mm it mh b gy mn mo l mp mq"># tokenize and pad every document to make them of the same size<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences</span><span id="e171" class="ml mm it mh b gy mr mo l mp mq">tokenizer=Tokenizer()<br/>tokenizer.fit_on_texts(documents_df.documents_cleaned)<br/>tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)<br/>tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')<br/>vocab_size=len(tokenizer.word_index)+1</span><span id="fd28" class="ml mm it mh b gy mr mo l mp mq">print (tokenized_paded_documents[0])</span></pre><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mv"><img src="../Images/76d35135b148ab7842d0e0c7038f786a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdH3O4iwWe6xrtt4zCx92w.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">标记化文档(作者图片)</p></figure><p id="b9ef" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们加载预训练的嵌入。每个单词都被表示为一个300维的向量。</p><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="27a2" class="ml mm it mh b gy mn mo l mp mq"># loading pre-trained embeddings, each word is represented as a 300 dimensional vector</span><span id="1e2c" class="ml mm it mh b gy mr mo l mp mq">import gensim</span><span id="40e5" class="ml mm it mh b gy mr mo l mp mq">W2V_PATH="GoogleNews-vectors-negative300.bin.gz"<br/>model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)</span></pre><p id="e79b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用这种嵌入，我们可以将文档语料库中的每个单词转换成300维向量。因为我们有6个文档，并且我们已经将每个文档填充为最大大小64，所以语料库的向量表示将是6X64X300的形状。</p><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="4bf2" class="ml mm it mh b gy mn mo l mp mq"># creating embedding matrix, every row is a vector representation from the vocabulary indexed by the tokenizer index. <br/>embedding_matrix=np.zeros((vocab_size,300))<br/>for word,i in tokenizer.word_index.items():<br/>    if word in model_w2v:<br/>        embedding_matrix[i]=model_w2v[word]</span><span id="d699" class="ml mm it mh b gy mr mo l mp mq"># creating document-word embeddings<br/>document_word_embeddings=np.zeros((len(tokenized_paded_documents),64,300))</span><span id="0ca6" class="ml mm it mh b gy mr mo l mp mq">for i in range(len(tokenized_paded_documents)):<br/>    for j in range(len(tokenized_paded_documents[0])):<br/>        document_word_embeddings[i][j]=embedding_matrix[tokenized_paded_documents[i][j]]</span><span id="7efb" class="ml mm it mh b gy mr mo l mp mq">document_word_embeddings.shape</span></pre><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mc"><img src="../Images/7ddd11b64927e18c9711b12becfb90ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZdDU0jX6U2Gp4hGSVXI-w.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">文档-单词嵌入形状(图片由作者提供)</p></figure><p id="62a5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们必须将每个文档表示为一个向量。我们可以对每个单词向量进行平均或求和，并将每个64X300表示转换为300维表示。但是对所有单词进行平均或求和将会失去文档的语义和上下文含义。文件的长度不同也会对此类行动产生不利影响。</p><p id="732b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一种更好的方法是使用tf-idf权重对单词向量进行加权平均。这可以在一定程度上处理可变长度问题，但是不能保持单词的语义和上下文含义。之后，我们可以使用成对距离来计算类似的文档，就像我们在tf-idf模型中所做的那样。</p><pre class="lw lx ly lz gt mg mh mi mj aw mk bi"><span id="91fb" class="ml mm it mh b gy mn mo l mp mq"># calculating average of word vectors of a document weighted by tf-idf</span><span id="af70" class="ml mm it mh b gy mr mo l mp mq">document_embeddings=np.zeros((len(tokenized_paded_documents),300))<br/>words=tfidfvectoriser.get_feature_names()</span><span id="31a4" class="ml mm it mh b gy mr mo l mp mq">for i in range(len(document_word_embeddings)):<br/>    for j in range(len(words)):<br/>        document_embeddings[i]+=embedding_matrix[tokenizer.word_index[words[j]]]*tfidf_vectors[i][j]</span><span id="9140" class="ml mm it mh b gy mr mo l mp mq">print (document_embeddings.shape)</span><span id="f2dd" class="ml mm it mh b gy mr mo l mp mq">pairwise_similarities=cosine_similarity(document_embeddings)<br/>pairwise_differences=euclidean_distances(document_embeddings)</span><span id="5ce9" class="ml mm it mh b gy mr mo l mp mq">most_similar(0,pairwise_similarities,'Cosine Similarity')<br/>most_similar(0,pairwise_differences,'Euclidean Distance')</span></pre><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mw"><img src="../Images/269e86b8fddef5f4b07ecc120ea77edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ci8IjuBC_FktVgNG0BLbg.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">基于余弦相似性和欧几里德距离与第一个文档相似的文档(图片由作者提供)</p></figure></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="0adb" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"><em class="lu">GloVe-</em></strong>Global Vectors for word Embedding(GloVe)是一种无监督学习算法，用于产生单词的向量表示。在来自语料库的聚集的全局单词-单词共现统计上执行训练，并且所得的表示展示了单词向量空间的有趣的线性子结构。</p><p id="475a" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将使用来自<a class="ae ko" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">斯坦福</a>的预训练手套嵌入。所有步骤都与word2vec嵌入相同，只是在这种情况下，我们将使用手套预训练模型。我们使用100维的手套嵌入，因为嵌入文件很大。你也可以使用更高的维度。</p><figure class="lw lx ly lz gt kd"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mx"><img src="../Images/fc84f5351569d39c83aa1c4894b4def8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKSETNXYo49r5GwyI_gpDQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">基于余弦相似性和欧几里德距离与第一个文档相似的文档(图片由作者提供)</p></figure></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="d6f1" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"><em class="lu">doc 2 vec-</em></strong><a class="ae ko" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">doc 2 vec</a>是一种无监督学习算法，产生句子/段落/文档的向量表示。这是word2vec的改编版。Doc2vec可以把一个完整的文档表示成一个向量。因此，我们不必取单词向量的平均值来创建文档向量。</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi my"><img src="../Images/ed13880051578a454de6d8cd7c3d346d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUL_B1Oyq5q5syH5O7vrrg.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">段落向量分布式单词包版本(PVDOBW)和段落向量分布式内存版本(PVDM)(图片来自<a class="ae ko" href="https://arxiv.org/pdf/1405.4053.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1405.4053.pdf</a>)</p></figure><p id="1954" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将使用gensim在我们的语料库上训练Doc2vec模型，并创建文档的向量表示。</p><figure class="lw lx ly lz gt kd"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi mz"><img src="../Images/d961bbaee30793463ccbdda6810ebd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWBVgrY7O8-cqoZpyWCfWA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">基于余弦相似性和欧几里德距离与第一个文档相似的文档(图片由作者提供)</p></figure></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="aa45" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kr jd"> <em class="lu"> BERT- </em> </strong> <a class="ae ko" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">来自变形金刚的双向编码器表示(BERT) </a>是Google开发的自然语言处理预训练的最新技术。BERT在包括维基百科和书籍语料库在内的未标记文本上接受训练。BERT使用transformer architecture，一种注意力模型来学习单词的嵌入。</p><p id="96f6" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">BERT包括两个预训练步骤:掩蔽语言建模(MLM)和下一句预测(NSP)。在BERT中，使用三种嵌入来表示训练文本，即标记嵌入+片段嵌入+位置嵌入。</p><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi na"><img src="../Images/23a9162f26594398d7e10117204e51e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RzQeTqlsABXJsLZLV0UjA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">伯特训练建筑(图片来自<a class="ae ko" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a>)</p></figure><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nb"><img src="../Images/1797ed05b3c814e1deced0aac1af9474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9HagJnUt6mAujVfY58nvA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">伯特输入表示(图片来自<a class="ae ko" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a>)</p></figure><p id="7436" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将使用来自<a class="ae ko" href="https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>的预训练BERT模型来嵌入我们的语料库。我们正在加载BERT基本模型，它有12层(变压器块)，12个注意头，1.1亿个参数，隐藏大小为768。</p><figure class="lw lx ly lz gt kd"><div class="bz fp l di"><div class="me mf l"/></div></figure><figure class="lw lx ly lz gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nc"><img src="../Images/5a1f17e12b6f0bd14b0a5b0b2cae9fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsGE4vz_rVJBkK1WEfGoxA.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">基于余弦相似性和欧几里德距离与第一个文档相似的文档(图片由作者提供)</p></figure></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="eefa" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您已经看到了用向量形式表示文档和度量相似性的多种方法。您可以针对自己的问题定制它们，看看哪种最适合您。</p><p id="3bd4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里有这个故事的完整代码-<a class="ae ko" href="https://github.com/varun21290/medium/blob/master/Document%20Similarities/Document_Similarities.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/varun 21290/medium/blob/master/Document % 20 similarities/Document _ similarities . ipynb</a></p><p id="266b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="lu">参考文献:</em></p><div class="nd ne gp gr nf ng"><a href="https://en.wikipedia.org/wiki/Law_of_cosines" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">余弦定律</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">在三角学中，余弦定律(也称为余弦公式、余弦法则或阿尔-卡希定理)与…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">en.wikipedia.org</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ki ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://code.google.com/archive/p/word2vec/" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">密码</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">编辑描述</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">code.google.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu ki ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://arxiv.org/abs/1301.3781" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">向量空间中单词表示的有效估计</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">我们提出了两种新的模型架构，用于从非常大的数据中计算单词的连续向量表示…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://nlp.stanford.edu/projects/glove/" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">GloVe:单词表示的全局向量</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">GloVe是一种无监督学习算法，用于获取单词的矢量表示。培训在…进行</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">nlp.stanford.edu</p></div></div><div class="np l"><div class="nw l nr ns nt np nu ki ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://arxiv.org/abs/1405.4053" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">句子和文档的分布式表示</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">许多机器学习算法要求将输入表示为固定长度的特征向量。当谈到…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://arxiv.org/abs/1810.04805" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">BERT:用于语言理解的深度双向转换器的预训练</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">我们介绍了一种新的语言表示模型，称为BERT，代表双向编码器表示…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">句子变形/bert-base-nli-mean-tokens拥抱脸</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">这是句子变形库的bert-base-nli-mean-tokens模型。句子-变形金刚…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">huggingface.co</p></div></div><div class="np l"><div class="nx l nr ns nt np nu ki ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://medium.com/analytics-vidhya/the-quantitative-value-of-text-tf-idf-and-more-e3c7883f1df3" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">文本、tf-idf等的量化价值…</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">定义、计算和变化</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="ny l nr ns nt np nu ki ng"/></div></div></a></div></div></div>    
</body>
</html>