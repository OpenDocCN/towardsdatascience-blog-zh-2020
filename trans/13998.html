<html>
<head>
<title>A Beginners Guide To Stemming In Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理词干初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-stemming-in-natural-language-processing-34ddee4acd37?source=collection_archive---------26-----------------------#2020-09-26">https://towardsdatascience.com/a-beginners-guide-to-stemming-in-natural-language-processing-34ddee4acd37?source=collection_archive---------26-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e81" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过减少单词的词干来降低维数并改善结果</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5ad3ed88dd4a4c66d26969af43affebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XV42Suwb3Xpp4KgH6Tx-NQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kz"> Pixabay </strong> </a>发自<a class="ae ky" href="https://www.pexels.com/photo/close-up-of-eyeglasses-256273/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kz"> Pexels </strong> </a></p></figure><p id="f259" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我的工作几乎完全专注于NLP。所以我用文字工作。很多。</p><p id="7a96" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">文本ML有它自己的挑战和工具。但一个反复出现的问题是维度过多。</p><p id="924a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在现实生活中(不是Kaggle)，我们通常只有有限数量的带注释的例子来训练模型。并且每个例子通常包含大量的文本。</p><p id="dcbb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这种稀疏性和缺乏共同特征使得很难在数据中发现模式。我们称之为<a class="ae ky" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维度诅咒</a>。</p><p id="9613" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一种降维技术是<strong class="lc iu">词干</strong>。</p><p id="215a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它从一个单词中去掉后缀，得到它的“词干”。</p><p id="8b33" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">于是<code class="fe lw lx ly lz b">fisher</code>变成了<code class="fe lw lx ly lz b">fish</code>。</p><p id="924f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们来学习词干是如何工作的，为什么要使用它，以及如何使用它。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="6f50" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">什么是词干？</h1><h2 id="62c2" class="mz mi it bd mj na nb dn mn nc nd dp mr lj ne nf mt ln ng nh mv lr ni nj mx nk bi translated">后缀</h2><p id="9de7" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">词干提取通过去除后缀来减少单词的词干(词根)。</p><p id="86cb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">后缀</strong>是修饰词义的词尾。这些包括但不限于...</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="96c0" class="mz mi it lz b gy nu nv l nw nx">-able<br/>-ation<br/>-ed<br/>-er<br/>-est<br/>-iest<br/>-ese<br/>-ily<br/>-ful<br/>-ing<br/>...</span></pre><p id="c7ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">显然，<code class="fe lw lx ly lz b">runner</code>和<code class="fe lw lx ly lz b">running</code>这两个词有不同的意思。</p><p id="6796" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">但当涉及到文本分类任务时，这两个词都暗示着一个句子在谈论类似的东西。</p><h2 id="ee65" class="mz mi it bd mj na nb dn mn nc nd dp mr lj ne nf mt ln ng nh mv lr ni nj mx nk bi translated">该算法</h2><p id="5347" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">词干算法是如何工作的？</p><p id="8fbd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">简而言之，算法包含一系列删除后缀的规则，这些规则有条件地应用于输入令牌。</p><p id="9100" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里没有任何统计数据，所以算法是非常基本的。这使得词干分析可以在大量文本中快速运行。</p><p id="b949" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果你感兴趣，NLTK的波特词干算法是免费的，这里是的<a class="ae ky" href="https://www.nltk.org/_modules/nltk/stem/porter.html" rel="noopener ugc nofollow" target="_blank">，解释它的论文是</a><a class="ae ky" href="https://tartarus.org/martin/PorterStemmer/def.txt" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><p id="7889" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">也就是说，有不同的词干算法。仅NLTK就包含波特、斯诺鲍和兰开斯特。一个比一个更积极地阻止。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="57f4" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">阻止文本的原因</h1><h2 id="0e15" class="mz mi it bd mj na nb dn mn nc nd dp mr lj ne nf mt ln ng nh mv lr ni nj mx nk bi translated">语境</h2><p id="281b" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">NLP的很大一部分是弄清楚一个文本主体在说什么。</p><p id="f639" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然不总是正确的，但包含单词<code class="fe lw lx ly lz b">planting</code>的句子经常谈论与包含单词<code class="fe lw lx ly lz b">plant</code>的另一个句子相似的事情。</p><p id="c4ca" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">考虑到这一点，为什么不在训练一个分类模型之前，先把所有的单词归到它们的词干中呢？</p><p id="5493" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">还有另一个好处。</p><h2 id="5b02" class="mz mi it bd mj na nb dn mn nc nd dp mr lj ne nf mt ln ng nh mv lr ni nj mx nk bi translated">维度</h2><p id="350c" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">大量的文本包含大量不同的单词。结合有限数量的训练示例，稀疏性使得模型很难找到模式并进行准确的分类。</p><p id="6ece" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">减少单词的词干减少了稀疏性，使发现模式和做出预测变得更容易。</p><p id="3d7e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">词干分析允许每个文本字符串用一个更小的单词包来表示。</p><p id="942d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">举例:</strong>词干化后的句子，<code class="fe lw lx ly lz b">"the fishermen fished for fish"</code>，可以用这样一包词来表示。</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="4e3e" class="mz mi it lz b gy nu nv l nw nx">[the, fisherman, fish, for]</span></pre><p id="0964" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">而不是。</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="deef" class="mz mi it lz b gy nu nv l nw nx">[the, fisherman, fished, for, fish]</span></pre><p id="8fb5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这既降低了样本之间的稀疏性，又提高了训练算法的速度。</p><p id="41bb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">根据我的经验</strong>，与使用单词嵌入相比，通过词干处理丢失的信号非常少。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="2cdd" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">Python示例</h1><p id="4c7a" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">我们将从一串单词开始，每个单词的词干都是“鱼”。</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="4143" class="mz mi it lz b gy nu nv l nw nx">'fish fishing fishes fisher fished fishy'</span></pre><p id="3cc5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后对该字符串中的标记进行词干处理。</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="5866" class="mz mi it lz b gy nu nv l nw nx">from nltk.tokenize import word_tokenize<br/>from nltk.stem.porter import PorterStemmer</span><span id="e4de" class="mz mi it lz b gy ny nv l nw nx"><br/>text = 'fish fishing fishes fisher fished fishy'</span><span id="0844" class="mz mi it lz b gy ny nv l nw nx"><br/># Tokenize the string<br/>tokens = word_tokenize(text)<br/>print(tokens) <br/>#=&gt; ['fish', 'fishing', 'fishes', 'fisher', 'fished', 'fishy']</span><span id="3c86" class="mz mi it lz b gy ny nv l nw nx"><br/>stemmer = PorterStemmer()<br/>stems = [stemmer.stem(w) for w in tokens]<br/>print(stems)<br/>#=&gt; ['fish', 'fish', 'fish', 'fisher', 'fish', 'fishi']</span></pre><p id="a3c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">结果是。</p><pre class="kj kk kl km gt nq lz nr ns aw nt bi"><span id="ffee" class="mz mi it lz b gy nu nv l nw nx">['fish', 'fish', 'fish', 'fisher', 'fish', 'fishi']</span></pre><p id="2454" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">请注意，它并没有将所有记号都简化为“fish”这个词干。波特算法是最保守的词干提取算法之一。也就是说，这仍然是一个进步。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="7529" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">结论</h1><p id="4bf3" class="pw-post-body-paragraph la lb it lc b ld nl ju lf lg nm jx li lj nn ll lm ln no lp lq lr np lt lu lv im bi translated">有趣的是，预处理是NLP管道中最重要的(也是被忽略的)部分。</p><p id="d0fe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它决定了最终馈入ML模型的数据的形状，以及馈入模型质量数据和垃圾之间的区别。</p><p id="cca5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如他们所说，垃圾进，垃圾出。</p><p id="582f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在删除标点符号和停用词之后，词干是大多数NLP管道的关键组成部分。</p><p id="f256" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">词干化(和它的表亲<a class="ae ky" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank">词汇化</a>)会给你更好的结果，使用更少的数据，并减少模型训练时间。</p></div></div>    
</body>
</html>