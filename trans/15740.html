<html>
<head>
<title>Breaking Down the Support Vector Machine (SVM) Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分解支持向量机(SVM)算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/breaking-down-the-support-vector-machine-svm-algorithm-d2c030d58d42?source=collection_archive---------33-----------------------#2020-10-29">https://towardsdatascience.com/breaking-down-the-support-vector-machine-svm-algorithm-d2c030d58d42?source=collection_archive---------33-----------------------#2020-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a66f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，我昨天在修改机器学习算法；我观察到SVM可能是一个广泛使用但复杂的机器学习算法。所以，在理解了其中的错综复杂并从头开始编码之后；我想我必须写一个帖子来帮助像我一样发现它比它的同行稍微复杂一点的人。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/fbfce234ffcd147c21c66db18b819b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jsdi4EdZDDozpp6ZpIzOIg.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="27a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，SVM基本上使用向量空间来执行指定的任务。当谈到识别手写数字和相关任务时，你会惊讶地发现SVM实际上比一些神经网络更好。让我们开始吧！</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="03d8" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">基本直觉</h1><p id="5d1f" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">在我们开始之前；你应该知道SVM一次只能划分两个组。但是嘿；这并不意味着它仅限于二元分类器(更强调单词‘一次’)。所以在直觉层面上。支持向量机在超平面的帮助下分离数据集中的两类数据。从超平面到数据点的垂直平分线表示最大可能距离。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/4133051de6881786dd14cdbd4aea22de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*t9u_9VlMAh5ctCm8m3fQXA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">SVM超平面(决定边界)和垂线。作者图片</p></figure><p id="1d5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之；即使可以有无限个超平面来分隔数据；最佳选择是可分离距离最大的一个(用数据点的垂线测量)。</p><p id="fe0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，如果一个数据点位于超平面的任一侧；假设超平面是一堵分隔墙(只是打个比方)，我们就可以很容易地说出它的类别。但是，这也可以在回归的帮助下完成，对吗？实际上是的，但是SVM可以处理更复杂的分色，因为它使用向量空间。不仅如此，SVM还在确定决策界限方面做了出色的工作。(这是最佳位置—垂直线的最大长度)。</p><p id="7a87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经清楚了我们的基本原则；让我们开始讨论它的技术细节。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="e9ba" class="lj lk iq bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">让我们多刺激一下我们的大脑</h1><p id="b952" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">简单来说，我们可以说向量是一个既有大小又有方向的量(基础物理/数学)。在SVM；幅度是数值(可以通过取x和y坐标的模或和的平方根来计算；类似毕达哥拉斯定理中的斜边)。</p><p id="1c6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除此之外；另一个要知道的是两个向量的点积，简单来说就是两个向量中的值相乘。可以通过下图来理解。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/ab0b6e84d3e0291d0e160d10020db4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6ojdKNh47PpVGMgHZPtYA.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="08b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，如果数据集中有一个数据点“u”(可以用u向量表示)，它被一个超平面(用红线表示)分隔开。现在，我们取另一个向量(w ),它是从超平面垂直画到原点的。现在，u和w的点积加上偏差决定了我们的支持向量断言的输出。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mo"><img src="../Images/cd024be18f05f963ca18701246a07c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y0C5PGIgbjj9IdaAYAB4Vw.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">SV断言。作者图片</p></figure><p id="e1de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果我们方程的结果大于或等于零；然后我们可以对另一边的数据点进行分类(+ve)。在上面的例子中；我们可以直观地看到事实并非如此(我们的数据点显然不在+ve一侧)。但是即使在复杂的情况下，这个等式也像魔法一样有效。</p><p id="3bd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有，你要注意，如果<strong class="jp ir"><em class="lb">u . w+b = 0</em></strong>；然后，该点出现在决策边界上(一种罕见的情况)。</p><p id="e193" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb">“但是萨蒂扬，这并不复杂。那就好理解了。”嗯，如果是这样的话，我会很高兴的。但是SVM在获取偏差和向量值时有一些限制。但是是的，一旦你熬过这一关，我相信你会爱上SVM的。</em></p><h1 id="255d" class="lj lk iq bd ll lm mp lo lp lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg bi translated">SVM的制约因素</h1><p id="8d09" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">SVM的约束是<strong class="jp ir"> <em class="lb"> u.w + b </em> </strong>必须是1或者-1。即使数学上这些值可以是-1到1之间的任何值，但是因为我们想用向量对值进行分类；我们假设它是绝对的1或-1。此外，这涉及到复杂的数学(拉格朗日乘数)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/2abe292c0d2cb0ddb39a2736868e5132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZM5RZQXOtJm-JPQqfkEwjw.jpeg"/></div></div></figure><p id="080c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，如果我们的等式是1；那么属于+类。如果它是-1，那么我们的数据点属于-class。但是嘿，我们还是不知道<em class="lb"> w </em>和<em class="lb"> b. </em>的值是多少</p><p id="7f3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">得到w；我们需要找到决策边界的最佳位置。因此，我们可以从它画一条垂线来测量矢量w。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2760ad8c0296e57674cc483d6e67df95.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*JbaLnAeIfW-9T-Cg64Y-lQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">SVM超平面和决策边界。作者图片</p></figure><p id="df63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经知道，决策边界应该与两侧的数据点有尽可能高的距离。(换句话说；决策边界和2个数据超平面之间的宽度必须最大)。</p><p id="77ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了实现这一目标；我们需要最大化宽度。那就是:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/56d04c01368ab52478ec5f01a5e84e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AzS84zvlClL6Pg1rJ2RLw.jpeg"/></div></div></figure><p id="4cd9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在简化并用等式<em class="lb"> u.w+b -1 = 0 </em>和<em class="lb"> u.w+b+1 = 0 </em>替换X正向量和X负向量的值后；我们得到:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mv"><img src="../Images/c7cede48f951bb436ed7578684074491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjNq46y1PIY-EkkmpuzD4Q.png"/></div></div></figure><p id="0628" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以便最大化宽度；向量w的值应该是最小值。如前所述；向量w的大小是<strong class="jp ir"> <em class="lb"> 1/2 * |w| </em> </strong>。</p><p id="98d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，我们在看一个想要最小化的方程。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/bddfd333dc98b19df3c15266303dfaed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ODlFGgtmiEdD9AFKhv1Gw.jpeg"/></div></div></figure><p id="67bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是额外需要阅读的内容。但是为了得到约束，我们必须插入拉格朗日乘数。不仅如此，我们还会看到一个向量w(我们希望最小化)和偏差(最大化)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/d81d16f44bb9fce13c2591f867987ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPaA9EpDC47JSEaQyvDfJg.jpeg"/></div></div></figure><p id="13a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">迷路了？坚持住。就差最后一步了。如果你对微积分感兴趣。我们需要对w和b做偏导数。这给了我们一个二次方程。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/cd320139c3c2c0b9628cc98ffedc1a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4N8xlMxr_-N_5migzZzEg.jpeg"/></div></div></figure><p id="5ac9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而这就是我们如何得到<strong class="jp ir">向量w </strong>和<strong class="jp ir"> b </strong> (bias)的值。天哪，我知道这很复杂。但仅此而已。</p><h1 id="4674" class="lj lk iq bd ll lm mp lo lp lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg bi translated">事物的利与弊</h1><p id="1780" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">除了表演；从计算的角度来看，训练SVM可能是一个繁琐的过程，因为二次方程。但是，一旦训练出来；它可以很快预测出分数。因为它只需要计算u.w+b的符号(不管它是否大于零)。</p><h2 id="55a3" class="mw lk iq bd ll mx my dn lp mz na dp lt jy nb nc lx kc nd ne mb kg nf ng mf nh bi translated">我的朋友是支持向量机。</h2><p id="6d4a" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">感谢sklearn让很多害怕数学的人只用一行代码就能使用这个漂亮的算法。无红利</p><p id="3da3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb">如果你想了解更多，你必须查看sentdex的解释和MITOCW讲座，了解更详细的数学解释。</em></p><p id="483c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直到下一次！</p></div></div>    
</body>
</html>