<html>
<head>
<title>How to Vectorize Text in DataFrames for NLP Tasks — 3 Simple Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何对自然语言处理任务的数据帧中的文本进行矢量化——3种简单的技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db?source=collection_archive---------6-----------------------#2020-10-12">https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db?source=collection_archive---------6-----------------------#2020-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4a78" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Texthero、Gensim和Tensorflow的简单代码示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4e35e7603a18cfb899d70afd7822f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bmw6IO1HsG17QU3X"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Justin Luebke 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="ac75" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">今天就开始学习NLP吧！</h1><p id="51a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可能听说过自然语言处理(NLP)是十年来最具T4变革性的技术之一。现在是进入NLP的绝佳时机，因为新的库已经抽象掉了许多复杂性，允许用户通过很少的模型训练和很少的代码获得最先进的结果。</p><p id="05f3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">虽然计算机实际上很擅长寻找模式和总结文档，但它必须先将单词转换成数字，然后才能理解它们。不同于<a class="ae ky" rel="noopener" target="_blank" href="/the-simple-approach-to-word-embedding-for-natural-language-processing-using-python-ae028c8dbfd2">像我过去所做的那样在单词级别对文本进行矢量化</a>，我将探索以下技术在句子、段落或文档级别对数据帧中的文本进行矢量化:</p><p id="8c1a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" href="https://texthero.org/docs/api/texthero.representation.tfidf" rel="noopener ugc nofollow" target="_blank">text hero:TF-IDF</a><br/><a class="ae ky" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank">Gensim:doc 2 vec</a><br/>T14】tensor flow 2:通用语句编码器4 </p><p id="8b0a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">作为奖励，我也将使用t-SNE可视化矢量！<strong class="lt iu">完整代码朝向底部！</strong></p><h1 id="da8f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">下载数据集</h1><p id="59c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我将不再从我最喜欢的来源中收集数据，而是从游戏《魔法:聚会中寻找<a class="ae ky" href="http://MTGJSON.com" rel="noopener ugc nofollow" target="_blank">卡片。我几乎一生都在玩纸牌游戏，并在90年代中期大约在二年级的时候开始玩</a><a class="ae ky" href="https://en.wikipedia.org/wiki/Magic:_The_Gathering" rel="noopener ugc nofollow" target="_blank">魔法:聚会(MTG) </a>。如果你有兴趣尝试一下，你可以<a class="ae ky" href="https://magic.wizards.com/en/mtgarena" rel="noopener ugc nofollow" target="_blank">在线免费玩</a>！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/40f109e68e256d9701c6728b4c79942d.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*NIxHeZ6R6f4j00xT3pY9-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Magic:_The_Gathering#/media/File:Magic_the_gathering-card_back.jpg" rel="noopener ugc nofollow" target="_blank">魔法:聚会</a> <a class="ae ky" href="https://company.wizards.com/fancontentpolicy" rel="noopener ugc nofollow" target="_blank">法律</a></p></figure><p id="cb08" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我很高兴发现所有卡片的免费数据集。要了解示例，请从<a class="ae ky" href="https://mtgjson.com/downloads/all-files/" rel="noopener ugc nofollow" target="_blank">MTGJSON.com</a>下载<strong class="lt iu"> AllPrintings SQLITE </strong>文件</p><blockquote class="mu mv mw"><p id="58a7" class="lr ls mn lt b lu mo ju lw lx mp jx lz mx mq mc md my mr mg mh mz ms mk ml mm im bi translated">MTGJSON是一个开源项目，它将所有魔术编目:以可移植的格式收集卡片。</p></blockquote></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="3472" class="kz la it bd lb lc nh le lf lg ni li lj jz nj ka ll kc nk kd ln kf nl kg lp lq bi translated">导入依赖项和数据</h1><p id="e7bf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">很容易将数据连接并加载到dataframe中，因为它已经是一个sqlite文件。按照三个步骤加载库、数据和数据框架！</p><p id="1c90" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">1.导入pandas和sqlite3库<br/> 2。连接到sqlite文件<br/> 3。将数据加载到熊猫数据框架中。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="e90c" class="nr la it nn b gy ns nt l nu nv">#<em class="mn">Import dependencies</em><br/>import pandas as pd<br/>import sqlite3</span><span id="af37" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#Establish connection to sqlite database</em><br/>conn = sqlite3.connect("AllPrintings.sqlite")</span><span id="5066" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#load the data into a pandas DataFrame</em><br/>df = pd.read_sql("select * from cards", conn)</span></pre><p id="ca48" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意，我使用pandas <strong class="lt iu"> read_sql </strong>函数来生成一个使用原始sql的数据帧。我可以看到数据集中有73列和54033行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/11301c9371645172a1fc77b3aac9d3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tzayzF7kU7RLtXOud5YLg.png"/></div></div></figure><p id="e9f5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我将减少列数和行数，因为我只关心不同的卡片。在游戏<em class="mn">魔法:聚会</em>中，有5种核心颜色:黑色、蓝色、绿色、红色和白色。</p><p id="ab7e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我将探索我最喜欢的颜色，<a class="ae ky" href="https://medium.com/swlh/how-to-create-a-dashboard-to-dominate-the-stock-market-using-python-and-dash-c35a12108c93" rel="noopener">绿色</a>。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="31b8" class="nr la it nn b gy ns nt l nu nv">df = pd.read_sql("select distinct name, text, convertedManaCost, power, toughness, keywords from cards where borderColor ='black' and colorIdentity = 'G'", conn)</span></pre><p id="03ac" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意，我用<strong class="lt iu"> SELECT DISTINCT </strong>开始SQL查询，以排除重复的卡。这是必要的，因为一些卡片多年来已经被重印了多套。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e1f5de100441495e5be543344944144d.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*FYdLwUBSHHNoVYuuK-xFUg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">df.shape返回数据帧的形状</p></figure><p id="ee25" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">删除重复项并将数据集限制为绿卡，这样我就有了3085行6列。</p><h1 id="69e0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">清理数据</h1><p id="6460" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了清理数据并为矢量化做准备，我将使用<a class="ae ky" rel="noopener" target="_blank" href="/try-texthero-the-absolute-simplest-way-to-clean-and-analyze-text-in-pandas-6db86ed14272"> <strong class="lt iu"> <em class="mn"> Texthero库</em> </strong> </a>，因为它简化了数据帧中文本的清理。要应用默认的文本清理脚本，运行<strong class="lt iu"> hero.clean(pandas。系列)。</strong></p><p id="5c3d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用<strong class="lt iu"> clean() </strong>时，默认运行以下七个功能:</p><ol class=""><li id="01d9" class="nz oa it lt b lu mo lx mp ma ob me oc mi od mm oe of og oh bi translated"><code class="fe oi oj ok nn b">fillna(s)</code>用空格替换未赋值的值。</li><li id="fea9" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated"><code class="fe oi oj ok nn b">lowercase(s)</code>小写所有文本。</li><li id="04fe" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated"><code class="fe oi oj ok nn b">remove_digits()</code>去掉所有的数字块。</li><li id="8a27" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated"><code class="fe oi oj ok nn b">remove_punctuation()</code>删除所有字符串.标点(！" #$% &amp; '()*+，-。/:;&lt; = &gt;？@[\]^_`{|}~).</li><li id="5f4c" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated"><code class="fe oi oj ok nn b">remove_diacritics()</code>去除琴弦上的所有重音。</li><li id="876b" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated">删除所有停用词。</li><li id="720e" class="nz oa it lt b lu ol lx om ma on me oo mi op mm oe of og oh bi translated"><code class="fe oi oj ok nn b">remove_whitespace()</code>删除单词之间的所有空格。</li></ol><h2 id="c1c2" class="nr la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">定制清洗管道</h2><p id="632a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我将构建自己的管道，而不是使用默认的clean函数。使用我自己的管道，我可以保留尽可能多的卡片上下文以实现相似性:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="663a" class="nr la it nn b gy ns nt l nu nv">import texthero as hero<br/>from texthero import preprocessing</span><span id="ca94" class="nr la it nn b gy nw nt l nu nv">custom_pipeline = [preprocessing.fillna,<br/>                   #preprocessing.lowercase,<br/>                   preprocessing.remove_whitespace,<br/>                   preprocessing.remove_diacritics<br/>                   #preprocessing.remove_brackets<br/>                  ]</span><span id="f13c" class="nr la it nn b gy nw nt l nu nv">df['clean_text'] = hero.clean(df['text'], custom_pipeline)<br/>df['clean_text'] = [n.replace('{','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace('}','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace('(','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace(')','') for n in df['clean_text']]</span></pre><p id="f562" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意我用了<strong class="lt iu"> fillna </strong>，因为有些卡片没有文字。<br/>注意我决定从管道中排除<strong class="lt iu">移除括号</strong>。这是因为Texthero也会删除括号中的数据！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/420c253f2cc2ed9f837d7fd044b04fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ab9jYnKgArw_QY8u4wTWPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">熊猫数据帧的前两行</p></figure><h1 id="7bdc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用TF-IDF生成矢量</h1><p id="983e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>代表<strong class="lt iu">词频-逆文档频</strong>。是一种<em class="mn">经典的</em>对单词值进行加权而不是简单计数的方法。它用于确定一个单词对集合文档中的文本有多重要。</p><p id="a77a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">TF-IDF是文本的一个<strong class="lt iu">词包(BoW) </strong>表示，描述文本语料库中<strong class="lt iu">词</strong>的出现。它不能解释单词的顺序。我将在下一节详细讨论这些限制…</p><p id="80ff" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">TextHero使得将TF-IDF应用到dataframe中的文本变得很容易。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="8230" class="nr la it nn b gy ns nt l nu nv">df['tfidf'] = (hero.tfidf(df['clean_text'], max_features=3000))</span></pre><p id="3d3d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">将值添加到dataframe实际上只有一行代码！我建议探索不同数量的<strong class="lt iu"> max_features </strong>，看看它如何影响向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/3e3e9b73144cb6e4f57508fe9a015731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6s16PkbQ08j1iNCz0VYZGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">df.head(2)显示dataframe中新的tfidf列</p></figure><p id="33cb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">TF-IDF生成一个稀疏矩阵，其中包含许多0，因为卡片上有各种各样的单词。</p><h1 id="9401" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用Doc2Vec生成矢量</h1><p id="68d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">虽然TF-IDF是使用经典矢量化技术建立基线的良好起点，但它有很大的局限性。由于它是一个单词包表示法，所以它没有保持单词的顺序，也没有说明太多的语义。作为稀疏矩阵，它也可能是资源密集型的。</p><h2 id="3927" class="nr la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">Word2Vec革命</h2><p id="6d71" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了解决语义关系和稀疏矩阵的问题，创建了像Word2Vec这样的模型。以前我写过关于使用<a class="ae ky" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim库</a>在文本数据上应用Word2Vec模型的文章。</p><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/the-simple-approach-to-word-embedding-for-natural-language-processing-using-python-ae028c8dbfd2"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">用Python实现自然语言处理中单词嵌入的简单方法</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">使用Gensim和Plotly探索单词嵌入</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu ks pg"/></div></div></a></div><p id="512b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Word2Vec模型为每个单词生成向量，但是为了计算整个文档的向量，我可以对文档中每个单词的向量进行平均。相反，我会使用更简单的路线:<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Doc2Vec </strong> </a>。</p><h2 id="d309" class="nr la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">使用Doc2Vec</h2><p id="2d45" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">2014年推出的<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py" rel="noopener ugc nofollow" target="_blank"> Doc2Vec </a>是一款<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-model" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>表示<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-document" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">文档</strong> </a> <strong class="lt iu"> s，段落或句子</strong>作为<a class="ae ky" href="https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-vector" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">向量</strong> </a>。Doc2Vec通常优于Word2Vec向量的简单平均，因此值得探索！</p><p id="df7a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在基本层面上，Doc2Vec算法为文档提供了另一个类似浮动单词的向量，称为doc-vector。它有助于所有的训练预测，并像其他词向量一样被更新。查看乐和米科洛夫的<a class="ae ky" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">全文，了解技术方面的内容。</a></p><p id="cec9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Doc2Vec需要训练。为了训练模型，标签/编号需要与训练的每个文档相关联。使用<strong class="lt iu"> doc2vec可以轻松完成标记。标签文档()。</strong></p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="cd8b" class="nr la it nn b gy ns nt l nu nv">from gensim.models.doc2vec import Doc2Vec, TaggedDocument</span><span id="eef1" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#tokenize and tag the card text</em><br/>card_docs = [TaggedDocument(doc.split(' '), [i]) <br/>             for i, doc in enumerate(df.clean_text)]</span><span id="c328" class="nr la it nn b gy nw nt l nu nv">#display the tagged docs<br/>card_docs</span></pre><p id="635d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意，我使用<strong class="lt iu">做了简单的标记化。拆分(' ')</strong>在清洗过的卡片文字上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/7ce8dcb5a036e7be1c0d8671a4387ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jP-rI57nZa9DWoTLoYS9yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标记和令牌化的卡文本</p></figure><p id="ff59" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">接下来，实例化一个<a class="ae ky" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> Doc2Vec模型</a>。</p><p id="7374" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">设置向量大小以确定将包括多少个维度。设置训练时期以确定它将迭代训练数据的次数。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="6fc2" class="nr la it nn b gy ns nt l nu nv">model = Doc2Vec(vector_size=64, min_count=1, epochs = 20)</span></pre><p id="4904" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当设置纪元时，考虑高的数字倾向于达到递减的回报。试验超参数！<a class="ae ky" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank">查看文档，获取所有模型参数的完整列表</a>。</p><p id="6d28" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在实例化模型后构建一个词汇表，然后训练模型。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4dbd" class="nr la it nn b gy ns nt l nu nv"><em class="mn">#instantiate model</em><br/>model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 40)</span><span id="6806" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#build vocab</em><br/>model.build_vocab(card_docs)</span><span id="5625" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#train model</em><br/>model.train(card_docs, total_examples=model.corpus_count<br/>            , epochs=model.epochs)</span></pre><p id="debd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">构建词汇表创建了一个字典(可通过<code class="fe oi oj ok nn b">model.wv.vocab</code>访问),包含从训练中提取的所有独特单词以及计数。</p><p id="f481" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在模型已经训练好了，使用<strong class="lt iu"> model.infer_vector </strong>将标记化的文本传递给模型以生成向量。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="62fb" class="nr la it nn b gy ns nt l nu nv"><em class="mn">#generate vectors</em><br/>card2vec = [model.infer_vector((df['clean_text'][i].split(' '))) <br/>            for i in range(0,len(df['clean_text']))]</span><span id="bde3" class="nr la it nn b gy nw nt l nu nv">card2vec</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/cbc9124bbeb224a229b022296c8298f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qBVZQt7t76X5tW7JS_P5KQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">矢量化卡片文本</p></figure><p id="1ec2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意数据输出为numpy数组。要将向量添加到数据帧中，使用<strong class="lt iu"> numpy.array()。【tolist().这将把它们保存为列表列表。然后，它们可以作为一列添加到数据帧中。</strong></p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="80fa" class="nr la it nn b gy ns nt l nu nv">import numpy as np</span><span id="2ca4" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#Create a list of lists</em><br/>dtv= np.array(card2vec).tolist()</span><span id="b4f5" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#set list to dataframe column</em><br/>df['card2vec'] = dtv</span><span id="e87c" class="nr la it nn b gy nw nt l nu nv">df.head(2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/7f1dc589c093e486111d4d7749234174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rW0xcEyZT1UsH8_zs_ozJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有Doc2Vec向量的数据帧</p></figure><p id="02a5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意card2vec列包含Doc2Vec向量。将向量添加到数据帧是存储它们的一种便捷方式。</p><p id="42b3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">虽然Doc2Vec方法只用了几行代码就完成了，而不是训练一个Doc2Vec模型，但是有一些预训练的选项甚至需要更少的代码！</p><h1 id="880c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用通用语句编码器生成向量</h1><p id="6233" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">比使用Doc2Vec更简单的是，托管在<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> Tensorflow-hub </a> (tfhub)上的<a class="ae ky" href="https://arxiv.org/pdf/1803.11175.pdf" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a> (USE)是一个预先训练好的模型，它将文本编码成512维向量。它在各种各样的文本资源上被训练。</p><blockquote class="mu mv mw"><p id="0c80" class="lr ls mn lt b lu mo ju lw lx mp jx lz mx mq mc md my mr mg mh mz ms mk ml mm im bi translated">通用句子编码器将文本编码成高维向量，这些向量可用于文本分类、语义相似性、聚类和其他自然语言任务。— <a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">文档</a></p></blockquote><p id="e335" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用模型的最新迭代使用TensorFlow2。我过去曾使用Tensorflow 1模型来<a class="ae ky" rel="noopener" target="_blank" href="/generating-wine-recommendations-using-the-universal-sentence-encoder-d086edd13d00">生成葡萄酒推荐</a>，这些新模型使用起来更简单，应用起来也更快。</p><p id="e4a6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">模型需要从tfhub下载。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="dd2e" class="nr la it nn b gy ns nt l nu nv">!pip install --upgrade tensorflow_hub</span><span id="f0d5" class="nr la it nn b gy nw nt l nu nv">import tensorflow_hub as hub</span><span id="2595" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#download the model</em><br/>embed = hub.load("<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-large/5</a>")</span></pre><p id="e696" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">接下来，通过模型运行<em class="mn"> cleaned_text </em>来生成向量。使用Doc2Vec示例中的相同技术，将numpy数组中的向量转换为list列表。</p><p id="d1c9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">由此，很容易将向量添加到数据帧中。就是这么简单！</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="63c8" class="nr la it nn b gy ns nt l nu nv"><em class="mn">#generate embeddings</em><br/>embeddings = embed(df['clean_text'])</span><span id="cd72" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#create list from np arrays</em><br/>use= np.array(embeddings).tolist()</span><span id="09ba" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#add lists as dataframe column</em><br/>df['use'] = use</span><span id="1ebc" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#check dataframe</em><br/>df.head(2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/d683c469c230ba8295b76bd9a22b7703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVUiSUAXdNXnS4qtCM1jkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通用句子编码器向量</p></figure><blockquote class="pz"><p id="b23a" class="qa qb it bd qc qd qe qf qg qh qi mm dk translated">恭喜你！您刚刚学习了3种快速简单的方法来矢量化数据帧中大于单词长度的文本！</p></blockquote><h1 id="25cf" class="kz la it bd lb lc ld le lf lg lh li lj jz qj ka ll kc qk kd ln kf ql kg lp lq bi translated">可视化矢量</h1><p id="891d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">既然我费了这么大的劲来生成向量，我还不如探索一下！使用<a class="ae ky" href="https://texthero.org/docs/api/texthero.representation.tsne" rel="noopener ugc nofollow" target="_blank"> Texthero库</a>，很容易应用<a class="ae ky" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> t-SNE算法</a>来降低向量的维度，并在2D空间中可视化它们。</p><p id="3f08" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">t-SNE(t-distributed random neighbor embedding)是一种用于可视化高维数据的<a class="ae ky" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法。t-SNE技术应用了<strong class="lt iu">非线性</strong>维度缩减。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="9246" class="nr la it nn b gy ns nt l nu nv">df['tsnetfidf'] = hero.tsne(df['tfidf'])<br/>df['tsnec2v'] = hero.tsne(df['card2vec'])<br/>df['tsneuse'] = hero.tsne(df['use'])</span></pre><p id="26ec" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">算法很复杂，可能需要一点时间才能完成！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qm"><img src="../Images/70deec022107c350cf8eadf6ce15ba4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5R02UBD7jhmPOVrSewJzBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">显示t-SNE结果的数据框</p></figure><p id="89ac" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">接下来，使用<a class="ae ky" href="https://texthero.org/docs/api/texthero.visualization.scatterplot" rel="noopener ugc nofollow" target="_blank"> Texthero构建t-sne列的散点图</a>。Texthero在引擎盖下使用Plotly，因此图形是交互式的。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="45c6" class="nr la it nn b gy ns nt l nu nv"><em class="mn">#create scatter plot of tfidf</em><br/>hero.scatterplot(df, col='tsnetfidf', color='convertedManaCost'<br/>                 , title="TF-IDF", hover_data = ['name','text'])</span><span id="9151" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#create scatter plot of doc2vec</em><br/>hero.scatterplot(df, col='tsnec2v', color='convertedManaCost'<br/>                 , title="Doc2Vec", hover_data = ['name','text'])</span><span id="1dd5" class="nr la it nn b gy nw nt l nu nv"><em class="mn">#create scatter plot of uni. sent. enc.</em><br/>hero.scatterplot(df, col='tsneuse', color='convertedManaCost'<br/>                 , title="U.S.E", hover_data = 'name','text'])</span></pre><p id="a071" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意我将<strong class="lt iu"> hover_data </strong>设置为<strong class="lt iu"> <em class="mn"> name </em> </strong>和<strong class="lt iu"> <em class="mn"> text </em> </strong>。将鼠标悬停在某个点上将显示卡片名称和卡片文本。这样就很容易比较各种技术如何将相似的卡片聚集在一起。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/c8931c66b17a6448dfa14133d0fd447b.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Owxcq3uiR7-il03dcU-uBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文本向量散点图</p></figure><p id="6cdf" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">每种技术产生不同的情节，但是把相似的卡片聚集在一起。举例来说，我发现有践踏，或加+1/+1，或横置以加魔法力的牌丛。</p><h1 id="cd8f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法和完整的代码</h1><p id="5e60" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自然语言处理可能是一个复杂的话题，但是这些技术应该给你一个跳板，让你可以更深入地研究这个领域。我介绍了三种在段落、句子或文档级别对文本进行矢量化的简单方法:</p><p id="f05b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" href="https://texthero.org/docs/api/texthero.representation.tfidf" rel="noopener ugc nofollow" target="_blank">text hero:TF-IDF</a><br/><a class="ae ky" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank">Gensim:doc 2 vec</a><br/><a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">tensor flow 2:通用语句编码器4 </a></p><p id="9283" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">从文本创建向量为机器学习任务做准备，如文本分类、语义相似性和聚类。在我的其他文章中了解关于自然语言处理和数据科学的更多信息:</p><div class="pd pe gp gr pf pg"><a href="https://medium.com/swlh/how-to-create-a-dashboard-to-dominate-the-stock-market-using-python-and-dash-c35a12108c93" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">如何使用Python和Dash创建控制股市的仪表板</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">自由期权订单流、价格、基本面、聊天集于一身</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="qo l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/3-super-simple-projects-to-learn-natural-language-processing-using-python-8ef74c757cd9"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">使用Python学习自然语言处理的3个超级简单的项目</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">单词云、垃圾邮件检测和情感分析的简单代码示例</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="qp l pr ps pt pp pu ks pg"/></div></div></a></div><h2 id="e19e" class="nr la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">完全码</h2><p id="f533" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这里是完整的代码或在我的github上找到它！</p><div class="pd pe gp gr pf pg"><a href="https://github.com/bendgame/Medium_doc2vec" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">bendgame/Medium_doc2vec</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码、管理项目和构建…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">github.com</p></div></div><div class="pp l"><div class="qq l pr ps pt pp pu ks pg"/></div></div></a></div><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="13c6" class="nr la it nn b gy ns nt l nu nv">#Import dependencies<br/>import pandas as pd<br/>import sqlite3<br/>import texthero as hero<br/>from texthero import preprocessing<br/>from gensim.models.doc2vec import Doc2Vec, TaggedDocument<br/>import numpy as np<br/>import tensorflow_hub as hub</span><span id="cd1c" class="nr la it nn b gy nw nt l nu nv">#Establish connection to sqlite database<br/>conn = sqlite3.connect("AllPrintings.sqlite")<br/>#load the data into a pandas DataFrame<br/>df = pd.read_sql("select * from cards", conn)</span><span id="0d95" class="nr la it nn b gy nw nt l nu nv">df = pd.read_sql("select distinct name, text, convertedManaCost, power, toughness, keywords from cards where borderColor ='black' and colorIdentity = 'G'", conn)</span><span id="d2b8" class="nr la it nn b gy nw nt l nu nv">custom_pipeline = [preprocessing.fillna,<br/>                   #preprocessing.lowercase,<br/>                   preprocessing.remove_whitespace,<br/>                   preprocessing.remove_diacritics<br/>                   #preprocessing.remove_brackets<br/>                  ]<br/>df['clean_text'] = hero.clean(df['text'], custom_pipeline)<br/>df['clean_text'] = [n.replace('{','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace('}','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace('(','') for n in df['clean_text']]<br/>df['clean_text'] = [n.replace(')','') for n in df['clean_text']]</span><span id="7b19" class="nr la it nn b gy nw nt l nu nv">df['tfidf'] = (hero.tfidf(df['clean_text'], max_features=3000))</span><span id="8354" class="nr la it nn b gy nw nt l nu nv">#tokenize and tag the card text<br/>card_docs = [TaggedDocument(doc.split(' '), [i]) <br/>             for i, doc in enumerate(df.clean_text)]<br/>card_docs</span><span id="3dae" class="nr la it nn b gy nw nt l nu nv">model = Doc2Vec(vector_size=64, min_count=1, epochs = 20)</span><span id="4333" class="nr la it nn b gy nw nt l nu nv">#instantiate model<br/>model = Doc2Vec(vector_size=64, window=2, min_count=1, workers=8, epochs = 40)</span><span id="334d" class="nr la it nn b gy nw nt l nu nv">#build vocab<br/>model.build_vocab(card_docs)</span><span id="a43f" class="nr la it nn b gy nw nt l nu nv">#train model<br/>model.train(card_docs, total_examples=model.corpus_count<br/>            , epochs=model.epochs)</span><span id="e67f" class="nr la it nn b gy nw nt l nu nv">#generate vectors<br/>card2vec = [model.infer_vector((df['clean_text'][i].split(' '))) <br/>            for i in range(0,len(df['clean_text']))]<br/>card2vec</span><span id="4f3c" class="nr la it nn b gy nw nt l nu nv">#Create a list of lists<br/>dtv= np.array(card2vec).tolist()</span><span id="a24d" class="nr la it nn b gy nw nt l nu nv">#set list to dataframe column<br/>df['card2vec'] = dtv</span><span id="feec" class="nr la it nn b gy nw nt l nu nv">df.head(2)</span><span id="6e49" class="nr la it nn b gy nw nt l nu nv">#download the model<br/>embed = hub.load("<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-large/5</a>")</span><span id="0564" class="nr la it nn b gy nw nt l nu nv">#generate embeddings<br/>embeddings = embed(df['clean_text'])</span><span id="534d" class="nr la it nn b gy nw nt l nu nv">#create list from np arrays<br/>use= np.array(embeddings).tolist()</span><span id="2cb7" class="nr la it nn b gy nw nt l nu nv">#add lists as dataframe column<br/>df['use'] = [v for v in use]</span><span id="a1b9" class="nr la it nn b gy nw nt l nu nv">#check dataframe<br/>df.head(2)</span><span id="1f72" class="nr la it nn b gy nw nt l nu nv">df['tsnetfidf'] = hero.tsne(df['tfidf'])<br/>df['tsnec2v'] = hero.tsne(df['card2vec'])<br/>df['tsneuse'] = hero.tsne(df['use'])</span><span id="e2c7" class="nr la it nn b gy nw nt l nu nv">#create scatter plot of tfidf<br/>hero.scatterplot(df, col='tsnetfidf', color='convertedManaCost'<br/>                 , title="TF-IDF", hover_data = ['name','text'])<br/>#create scatter plot of doc2vec<br/>hero.scatterplot(df, col='tsnec2v', color='convertedManaCost'<br/>                 , title="Doc2Vec", hover_data = ['name','text'])<br/>#create scatter plot of uni. sent. enc.<br/>hero.scatterplot(df, col='tsneuse', color='convertedManaCost'<br/>                 , title="U.S.E", hover_data = 'name','text'])</span></pre><div class="pd pe gp gr pf pg"><a href="https://medium.com/swlh/become-a-sql-wizard-using-these-query-optimization-tips-a932d18c762f" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">使用这些查询优化技巧成为SQL向导</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">如何对SQL Server查询进行性能优化</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="qr l pr ps pt pp pu ks pg"/></div></div></a></div><h1 id="5d81" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">谢谢大家！</h1><ul class=""><li id="5ab5" class="nz oa it lt b lu lv lx ly ma qs me qt mi qu mm qv of og oh bi translated"><em class="mn">如果你喜欢这个，</em> <a class="ae ky" href="https://medium.com/@erickleppen" rel="noopener"> <em class="mn">在Medium上关注我</em> </a> <em class="mn">获取更多</em></li><li id="ab73" class="nz oa it lt b lu ol lx om ma on me oo mi op mm qv of og oh bi translated"><a class="ae ky" href="https://erickleppen.medium.com/membership" rel="noopener"> <em class="mn">通过订阅</em> </a>获得对我的内容的完全访问和帮助支持</li><li id="3430" class="nz oa it lt b lu ol lx om ma on me oo mi op mm qv of og oh bi translated"><em class="mn">我们来连线一下</em><a class="ae ky" href="https://www.linkedin.com/in/erickleppen01/" rel="noopener ugc nofollow" target="_blank"><em class="mn">LinkedIn</em></a></li><li id="602c" class="nz oa it lt b lu ol lx om ma on me oo mi op mm qv of og oh bi translated"><em class="mn">用Python分析数据？查看我的</em> <a class="ae ky" href="https://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mn">网站</em> </a></li></ul><p id="c68d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><a class="ae ky" href="http://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> —埃里克·克莱本</strong> </a></p></div></div>    
</body>
</html>