<html>
<head>
<title>NLP in the Financial Market — Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">金融市场中的NLP情绪分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-in-the-financial-market-sentiment-analysis-9de0dda95dc?source=collection_archive---------8-----------------------#2020-10-03">https://towardsdatascience.com/nlp-in-the-financial-market-sentiment-analysis-9de0dda95dc?source=collection_archive---------8-----------------------#2020-10-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9176" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最新的自然语言处理模型在多大程度上优于传统模型——从词汇方法到用于金融文本情感分析任务的BERT</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/45ba92b7b277c59e925f7727a2830999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AJr07x6ChJ6o_HQM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ec29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> D </span>自2012年先驱CNN在ImageNet上推出AlexNet以来，计算机视觉中的eep学习已经成功应用于各种应用中。相反，NLP在深度神经网络应用方面已经落后了。许多声称使用人工智能的应用程序通常使用某种基于规则的算法和传统的机器学习，而不是深度神经网络。2018年，一种名为BERT的最先进(STOA)模型在一些自然语言处理任务中的表现超过了人类的分数。在这里，我将几个模型用于情绪分析任务，看看它们在我所在的金融市场中有多大用处。代码在jupyter笔记本中，可以在git <a class="ae ky" href="https://github.com/yuki678/financial-phrase-bert" rel="noopener ugc nofollow" target="_blank"> repo </a>中获得。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="b0af" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">1.介绍</h1><p id="f536" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">NLP任务可以大致分为以下几类。</p><ol class=""><li id="2708" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">文本分类—过滤垃圾邮件，对文档进行分类</li><li id="d342" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">单词序列—单词翻译、词性标签、命名实体识别</li><li id="56a8" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">文本含义—主题建模、搜索、问题回答</li><li id="1b5b" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">序列对序列—机器翻译、文本摘要、问答</li><li id="cd2d" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">对话系统</li></ol><p id="a37a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同的任务需要不同的方法，在大多数情况下是多种自然语言处理技术的结合。开发机器人时，后端逻辑通常是基于规则的搜索引擎和排名算法，以形成自然的交流。</p><p id="eb47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是有充分理由的。语言有语法和词序，这可以通过基于规则的方法更好地处理，而机器学习方法可以更好地学习搭配和单词相似性。word2vec、词袋等矢量化技术有助于模型以数学方式表达文本。最著名的例子是:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="e577" class="ob mm it nx b gy oc od l oe of">King - Man + Woman = Queen</span><span id="a168" class="ob mm it nx b gy og od l oe of">Paris - France + UK = London</span></pre><p id="6eb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个例子描述了性别关系，第二个例子描述了首都的概念。然而，在这些方法中，由于在任何文本中相同的单词总是由相同的向量表示，所以上下文没有被捕获，这在许多情况下是不正确的。递归神经网络(<strong class="lb iu"> RNN </strong>)架构使用来自输入序列的先前信息并处理时间序列数据，在捕获和记忆上下文方面表现良好。典型的架构之一是长短期记忆(<strong class="lb iu"> LSTM </strong>)，由输入门、输出门和遗忘门组成，以克服RNN的清漆梯度问题。有许多基于LSTM的改进模型，例如双向LSTM，不仅可以从前面的单词中捕捉上下文，还可以从后面捕捉上下文。这些对于一些特定的任务是好的，但是在实际应用中不太好。</p><p id="f105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2017年，我们看到了一种无需递归或卷积架构即可解决该问题的新方法。<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你需要的全部</a>提出了一种transformer架构，这是一种基于注意力机制的编解码栈。<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">来自Transformers ( <strong class="lb iu"> BERT </strong> ) </a>的双向编码器表示，是Google在2018年推出的具有多个编码器堆栈的掩码语言模型，在GLUE、SQuAD和SWAG基准测试中实现了较大的改进。有很多文章和博客解释这个架构，比如Jay Alammar的文章。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="ee2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在金融行业工作，我很难看到过去几年在交易系统中生产使用的NLP上的机器学习模型在我们过去的R&amp;D中足够稳健的性能。现在，由于<a class="ae ky" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>的实施，基于BERT的模型变得越来越成熟和易于使用，并且许多预先训练的模型已经公开。我的目标是看看NLP的最新发展是否达到了在我的领域中使用的良好水平。在这篇文章中，我在一个相当简单的金融文本情感分析任务上比较了不同的模型，作为判断是否值得在实际解决方案中尝试另一个R &amp; D的基线。</p><p id="5628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里比较的模型有:</p><ol class=""><li id="2411" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">使用词典的基于规则的方法</li><li id="9139" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">使用Tfidf的传统机器学习方法</li><li id="ab81" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">作为递归神经网络结构的LSTM</li><li id="b1fb" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">伯特(和艾伯特)</li></ol></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="8c8a" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">2.输入数据</h1><p id="911e" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">对于情感分析任务，我采用以下两种输入来代表行业中的不同语言。</p><ol class=""><li id="fd4c" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">金融标题作为更正式的风格</li><li id="5b5e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">来自Stocktwits的Tweets作为交易者的非正式谈话</li></ol><p id="68e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">后者我会再写一篇，所以这里重点说一下前者的数据。这是一个包含更正式的金融领域特定语言的文本示例，我使用了Malo等人(2014) 的<a class="ae ky" href="https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts" rel="noopener ugc nofollow" target="_blank"> FinancialPhraseBank，它由16个人的4845个手写标题文本组成，并提供了agree级别。我用75%同意的标签和3448个文本作为训练数据。</a></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="1f18" class="ob mm it nx b gy oc od l oe of">## Input text samples</span><span id="2074" class="ob mm it nx b gy og od l oe of"><strong class="nx iu">positive</strong> "Finnish steel maker Rautaruukki Oyj ( Ruukki ) said on July 7 , 2008 that it won a 9.0 mln euro ( $ 14.1 mln ) contract to supply and install steel superstructures for Partihallsforbindelsen bridge project in Gothenburg , western Sweden."</span><span id="2f7b" class="ob mm it nx b gy og od l oe of"><strong class="nx iu">neutral</strong> "In 2008 , the steel industry accounted for 64 percent of the cargo volumes transported , whereas the energy industry accounted for 28 percent and other industries for 8 percent."</span><span id="167d" class="ob mm it nx b gy og od l oe of"><strong class="nx iu">negative</strong> "The period-end cash and cash equivalents totaled EUR6 .5 m , compared to EUR10 .5 m in the previous year."</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/8b472c7499e41aa99d1d071d7eb23e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwzEY8r0lnmA7S6YqUjvow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">词云为第一次输入训练文本，图片由作者提供</p></figure><p id="2534" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，所有的数据属于来源，人们必须尊重他们的版权和许可条款。</p><h1 id="d866" class="ml mm it bd mn mo oi mq mr ms oj mu mv jz ok ka mx kc ol kd mz kf om kg nb nc bi translated">3.模型</h1><p id="e01f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">下面是我对比的四款机型的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/f3d0a27b527cab250f35e91280222e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7G11Iaj5SbHopn07z_mkLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">NLP模型评估，图片由作者提供</p></figure><h2 id="d0ef" class="ob mm it bd mn oo op dn mr oq or dp mv li os ot mx lm ou ov mz lq ow ox nb oy bi translated">A.基于词典的方法</h2><p id="769f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">创建特定领域的词典是一种传统的方法，在某些情况下，当来源来自特定的人或媒体时，这种方法简单而有效。<a class="ae ky" href="https://sraf.nd.edu/textual-analysis/resources/" rel="noopener ugc nofollow" target="_blank">拉夫兰和麦克唐纳感情词表</a>。这个列表包含超过4k个单词，出现在带有情绪标签的财务报表上。注意:这些数据需要许可证才能用于商业应用。请在使用前查看他们的网站。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="430c" class="ob mm it nx b gy oc od l oe of">## Sample</span><span id="3a0c" class="ob mm it nx b gy og od l oe of">negative: ABANDON<br/>negative: ABANDONED<br/>constraining: STRICTLY</span></pre><p id="27d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我用了负2355字，正354字。它包括单词形式，所以不要对输入进行词干分析和词条解释。对于这种方法，考虑否定是很重要的。不，不，不要等词语。把否定词的意思改成肯定词，如果否定词之一出现在肯定词之前的三个词里，我就简单地颠倒一下情绪。</p><p id="bddf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，音调分数定义如下，并沿正/负计数送入分类器。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="56a3" class="ob mm it nx b gy oc od l oe of">tone_score = 100 * (pos_count — neg_count) / word_count</span></pre><p id="eeb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用默认参数训练14个不同的分类器，然后使用网格搜索交叉验证进行随机森林的超参数调整。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h2 id="4ff8" class="ob mm it bd mn oo op dn mr oq or dp mv li os ot mx lm ou ov mz lq ow ox nb oy bi translated">B.基于Tfidf向量的传统机器学习</h2><p id="a099" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">输入由NLTK word_tokenize()进行标记化，然后移除词条和停用词。然后输入到tfidf矢量器，通过逻辑回归和随机森林分类器进行分类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h2 id="dd0e" class="ob mm it bd mn oo op dn mr oq or dp mv li os ot mx lm ou ov mz lq ow ox nb oy bi translated">C.LSTM——一种递归神经网络</h2><p id="ee85" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">由于LSTM被设计为记住表达上下文的长期记忆，所以使用定制的标记器来提取字母，因为它们没有词条满足或停用词移除。然后输入被送入一个嵌入层，然后是两个lstm层。为了避免过度拟合，通常情况下应用dropout，然后是完全连接的层，最后是log softmax。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="28dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为替代，也尝试了斯坦福的<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe word embedding，这是一种无监督学习算法，用于获取单词的矢量表示。在这里，采取了预先训练的维基百科和千兆字6B令牌，40万词汇大小和300维向量。我们的词汇库中大约90%的单词是在这个手套词汇库中找到的，其余的是随机初始化的。</a></p><h2 id="78f6" class="ob mm it bd mn oo op dn mr oq or dp mv li os ot mx lm ou ov mz lq ow ox nb oy bi translated">D.伯特(还有阿尔伯特作为伯特模型的替代)</h2><p id="1f2a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我用pytorch实现了来自Huggingface 的<a class="ae ky" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">变形金刚的BERT模型。现在(v3)他们提供了标记器和编码器，可以生成文本id、填充掩码和段id，可以直接在他们BertModel中使用，标准训练过程不需要定制实现。</a></p><p id="3aa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于LSTM模型，来自BERT的输出然后被传递到退出的、完全连接的layters，然后应用log softmax。如果没有足够的计算资源预算，也没有足够的数据，从头开始训练模型不是一个选项，所以我使用预训练的模型并进行微调。预训练模型使用如下:</p><ul class=""><li id="be42" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu pb no np nq bi translated">伯特:伯特-基地-无壳</li><li id="de27" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu pb no np nq bi translated">艾伯特:艾伯特-基地-v2</li></ul><p id="e2e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预训练的bert的训练过程如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h1 id="5c5d" class="ml mm it bd mn mo oi mq mr ms oj mu mv jz ok ka mx kc ol kd mz kf om kg nb nc bi translated">4.估价</h1><p id="3736" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">首先，输入数据以8:2的比例分成训练集和测试集。测试集保持不变，直到所有参数都固定下来，并且每个模型只使用一次。由于数据集不大，交叉验证用于评估参数集。此外，为了克服不平衡和较小数据集的问题，分层K-Fold交叉验证被用于超参数调整。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/b18e8c039533533002aa3630b83ab8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMJkl8nWrEtBImnkZwHX0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉验证和测试集分割，按作者分类的图像</p></figure><p id="4236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于输入数据不平衡，评估基于F1分数，同时也参考了准确性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="75d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网格搜索交叉验证用于模型A和B，而定制交叉验证用于深度神经网络模型C和d</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oz pa l"/></div></figure><h1 id="fd05" class="ml mm it bd mn mo oi mq mr ms oj mu mv jz ok ka mx kc ol kd mz kf om kg nb nc bi translated">5.结果</h1><p id="8dd0" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在花费或多或少相似的时间进行超参数调优后，基于微调的BERT模型明显优于其他模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/e50b87e8951ed93be1b6e5bff3b0b4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*jtKsio38Em9vsSxSriaNCQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个模型的评估分数，按作者排序的图像</p></figure><p id="b6f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">模型A </strong>表现不佳，因为输入过于简化为音调得分，这是判断情绪的单个值，随机森林模型最终对大多数数据标注了中性的多数类别。简单线性模型仅通过将阈值应用于音调得分而表现得更好，但是在准确性和f1得分方面仍然相当低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/be43752977f64b5ddcb024dd56fc9dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LcFIJZshkscOgcZ3SIGW9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个分类器的准确性，按作者分类的图像</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/8620aaa11fd2880a301aaf02ccd30dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y4eBNz0r_90XEKdJ5pSGZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵，作者图片</p></figure><p id="b5c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们没有使用欠采样/过采样或SMOTE等方法来平衡输入数据，因为它可以纠正这个问题，但会偏离不平衡存在的实际情况。对该模型的潜在改进是，如果为要解决的每个问题建立词典的成本是合理的，则建立定制词典来代替L-M词典。更复杂的否定也可以提高预测的准确性。</p><p id="9b5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">模型B </strong>比前一个模型好得多，但是它以几乎100%的准确度和f1分数过度适合训练集，并且未能被通用化。我试图降低模型的复杂性，以避免过度拟合，但它最终在验证集中得分较低。平衡数据可以帮助解决这个问题或收集更多的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/496936e35991afc2f7be62d84ccb3dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h3cZcZZE8-DRGA-0G0_W2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网格搜索交叉验证结果，按作者排序的图像</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/0e93bdbea8dfc982477047373304323b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17nPf_ga5w8PABKKLYKOKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵，作者图片</p></figure><p id="700d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">模型C </strong>产生了与先前模型相似的结果，但改进不大。事实上，训练数据的数量不足以从零开始训练神经网络，并且需要多个时期，这往往会过度拟合。预训练的手套嵌入不会改善结果。对后一种模型的一个可能的改进是使用来自类似领域(如10K、10Q财务报表)的一串文本来训练手套，而不是使用来自维基百科的预训练模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/5e6cddf3069be6ef3fce919953450c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNAU965WCwTdgrhQGb7YkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵，作者图片</p></figure><p id="1468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">D型</strong>表现相当不错，在交叉验证和最终测试中，准确率和f1分均超过90%。它对负面文本的正确分类率为84%，而对正面文本的正确分类率为94%，这可能是由于输入的数量，但最好仔细观察以进一步提高性能。这表明，由于迁移学习和语言模型，预训练模型的微调在这个小数据集上表现良好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/8af42bb721b8720662246773d2bdee0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNUvUPA5-eYFG0ilVKn_Bw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终测试集上的混淆矩阵显示了良好的性能，图片由作者提供</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/aa3cbe785ace60a6fe17baa7d5cfd5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oq2ELM3VrwKRcpuXv1rVUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练数据的丢失随着训练的进行而减少，图片由作者提供</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/0a47dbaad52b892fb6f10ac9e3503128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vl1sW4nCD_VUo3-oS6ejsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">验证数据的损失并没有像训练数据那样下降</p></figure><h1 id="3e9d" class="ml mm it bd mn mo oi mq mr ms oj mu mv jz ok ka mx kc ol kd mz kf om kg nb nc bi translated">5.结论</h1><p id="7a41" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这个实验显示了基于BERT的模型在我的领域中的应用潜力，在这个领域中，以前的模型未能产生足够的性能。然而，结果是不确定的，并且基于在自由层GPU上的相当简单的手动超参数调整，并且它可能根据输入数据和调整方法而不同。</p><p id="9194" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样值得注意的是，在实际应用中，获得适当的输入数据也是相当重要的。如果没有高质量的数据，就不能很好地训练模型，这通常被称为“垃圾输入，垃圾输出”。</p><p id="ef09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在下次讨论这些问题。这里使用的所有代码都可以在git <a class="ae ky" href="https://github.com/yuki678/financial-phrase-bert" rel="noopener ugc nofollow" target="_blank"> repo </a>中获得。</p></div></div>    
</body>
</html>