<html>
<head>
<title>Multiple Linear Regression model using Python: Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的多元线性回归模型:机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiple-linear-regression-model-using-python-machine-learning-d00c78f1172a?source=collection_archive---------4-----------------------#2020-10-15">https://towardsdatascience.com/multiple-linear-regression-model-using-python-machine-learning-d00c78f1172a?source=collection_archive---------4-----------------------#2020-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c79a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习如何使用python中的Jupyter notebook在机器学习中构建基本的多元线性回归模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/28148c0733ffedd055d86aeff45ffd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6OCO74uqDaHrGdyYvfXRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/vectors/a-i-ai-anatomy-2729781/" rel="noopener ugc nofollow" target="_blank">图片由来自Pixabay的Gordon Johnson拍摄</a></p></figure><p id="a7e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归根据给定数据中的独立变量对目标变量执行回归任务。它是一种机器学习算法，通常用于查找目标和独立变量之间的关系。</p><p id="893e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> </strong> <a class="ae ky" rel="noopener" target="_blank" href="/simple-linear-regression-model-using-python-machine-learning-eab7924d18b4"> <strong class="lb iu">简单线性回归</strong> </a> <strong class="lb iu"> </strong>模型是用一个自变量来预测目标变量。</p><p id="ff3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当数据集中的一个变量/列不足以创建好的模型并做出更准确的预测时，我们将使用多元线性回归模型，而不是简单的线性回归模型。</p><p id="9081" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多元线性回归模型的直线方程为:</p><p id="d7d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">y = β0 + β1X1 + β2X2 + β3X3 + .... + βpXp + e</code></p><p id="a9b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在继续使用python构建模型之前，我们需要考虑一些事情:</p><ol class=""><li id="2bb1" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">添加更多的变量并不总是有用的，因为模型可能会“过度拟合”，而且会太复杂。经过训练的模型不会使用新数据进行归纳。它只对训练好的数据起作用。</li><li id="c833" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">数据集中的所有变量/列可能都不是独立的。这种情况称为<code class="fe lv lw lx ly b"><strong class="lb iu">multicollinearity</strong></code>，其中预测变量之间存在关联。</li><li id="3286" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">我们必须选择合适的变量来构建最佳模型。这个选择变量的过程叫做<code class="fe lv lw lx ly b"><strong class="lb iu">Feature selection</strong></code> <strong class="lb iu">。</strong></li></ol><p id="74d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用python代码讨论第2点和第3点。</p><p id="dfb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们深入到<code class="fe lv lw lx ly b">Jupyter notebook</code>中，看看我们如何构建Python模型。</p><h2 id="aee2" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">阅读和理解数据集</h2><p id="d67a" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们将数据读入我们的系统，了解数据是否有任何异常。</p><blockquote class="nl"><p id="a00a" class="nm nn it bd no np nq nr ns nt nu lu dk translated">对于本文的其余部分，我们将使用数据集，可以从这里的<a class="ae ky" href="https://github.com/Kaushik-Varma/mlr" rel="noopener ugc nofollow" target="_blank">下载。</a></p></blockquote><p id="2287" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">数据集中的目标变量/列是<code class="fe lv lw lx ly b">Price</code>。</p><p id="c5ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将导入必要的库来读取数据并将其转换成pandas数据帧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="2ffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">样本数据帧看起来像这样，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/0e35dcc230f804fa25e52fee52233a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNQeuhQpIG65QGSTlucZHw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-样本数据集</p></figure><p id="81bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用<code class="fe lv lw lx ly b">.info()</code>查看数据集中的任何空值，并且我们必须使用<code class="fe lv lw lx ly b">.describe()</code>检查任何异常值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="af56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/80298833f2be6ccdf5cf0c10ddb2c9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6K8lXrNKvxd6QuqV4usXcA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-检查空值和异常值</p></figure><p id="1365" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，数据中没有空值，数据中也没有异常值。</p><h2 id="7c8b" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">数据准备</h2><p id="5b1f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">如果我们观察数据集，会发现有数值和值为“是”或“否”的列。但是为了拟合回归线，我们需要数值，因此我们将“是”和“否”转换为1和0。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="0520" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看数据集，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ba545b8a4e8329980ff9a4c6c39b75af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*koHZWE1uDTEwoEubF4BTwA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像—将类别变量转换为数字变量</p></figure><p id="9a1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">furnishingstatus</code>列有三个级别<code class="fe lv lw lx ly b">furnished</code>、<code class="fe lv lw lx ly b">semi_furnished</code>和<code class="fe lv lw lx ly b">unfurnished</code>。</p><p id="267f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要将这个列转换成数字。为此，我们将使用<code class="fe lv lw lx ly b">dummy</code>变量。</p><p id="7fbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您有一个带有<code class="fe lv lw lx ly b">n-levels</code>的分类变量时，创建虚拟变量的想法是构建<code class="fe lv lw lx ly b">‘n-1’ variables</code>，指示级别。</p><p id="968b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在pandas中使用<code class="fe lv lw lx ly b">get_dummies</code>方法创建一个虚拟变量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="35b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看<code class="fe lv lw lx ly b">furnishstatus</code>列在数据集中是什么样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c24fe2874845a58f4c33087412f0f3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*wkocFoyTzlmTrteW00MPkg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片—提供状态<code class="fe lv lw lx ly b"> column into dataset</code></p></figure><p id="9a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们不需要三列。我们可以删除<code class="fe lv lw lx ly b">furnished</code>列，因为它可以用最后两列值来标识，其中:</p><ul class=""><li id="2fd6" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu og mf mg mh bi translated"><code class="fe lv lw lx ly b">00</code>将对应于<code class="fe lv lw lx ly b">furnished</code></li><li id="7c6f" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><code class="fe lv lw lx ly b">01</code>将对应于<code class="fe lv lw lx ly b">unfurnished</code></li><li id="7438" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><code class="fe lv lw lx ly b">10</code>将对应于<code class="fe lv lw lx ly b">semi-furnished</code></li></ul><p id="087a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们删除<code class="fe lv lw lx ly b">furnished</code>列，并将状态数据集添加到原始数据集中。之后，我们将从数据集中删除<code class="fe lv lw lx ly b">furnishstatus</code>列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="c352" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">修改后的数据集看起来像，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/7fa919900655c1aa747e8b12f0da630b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMm86sYneJ2Ik38i6lI84A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-添加虚拟变量后的样本数据集</p></figure><p id="83fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们建立模型。正如我们在<a class="ae ky" rel="noopener" target="_blank" href="/simple-linear-regression-model-using-python-machine-learning-eab7924d18b4"> <strong class="lb iu">简单线性回归</strong> </a>模型文章中看到的，第一步是将数据集拆分成训练和测试数据。</p><h2 id="ef9a" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">将数据分成两个不同的集合</h2><p id="5671" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们将以7:3的比例将数据分成两个数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="5704" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">重新缩放特征</h2><p id="84f7" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们可以看到，除了<code class="fe lv lw lx ly b">area</code>列之外，数据集中的所有列都具有较小的整数值。因此，重要的是重新调整变量，使它们都有一个可比较的范围。如果我们没有相对比例，那么与其他系数相比，一些回归模型系数将具有不同的单位。</p><p id="2867" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们使用<code class="fe lv lw lx ly b">MinMax</code>缩放方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="1168" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练数据集看起来像这样，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/978f22cd87e8fc9900d627325547c1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywv_z6qcPBI6E06e6Dg-gA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-重新缩放后的训练数据集</p></figure><h2 id="8a2a" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">构建线性模型</h2><p id="da3e" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在建立模型之前，我们需要将数据分为X和Y两个集合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="0cfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将除目标变量之外的变量添加到模型中。</p><p id="c566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">将所有变量添加到模型中</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f34f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的总结是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/517fb88fd19708aebee86fc896fba90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSX_gviYNM3zP7FocV1c2A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图片-模型摘要</p></figure><p id="83eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们观察一些变量的<code class="fe lv lw lx ly b">p-values</code>，这些值似乎很高，这意味着它们并不重要。这意味着我们可以从模型中去掉这些变量。</p><p id="d951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在丢弃变量之前，如上所述，我们必须看到变量之间的<code class="fe lv lw lx ly b">multicollinearity</code>。我们通过计算<strong class="lb iu"> VIF </strong>值来实现。</p><p id="169d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">方差膨胀因子</strong>或<strong class="lb iu"> VIF </strong>是一个量化值，表示特征变量之间的相关程度。这是检验我们的线性模型的一个极其重要的参数。<code class="fe lv lw lx ly b">VIF</code>的公式为:</p><p id="92a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">VIF = 1/(1-Ri²)</code></p><p id="43ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在python中，我们可以通过从<code class="fe lv lw lx ly b">statsmodels</code>导入<code class="fe lv lw lx ly b">variance_inflation_factor</code>来计算VIF值</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="fb7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每列的VIF值是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7c66567573d43a031bb98edce03c8f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*8jStfLWu92P9DsV62Bl1mg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图片-每个变量的VIF值</p></figure><p id="c4e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们认为变量通常有一个值&lt;5. If we observe the above image clearly, there are some variables we need to drop.</p><p id="0a33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">While dropping the variables, the first preference will go to the  【T7】 . Also, we have to drop one variable at a time.</p><p id="df5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">丢弃变量并更新模型</strong>从总结和VIF中可以看出，一些变量仍然是无关紧要的。其中一个变量是<code class="fe lv lw lx ly b">semi-furnished</code>，因为它具有非常高的p值<code class="fe lv lw lx ly b">0.938</code>。让我们继续下去，放弃这个变量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="9f79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新创建的模型的概要是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/4f1dc255b7fa1fe9c93e57ed47bdc3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TeIhohWp_LALrvlHn7aBQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片—删除<code class="fe lv lw lx ly b">semi-furnished</code>变量后的模型摘要</p></figure><p id="1b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们计算新模型的VIF值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="9764" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新模型的VIF值是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/06cb80b645fe61b4dbfd73d58d184afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*AnbqPAWx0bgqI7iTckXHWg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者提供的图片—丢弃半装配色谱柱后的新VIFs</p></figure><p id="75ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，变量<code class="fe lv lw lx ly b">bedroom</code>具有高VIF <code class="fe lv lw lx ly b">(6.6)</code>和p值<code class="fe lv lw lx ly b">(0.206)</code>。因此，它没有多大用处，应该从模型中删除。我们将重复和以前一样的过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="37aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型的总结，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/2b55c4251de43e83b2e651af994898b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ckoe7ibWKc4j2cCiM13MHQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图片-模型摘要</p></figure><p id="68c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是计算VIF，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="cded" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">VIF值如下所示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a38370dd1f29699a1c031e5e5788e7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*lizcV7LBLuI28TfSGxugpA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片-删除卧室栏后的VIF值</p></figure><p id="688f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将重复这个过程，直到每一列的<code class="fe lv lw lx ly b">p-value is &lt;0.005</code>和<code class="fe lv lw lx ly b">VIF is &lt;5</code></p><p id="5f33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个接一个地去掉所有必要的变量之后，最终的模型将会是，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f07b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终模型的概要看起来像，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/1de95176b1a2efd35f3419eb8a13d89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49WLSorR83DiL4hpA5QMjQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者提供的图片——删除所有必要变量后的最终模型</p></figure><p id="a79c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终模型的VIFs是，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="386d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">VIF值如下所示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/67c7d574522119fc7cf70f5fe671cefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*rzIE926Z6MJGVFU8zyWqzA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片—最终模型的VIF值</p></figure><p id="df58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，<code class="fe lv lw lx ly b">p-value</code>和<code class="fe lv lw lx ly b">VIF</code>都在可接受的范围内。现在是我们继续使用最终模型进行预测的时候了。这就是我们选择<code class="fe lv lw lx ly b">Feature variables</code>的方式，我们之前已经讨论过了。</p><p id="2cca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在进行预测之前，我们必须看看<code class="fe lv lw lx ly b">error terms</code>是否正态分布。我们将通过使用<code class="fe lv lw lx ly b">Residual Analysis</code>来做到这一点。</p><p id="6e54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">Error-terms = y_actual - y_predicted</code></p><p id="afe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际y值与使用该特定x值的模型预测的y值之间的差异就是误差项。</p><h2 id="7b60" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">训练数据的残差分析</h2><p id="ab2c" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们要检查误差项是否正态分布(这是线性回归的主要假设之一)；让我们画出误差项的直方图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="0755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直方图如下所示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0388582486afeb8f9568e7899f27dfab.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*q37SGzEcwdZ673wDHPGzUg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-错误术语直方图</p></figure><p id="ad5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，误差项非常接近正态分布。因此，我们可以继续使用测试数据集中的模型进行预测。</p><h2 id="10d4" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">使用最终模型进行预测</h2><p id="bd2a" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们拟合了模型并检验了误差项的正态性。让我们使用最终模型进行预测。</p><p id="68cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于训练数据集。首先，我们必须缩放测试数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="129c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试数据集看起来像这样，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/e7b0c7ee7061d44ef90ff8130ad3fabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2pPpuWjyNuKZocVnXHkBfg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-测试数据集</p></figure><p id="66eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将测试数据分为X和Y，之后，我们将根据我们的模型从测试数据中删除不必要的变量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="ebfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们必须看看最终预测的模型是否是最佳拟合的。为此，我们将计算预期测试模型的R值。</p><p id="b755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过从<code class="fe lv lw lx ly b">sklearn</code>导入<code class="fe lv lw lx ly b">r2_score</code>库来做到这一点</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="0fb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试数据的R值= 0.660134403021964，<br/>训练数据的R值= 0.667；我们可以从上面的最终模型总结中看到价值。</p><p id="deb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为训练和测试数据的R值几乎相等，所以我们建立的模型是最适合的模型。</p><p id="3219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一种建立多元线性回归模型的过程，我们手动选择和删除变量。还有一个过程叫做<strong class="lb iu">递归特征消除(RFE)。</strong></p><h2 id="391c" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">递归特征消除(RFE)</h2><p id="5144" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">RFE是一个自动过程，我们不需要手动选择变量。我们遵循与之前相同的步骤，直到<strong class="lb iu">重新缩放特征</strong>和<strong class="lb iu">将数据分成X和y。</strong></p><p id="2259" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用来自<strong class="lb iu"> </strong> <code class="fe lv lw lx ly b"><strong class="lb iu">sklearn</strong></code>的<code class="fe lv lw lx ly b"><strong class="lb iu">LinearRegression</strong></code> <strong class="lb iu"> </strong>函数<strong class="lb iu"> </strong>用于RFE(这是来自<code class="fe lv lw lx ly b">sklearn</code>的一个实用程序)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="1dd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须跑RFE。在代码中，我们必须提供RFE在构建模型时必须考虑的变量数量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="2f84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面代码的输出是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/5ff5e3d9464c817de9fe001967e07115.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*beUqIjiEVzyCjmaVenftDw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图像-所有变量的RFE值</p></figure><p id="e38b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，显示<code class="fe lv lw lx ly b">True</code>的变量对于模型是必不可少的，而<code class="fe lv lw lx ly b">False</code>变量是不需要的。如果我们想将<code class="fe lv lw lx ly b">False</code>变量添加到模型中，那么也有一个与它们相关联的等级来按照这个顺序添加变量。</p><h2 id="178a" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">建筑模型</h2><p id="7254" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">现在，我们使用<code class="fe lv lw lx ly b">statsmodel</code>建立模型，进行详细的统计。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="c0fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的总结是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/e198f7889b56502c33553a437ba31241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4b2i2BXxj28R7HFi025Bnw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片—初始模型摘要</p></figure><p id="0f7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<code class="fe lv lw lx ly b">bedrooms</code>列对于其他变量来说无关紧要，因此可以将其从模型中删除。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="d96f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">删除<code class="fe lv lw lx ly b">bedroom</code>变量后的模型摘要</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/34f2c7bb6918eed62d8ede0a3642e6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sLoqp2WXi3Cf0MkpfGFZA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片-删除卧室栏后的模型摘要</p></figure><p id="0fa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们计算模型的Vif。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="3075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述代码的VIF值是，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ce1f14001c18396a44054028904f2842.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*Q8IUUd_FGfs4lNmQBtIe1A.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片— VIF价值观</p></figure><p id="bd49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于p值和VIF在理想范围内，我们将继续进行分析。</p><p id="6ebe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是误差项的残差分析。</p><h2 id="448a" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">残差分析</h2><p id="bb17" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">因此，让我们使用直方图来检查误差项是否也呈正态分布。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="6a3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直方图看起来像，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e4ffdfb3f48d4468f0a468aa8d6d73df.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*MzCsP2cAJSKImW61dUPu1g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="65d3" class="mn mo it bd mp mq mr dn ms mt mu dp mv li mw mx my lm mz na nb lq nc nd ne nf bi translated">根据测试数据评估模型</h2><p id="33a5" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">对测试集应用缩放并将数据分成X和y。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f57d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，我们来评估模型，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="c7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试数据的R值= 0.6481740917926483，这与训练数据非常相似。</p><p id="a691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为训练和测试数据的R值几乎相等，所以我们建立的模型是最适合的模型。</p><h1 id="998d" class="oy mo it bd mp oz pa pb ms pc pd pe mv jz pf ka my kc pg kd nb kf ph kg ne pi bi translated">结论</h1><p id="fc98" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">我们在机器学习中手动建立了一个基本的多元线性回归模型，并使用了自动RFE方法。大多数时候，我们使用多元线性回归而不是简单的线性回归模型，因为目标变量总是依赖于多个变量。</p><p id="cad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，了解多元线性回归在机器学习中的工作方式至关重要，如果不知道简单的线性回归，理解多元线性回归模型是很有挑战性的。</p><p id="ef65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">感谢您阅读</strong>和<strong class="lb iu">快乐编码！！！</strong></p><h1 id="304d" class="oy mo it bd mp oz pa pb ms pc pd pe mv jz pf ka my kc pg kd nb kf ph kg ne pi bi translated">点击这里查看我以前的文章</h1><ul class=""><li id="f5b2" class="lz ma it lb b lc ng lf nh li pj lm pk lq pl lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/simple-linear-regression-model-using-python-machine-learning-eab7924d18b4"> <strong class="lb iu">使用Python的简单线性回归模型:机器学习</strong> </a></li><li id="8339" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-model-machine-learning-9853450c8bce"> <strong class="lb iu">线性回归模型:机器学习</strong> </a></li><li id="7e5b" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/exploratory-data-analysis-eda-python-87178e35b14"> <strong class="lb iu">探索性数据分析(EDA): Python </strong> </a></li><li id="5b33" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/central-limit-theorem-clt-data-science-19c442332a32"> <strong class="lb iu">中心极限定理(CLT):数据科学</strong> </a></li><li id="a826" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/inferential-statistics-data-analysis-e59adc75c6eb"> <strong class="lb iu">推断统计:数据分析</strong> </a></li><li id="4639" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/seaborn-python-8563c3d0ad41"><strong class="lb iu">Seaborn:Python</strong>T27】</a></li><li id="06cb" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" href="https://levelup.gitconnected.com/pandas-python-e69f4829fee1" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">熊猫:蟒蛇</strong> </a></li><li id="45fb" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" href="https://levelup.gitconnected.com/matplotlib-python-ecc7ba303848" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Matplotlib:Python</strong></a></li><li id="1134" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><a class="ae ky" href="https://medium.com/coderbyte/numpy-python-f8c8f2bbd13e" rel="noopener"> <strong class="lb iu"> NumPy: Python </strong> </a></li></ul><h1 id="16d8" class="oy mo it bd mp oz pa pb ms pc pd pe mv jz pf ka my kc pg kd nb kf ph kg ne pi bi translated">参考</h1><ul class=""><li id="4868" class="lz ma it lb b lc ng lf nh li pj lm pk lq pl lu og mf mg mh bi translated"><strong class="lb iu">多元线性回归:</strong>【https://acadgild.com/blog/multiple-linear-regression】T42</li><li id="9c86" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><strong class="lb iu"> ML |多元线性回归:</strong><a class="ae ky" href="https://www.geeksforgeeks.org/ml-multiple-linear-regression-using-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/ML-Multiple-Linear-Regression-using-python/</a></li><li id="95ea" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><strong class="lb iu">多元线性回归:</strong><a class="ae ky" href="https://www.coursera.org/lecture/machine-learning-with-python/multiple-linear-regression-0y8Cq" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/lecture/machine-learning-with-python/Multiple-Linear-Regression-0y8Cq</a></li><li id="e540" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu og mf mg mh bi translated"><strong class="lb iu">多元线性回归:</strong><a class="ae ky" href="https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_multiple_linear_regression.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/machine _ learning _ with _ python/machine _ learning _ with _ python _ Multiple _ Linear _ Regression . htm</a></li></ul></div></div>    
</body>
</html>