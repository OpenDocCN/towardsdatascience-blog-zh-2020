<html>
<head>
<title>How to turn Text into Features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将文本转化为特征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-turn-text-into-features-478b57632e99?source=collection_archive---------1-----------------------#2020-11-09">https://towardsdatascience.com/how-to-turn-text-into-features-478b57632e99?source=collection_archive---------1-----------------------#2020-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7229" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="3c87" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用NLP进行机器学习的综合指南</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/178ac6e9c2cf85af8a00575e04cdd219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*9mTcKmJdqP3-olFLwyUFjw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">简单问题:如何把文字变成特征？作者图片</p></figure><p id="1ba0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">假设你被分配了一项任务，为你的公司产品评论建立一个情感分析工具。作为一名经验丰富的数据科学家，您构建了许多关于未来销售预测的见解，甚至能够根据客户的购买行为对他们进行分类。</p><p id="c7d2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是现在，你很感兴趣:你有一堆文本条目，必须将它们转化为机器学习模型的特征。如何做到这一点？当数据科学家第一次遇到文本时，这是一个常见的问题。</p><p id="69a3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于有经验的NLP数据科学家来说，将文本转换成特征可能看起来很简单，但对于该领域的新手来说，这并不容易。这篇文章的目的是提供一个将文本转化为特征的指南，作为我在过去几个月中构建的NLP系列的继续(我知道，距离上一篇文章已经有一段时间了)。</p><p id="ab6c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">之前，我已经讨论了NLP预处理管道中的几个步骤。现在，<strong class="lc ja">这是预处理管道</strong>的最后一步，当你精心策划的文本最终变成机器学习模型的可用特征时(如果这是你的目的——没有机器学习也可以有NLP，这是另一个话题)。</p><p id="8e80" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">像往常一样，除了解释如何做之外，我还将介绍三种最常用的建模技术:单词袋模型、基于TF-IDF算法的模型和Word2Vec模型。让我们开始讨论吧。</p><h1 id="1357" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">特色？</h1><p id="b241" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">对于那些不习惯这个词的人，让我稍微离题一下。特征是给予<strong class="lc ja">选择或处理的数据</strong>的名称，准备用作算法(通常是机器学习算法)的输入。特征可以是房子的价格、像素的RGB值，或者在我们的例子中是单词的表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d17b69093ffc6182b9548ea8ed33bf30.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*u4h9GqHMY1bwetBGLfdJTw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">来自<a class="ae mv" href="https://developers.google.com/machine-learning/crash-course/representation/feature-engineering" rel="noopener ugc nofollow" target="_blank">谷歌开发者机器学习速成班</a>的漂亮图片描述了什么是特性。</p></figure><p id="0d30" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">甚至有一种叫做“特征工程”的很酷的技能，数据科学家研究数据，从数据中获得特征。这些特征甚至可能在数据中不明显，但可以通过修改现有数据或添加新数据使其更完整来获得，从而有助于实现更稳健的决策。</p><p id="37fa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，最终，我们的目标是获得原始数据(文本)并转化为特征(计算机算法可以处理的东西)。</p><h1 id="4a6c" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">文本矢量化</h1><p id="a4e1" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">用于将文本转换成<strong class="lc ja">特征</strong>的技术可以被称为“<em class="mt">文本矢量化</em>技术，因为它们都旨在一个目的:将文本转换成向量(或者数组，如果你想更简单的话；或者张量，如果你想要更复杂的话)，然后可以以经典的方式输入到机器学习模型中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/4ff3e2753d85b84ce5e1cbdf6e7c8298.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vNrZY9WPEcAODpJbFgJV5Q.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">文本矢量化旨在将文本转换为整数(或布尔或浮点数)向量。图片作者。</p></figure><p id="a5a2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">把我们的预期结果想成一个<strong class="lc ja">向量</strong>是一个很好的起点，来想象我们如何把文本变成特征。让我们对此进行更多的思考。考虑下面的短语:</p><blockquote class="mx my mz"><p id="52a4" class="la lb mt lc b ld le ka lf lg lh kd li na lk ll lm nb lo lp lq nc ls lt lu lv ij bi translated">我想把我的文本转换成数据。</p></blockquote><p id="9037" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">用简单的计算术语来说，向量是具有<strong class="lc ja"> <em class="mt"> n </em> </strong>个位置的列表。考虑如何将文本转换为向量的一种自然方法是创建一个所有单词的有序列表，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/87ccf4fdc3b77f67a20df9202aca5d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rou4NteMNnJfGE_KpZ7lfA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片作者。</p></figure><p id="adef" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是，如果条目更短或更大，会发生什么情况呢？此外，你的机器学习算法(基本上是一系列矩阵和向量计算)如何比较两个单词——这些由人类发明的具有特殊含义的符号？</p><p id="d9c3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">既然你很聪明，你已经想到了:让我做一个字典或者一些类似的结构(一般来说，一个词汇图)，用单词索引代替单词！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/21ffefc4977addc81ba706a0e49019ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*sEbUj8Fispi0Ltz0j0y9yw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">一个假设的解决方案是为每个单词创建一个映射……作者的图片。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/4f1b9fc683fb6ffd370c554d6d0e7aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJQKQJyR9X1I0wSaqvfj0g.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">然后用单词索引对向量进行编码。图片作者。</p></figure><p id="8209" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你在正确的道路上，但是让我们考虑这里的一些问题:单词“我的”比“想要的”更重要吗？对于机器学习算法来说，这就是你的数据“告诉”的。机器学习算法不关心数字是否是一个索引，只关心它是一个要计算的值(当然，你可以有一个分类特征，但我们会在下面看到)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/47b348190e1bbe9dd22b19dc3f9fff7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*8Tl8DhIMvwjXoQreg0_vBw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">不要吼，言语不是价值观！图片来自海绵宝宝电视节目，由作者编辑。</p></figure><p id="f2a7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">“正常化，正常化，正常化！！!"，可以认为。但是，记住:这些都不是价值观！它们是指数。</p><p id="aba9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你玩数据科学和机器学习已经够久了，你可能会想到<em class="mt"> one </em>解决方案:使用<em class="mt"> One Hot Encoding </em>。</p><p id="e48b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">一个热编码</strong>是对分类特征进行编码的过程，其中该特征的每个可能值被映射到一个新的“列”,如果存在，则该列被设置为1，否则为0。</p><p id="f598" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们使用前面提到的词汇表“map”和建议的短语来考虑这一点(在这种情况下，词汇表与短语的单词相同)。我们得到这个:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/07eade824feb95574c0e2f45f6d48eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZynDy20IjKmSMirxps3yQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">“我想把我的文本转换成数据”的一键编码。图片作者。</p></figure><p id="29cc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，如果我们想编码:“我想要我的数据”，我们会得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/b8f1d7661a3b9b244897b64ad7d36503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PWCCIgvrg7PsdhXNV_d1Gg.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">“我要我的数据”的一键编码。图片作者。</p></figure><p id="3cb1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">太好了，我们找到了一种将我们的数据编码成机器学习方式的方法！但是这里有很多问题需要解决:让我们考虑第一个问题——<strong class="lc ja">词频——</strong>这就是<strong class="lc ja">词汇袋</strong>模型的用武之地！</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="b092" class="lw lx iq bd ly lz ns mb mc md nt mf mg kf nu kg mi ki nv kj mk kl nw km mm mn bi translated">单词袋(蝴蝶结)模型</h1><p id="1bd4" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">一种热编码只将值视为“存在”和“不存在”。这不适合发短信。在许多文本应用中，词频起着重要的作用。考虑以下两段:</p><blockquote class="mx my mz"><p id="2050" class="la lb mt lc b ld le ka lf lg lh kd li na lk ll lm nb lo lp lq nc ls lt lu lv ij bi translated">狗是犬科的家养食肉动物。它是像狼一样的犬科动物的一部分，是最广泛丰富的陆地食肉动物。狗和大灰狼是姐妹分类群，因为现代狼与最初被驯化的狼没有密切关系，这意味着狗的直接祖先已经灭绝。狗是第一个被驯化的物种，几千年来因为各种行为、感觉能力和身体特征而被选择性繁殖。</p><p id="18ae" class="la lb mt lc b ld le ka lf lg lh kd li na lk ll lm nb lo lp lq nc ls lt lu lv ij bi translated">今天我带着我的狗出去，我在公园里发现了100美元。想到这些钱可能是某个可怜的老太太一周的午餐钱，我很难过。</p></blockquote><p id="547d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">第一个是维基百科关于狗的文章的第一段，第二个是我为了证明这个问题而写的一个伪博客。问题:你会向一个用“狗”这个词搜索假想引擎的用户推荐哪个网页？</p><p id="8c84" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一个热编码将在两个条目中为“dog”给出相同的值，所以这不好。</p><p id="3402" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="mt">进入词汇袋模式！</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8185633b2295031e72a978150e6ea388.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*Zb7ee86S7FqjVXwXEy1hww.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">一袋单词😜。图片作者。</p></figure><p id="a515" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个模型建议，<strong class="lc ja">使用一个词频矢量</strong>，而不是一个布尔值矢量。在上面的示例中，单词“dog”的列在第一个文本中将接收值“4 ”,而在第二个文本中仅接收值“1”。现在，规范化这些值是可以的，但不是必须的(只是为了更快的处理)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8c472db65928d087ee28411b741c55fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*2YDpxvAbkqyR6qQzBbpu2g.jpeg"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">一个乱七八糟的包！图片由Pinterest上的<a class="ae mv" href="https://br.pinterest.com/pin/348747564875387277/" rel="noopener ugc nofollow" target="_blank"> Cocoon Innovations </a>提供。</p></figure><p id="476f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这种模型被称为“包”，因为它不保持单词“秩序”(就像我们妈妈在90年代的包总是乱七八糟)。</p><p id="7443" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是在讨论单词袋模型的缺点之前，我们先来看看如何使用Python和Numpy实现它。</p><p id="f063" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这很简单，但是让我们一步一步来看。我决定让它成为一个类，这样我就可以在一个模型中实例化多个BoW。这是类构造函数:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="d0dc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">基本上，我们有一组所有单词和几个字典来存储单词索引。</p><p id="b7cb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">接下来，我们通过<em class="mt">拟合</em>来准备我们的单词包(一次添加所有的文档，所以它可以‘学习’我们可以使用的单词)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="2728" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后，我们可以<em class="mt">转换</em>新的输入，返回整个词汇表大小的数组，并计算该单词出现的次数。为此，我使用numpy，这是一个数学和代数库，专门研究向量/矩阵代数。这是python目前针对这类任务的默认库(并被用作大多数机器学习库的输入格式)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="b4c7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是一个用法和输出示例:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="8fde" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在我们已经看到了它是如何完成的，我们可以讨论模型中的问题了:</p><p id="4381" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">第一个</strong>:完全无视语序。您可以通过查看下图来理解这一点:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ob"><img src="../Images/259ee662bf569e8d6a4302a47b4c5ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JmhhcXRTeE6b4J57zy3czA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片作者。</p></figure><p id="e6ae" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ja">第二</strong>:趋于<strong class="lc ja">非常高维度</strong>。根据<a class="ae mv" href="https://web.archive.org/web/20120104170705/http://oxforddictionaries.com/words/the-oec-facts-about-the-language" rel="noopener ugc nofollow" target="_blank">牛津英语语料库</a>，如果你的语料库只包含90%最常见的英语单词至少一次，那将产生至少7000维的向量(其中大部分为零，但仍然是很多维)！</p><p id="488b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是弓很简单，能为简单的问题带来快速的结果。如果在构建NLP解决方案时，您不确定从哪里开始，请尝试最基本的方法:使用BoW模型。它是一个很好的基线估计器。</p><p id="c4bc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">仅指出一点，有一个变体在<strong class="lc ja">大数据案例</strong>中是有用的，特别是在您必须在字符库中比较文本的情况下(不考虑语义)。该方法使用文本瓦片区(<em class="mt">收缩</em>)。在这种情况下，不要将句子拆分成单词，而是在每k个字符或每个停用词处进行拆分。点击此处阅读更多内容:</p><div class="oc od gp gr oe of"><a href="https://nlp.stanford.edu/IR-book/html/htmledition/near-duplicates-and-shingling-1.html" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ja gy z fp ok fr fs ol fu fw iz bi translated">近似重复和重叠</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">下一篇:参考资料和进一步阅读:网络搜索基础知识上一篇:索引大小和估计内容索引一…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">nlp.stanford.edu</p></div></div><div class="oo l"><div class="op l oq or os oo ot ku of"/></div></div></a></div></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="2f48" class="lw lx iq bd ly lz ns mb mc md nt mf mg kf nu kg mi ki nv kj mk kl nw km mm mn bi translated">TF-IDF模型</h1><p id="c401" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">这个<em class="mt">实际上并不是一个模型</em>，而是对计算单词相对于文档的“相关性”的改进。为简单起见，我称之为模型。</p><p id="9da7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在TF-IDF模型中，我们不是存储单词的频率，而是存储对输入数据执行<em class="mt"> tf-idf算法</em>的结果。<strong class="lc ja"> TF </strong>代表<strong class="lc ja">词频</strong>，<strong class="lc ja"> IDF </strong>代表<strong class="lc ja">逆文档频率</strong>。</p><p id="61ee" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">简而言之，TF-IDF计算一个词在特定文档中的权重，同时考虑词的总体分布。结果的呈现方式与单词包相同:一个稀疏的向量(0表示没有出现的单词，否则有些单词会浮动)。</p><p id="2d69" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">例如，在情感分析任务中，与单词袋相比，TF-IDF模型可以返回更好的结果。</p><p id="6295" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">为了更好地理解，让我们把这个算法分成两部分:计算全局逆文档频率，然后计算单独的TF-IDF分数。</p><h2 id="c981" class="ou lx iq bd ly ov ow dn mc ox oy dp mg lj oz pa mi ln pb pc mk lr pd pe mm iw bi translated">逆文档频率</h2><p id="aec0" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">第一部分是计算每个单词的全局IDF值。这个值表示每个单词在所有文档中的权重(在整个语料库中)。这意味着<strong class="lc ja">非常常见的单词将具有较小的总权重</strong>，而罕见的单词将具有较大的权重(这甚至消除了去除停用词的需要，因为它们非常常见，所以权重较小)。</p><p id="73da" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">有许多不同的方法来计算TF和IDF(例如，看看算法<a class="ae mv" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">维基百科页面</a>)。我选择使用对数标度的IDF，其计算公式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/fdd7c7477932bc7ca632786a03df511a.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/1*1LUFzrkSliUTiLU4x9WM3Q.gif"/></div></figure><p id="9096" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中:</p><ul class=""><li id="37c1" class="pg ph iq lc b ld le lg lh lj pi ln pj lr pk lv pl pm pn po bi translated">D =文件总数</li><li id="2972" class="pg ph iq lc b ld pp lg pq lj pr ln ps lr pt lv pl pm pn po bi translated">freq =出现该术语的文档数。</li></ul><p id="e752" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">首先，我们定义一个助手方法:</p><p id="237a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">1)将句子转换成单词和频率的字典的简单方法(可以使用Python集合“Counter”来获得最佳性能，但为了简单起见，我将使用旧的字典方法):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="b7df" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后我们初始化IDF类，类似于BoW类:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="03a4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于拟合，我们根据上面提到的公式计算全局词频率，然后是每个词的idf。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="16ee" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">太好了，我们的文档有了全球IDF。现在，通过找到每个句子中每个术语的术语频率分数并乘以全局术语IDF，为每个句子计算TF-IDF。</p><h2 id="7985" class="ou lx iq bd ly ov ow dn mc ox oy dp mg lj oz pa mi ln pb pc mk lr pd pe mm iw bi translated">计算词频</h2><p id="3ba4" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">简而言之，我们通过以下方式获得一个句子的TF-IDF得分:</p><ol class=""><li id="0b4a" class="pg ph iq lc b ld le lg lh lj pi ln pj lr pk lv pu pm pn po bi translated">统计句子中术语的出现频率。</li><li id="b886" class="pg ph iq lc b ld pp lg pq lj pr ln ps lr pt lv pu pm pn po bi translated">将每个术语的术语频率乘以逆文档频率。</li></ol><p id="5772" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">下面是代码(注意，我添加了一些条件和私有方法来说明批量转换):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="da13" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">就这样，我们的TF-IDF <em class="mt">特征化</em>完成了！</p><p id="e7bb" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">现在，对于每一个句子，我们得到一个数组，它的大小相当于整个词汇表的大小，包含每个单词与该句子的相关性(如果缺少，则为0)。</p><p id="ca12" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">下面是一个输出示例:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="cd24" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">或者更“可读”一点:</p><pre class="kp kq kr ks gt pv pw px py aw pz bi"><span id="cb84" class="ou lx iq pw b gy qa qb l qc qd">{‘of’: 0.0, ‘list’: 0.0, ‘sentence’: 0.08664339756999316, ‘a’: 0.07192051811294521, ‘second’: 0.0, ‘sentences’: 0.0, ‘in’: 0.0, ‘complexity’: 0.0, ‘is’: 0.08664339756999316, ‘this’: 0.08664339756999316, ‘word’: 0.08664339756999316, ‘for’: 0.0}</span></pre><p id="216b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你可以在这里找到用于生成上述‘解释’的代码。</p><p id="8dcc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是到目前为止实现的提交，请注意代码中会有一些不同，因为我也将使用我的工具集中内置的一些结构。：</p><div class="oc od gp gr oe of"><a href="https://github.com/Sirsirious/NLPTools/tree/b48910b1db6e0de463b1713fcb65521df98e58e6" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ja gy z fp ok fr fs ol fu fw iz bi translated">各种/各种工具</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">github.com</p></div></div><div class="oo l"><div class="qe l oq or os oo ot ku of"/></div></div></a></div><h2 id="1d3e" class="ou lx iq bd ly ov ow dn mc ox oy dp mg lj oz pa mi ln pb pc mk lr pd pe mm iw bi translated">关于BoW和TF-IDF的一些讨论:</h2><p id="2a57" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">正如我们所看到的，BoW和TF-IDF方法为一个句子生成一个向量，其大小相当于整个词汇表。这不考虑词序或位置，使得这些技术不适合连续性敏感的应用程序(大多数NLP应用程序都是如此)。</p><p id="edbf" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">一种可能的解决方法是对每个单词使用BoW/TFIDF数组，将它们堆叠起来并作为特征传递，如下图所示(以人工神经网络的输入层为例):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/42e0cc50ba8c6c7dbb48fdef411186a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*q3E9XBYDWdbHlH89LY6Ycg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片作者。</p></figure><p id="b7f2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">对于上面的图像，你有一个10000*4的稀疏矩阵，用于一个4个单词的句子(即使每个布尔使用一个位，你会得到大约1kb每个单词每个句子！想象一个大型语料库？).用于训练简单情感分析模型的计算时间和存储将使其过于昂贵或不可行(事实上，几年前，由于缺乏足够的内存和处理能力，当问题是机器学习时，文本几乎是一个未触及的话题)。</p><p id="36c4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然而，这种方法允许我们保持单词顺序。如果我们能降低这个向量的维数呢？</p><p id="baf2" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">输入单词嵌入！</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="e1fc" class="lw lx iq bd ly lz ns mb mc md nt mf mg kf nu kg mi ki nv kj mk kl nw km mm mn bi translated">单词嵌入</h1><p id="b01e" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">我不会深入解释单词嵌入，因为有几种计算它们的方法，并且大多数涉及深度神经网络，这本身需要时间来解释(这不是本文的重点)。</p><p id="d02d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但我不会克制自己给你关于它的基本和最重要的信息。</p><p id="b99e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们这样陈述:</p><blockquote class="qf"><p id="c00d" class="qg qh iq bd qi qj qk ql qm qn qo lv dk translated">单词嵌入是从上下文训练中学习到的单词的向量表示。它不是每个单词的分数，相反，它更像是单词的“坐标”。</p></blockquote><p id="bfd0" class="pw-post-body-paragraph la lb iq lc b ld qp ka lf lg qq kd li lj qr ll lm ln qs lp lq lr qt lt lu lv ij bi translated">因此，当训练一个模型而不是词汇大小的一次性编码时，您需要输入一个表示输入的单词嵌入数组。这个数组有一个预定义的<em class="mt"> d </em>维深度，它通常比词汇表的大小小得多。</p><p id="35f3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最著名的生成单词嵌入的技术之一是Word2Vec，它是该方法的创始人。Word2Vec本身可以使用两种不同的技术来计算，但是细节在这里并不重要。</p><p id="0088" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">相反，您最好知道训练/使用嵌入的方式有多种变化。这里总结了用于创建嵌入的技术:</p><ul class=""><li id="159a" class="pg ph iq lc b ld le lg lh lj pi ln pj lr pk lv pl pm pn po bi translated">Word2vec (Google) - 2技术:连续词包(CBoW)和Skip-Gram；</li><li id="21b4" class="pg ph iq lc b ld pp lg pq lj pr ln ps lr pt lv pl pm pn po bi translated">全局向量或手套(斯坦福)；</li><li id="f7f3" class="pg ph iq lc b ld pp lg pq lj pr ln ps lr pt lv pl pm pn po bi translated">快速文本(脸书)——有趣的事实:解释了词汇之外的单词。</li></ul><p id="f8d0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你想了解更多，我推荐这个来自DeepLearning的神奇课程。艾出席Coursera:【https://www.coursera.org/learn/probabilistic-models-in-nlp】T2</p><p id="a55e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">生成单词嵌入的方式<strong class="lc ja">通常通过训练过程</strong>，允许单词根据其上下文进行编码。这导致了嵌入也能够表示单词语义(在一定程度上)的有趣效果。</p><p id="d0ed" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因为单词是用坐标表示的，所以要进行比较(比较相似性)。如果使用诸如<a class="ae mv" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA)</a>之类的技术来适当地降低维度，则可以绘制单词，并且绘制通常会将具有相似含义的<strong class="lc ja">单词更紧密地显示在一起</strong>，如下图所示，摘自<a class="ae mv" href="https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/" rel="noopener ugc nofollow" target="_blank"> IBM研究博客</a>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi qu"><img src="../Images/3ef6f742b25549811deeab0f420a085b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fWhKumq1KRuKMr0L.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">单词嵌入能够在一定程度上捕捉单词语义。句子甚至可以用嵌入来比较。图片由<a class="ae mv" href="https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/" rel="noopener ugc nofollow" target="_blank"> IBM Research的编辑人员提供。</a></p></figure><p id="400e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">坐标通常以大量的维度给出，通常在8到1024之间。这样，我们就有了一个8到1024维的非稀疏数组，而不是一堆10000维的填充0的数组。这更适合计算机使用。</p><p id="84d6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这是另一个很酷的例子，摘自大卫·罗扎多在《公共科学图书馆综合》杂志上的一篇文章:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi qv"><img src="../Images/2f85f63a68bfe31ede438a1f98649f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*trIFGxQI0BWA0QS0"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">大卫·罗扎多拍摄。</p></figure><p id="d905" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当考虑人们如何使用嵌入时，重要的是指出单词嵌入可以通过两种不同的方式产生:</p><ol class=""><li id="827c" class="pg ph iq lc b ld le lg lh lj pi ln pj lr pk lv pu pm pn po bi translated">通过与正在进行的过程(如情感分析任务)一起接受训练，并根据该特定任务捕捉单词之间的关系。这就是当您使用诸如<a class="ae mv" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja"> Keras </strong> </a> <strong class="lc ja">、</strong> <a class="ae mv" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lc ja"> Pytorch </strong> </a> <strong class="lc ja">或Trax </strong>等库来定义嵌入层时会发生的情况。这些层将单词索引映射到嵌入值。</li><li id="f43f" class="pg ph iq lc b ld pp lg pq lj pr ln ps lr pt lv pu pm pn po bi translated">通过在一个巨大的语料库上进行预处理来捕捉语言中最常见的关系。然后，可以将这些经过预训练的嵌入加载到模型中，以帮助加快学习过程或获得更好的结果。这就是fastText、Word2vec或GloVe的用武之地。诸如<a class="ae mv" href="https://spacy.io/usage/vectors-similarity" rel="noopener ugc nofollow" target="_blank"><strong class="lc ja">spaCy</strong></a><strong class="lc ja"/>等库提供了这种嵌入用法。</li></ol><p id="5d0d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我不会在这里讨论如何预训练一个嵌入，但是我准备了一个专门的帖子来解释这个实践。我建议你订阅我的帐户，以便在文章发表时得到通知，或者留意这一段，因为当它准备好时，我会在它下面发表一个链接。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="114d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">关于这个主题再补充一点，当把文本转换成特征时，也可以进行特征工程。换句话说，数据科学家可以应用自己的规则(通常通过<a class="ae mv" href="https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f" rel="noopener">预处理</a>)来定义在将文本转换成数字数组之前应该提取什么。</p><p id="7159" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">正如我们所看到的，将文本转换成功能看起来可能是一件简单而琐碎的事情，但是需要考虑很多事情。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/1094b8a4617bcbecde1f44086e02ae03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*DWLHqfd657M8v-IVwMzrcA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">在被输入编码器之前，单词索引被映射到它们各自的嵌入，这是在训练过程中使用的真实值。图片取自<a class="ae mv" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener ugc nofollow" target="_blank">py torch seq seq车型教程</a>。</p></figure><p id="36a8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">目前，NLP中的大多数现有技术仅使用单词嵌入，因为它们更健壮并且能够以顺序方式使用。</p><p id="189b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">当然，每个算法和问题都需要对输入文本进行特定的操作。例如，Seq2Seq模型通常有一个固定的序列</p><p id="b6a8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">长度。为了加强这一长度，使用了填充或压缩。但最后，填充和文字都转换成了嵌入。</p><p id="548a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">既然我们已经完成了NLP预处理中最基本的主题，我们可以开始讨论应用程序和技术了。别忘了订阅敬请关注，收到下一个话题的通知！</p><p id="73e4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">此外，请经常查看我的存储库中的新代码。根据当前的许可，随意重用和贡献代码。如果你需要帮助，可以通过Linkedin联系我，只要查看我的个人资料就可以了！下一篇文章再见。</p><div class="oc od gp gr oe of"><a href="https://github.com/Sirsirious/NLPTools" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ja gy z fp ok fr fs ol fu fw iz bi translated">各种/各种工具</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">github.com</p></div></div><div class="oo l"><div class="qx l oq or os oo ot ku of"/></div></div></a></div></div></div>    
</body>
</html>