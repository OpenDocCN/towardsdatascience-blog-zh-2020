<html>
<head>
<title>Supervised Machine Learning Technique for Anomaly Detection: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于异常检测的监督机器学习技术:逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-machine-learning-technique-for-anomaly-detection-logistic-regression-97fc7a9cacd4?source=collection_archive---------12-----------------------#2020-11-01">https://towardsdatascience.com/supervised-machine-learning-technique-for-anomaly-detection-logistic-regression-97fc7a9cacd4?source=collection_archive---------12-----------------------#2020-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4393" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于异常检测的小型数据科学文章</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9fef90ea728792941ee5abc26e880e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XMW3X0hxAGfzTn_Q"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">本·威克斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="9e6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我写的关于异常检测算法的系列文章的第11篇(也是最后一篇)。以下是之前的10篇文章，如果您想了解的话，每篇文章都侧重于不同的异常检测算法:</p><ul class=""><li id="939a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/isolation-forest-a-tree-based-algorithm-for-anomaly-detection-4a1669f9b782"> 1。隔离林</a></li><li id="4242" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/k-nearest-neighbors-knn-for-anomaly-detection-fdf8ee160d13"> 2。k-最近邻(kNN) </a></li><li id="d382" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-svm-for-anomaly-detection-73a8d676c331"> 3。支持向量机(SVM) </a></li><li id="2119" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/dbscan-a-density-based-unsupervised-algorithm-for-fraud-detection-887c0f1016e9"> 4。无监督算法</a></li><li id="98f9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/machine-learning-for-anomaly-detection-elliptic-envelope-2c90528df0a6"> 5。椭圆形信封</a></li><li id="b28c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/anomaly-detection-with-local-outlier-factor-lof-d91e41df10f2"> 6。局部异常值因子(LOF) </a></li><li id="ec53" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/z-score-for-anomaly-detection-d98b0006f510"> 7。z分数</a></li><li id="3a72" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/boxplot-for-anomaly-detection-9eac783382fd"> 8。箱线图</a></li><li id="64af" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">9。统计技术</li><li id="02e2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">10。时间序列异常检测</li></ul><p id="f573" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仔细观察，你会发现所有这些算法要么是统计的，要么是无监督的ML技术。无监督的，但许多这些算法实际上是围绕监督分类理论(如kNN，SVM或DBSCAN)建立的。今天，我将采用一种“纯粹的”机器学习方法来进行异常检测，这意味着数据集将具有0和1标签，分别代表异常和非异常。</p><p id="c056" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么为什么监督分类在这个领域如此晦涩难懂呢？这背后有几个原因，但一个关键原因是严重的类别不平衡，这意味着只有一小部分数据代表异常。异常是罕见的事件，找到它们就像大海捞针。对于监督算法来说，这是一个很大的障碍，因为没有足够的例子可以学习！</p><p id="233e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，下面我将使用一种流行的算法——逻辑回归，用公开可用的数据编写一个监督分类的实现。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="59a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在以前的文章中，我通常动态地创建一个小的合成数据集，并用最少的代码实现算法，以给出它们如何工作的直觉。今天会有所不同，因为这是一个监督分类问题，我必须遵循所有的基本步骤。正如您将看到的，光是数据准备就要占用相当多的空间。</p><h1 id="b6ef" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">步骤1:导入库</h1><p id="6fb1" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们需要相当多的库来进行数据辩论、准备模型输入、模型构建和验证——所有的库都来自三个大的包:<code class="fe nn no np nq b">pandas</code>、<code class="fe nn no np nq b">nunpy</code>和<code class="fe nn no np nq b">sklearn</code>。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="2532" class="nv mr it nq b gy nw nx l ny nz"># data wrangling<br/>import pandas as pd<br/>import numpy as np</span><span id="5848" class="nv mr it nq b gy oa nx l ny nz"># inputs data preparation<br/>from sklearn.preprocessing import RobustScaler<br/>from sklearn.model_selection import train_test_split</span><span id="0b21" class="nv mr it nq b gy oa nx l ny nz"># modeling<br/>from sklearn.linear_model import LogisticRegression</span><span id="36dc" class="nv mr it nq b gy oa nx l ny nz"># model validation<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix</span></pre><h1 id="ff86" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">第二步:数据争论</h1><p id="b077" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我正在使用一个来自<a class="ae ky" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的关于信用卡欺诈检测的流行数据集。欧洲持卡人在删除任何用户标识符后，在公共领域发布了该数据集。让我们导入并查看一下数据集，看看特征。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="a16f" class="nv mr it nq b gy nw nx l ny nz"># import data<br/>df = pd.read_csv("..\creditcard.csv")</span><span id="3962" class="nv mr it nq b gy oa nx l ny nz"># view the column names<br/>df.columns</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2dd8b501422f87638c65dd841fd34068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*Mg-xP9wBsXpLT2krgpmUnQ.png"/></div></figure><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="dd8b" class="nv mr it nq b gy nw nx l ny nz"># view the column names<br/>df.columns</span><span id="5716" class="nv mr it nq b gy oa nx l ny nz">&gt;&gt; Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']</span></pre><p id="4ea0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集有31列。第一列“时间”是交易时间戳，倒数第二列“金额”是交易金额，最后一列“类别”表示交易是否欺诈(欺诈= 1，非欺诈= 0)。其余的列，“V1”到“V28”是未知的特征，并且值被缩放。</p><p id="29ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在看看欺诈和非欺诈案例的数量。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="846f" class="nv mr it nq b gy nw nx l ny nz"># number of fraud and non-fraud observations <br/>frauds = len(df[df.Class == 1])<br/>nonfrauds = len(df[df.Class == 0])</span><span id="f8a2" class="nv mr it nq b gy oa nx l ny nz">print("Frauds", frauds); print("Non-frauds", nonfrauds)</span><span id="b9ca" class="nv mr it nq b gy oa nx l ny nz">&gt;&gt; Frauds 492<br/>&gt;&gt; Non-frauds 284315</span></pre><p id="28dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我所说，这是一个相当不平衡的数据集，在25万个观察中只有492个欺诈案例。这只占所有交易中欺诈案件的0.17%。</p><h1 id="2175" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">步骤3:输入数据准备</h1><p id="f03a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">数据集非常干净，所以在下一步中，我们将缩放“Amount”和“Time”列，以确保它们与其他列的格式相似。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="22a1" class="nv mr it nq b gy nw nx l ny nz">## scaling the "Amount" and "Time" columns <br/>rob_scaler = RobustScaler()</span><span id="51ec" class="nv mr it nq b gy oa nx l ny nz">df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))<br/>df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))</span><span id="367f" class="nv mr it nq b gy oa nx l ny nz"># now drop the original columns<br/>df.drop(['Time','Amount'], axis=1, inplace=True)</span></pre><p id="25df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们定义X和y输入变量。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="e229" class="nv mr it nq b gy nw nx l ny nz"># define X and y variables<br/>X = df.loc[:, df.columns != 'Class']<br/>y = df.loc[:, df.columns == 'Class']</span></pre><p id="19e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这是一个不平衡的数据集，我们需要采取额外的步骤来进行子采样——称为欠采样——这样机器就不必大海捞针来学习示例。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="c51d" class="nv mr it nq b gy nw nx l ny nz"># number of fraud cases<br/>frauds = len(df[df.Class == 1])</span><span id="57b7" class="nv mr it nq b gy oa nx l ny nz"># selecting the indices of the non-fraud classes<br/>fraud_indices = df[df.Class == 1].index<br/>nonfraud_indices = df[df.Class == 0].index</span><span id="ff52" class="nv mr it nq b gy oa nx l ny nz"># from all non-fraud observations, randomly select observations equal to number of fraud observations<br/>random_nonfraud_indices = np.random.choice(nonfraud_indices, frauds, replace = False)<br/>random_nonfraud_indices = np.array(random_nonfraud_indices)</span><span id="ea5f" class="nv mr it nq b gy oa nx l ny nz"># appending the 2 indices<br/>under_sample_indices = np.concatenate([fraud_indices,random_nonfraud_indices])</span><span id="a450" class="nv mr it nq b gy oa nx l ny nz"># undersample dataset<br/>under_sample_data = df.iloc[under_sample_indices,:]</span><span id="071f" class="nv mr it nq b gy oa nx l ny nz"># now split X, y variables from the under sample data<br/>X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']<br/>y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']</span></pre><p id="398f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在将数据输入模型之前，将数据拆分为训练集和测试集。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="b11c" class="nv mr it nq b gy nw nx l ny nz"># split dataset<br/>X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample, y_undersample, test_size = 0.3, random_state = 0)</span></pre><h1 id="7313" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">步骤4:模型构建</h1><p id="e1cf" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">像往常一样，实际的模型构建只需要3行代码，就可以对给定的数据集进行实例化、拟合和预测。由于这只是为了演示，我们将使用默认参数，不做任何调整。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="723d" class="nv mr it nq b gy nw nx l ny nz"># instantiate model<br/>model = LogisticRegression()</span><span id="c9de" class="nv mr it nq b gy oa nx l ny nz"># fit <br/>model.fit(X_train_undersample, y_train_undersample)</span><span id="b47f" class="nv mr it nq b gy oa nx l ny nz"># predict<br/>y_pred = model.predict(X_test_undersample)</span></pre><h1 id="7ec3" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">第五步:模型评估</h1><p id="05fa" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">作为标准模型评估指标，我们正在制作分类报告和混淆指标。</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="2bfc" class="nv mr it nq b gy nw nx l ny nz">classification_report = classification_report(y_test_undersample, y_pred)<br/>confusion_matrix = confusion_matrix(y_test_undersample, y_pred)</span><span id="be6e" class="nv mr it nq b gy oa nx l ny nz">print("CLASSIFICATION REPORT")<br/>print(classification_report)<br/>print("CONFUSION MATRIX") <br/>print(confusion_matrix)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d7b3d309c48f1eee57fd08a38c8001af.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*EM0mwpDGlxg4UdKD82wwwg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/aa255c9cf6e73a79a6b726fbbb419fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*JRLMnDdjnBtkotxTcozSFw.png"/></div></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="bb92" class="mq mr it bd ms mt oe mv mw mx of mz na jz og ka nc kc oh kd ne kf oi kg ng nh bi translated">离别赠言</h1><p id="c674" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">本文的目的是展示一种用于异常检测的纯监督机器学习方法。由于严重的阶级不平衡，在这个领域很少使用监督技术。由于一个数据集中只有少数异常观测值，算法没有足够的例子可以学习。</p><p id="828f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我写的11个算法没有一个是绝对好或更好的，这都归结于数据集的性质和它来自的领域。通常，一个简单的统计算法(如箱线图)足以过滤可能的候选者，而在其他情况下，一个复杂的算法可能是无用的。</p><p id="f9d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你觉得这个系列有用，欢迎在下面发表评论，关注我的<a class="ae ky" href="https://medium.com/@mab.datasc" rel="noopener">媒体</a>、<a class="ae ky" href="https://twitter.com/DataEnthus" rel="noopener ugc nofollow" target="_blank">推特</a>或<a class="ae ky" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</p></div></div>    
</body>
</html>