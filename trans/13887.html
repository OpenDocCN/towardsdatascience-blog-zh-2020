<html>
<head>
<title>Fine-tuning Mozilla DeepSpeech for the Indian Accent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">针对印度口音微调Mozilla DeepSpeech</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-speech-recognition-for-the-indian-accent-91bb011ad169?source=collection_archive---------9-----------------------#2020-09-24">https://towardsdatascience.com/automatic-speech-recognition-for-the-indian-accent-91bb011ad169?source=collection_archive---------9-----------------------#2020-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ed77" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌语音识别API的最佳开源替代品——现在是第二大英语国家！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/078886b3d595a52ee74cd6d62a35e5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M0dk2kSK-xkCYU8p"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@kennyzhang29?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">张肯尼</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="8032" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让计算机识别语音的最初尝试之一是专注于识别数字！贝尔实验室在1952年设计了<a class="ae ky" href="http://www.icsi.berkeley.edu/pubs/speech/audreytosiri12.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">奥黛丽系统</strong> </a>，它可以识别单个语音说出的数字。从那以后，维基百科的这篇文章中详细记录了许多其他的实验。快进到今天，我们有最先进的自动语音识别引擎(ASR ),如苹果的Siri、谷歌助手和亚马逊的Alexa。</p><p id="aaff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很长一段时间，谷歌的语音转文本API (STT)是任何ASR任务事实上的选择。当像<a class="ae ky" href="https://github.com/mozilla/DeepSpeech" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Mozilla deep speech</strong></a>这样的开源替代品在2017年末问世时，这种情况慢慢改变了。它基于百度的原始<a class="ae ky" href="https://arxiv.org/abs/1412.5567" rel="noopener ugc nofollow" target="_blank">深度语音研究论文，并使用(大部分)美国英语数据集进行训练，导致对其他英语口音的泛化能力较差。</a></p><p id="206b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在最近的一次实习中，我不得不为一个视频会议平台集成一个ASR引擎，这个平台主要由印度人使用。我们更倾向于寻找开源替代方案，但是大多数通用方案在实时会议中表现不佳。就在那时，我看到了DeepSpeech和IITM的<a class="ae ky" href="https://www.iitm.ac.in/donlab/tts/index.php" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Indic TTS</strong></a><strong class="lb iu"/>项目。</p><p id="8c85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Indic数据集包含超过50 GB的来自印度13个邦的说话者的语音样本。它由10000多个英语口语句子组成，既有男性也有女性母语者。这些文件可以在<em class="lv">中找到。wav </em>格式连同相应的文本。</p><p id="b36c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将向您展示使用Indic数据集对DeepSpeech进行微调的过程，但是您也可以轻松地对其他英语数据集进行微调。你可以在<a class="ae ky" href="https://www.iitm.ac.in/donlab/tts/database.php" rel="noopener ugc nofollow" target="_blank"> IITM网站</a>上注册，向他们索取数据集。</p><p id="a9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先决条件:熟悉ASR引擎、语音处理，并对递归神经网络和张量流有基本的了解。</p><p id="7049" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:我所有的训练和预处理都是在Google Colab上用<strong class="lb iu"> DeepSpeech版本0.7.4 </strong>完成的</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="e384" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">预处理数据集</h1><p id="15a0" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在你申请数据集后，IITM会给你七天的时间访问他们的Google Drive链接。因为我长期需要这些数据，所以我把所有的ZIP文件都转移到了Google Cloud Bucket中。每个ZIP文件都会有一个包含<em class="lv">的文件夹。wav </em>文件和对应的名为<em class="lv"> txt.done.data </em>的元数据文件。</p><p id="d3ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要处理元数据文件，并为数据集生成<em class="lv">培训/开发/测试</em>分割。我们可以一次为一个州训练模型，或者将几个州分组，然后训练模型。下图显示了如何处理元数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/f7228bee088663d8e9ac9bcdeb93877b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Iel7x9wstcL3F_HRSR2mQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面给出的GitHub要点包含了生成单个CSV文件的完整代码，我们稍后需要对其进行拆分。大部分代码都是不言自明的，有足够的注释。确保先安装好<strong class="lb iu"> Librosa </strong>和<strong class="lb iu"> num2words </strong>。</p><p id="908c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行脚本时，<em class="lv"> wav </em>参数指向包含所有音频文件的文件夹，而<em class="lv"> meta </em>参数指向包含<em class="lv"> txt.done.data </em>文件的文件夹。这里，第一部分将数据从Google Bucket复制到Colab。第二部分创建一个CSV文件，最后一个命令追加到这个CSV文件中，依此类推。如果您想单独训练每个ZIP文件，只运行一个命令并继续分割CSV(尽管我不建议这样做)。如果附加到同一个文件，小心注释掉第45行。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="742a" class="ng me it nc b gy nh ni l nj nk">$ gsutil cp gs://bucket-name/hindi_female_english.zip /content/hindi_female_english.zip<br/>$ gsutil cp gs://bucket-name/hindi_male_english.zip /content/hindi_male_english.zip</span><span id="4595" class="ng me it nc b gy nl ni l nj nk">$ unzip hindi_female_english.zip -d /content/hindi_female_english<br/>$ unzip hindi_male_english.zip -d /content/hindi_male_english</span><span id="33f8" class="ng me it nc b gy nl ni l nj nk">--------------------------------------------------------------------</span><span id="ad60" class="ng me it nc b gy nl ni l nj nk">$ python preProcess.py --wav /content/hindi_female_english/english/wav --meta /content/hindi_female_english/english</span><span id="dc31" class="ng me it nc b gy nl ni l nj nk">$ python preProcess.py --wav /content/hindi_male_english/english/wav --meta /content/hindi_male_english/english</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="9f20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们有一个CSV文件，需要将它分成三个单独的文件。在左边的要点中，我们首先将主CSV文件分割成<em class="lv">序列</em>和<em class="lv">中间</em>，然后将<em class="lv">中间</em>分割成<em class="lv">开发</em>和<em class="lv">测试</em>。然后我们最终有了三个文件，对应于DeepSpeech训练所需的三个拆分。</p><h1 id="59b6" class="md me it bd mf mg no mi mj mk np mm mn jz nq ka mp kc nr kd mr kf ns kg mt mu bi translated">微调深度演讲</h1><p id="240d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这里给出了官方培训文档<a class="ae ky" href="https://deepspeech.readthedocs.io/en/v0.7.4/TRAINING.html" rel="noopener ugc nofollow" target="_blank">。它非常详细，但跳过了一些可能会让你沮丧很长时间的细节😩。我假设您已经将数据集传输到Google Cloud bucket，并且有适当的互联网连接，因为如果您不快速重新连接，Colab会终止您的实例。<strong class="lb iu">以下所有步骤均摘自Colab上的</strong> <strong class="lb iu">培训笔记本</strong> </a><a class="ae ky" href="https://colab.research.google.com/drive/15s7ZcV-MxOOMIvQSL48KYTSFcDmjc4gi?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">此处</strong> </a> <strong class="lb iu">。</strong></p><ol class=""><li id="591b" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated"><strong class="lb iu">授权Colab访问您的Google Cloud Bucket: </strong>下载您项目的IAM访问凭证文件并上传到Colab。</li></ol><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="870e" class="ng me it nc b gy nh ni l nj nk">import os<br/>from google.colab import auth</span><span id="d7c8" class="ng me it nc b gy nl ni l nj nk">auth.authenticate_user()</span><span id="68d7" class="ng me it nc b gy nl ni l nj nk">os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "creds.json"</span><span id="794d" class="ng me it nc b gy nl ni l nj nk">#ensure the path is set correctly<br/>!echo $GOOGLE_APPLICATION_CREDENTIALS</span><span id="4f24" class="ng me it nc b gy nl ni l nj nk">project_id = '&lt;gcloud-project-id&gt;'<br/>bucket_name = 'gs://&lt;bucket-name&gt;'<br/>!gcloud config set project {project_id}</span></pre><p id="7c98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。从GitHub: </strong>克隆DeepSpeech v0.7.4，下载相应的检查点。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="bde6" class="ng me it nc b gy nh ni l nj nk">!git clone --branch v0.7.4 <a class="ae ky" href="https://github.com/mozilla/DeepSpeech" rel="noopener ugc nofollow" target="_blank">https://github.com/mozilla/DeepSpeech</a></span><span id="15f7" class="ng me it nc b gy nl ni l nj nk">!wget <a class="ae ky" href="https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-checkpoint.tar.gz" rel="noopener ugc nofollow" target="_blank">https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-checkpoint.tar.gz</a></span><span id="3486" class="ng me it nc b gy nl ni l nj nk">!tar -xvf deepspeech-0.7.4-checkpoint.tar.gz -C \<br/>/content/model_checkpoints/</span></pre><p id="7961" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。安装DeepSpeech依赖项:</strong>每次训练模型时，我们都需要安装很多东西。详细步骤在笔记本<a class="ae ky" href="https://colab.research.google.com/drive/15s7ZcV-MxOOMIvQSL48KYTSFcDmjc4gi#scrollTo=In4HqOMdrg8n&amp;line=2&amp;uniqifier=1" rel="noopener ugc nofollow" target="_blank">这里</a>给出。</p><p id="22e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 4。设置默认的CUDA版本:</strong>如果你第一次没有做好，这部分将会非常令人沮丧😤。出于某种原因，每当你试图在Colab中设置默认的CUDA版本为10.0 (DeepSpeech需要CUDA 10.0和CuDNN v7.6)，它总是试图恢复到10.1🤷。我发现了一系列看似有效的步骤，但我不能保证每次都有效——你可能需要挖掘一点点，才能找到正确的方法。详细步骤在笔记本<a class="ae ky" href="https://colab.research.google.com/drive/15s7ZcV-MxOOMIvQSL48KYTSFcDmjc4gi#scrollTo=pQIMGUGzU09V&amp;line=3&amp;uniqifier=1" rel="noopener ugc nofollow" target="_blank">这里</a>给出。如果你仍然不能让它工作，我会非常乐意帮助:)</p><p id="8c13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 5。安装TensorFlow-GPU: </strong></p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="00e2" class="ng me it nc b gy nh ni l nj nk">!pip3 uninstall tensorflow<br/>!pip3 install 'tensorflow-gpu==1.15.2'</span></pre><p id="c705" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6。将数据从Google Bucket复制到Colab:这里需要小心，将文件复制到您生成CSV文件时的位置，因为模型使用CSV文件中声明的音频文件的绝对路径。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ab06" class="ng me it nc b gy nh ni l nj nk">%cd /content/</span><span id="0d65" class="ng me it nc b gy nl ni l nj nk">!gsutil cp gs://bucket-name/hindi_female_english.zip /content/hindi_female_english.zip<br/>!gsutil cp gs://bucket-name/hindi_male_english.zip /content/hindi_male_english.zip</span><span id="414d" class="ng me it nc b gy nl ni l nj nk">!unzip hindi_female_english.zip -d /content/hindi_female_english<br/>!unzip hindi_male_english.zip -d /content/hindi_male_english</span></pre><p id="6aa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 7。检查不在<em class="lv"> alphabet.txt </em> : </strong>中的额外字符这个文件(存在于<em class="lv"> /DeepSpeech/data </em>中)定义了DeepSpeech正在被训练的语言的字母表。因为我们的任务语言是一样的，我们不需要改变它。如果我们想用一种完全不同的语言训练一个新的模型，我们需要重新定义字母表并遵循这些步骤。将三个CSV文件上传到<em class="lv"> /content/ </em>并对其运行<em class="lv"> check_parameters.py </em>。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="4022" class="ng me it nc b gy nh ni l nj nk">!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/train.csv -alpha</span><span id="b9de" class="ng me it nc b gy nl ni l nj nk">!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/dev.csv -alpha</span><span id="6814" class="ng me it nc b gy nl ni l nj nk">!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/test.csv -alpha</span></pre><p id="42a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>您可能需要注释掉<code class="fe oc od oe nc b">check_characters.py</code>中的一些行才能让它工作。在注释掉一些行之后，这个文件中的第44行应该是这样的</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="0ead" class="ng me it nc b gy nh ni l nj nk">if not args.disable_unicode_variants:<br/>  unicode_transcript = unicodedata.normalize("NFKC", row[2])<br/>  #if row[2] != unicode_transcript:<br/>    #print("Your input file", in_file, "contains at least one transript with unicode chars on more than one code-point: '{}'. Consider using NFKC normalization: unicodedata.normalize('NFKC', str).".format(row[2]))<br/>    #sys.exit(-1)<br/>all_text |= set(row[2])</span></pre><p id="f1a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">8.训练模型(最后😌):这一步非常简单。尝试不同的超参数，并仔细检查路径。增强参数可以帮助您的模型更好地进行概化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="99f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.导出模型进行推理:训练过程完成后，a <em class="lv">。pb </em>模型文件被导出到<em class="lv"> export_dir </em>。但是这个模型非常大(~700MB)，并且对于批量预测不是非常有效。幸运的是，有一个选项可以将导出的模型转换成内存映射模型，之后我们会得到一个<em class="lv">。pbmm </em>模型文件。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="db16" class="ng me it nc b gy nh ni l nj nk">%cd /content/DeepSpeech/</span><span id="92d4" class="ng me it nc b gy nl ni l nj nk">!python3 util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target .</span><span id="a14b" class="ng me it nc b gy nl ni l nj nk">!./convert_graphdef_memmapped_format --in_graph=/content/models/ft_model.pb --out_graph=/content/models/ft_model.pbmm</span></pre><p id="45c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个<em class="lv">。pbmm </em>文件相对较小(~180MB)且高效，可与相应的<a class="ae ky" href="https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.scorer" rel="noopener ugc nofollow" target="_blank">划线器文件</a>一起使用，以改善推断结果。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="1e9f" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="cb72" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">唷！那篇文章很长😵。如果您已经到达这里，感谢您的坚持:)</p><p id="31e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我使用DeepSpeech的过程中，我发现它是ASR最容易使用的库之一。我希望这篇文章能帮助任何自己尝试的人。</p><p id="f97d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以通过<a class="ae ky" href="https://www.linkedin.com/in/abhiroop1999/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae ky" href="https://github.com/abhirooptalasila" rel="noopener ugc nofollow" target="_blank"> GitHub </a>联系我</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/generating-subtitles-automatically-using-mozilla-deepspeech-562c633936a7"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">使用Mozilla DeepSpeech自动生成字幕</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">对于那些嘴里薯条的噪音让你无法看电影的时候:)</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ks oi"/></div></div></a></div></div></div>    
</body>
</html>