<html>
<head>
<title>NLP Text Preprocessing: Steps, tools, and examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理文本预处理:步骤、工具和例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-text-preprocessing-steps-tools-and-examples-94c91ce5d30?source=collection_archive---------23-----------------------#2020-10-21">https://towardsdatascience.com/nlp-text-preprocessing-steps-tools-and-examples-94c91ce5d30?source=collection_archive---------23-----------------------#2020-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="46b7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为自然语言处理任务预处理文本的标准逐步方法。</h2></div><p id="e678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文本数据无处不在，从你每天的脸书或Twitter新闻，到教科书和客户反馈。数据是新的石油，文本是我们需要钻得更深的油井。在我们真正使用油之前，我们必须对它进行预处理，使它适合我们的机器。对于数据也是一样，我们必须清理和预处理数据以符合我们的目的。这篇文章将包括一些简单的方法来清理和预处理文本分析任务的文本数据。</p><p id="b6d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在新冠肺炎推特数据集上模拟这种方法。这种方法有三个主要组成部分:<br/>首先，我们清理和过滤所有非英语的推文/文本，因为我们希望数据的一致性。<br/>其次，我们为复杂的文本数据创建一个简化版本。<br/>最后，我们对文本进行矢量化处理，并保存它们的嵌入内容以供将来分析。</p><p id="d018" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您想查看代码:请随意查看<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets/blob/master/1.1.%20clean%20%26%20filter%20english%20tweets.ipynb" rel="noopener ugc nofollow" target="_blank">第1部分</a>、<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets/blob/master/1.2.%20clean%20text%2C%20tags%20%26%20date.ipynb" rel="noopener ugc nofollow" target="_blank">第2部分</a>和<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets/blob/master/1.3.%20clean%20%26%20get_embeddings.ipynb" rel="noopener ugc nofollow" target="_blank">第3部分</a>嵌入<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets" rel="noopener ugc nofollow" target="_blank">这里</a>的代码。你也可以在这里查看整个项目的blogpost和<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets" rel="noopener ugc nofollow" target="_blank">代码</a> <a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/3819a74b37630bf9512b72936187297e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzaOJVBd-gZCZ94PLyYnVw.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">(<a class="ae lb" href="https://pixabay.com/illustrations/personal-data-data-click-3914806/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="fcae" class="lz ma iq bd mb mc md dn me mf mg dp mh ko mi mj mk ks ml mm mn kw mo mp mq mr bi translated"><strong class="ak">第1部分:清洁&amp;过滤器文本</strong></h2><p id="14d4" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">首先，为了简化文本，我们希望将文本标准化为只有英文字符。此功能将删除所有非英语字符。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="33a4" class="lz ma iq my b gy nc nd l ne nf">def clean_non_english(txt):<br/>    txt = re.sub(r'\W+', ' ', txt)<br/>    txt = txt.lower()<br/>    txt = txt.replace("[^a-zA-Z]", " ")<br/>    word_tokens = word_tokenize(txt)<br/>    filtered_word = [w for w in word_tokens if all(ord(c) &lt; 128 for c in w)]<br/>    filtered_word = [w + " " for w in filtered_word]<br/>    return "".join(filtered_word)</span></pre><p id="04d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们甚至可以通过删除停用词做得更好。停用词是英语句子中出现的对意思没有多大贡献的常用词。我们将使用nltk包来过滤停用词。由于我们的主要任务是使用词云可视化推文的共同主题，这一步是必要的，以避免常见的词，如“the”、“a”等。<br/>然而，如果你的任务需要完整的句子结构，比如下一个单词预测或语法检查，你可以跳过这一步。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="8ba5" class="lz ma iq my b gy nc nd l ne nf">import nltk<br/>nltk.download('punkt') # one time execution<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>stop_words = set(stopwords.words('english'))</span><span id="9b84" class="lz ma iq my b gy ng nd l ne nf">def clean_text(english_txt):<br/>    try:<br/>       word_tokens = word_tokenize(english_txt)<br/>       filtered_word = [w for w in word_tokens if not w in stop_words]<br/>       filtered_word = [w + " " for w in filtered_word]<br/>       return "".join(filtered_word)<br/>    except:<br/>       return np.nan</span></pre><p id="182b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于tweets，在清理之前我们需要考虑一个特殊的特性:提及。您的数据可能有(或没有)这样的特殊特性，这是具体情况具体分析，而不是普遍要求。因此，在盲目清理和预处理之前，要充分了解您的数据！</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="7a51" class="lz ma iq my b gy nc nd l ne nf">def get_mention(txt):<br/>    mention = []<br/>    for i in txt.split(" "):<br/>        if len(i) &gt; 0 and i[0] == "@":<br/>            mention.append(i)<br/>            return "".join([mention[i] + ", " if i != len(mention) - 1 else mention[i] for i in range(len(mention))]</span></pre><p id="1957" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之前，我们清理非英语字符。现在，我们移除非英语文本(语义上)。Langdetect是一个python包，允许检查文本的语言。它是Google的<a class="ae lb" href="https://code.google.com/archive/p/language-detection/" rel="noopener ugc nofollow" target="_blank">语言检测</a>库从Java到Python的直接端口。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="41c5" class="lz ma iq my b gy nc nd l ne nf">from langdetect import detect<br/>def detect_lang(txt):<br/>    try:<br/>        return detect(txt)<br/>    except:<br/>        return np.nan</span></pre><p id="6bba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们过滤掉所有非英语的列。</p><p id="afda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一部分的所有代码都可以在<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets/blob/master/1.1.%20clean%20%26%20filter%20english%20tweets.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="5ed9" class="lz ma iq bd mb mc md dn me mf mg dp mh ko mi mj mk ks ml mm mn kw mo mp mq mr bi translated">第2部分:简化复杂的数据—位置？</h2><p id="d6e1" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">对于数字数据，好的处理方法是缩放、标准化和规范化。这个<a class="ae lb" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">资源</a>有助于理解并将这些方法应用于您的数据。在这篇文章的范围内，我不会进一步讨论，因为其他<a class="ae lb" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">资源</a>已经做了很好的工作。</p><p id="e0bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于分类数据，有许多方法。两种名义上的方法是标签编码器(为每个标签分配不同的数字)和一种热编码(用0和1的向量表示)。关于这些分类值的更多细节可以在<a class="ae lb" rel="noopener" target="_blank" href="/all-about-categorical-variable-encoding-305f3361fd02">这里</a>找到。这个<a class="ae lb" rel="noopener" target="_blank" href="/all-about-categorical-variable-encoding-305f3361fd02">资源</a>非常丰富，比我提到的这两种编码类型更多。</p><p id="5179" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本帖将介绍一些降低数据复杂度的方法，尤其是位置数据。在我的数据集中，有一个位置栏，有作者的地址。然而，我不能对这些原始数据进行太多的分析，因为它们太杂乱和复杂了(有城市、县、州、国家)。因此，我们可以对文本进行标准化，并将其简化到“国家”级别(如果您感兴趣，也可以是州)。处理位置数据的包是<a class="ae lb" href="https://pypi.org/project/geopy/" rel="noopener ugc nofollow" target="_blank"> geopy </a>。它可以识别正确的地址并将这些位置重新格式化为标准格式。然后，您可以选择保留您需要的任何信息。对我来说，国家已经够体面了。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="ec7d" class="lz ma iq my b gy nc nd l ne nf">from geopy.geocoders import Nominatim<br/>geolocator = Nominatim(user_agent="twitter")</span><span id="84e8" class="lz ma iq my b gy ng nd l ne nf">def get_nation(txt):<br/>    try:<br/>        location = geolocator.geocode(txt)<br/>        x = location.address.split(",")[-1]<br/>        return x<br/>    except:<br/>        return np.nan</span></pre><p id="8974" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Python有这么多包，太牛逼了。我相信你总能找到满足你特定需求的东西，就像我处理我杂乱的位置数据一样。祝你好运，简化这些杂乱的数据。</p><p id="71c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二部分的代码可以在<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets/blob/master/1.2.%20clean%20text%2C%20tags%20%26%20date.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="a892" class="lz ma iq bd mb mc md dn me mf mg dp mh ko mi mj mk ks ml mm mn kw mo mp mq mr bi translated">第3部分:矢量化和嵌入</h2><p id="07a4" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">文本矢量化是将文本转换成值的向量来表示它们的含义。早期，我们有一种热门的编码方法，它使用一个向量，这个向量的大小是我们的词汇量，在文本出现的地方取值1，在其他地方取值0。如今，我们有更先进的方法，如<a class="ae lb" href="https://spacy.io/usage/vectors-similarity" rel="noopener ugc nofollow" target="_blank">空间</a>、<a class="ae lb" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>，甚至<a class="ae lb" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特</a>嵌入。对于这个项目的范围，我将向您介绍GloVe in python和Jupiter笔记本。</p><p id="1fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们下载嵌入。你可以在这里手动下载或者直接在笔记本上下载。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="1195" class="lz ma iq my b gy nc nd l ne nf">!wget <a class="ae lb" href="http://nlp.stanford.edu/data/glove.6B.zip" rel="noopener ugc nofollow" target="_blank">http://nlp.stanford.edu/data/glove.6B.zip</a><br/>!unzip glove*.zip</span></pre><p id="fb83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们创建一个函数来矢量化每个数据点。这个句子是每个单词的意思表示。对于空句子，我们将其默认为零向量。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="2997" class="lz ma iq my b gy nc nd l ne nf">def vectorize(value, word_embeddings, dim = 100):<br/>    sentences = value.to_list()<br/>    sentence_vectors = []<br/>    for i in sentences:<br/>        if len(i) != 0:<br/>            v = sum([word_embeddings.get(w, np.zeros((dim,))) for w in      i.split()])/(len(i.split())+0.001)<br/>        else:<br/>            v = np.zeros((dim,))<br/>        sentence_vectors.append(v)<br/>    sentence_vectors = np.array(sentence_vectors)<br/>    return sentence_vectors</span></pre><p id="1514" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们对整个数据集进行矢量化，并将矢量化的numpy数组保存为一个文件，这样我们就不必在每次运行代码时都重复这个过程。矢量化版本将保存为numpy数组，格式为。npy文件。Numpy包便于存储和处理海量数组数据。</p><p id="7ce3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为我个人的标准实践，我尝试在每个部分之后将所有数据保存为单独的文件，以便更灵活地评估数据和修改代码。</p><pre class="ld le lf lg gt mx my mz na aw nb bi"><span id="b2ca" class="lz ma iq my b gy nc nd l ne nf">def vectorize_data(data = data, value = 'english_text', dim = 100):<br/>    # Extract word vectors<br/>    word_embeddings = {}<br/>    f = open('glove.6B.{}d.txt'.format(str(dim)), encoding='utf-8')<br/>    for line in f:<br/>        values = line.split()<br/>        word = values[0]<br/>        coefs = np.asarray(values[1:], dtype='float32')<br/>        word_embeddings[word] = coefs<br/>    f.close()</span><span id="c5eb" class="lz ma iq my b gy ng nd l ne nf">    text_vec = vectorize(data[value], word_embeddings, dim)<br/>    np.save("vectorized_{}.npy".format(str(dim)), text_vec)<br/>    print("Done. Data:", text_vec.shape)<br/>    return True</span></pre><p id="c35d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三部分的代码在这里。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="122d" class="lz ma iq bd mb mc md dn me mf mg dp mh ko mi mj mk ks ml mm mn kw mo mp mq mr bi translated">结论</h2><p id="4fde" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">数据预处理，特别是文本，可能是一个非常麻烦的过程。你的机器学习工程师工作流程的很大一部分将是这些清理和格式化数据(如果你的数据已经非常干净，那你是幸运的&amp;所有数据工程师都为此做出了贡献)。</p><p id="44ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本帖所有的<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets" rel="noopener ugc nofollow" target="_blank">代码</a>都很抽象，可以应用到很多数据项目中(只需要改成列名，应该都可以正常工作)。在笔记本中，我还添加了异常函数来处理失败情况，确保您的代码不会中途崩溃。我希望它对你的项目有所帮助，就像我对我的项目有所帮助一样。</p><p id="37fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这里查看项目博客<a class="ae lb" href="https://github.com/viethoangtranduong/covid19-tweets" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="757d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">祝你的项目好运，如果有什么需要改变和改进的地方，请告诉我！谢谢，下一篇帖子再见！</p><p id="8975" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考资料:<br/>黑尔，J. (2020年2月21日)。使用Scikit-Learn进行扩展、标准化或规范化。检索自<a class="ae lb" rel="noopener" target="_blank" href="/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02">https://towards data science . com/scale-standard-or-normalize-with-scikit-learn-6 CCC 7d 176 a 02</a><br/>KazAnova。(2017年9月13日)。包含160万条推文的Sentiment140数据集。检索自<a class="ae lb" href="https://www.kaggle.com/kazanova/sentiment140" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/kazanova/sentiment140</a><br/>普雷达，G. (2020年8月30日)。COVID19推文。检索自https://www.kaggle.com/gpreda/covid19-tweets(2020年4月25日)。关于分类变量编码的一切。检索自<a class="ae lb" rel="noopener" target="_blank" href="/all-about-categorical-variable-encoding-305f3361fd02">https://towardsdatascience . com/all-about-category-variable-encoding-305 f 3361 FD 02</a></p></div></div>    
</body>
</html>