<html>
<head>
<title>6 Tips for Interpretable Topic Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释主题模型的6个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2?source=collection_archive---------3-----------------------#2020-11-01">https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2?source=collection_archive---------3-----------------------#2020-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d9df" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于调整LDA主题模型以获得易于理解的输出的Python实践教程。</h2></div><p id="3b5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着如此多的文本在数字平台上输出，自动理解关键主题趋势的能力可以揭示巨大的洞察力。例如，企业可以从了解客户围绕其品牌和产品的对话趋势中受益。提取关键主题的一种常用方法是潜在狄利克雷分配(LDA)。然而，输出通常很难解释为有用的见解。我们将探索提高可解释性的技术。</p><h1 id="2af7" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak">什么是潜在狄利克雷分配(LDA)？</strong></h1><p id="1e00" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">潜在狄利克雷分配(LDA)是一种生成统计模型，有助于在不同数据部分的集合中提取相似性。在主题建模中，每个数据部分是一个word文档(例如产品页面上的单个评论)，文档集合是一个语料库(例如产品页面上的所有用户评论)。重复出现的相似单词集合可能指示主题。</p><p id="7d7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LDA假设每个<strong class="kh ir">文档由固定数量的主题分布</strong>表示，<strong class="kh ir">每个主题是单词分布</strong>。</p><p id="d7fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">近似这些分布的算法的高级关键步骤:</p><ol class=""><li id="447c" class="ly lz iq kh b ki kj kl km ko ma ks mb kw mc la md me mf mg bi translated">用户选择K，即当前主题的数量，调整到适合每个数据集。</li><li id="38be" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">浏览每份文件，并随机将每个单词分配到K个主题中的一个。由此，我们有了计算主题p的<strong class="kh ir">文档分布(主题t |文档d) </strong>、文档d中分配给主题t的单词的比例的起点。我们还可以计算单词p的<strong class="kh ir">主题分布(单词w |主题t) </strong>、单词w在分配给主题t的所有文档单词中的比例。由于随机性，这些将是较差的近似。</li><li id="1e80" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">为了提高近似性，我们遍历每个文档。对于每个文档，浏览每个单词并重新分配一个新主题，其中我们根据上一轮的分布以概率<strong class="kh ir"> p(主题t |文档d)÷p(单词w |主题t) </strong>选择主题t。这本质上就是题目t生成单词w 的<strong class="kh ir">概率。从这些新作业中重新计算p(题目t |文档d)和p(单词w |题目t)。</strong></li><li id="a89c" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">不断迭代，直到主题/单词分配达到稳定状态，不再有太大变化，(即收敛)。使用最终分配来估计每个文档的主题混合(该文档中分配给每个主题的单词的百分比)和与每个主题相关的单词(该单词分配给每个主题的总次数)。</li></ol><h1 id="f5a9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak">数据准备</strong></h1><p id="8ac4" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我们将探索在亚马逊Office产品评论上使用LDA优化可解释性的技术。为了准备评论数据，我们使用典型的文本清理步骤来清理评论文本:</p><ol class=""><li id="7127" class="ly lz iq kh b ki kj kl km ko ma ks mb kw mc la md me mf mg bi translated">删除非ascii字符，例如à∅</li><li id="176d" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">将单词转换为最基本的形式，如“running”和“ran”到“run ”,以便它们被识别为同一个单词</li><li id="c2c5" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">删除标点符号</li><li id="8065" class="ly lz iq kh b ki mh kl mi ko mj ks mk kw ml la md me mf mg bi translated">删除非英语注释(如果有)</li></ol><p id="073d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">教程中的所有代码都可以在<a class="ae mm" href="https://nicharuc.github.io/topic_modeling/" rel="noopener ugc nofollow" target="_blank">这里</a>找到，这里清理的函数位于clean_text.py，整个过程的主笔记本是<a class="ae mm" href="https://nicharuc.github.io/topic_modeling/" rel="noopener ugc nofollow" target="_blank"> topic_model.ipynb </a>。</p><h1 id="9c0a" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak">优化可解释性的步骤</strong></h1><p id="aa7f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated"><strong class="kh ir">提示1:通过n元语法识别短语，过滤名词型结构</strong></p><p id="238b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们想要识别短语，以便主题模型能够识别它们。二元词是包含两个词的短语，例如“社交媒体”。同样，三元组是包含3个单词的短语，例如“Proctor and Gamble”。有很多方法可以检测n-gram，这里解释<a class="ae mm" href="http://bit.ly/2HGWhl8" rel="noopener ugc nofollow" target="_blank">这里</a>。在本例中，我们将使用<em class="mn">点态互信息(PMI)得分。</em>这衡量了单词比独立出现时更有可能同时出现。该度量对罕见的单词组合敏感，因此它与出现频率过滤器一起使用，以确保短语相关性。下面的二元模型示例(三元模型代码包含在Jupyter笔记本中):</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="27d0" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir"># Example for detecting bigrams <br/></strong>bigram_measures = nltk.collocations.BigramAssocMeasures()</span><span id="2650" class="mx lc iq mt b gy nc mz l na nb">finder =nltk.collocations.BigramCollocationFinder\<br/>.from_documents([comment.split() for comment in\ clean_reviews.reviewText])</span><span id="0e03" class="mx lc iq mt b gy nc mz l na nb"><strong class="mt ir"># Filter only those that occur at least 50 times<br/></strong>finder.apply_freq_filter(50)<br/>bigram_scores = finder.score_ngrams(bigram_measures.pmi)</span></pre><p id="62d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们过滤带有名词结构的二元或三元模型。这有助于LDA模型更好地聚类主题，因为名词是正在谈论的主题的更好指示符。我们使用NLTK包来标记词性并过滤这些结构。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="fa36" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir"><em class="mn"># Example filter for noun-type structures bigrams<br/></em>def</strong> bigram_filter(bigram):<br/>    tag = nltk.pos_tag(bigram)<br/>    <strong class="mt ir">if</strong> tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:<br/>        return False<br/>    <strong class="mt ir">if</strong> bigram[0] in stop_word_list or bigram[1] in stop_word_list:<br/>        return False<br/>    <strong class="mt ir">if</strong> 'n' in bigram or 't' in bigram:<br/>        return False<br/>    <strong class="mt ir">if</strong> 'PRON' in bigram:<br/>        return False<br/>    <strong class="mt ir">return</strong> True</span><span id="5384" class="mx lc iq mt b gy nc mz l na nb"><strong class="mt ir"><em class="mn"># Can eyeball list and choose PMI threshold where n-grams stop making sense<br/># In this case, get top 500 bigrams/trigrams with highest PMI score<br/></em></strong>filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\                                             bigram_filter(bigram['bigram'])\<br/>and bigram.pmi &gt; 5, axis = 1)][:500]</span><span id="60dc" class="mx lc iq mt b gy nc mz l na nb">bigrams = [' '.join(x) for x in filtered_bigram.bigram.values\<br/>if len(x[0]) &gt; 2 or len(x[1]) &gt; 2]</span></pre><p id="9584" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将这些短语连接成一个单词。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="3432" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> replace_ngram(x):<br/>    <strong class="mt ir">for</strong> gram <strong class="mt ir">in</strong> bigrams:<br/>        x = x.replace(gram, '_'.join(gram.split()))<br/>    <strong class="mt ir">for</strong> gram <strong class="mt ir">in</strong> trigrams:<br/>        x = x.replace(gram, '_'.join(gram.split()))<br/>    <strong class="mt ir">return</strong> x</span><span id="af0e" class="mx lc iq mt b gy nc mz l na nb">reviews_w_ngrams = clean_reviews.copy()<br/>reviews_w_ngrams.reviewText = reviews_w_ngrams.reviewText\<br/>.map(lambda x: replace_ngram(x))</span></pre><p id="5af7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提示2:过滤名词的剩余单词</strong></p><p id="d9d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在“商店很好”这个句子中，我们知道这个句子是在说“商店”。句子中的其他单词提供了关于主题(“商店”)本身的更多上下文和解释。因此，对名词进行过滤可以提取出对主题模型更具解释力的单词。另一种方法是过滤名词<strong class="kh ir"> <em class="mn">和动词</em> </strong>。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d52a" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir"># Tokenize reviews + remove stop words + remove names + remove words with less than 2 characters</strong><br/>reviews_w_ngrams = reviews_w_ngrams.reviewText.map(lambda x: [word for word in x.split()\<br/>if word not in stop_word_list\<br/>and word not in english_names\<br/>and len(word) &gt; 2])</span><span id="a661" class="mx lc iq mt b gy nc mz l na nb"><strong class="mt ir"># Filter for only nouns<br/>def</strong> noun_only(x):<br/>    pos_comment = nltk.pos_tag(x)<br/>    filtered =[word[0] for word in pos_comment if word[1] in ['NN']]<br/>    <strong class="mt ir">return</strong> filtered</span><span id="a7bc" class="mx lc iq mt b gy nc mz l na nb">final_reviews = reviews_w_ngrams.map(noun_only)</span></pre><p id="d045" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提示#3:通过连贯性测量优化主题数量的选择</strong></p><p id="3f84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LDA要求指定主题的数量。我们可以通过优化预测似然性、困惑度和一致性等指标来调整这一点。许多文献表明，最大化一个被命名为<a class="ae mm" href="https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" rel="noopener ugc nofollow" target="_blank">Cv</a>【1】的一致性度量，会导致更好的人类可解释性。我们可以测试许多主题并评估Cv衡量标准:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="a9e2" class="mx lc iq mt b gy my mz l na nb">coherence = []<br/>for k in range(5,25):<br/>    print('Round: '+str(k))<br/>    Lda = gensim.models.ldamodel.LdaModel<br/>    ldamodel = Lda(doc_term_matrix, num_topics=k, \<br/>               id2word = dictionary, passes=40,\<br/>               iterations=200, chunksize = 10000, eval_every = None)<br/>    <br/>    cm = gensim.models.coherencemodel.CoherenceModel(\<br/>         model=ldamodel, texts=final_reviews,\<br/>         dictionary=dictionary, coherence='c_v')   <br/>                                                <br/>    coherence.append((k,cm.get_coherence()))</span></pre><p id="a96c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">绘制此显示:</strong></p><figure class="mo mp mq mr gt ne gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/819c53fdcd480fa92406dccaa1d83c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*E1LxUKQ1X-vnCHkqqYBFAA.png"/></div></figure><p id="17fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在15个主题后，改善停止。Cv最高的地方并不总是最好的，所以我们可以尝试多个来找到最好的结果。我们在这里尝试了15和23，23产生了更清晰的结果。添加主题有助于揭示更多的子主题。尽管如此，如果相同的单词开始出现在多个主题中，那么主题的数量就太多了。</p><p id="bba1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提示#4:调整LDA超参数</strong></p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="faea" class="mx lc iq mt b gy my mz l na nb">Lda2 = gensim.models.ldamodel.LdaModel<br/>ldamodel2 = Lda(doc_term_matrix, num_topics=23, id2word = dictionary, passes=40,iterations=200,  chunksize = 10000, eval_every = None, random_state=0)</span></pre><p id="b36e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您的主题仍然没有意义，尝试增加遍数和迭代次数，同时将chunksize增加到您的内存可以处理的范围。 </p><p id="f502" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nh ni nj mt b"><strong class="kh ir">chunksize</strong></code>是每次训练要加载到内存中的文档数。<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code> <strong class="kh ir"> </strong>是通过整个语料库的训练迭代次数。<code class="fe nh ni nj mt b"><strong class="kh ir">iterations</strong></code>是每个文档达到收敛的最大迭代次数——限制这意味着一些文档可能无法及时收敛。如果训练语料有200个文档，<code class="fe nh ni nj mt b"><strong class="kh ir">chunksize</strong></code> <strong class="kh ir"> </strong>为100，<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>为2，<code class="fe nh ni nj mt b"><strong class="kh ir">iterations</strong></code>为10，算法经过这几轮:</p><p id="a680" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一轮:文件0–99<br/>第二轮:文件100–199<br/>第三轮:文件0–99<br/>第四轮:文件100–199</p><p id="dac6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每一轮将迭代每个文档的概率分布分配最多10次，如果已经达到收敛，则在10次之前移动到下一个文档。这基本上是前面解释的算法的关键步骤2-4，重复<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>的数量，而步骤3重复10 <code class="fe nh ni nj mt b"><strong class="kh ir">iterations</strong></code>或更少。</p><p id="3595" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个<code class="fe nh ni nj mt b"><strong class="kh ir">chunksize</strong></code>和每个<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>之后，更新整个语料库的主题分布。增加<code class="fe nh ni nj mt b"><strong class="kh ir">chunksize</strong></code>到你的内存可以处理的程度将会提高速度，因为主题分布更新是昂贵的。然而，增加<code class="fe nh ni nj mt b"><strong class="kh ir">chunksize</strong></code>需要增加<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>的数量，以确保足够的语料库主题分布更新，特别是在小语料库中。<code class="fe nh ni nj mt b"><strong class="kh ir">iterations</strong></code>还需要足够高，以确保大量文档在继续前进之前达到收敛。当主题仍然没有意义时，我们可以尝试增加这些参数，但是日志记录也可以帮助调试:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="06bd" class="mx lc iq mt b gy my mz l na nb">import logging<br/>logging.basicConfig(filename='gensim.log',<br/>                    format="%(asctime)s:%(levelname)s:%(message)s",<br/>                    level=logging.INFO)</span></pre><p id="a0ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在日志中查找如下所示的行，它将重复您设置的<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>的数量:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="37ae" class="mx lc iq mt b gy my mz l na nb">2020-07-21 06:44:16,300 - gensim.models.ldamodel - DEBUG - 100/35600 documents converged within 200 iterations</span></pre><p id="77a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>结束时，大部分文档应该已经收敛了。如果没有，增加<code class="fe nh ni nj mt b"><strong class="kh ir">passes</strong></code>和<code class="fe nh ni nj mt b"><strong class="kh ir">iterations</strong></code>。</p><p id="7394" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提示5:使用pyLDAvis可视化主题关系</strong></p><figure class="mo mp mq mr gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/cdd0f83ed1de30a1e35f5570b055bd09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7wuMVJHP943MhXRuydKnBw.png"/></div></div></figure><p id="a412" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Python中的pyLDAvis包给出了两条重要的信息。圆圈代表每个主题。圆圈之间的距离显示了主题的关联性。这些通过对每个主题的概率分布之间的距离进行降维(PCA/t-sne)映射到2D空间。这表明我们的模型是否开发了不同的主题。我们希望调整模型参数和主题数量，以最小化圆圈重叠。</p><p id="f3fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主题距离也显示了主题的相关程度。专题1、2、13讨论电子产品(打印机、扫描仪、电话/传真)。象限3中的主题如6、14、19是关于办公文具(包装材料、便利贴、文件管理器)。此外，圆圈大小代表主题流行度。例如，主题1构成了文档中被谈论的主题的最大部分，构成了标记的17.1%。</p><p id="7e6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">提示#6:调整相关性分数，以优先考虑更专属于某个主题的术语</strong></p><p id="0740" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代表给定主题的单词可以排名较高，因为它们在语料库中是全局频繁的。相关性分数有助于优先考虑更专门属于给定主题的术语，使主题更加明显。术语w与主题k的相关性被定义为:</p><figure class="mo mp mq mr gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi np"><img src="../Images/690446ca491f63fa42e101f8d95fe52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*XwRPFZXPlskMBouZRMytbw.png"/></div></div></figure><p id="f1bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，ϕ_kw是单词w在主题k中的概率，ϕ_kw/p_kw是术语在主题中的概率相对于其在整个语料库中的边际概率的提升(这有助于丢弃全局频繁出现的术语)。λ越低，第二项(ϕ_kw/p_kw)越重要，第二项越重要，主题排他性越重要。为此我们可以再次使用pyLDAvis。例如，当将λ降低到0.6时，我们可以看到主题13对与电话主题更相关的术语进行了排序。</p><figure class="mo mp mq mr gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nq"><img src="../Images/f884abaeedf88288c58403adf80ba4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEWSLpw5_grU8Nhy31aScg.png"/></div></div></figure><p id="755a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">调整λ以获得最有意义的结果，并应用最佳λ值以获得输出:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="7138" class="mx lc iq mt b gy my mz l na nb">all_topics = {}<br/>lambd = 0.6  <strong class="mt ir"><em class="mn"># Adjust this accordingly</em></strong><br/><strong class="mt ir">for</strong> i <strong class="mt ir">in</strong> range(1,22): <strong class="mt ir"><em class="mn">#Adjust number of topics in final model</em></strong><br/>    topic = topic_data.topic_info[topic_data.topic_info\<br/>            .Category == 'Topic'+str(i)]<br/>    topic['relevance'] = topic['loglift']*(1-lambd)\<br/>                         +topic['logprob']*lambd<br/>    all_topics['Topic '+str(i)] = topic.sort_values(by='relevance\<br/>    , ascending=False).Term[:10].values</span></pre><h1 id="630b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">决赛成绩</h1><figure class="mo mp mq mr gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nr"><img src="../Images/e7ad50d70295cefa8066ffbd666a8f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3OAxcc62BWOiGnR3iq17Lw.png"/></div></div></figure><p id="91f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这里，我们可以围绕这些主题关键词进一步分析情感(例如，搜索相关的形容词或评论星级)。在业务应用程序中，这提供了对客户认为重要的主题的洞察，以及他们对此的感受。这使得有针对性的产品开发和客户体验的改善成为可能。这个例子包含了各种各样的产品，但是每个产品的单独主题模型可能揭示了客户关心的方面。例如，这一分析已经开始揭示计算器(主题21)的重要方面，如显示器、易按按钮、电池、重量。然后，销售者需要确保在他们的产品描述中突出这些特征，或者提高这些方面的竞争力。</p><h1 id="37a3" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated"><strong class="ak">来源:</strong></h1><p id="af56" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">[1]迈克尔·罗德尔、安德里亚斯都、亚历山大·欣内堡、<a class="ae mm" href="https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">探索话题连贯的空间措施</strong> </a></p><p id="af26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]卡森·西维尔，肯尼斯·e·雪莉，LDAvis: <a class="ae mm" href="https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">一种可视化和解释主题的方法</strong> </a></p></div></div>    
</body>
</html>