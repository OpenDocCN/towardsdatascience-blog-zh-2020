<html>
<head>
<title>Improved Naïve Bayes Classifier to Solve Text Classification Problems.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进的朴素贝叶斯分类器解决文本分类问题。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improved-na%C3%AFve-bayes-classifier-to-solve-classification-problems-a34d7a32e863?source=collection_archive---------32-----------------------#2020-11-03">https://towardsdatascience.com/improved-na%C3%AFve-bayes-classifier-to-solve-classification-problems-a34d7a32e863?source=collection_archive---------32-----------------------#2020-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="611b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/4d51bba9400a1bb840676112c96dac2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GiDcBfcMuqRLPQSV"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">斯蒂芬·菲利普斯-Hostreviews.co.uk在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片<a class="ae kl" href="https://unsplash.com/@hostreviews?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="2875" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本文致力于解释最经典的机器学习分类器之一——朴素贝叶斯。在这个故事中，我将解释朴素贝叶斯和<strong class="ko ja">背后的理论，最重要的是</strong>将呈现这个模型的一个简单的从零开始的<strong class="ko ja"> <em class="lk">实现</em> </strong>。对于进入机器学习领域的人来说，这篇文章将会非常有用和有趣。我也相信有ML经验的读者也会学到新的东西。所以，事不宜迟，让我们开始吧。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="ebca" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如艾萨克·纽敦曾经说过的那样:“<em class="lk">如果我比别人看得更远，那是因为我站在巨人的肩膀上</em>”，这是我自己相信的事情，就好像你想创造新的东西，你必须了解以前已经做过的事情。这可以让你不要在发明的东西上浪费时间，或者寻找机会来改进以前的方法和挑战现状。</p><p id="f9fb" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，我想谈谈如何使用朴素贝叶斯来解决文本分类问题，但不会触及最新的深度学习方法。这将使大多数读者能够跟踪和理解这篇文章，无论其教育背景如何。</p><p id="d402" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你是自然语言处理(NLP)领域的新手，我建议你在阅读这篇文章之前，先看看我的<a class="ae kl" rel="noopener" target="_blank" href="/text-generation-using-n-gram-model-8d12d9802aa0">上一篇文章，在那里我讨论了我们如何使用简单的概率规则来执行文本生成</a>。</p><p id="a596" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我们将从头开始使用朴素贝叶斯分类器<strong class="ko ja">创建一个垃圾邮件过滤器，而不使用任何</strong><strong class="ko ja"/><strong class="ko ja">外部库</strong>。首先，我想介绍一些理论，然后我们将讨论实现部分。</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ls"><img src="../Images/cd808c7ef17330d429873c3f9f908005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q17InKV9PXTNb5eG"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">照片由<a class="ae kl" href="https://unsplash.com/@hannes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">汉尼斯·约翰逊</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="5650" class="lx ly iq bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">理论。</h1><h2 id="6740" class="mv ly iq bd lz mw mx dn md my mz dp mh kx na nb ml lb nc nd mp lf ne nf mt iw bi translated">贝叶斯公式。</h2><p id="3322" class="pw-post-body-paragraph km kn iq ko b kp ng kr ks kt nh kv kw kx ni kz la lb nj ld le lf nk lh li lj ij bi translated">有很多文章讨论了朴素贝叶斯以及它在概念上是如何工作的，所以我不打算深入进行理论解释，而是讨论最基本和最重要的部分。</p><p id="44c4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们都知道贝叶斯公式是这样的:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/decbc03c3674699bd97e2a2d5b3528e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*2iCg_zTehdItcQoJU8ivhQ.png"/></div></figure><p id="faf0" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们的NLP设置中，我们想要计算什么是<em class="lk"> P(垃圾邮件|文本)</em>和什么是<em class="lk"> P(非垃圾邮件|文本)</em>。</p><blockquote class="nm nn no"><p id="9dc5" class="km kn lk ko b kp kq kr ks kt ku kv kw np ky kz la nq lc ld le nr lg lh li lj ij bi translated">对于常用的术语，我将把“非垃圾邮件”称为“火腿”。</p></blockquote><p id="d5a3" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">众所周知，任何电子邮件的文本通常由单词组成。因此，我们有兴趣计算<em class="lk"> P(spam | word1，word2，…，wordN) </em>。以贝叶斯公式的形式来说:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/706cd27b549736f66024ae33a06a9dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/1*Vtv3kyg8J3v5PnjayxkjyA.gif"/></div></figure><p id="027f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当我们计算类似的概率时，但是对于<em class="lk">火腿</em>的情况，我们将得到以下公式:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6a7ec490f513b90bd176db86461075c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/1*V65ahi8Bvne498mIvBcMNw.gif"/></div></figure><p id="79dc" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如您所见，分母<em class="lk"> P(w1，w2，…，wN) </em>【也称为<strong class="ko ja">归一化</strong>】在两种情况下是相同的，因为我们的任务是确定哪个概率更高(而不是确切的值)，所以我们可以去掉归一化部分。推理如下所示:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4312c904cccd366ac669fb7150bddf56.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/1*z3RsF_BfUuPEuH2qUYYK-w.gif"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">奇怪的符号表示不确定左侧(LHS)是大于还是小于右侧(RHS)。</p></figure><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a468102b10e8d98aa084ccb845a82273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/1*fdz3t328djmxevFjDyXLVg.gif"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">取消分母，因为我们知道概率是非负的。</p></figure><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a02078cefa0963ccbf58d86c2e288466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/1*hioyLHWqAOpmivFWJUQM3w.gif"/></div></figure><p id="ef2c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以最终，在贝叶斯分类中，我们得到了以下简化:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/04aeda38f2c060586553dfbd05f92191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/1*XkEuyer6_tRC6OidWnZuiA.gif"/></div></figure><blockquote class="nm nn no"><p id="88db" class="km kn lk ko b kp kq kr ks kt ku kv kw np ky kz la nq lc ld le nr lg lh li lj ij bi translated">旁注:标准化部分在大多数情况下被忽略，不仅因为它是多余的，而且因为从数据中计算通常是非常复杂的，所以不必处理它简化了我们的生活！</p></blockquote><h2 id="230b" class="mv ly iq bd lz mw mx dn md my mz dp mh kx na nb ml lb nc nd mp lf ne nf mt iw bi translated">我们为什么幼稚？</h2><p id="84b0" class="pw-post-body-paragraph km kn iq ko b kp ng kr ks kt nh kv kw kx ni kz la lb nj ld le lf nk lh li lj ij bi translated">概括地说，为了判断电子邮件是否是垃圾邮件，我们需要查看哪个值更高<em class="lk"> P(spam|w1，…，wN) </em>或<em class="lk"> P(ham|w1，…，wN) </em>。</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/04aeda38f2c060586553dfbd05f92191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/1*XkEuyer6_tRC6OidWnZuiA.gif"/></div></figure><p id="b62e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了计算<em class="lk"> P(spam|w1，…，wN) </em>，我们需要:</p><ol class=""><li id="3200" class="ny nz iq ko b kp kq kt ku kx oa lb ob lf oc lj od oe of og bi translated"><em class="lk"> P(垃圾邮件)–</em>很容易找到，因为它只是我们数据集中<strong class="ko ja">垃圾邮件</strong> <strong class="ko ja">电子邮件</strong>与<strong class="ko ja">所有电子邮件</strong>的比率。</li><li id="7340" class="ny nz iq ko b kp oh kt oi kx oj lb ok lf ol lj od oe of og bi translated"><em class="lk"> P(w1，…，wN | spam)</em>–直接计算有点困难，所以让我们使用“天真”的假设，即文本中的单词是独立的，这样我们可以将这个术语简化为:</li></ol><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0b5691e303cb74beae332058d9fc2174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/1*zAoD_SFhs9KDYRzsxWWnkg.gif"/></div></figure><p id="93b5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以最后为了计算<em class="lk"> P(spam|w1，…，wN) </em>我们会做:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4722f614de684c94f25f8ed706d3f2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/1*5U6z_6Hojsg2OVbisakPAg.gif"/></div></figure><p id="f5ab" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">看起来很简单，但是，让我们不要犯一个会破坏我们结果的错误。考虑一下，如果我们需要检查一个单词<em class="lk"> w_i </em>但它从未出现在垃圾邮件中，会发生什么？</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/075871cb6165e66f6b9329b80f83ffea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/1*I994NeYT0jh-DeCCcLHogg.gif"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">我们如何计算一般情况下的概率？</p></figure><p id="c14e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">换句话说:<em class="lk"> P(w_i|spam) = 0。</em></p><p id="99ab" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">显然，在这种情况下，由于乘法运算，我们最终会得到<em class="lk"> P(spam|w1，…，wN) = 0 </em>，这可能会导致错误的结论，因为我们永远无法假设拥有包含所有单词的完美训练数据。也有可能一个单词包含了错别字，因此系统显然不知道它。(还要考虑这个词从未在<em class="lk">垃圾邮件</em>和<em class="lk">火腿</em>邮件中出现的情况，那么我们该如何对包含这个词的邮件进行分类呢？)</p><p id="51d4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了解决这个问题，使用了一种<strong class="ko ja">平滑</strong>方法，确保我们将<em class="lk">非零</em>概率分配给任何项:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi op"><img src="../Images/09e18efba8baaecca1424863133d6de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iLwG3Vkm-oSLMOsPHBp2lQ.gif"/></div></div></figure><p id="032c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里α ( <em class="lk"> alpha </em>)是一个平滑参数，应该是一个非负数。</p><h2 id="2ce8" class="mv ly iq bd lz mw mx dn md my mz dp mh kx na nb ml lb nc nd mp lf ne nf mt iw bi translated">提高效率——使用对数。</h2><p id="d05e" class="pw-post-body-paragraph km kn iq ko b kp ng kr ks kt nh kv kw kx ni kz la lb nj ld le lf nk lh li lj ij bi translated">正如我们之前所看到的，为了计算概率，我们将不得不进行大量的乘法运算<strong class="ko ja"/>(与我们要评估的电子邮件中的单词一样多的次数):</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4722f614de684c94f25f8ed706d3f2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/1*5U6z_6Hojsg2OVbisakPAg.gif"/></div></figure><p id="cef4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">问题是所有的概率都小于1，一般来说都很小。众所周知，2个小值0 &lt; <em class="lk"> a，b </em> &lt; 1的乘积会产生一个更小的数。因此，在朴素贝叶斯方法的情况下，我们将受到我们计算机的浮点精度的限制(因为在某些情况下，spam和ham方程将会收敛到零，只是因为计算机在处理非常小的数字时的限制)。</p><p id="661b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们该怎么办？我们需要把我们的计算转移到能够记录非常小的数字的超级计算机上吗？幸运的是，这是不需要的，对数来帮助！而且我们会用对数的<strong class="ko ja">乘积法则</strong>。</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/86138e23e68caa80d9f0146006aef3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/1*9aqxNmQJliYzJ68_nRZRjA.gif"/></div></figure><p id="8cd8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们使用对数的另一个原因是:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9b40444df649fa0800508574141abb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/1*t449sY3vD2lbbzCZLB77rA.gif"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">这意味着如果A &gt; B，则log(A) &gt; log(B)，反之亦然</p></figure><pre class="lt lu lv lw gt os ot ou ov aw ow bi"><span id="aad3" class="mv ly iq ot b gy ox oy l oz pa">Hence, <strong class="ot ja">if</strong> <em class="lk">log(P(spam|text)) &gt; log(P(ham|text)) <br/></em><strong class="ot ja">then</strong>  <em class="lk">P(spam|text) &gt; P(ham|text)</em></span></pre><p id="d3f7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以对于我们来说，用对数来计算概率会容易得多，对数的形式如下:</p><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/80d3adee36321160d934f8c734699a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*3liDcZ5MLWp7T-kyBQLmwg.gif"/></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="9192" class="lx ly iq bd lz ma pc mc md me pd mg mh mi pe mk ml mm pf mo mp mq pg ms mt mu bi translated">实施。</h1><figure class="lt lu lv lw gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ph"><img src="../Images/d9d2d903fd0b6b0afbd572ad704a3b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4VCso-KYvXl5O2HB"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">照片由<a class="ae kl" href="https://unsplash.com/@swimstaralex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alexander Sinn </a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="cec7" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">理论说够了，让我们把理论带入生活。正如我之前提到的，我将从头开始实现朴素贝叶斯分类器，并且不会使用任何python的外部库，因为它允许实现多种功能:</p><ol class=""><li id="2b76" class="ny nz iq ko b kp kq kt ku kx oa lb ob lf oc lj od oe of og bi translated">它帮助我们更好地理解朴素贝叶斯背后的理论。对于刚接触机器学习领域的读者来说是非常有益的。</li><li id="6043" class="ny nz iq ko b kp oh kt oi kx oj lb ok lf ol lj od oe of og bi translated">它允许我们有一些灵活性并改进标准方法。在本文中，我们将同时使用单字和双字(如果你不知道什么是双字，请阅读本文)。</li></ol><p id="8c9e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">完整的代码和数据集可以在我的<a class="ae kl" href="https://github.com/olegborisovv/NaiveBayesSpamClassifier" rel="noopener ugc nofollow" target="_blank"> github库</a>上找到。</p><p id="436a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">数据集包含原始格式的电子邮件，因此为了处理电子邮件数据，我们必须使用python的<code class="fe pi pj pk ot b">email</code>库。</p><p id="d0ea" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们看看<strong class="ko ja">支持功能</strong>:</p><figure class="lt lu lv lw gt ka"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="8ed6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一个是记号化器，在你的修改中可以随意使用任何其他方法，但是这里这个方法记号化方法工作得足够好(你会看到我们的精度会有多好)。</p><p id="3881" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面你可以看到我们将用来计算垃圾邮件的对数概率的函数。</p><figure class="lt lu lv lw gt ka"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="e732" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如前所述，我将在这里使用unigrams <strong class="ko ja">和</strong> bigrams来识别电子邮件是否是垃圾邮件，因此我们正在计算</p><p id="8901" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk"> P(spam|token_1，token_2，… token_n) </em>，其中<em class="lk"> token_i </em>可以由一元或二元表示。<strong class="ko ja">这比只在单词(单字)层面上工作要好，因为我们变得不那么幼稚</strong>(因为我们接受文本中单词的某种依赖性)。</p><p id="7669" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了我们在理论部分讨论的内容，我还介绍了一些特殊的标记，如<code class="fe pi pj pk ot b">&lt;UNK&gt;</code>，如果我们偶然发现在训练集中没有见过的<em class="lk">未知</em>单词，我们将在推理部分使用这些标记。</p><p id="7eb6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除此之外，我创建了一个记号<code class="fe pi pj pk ot b">&lt;LONG_W&gt;</code>，它对应于大于12个字符的长单词<em class="lk">和</em>，其概率的计算与所有其他记号类似。</p><h2 id="3fd0" class="mv ly iq bd lz mw mx dn md my mz dp mh kx na nb ml lb nc nd mp lf ne nf mt iw bi translated">垃圾邮件过滤器类别。</h2><p id="9579" class="pw-post-body-paragraph km kn iq ko b kp ng kr ks kt nh kv kw kx ni kz la lb nj ld le lf nk lh li lj ij bi translated">现在让我们转到垃圾邮件过滤器类的实现:</p><figure class="lt lu lv lw gt ka"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="4ce9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该类的初始化需要大约10秒钟，因为我们需要遍历所有垃圾邮件和业余邮件，并计算所有词类相关概率。</p><p id="3cf5" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><code class="fe pi pj pk ot b">is_spam</code>函数完成所有的推理工作，并负责根据文本中出现的标记判断电子邮件是否是垃圾邮件。因为这个函数确实做了我们在理论部分讨论过的事情，所以我不打算提供实现的详细解释。</p><p id="542a" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们创建主函数，并测试我们的解决方案有多好。</p><figure class="lt lu lv lw gt ka"><div class="bz fp l di"><div class="pl pm l"/></div></figure><p id="7417" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">结果我们得到:</p><pre class="lt lu lv lw gt os ot ou ov aw ow bi"><span id="0edc" class="mv ly iq ot b gy ox oy l oz pa">done with initialization!    9.24 s<br/>spam errors: 1 <br/>ham errors: 1<br/>correct identified: 0.995</span></pre><p id="ad59" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">99.5%的准确率，只有1个假阴性和1个假阳性，这是一个<strong class="ko ja">非常</strong> <strong class="ko ja">骄人的成绩</strong>！</p><p id="f926" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">随意改变一些参数，比如平滑，或者不使用二元模型，而只使用一元模型，看看模型的准确性降低了多少。在这里，我已经提出了达到完美效果的最佳模型。</p><p id="e298" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如您从本文中看到的，朴素贝叶斯是一个非常简单且易于实现的机器学习模型，它能够在语言分类任务中实现一些令人难以置信的结果。</p><p id="976f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最不可思议的是，我们只用了10秒钟就训练好了模型，并且达到了非常好的精度。将其与深度学习方法进行比较，深度学习方法需要几个小时的训练，并且很可能只能达到与我们的朴素贝叶斯相似的性能。</p><p id="4bc8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你想让我在下一篇文章中涉及一些ML话题，请在评论中告诉我。</p><p id="9785" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">敬请关注更多关于人工智能和自然语言处理的文章。</p></div></div>    
</body>
</html>