<html>
<head>
<title>A Deep Dive Into The Concept of Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对回归概念的深入探究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deep-dive-into-the-concept-of-regression-fb912d427a2e?source=collection_archive---------33-----------------------#2020-09-24">https://towardsdatascience.com/a-deep-dive-into-the-concept-of-regression-fb912d427a2e?source=collection_archive---------33-----------------------#2020-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0caa3141cefc25befc38b8991ddb998c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPZap7X4nhqiLrkQZe-6wQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">弗兰基·查马基在<a class="ae jg" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="896d" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">回归背后的数学</h2></div><p id="20f7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回归是机器学习中最重要的概念之一。在这篇博客中，我们将讨论不同类型的回归和基本概念。</p><p id="5b96" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回归类型的变化如图所示:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/65c12c798fa868b8e12b9f3d6ddee5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSFn-uIYDhDfdaG5GXlB3A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">回归的详细表示</p></figure><p id="3b8b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将详细讨论所有要点。</p><h2 id="5555" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">线性回归</h2><p id="8415" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">回归任务处理从一组独立变量(即提供的特征)中预测因变量的值。比方说，我们想预测一辆汽车的价格。因此，它成为一个因变量，比如Y，而发动机容量、最高速度、级别和公司等特征成为自变量，这有助于构建方程以获得价格。</p><p id="ffd8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果有一个特征，比如说x。如果因变量y线性依赖于x，那么它可以由y=mx+c给出，其中m是方程中该特征的系数，c是截距或偏差。M和C都是模型参数。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/6dfdd7bb0d5bace013d83be1f701baec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*mNd3iHeT5QE89qmsPBl5uw.png"/></div></figure><p id="a615" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">说红叉是点。X1是特征值，Y1是预测值，Y_pred 1是实际目标值。这里我们可以看到X1的预测值和实际值是相同的。但是，对X2来说，它们是不同的。因此，我们的工作是给出一条最佳拟合线，准确地描绘出因变量和目标变量之间的关系。现在，首先随机初始化模型参数M和C，比如M1和C1，如图中第1行所示。很明显，这条线不太合适。然后我们移到由参数M2和C2给出的第2行。这条线提供了一个更好的拟合，并作为我们的回归。它不会总是给出精确的拟合，但它给出了可能的最佳拟合。</p><p id="ad6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，问题就来了，我们如何从第一行到达第二行？答案在于成本函数和梯度下降的概念。为了获得最佳拟合线，我们在一组给定值上训练模型，该给定值由特征值和相应的目标值(Y实际值)组成。因此，我们在目标方程y=Mx+C中拟合特征值，我们获得Y _预测值。我们的目标是减少Y _预测值和Y _实际值之间的差异。</p><p id="8069" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们使用一个损失函数或成本函数，称为均方误差(MSE)。它由因变量的实际值和预测值之差的平方给出。</p><blockquote class="my"><p id="9119" class="mz na jj bd nb nc nd ne nf ng nh lt dk translated">MSE = 1/2m *(Y _ actual-Y _ pred)</p></blockquote><p id="41d3" class="pw-post-body-paragraph ky kz jj la b lb ni kk ld le nj kn lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">如果我们观察这个函数，我们会看到它是一条抛物线，也就是说，这个函数本质上是凸的。这个凸函数就是梯度下降法中用来获得M2和C2值的原理。</p><h2 id="ccb7" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">梯度下降</h2><p id="51e0" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">这种方法是最小化损失函数和实现我们目标的关键，我们的目标是预测接近原始值。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/dc931664213865c461d51bed2ec8dd4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kxcxtj3TP4rpMkHQ.png"/></div></div></figure><p id="b515" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这张图中，我们看到了损失函数图。为了找到最小损失函数值，我们需要找到一个特定的全局最小值。因此，我们总是试图使用一个凸形的损失函数来得到一个合适的最小值。现在，我们看到预测结果取决于等式Y=Mx +C或Y=Wx+C中的权重/系数。图中X轴上的权重和Y轴上的相应损失值。最初，模型为特征分配随机权重。假设它初始化了权重=a，我们可以看到它产生了一个损失，这个损失远离最小点L-min。</p><p id="d37f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们可以看到，如果我们将权重更多地移向x轴的正方向，我们可以优化损失函数并实现最小值。但是，模型怎么知道呢？我们需要优化权重以最小化误差，因此，显然，我们需要检查误差如何随权重变化。为此，我们需要找到误差相对于重量的导数。这个导数叫做梯度。</p><blockquote class="no np nq"><p id="e23c" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated"><strong class="la jk"> <em class="jj">渐变= dE/dw </em> </strong></p></blockquote><p id="7073" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中E是误差，w是重量。</p><p id="712a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看这是如何工作的。比方说，<strong class="la jk">如果损失随着重量的增加而增加，那么梯度将是正的，</strong>那么我们基本上在C点，我们可以看到这个陈述是正确的。<strong class="la jk">如果损失随着重量的增加而减少，那么梯度将为负</strong>。我们可以看到A点，对应着这样一种情况。现在，从A点我们需要向x轴的正方向移动，梯度是负的。从C点开始，我们需要向负x轴移动，但是梯度是正的。<strong class="la jk">因此，梯度的负值总是表示权重应该移动的方向，以便优化损失函数。</strong>因此，梯度以这种方式指导模型是增加还是减少权重，以便优化损失函数。</p><p id="0077" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型找到了移动的方向，现在模型需要找到应该移动多少重量。这由称为<strong class="la jk">的参数决定，学习率由α</strong>表示。我们看到的图表中，重量从A点移动到距离为<em class="nr"> dx的B点。</em></p><blockquote class="no np nq"><p id="4629" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated"><strong class="la jk"><em class="jj">dx = alpha * | dE/dw |</em></strong></p></blockquote><p id="d11c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，移动的距离是学习率参数α和误差变化幅度与该点权重变化的乘积。</p><p id="412c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们需要非常仔细地决定学习率。如果它很大，权值会有很大的变化，会超过最佳值。如果它非常低，它需要很小的步骤，需要很多步骤来优化。根据以下公式改变更新的权重。</p><blockquote class="no np nq"><p id="99ef" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated"><strong class="la jk"><em class="jj">w = w—alpha * | dE/dw |</em></strong></p></blockquote><p id="9d77" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中w是之前的权重。</p><p id="34e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每个时期，模型根据梯度移动权重以找到最佳权重。</p><p id="3f86" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，使用该过程，我们获得了模型参数M2和C2的最终值，这给了我们实际的最佳拟合线。</p><h2 id="b7dc" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">回归的类型</h2><ol class=""><li id="3584" class="nv nw jj la b lb ms le mt lh nx ll ny lp nz lt oa ob oc od bi translated"><strong class="la jk">单变量、双变量和多变量</strong></li></ol><p id="81cf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在上面的例子中看到，我们将因变量表示为单个自变量的函数，因此我们使用y=wx+c。这种类型的回归称为<strong class="la jk">单变量</strong>，因为只有一个自变量。</p><p id="544c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果方程中有两个自变量，则变成:</p><blockquote class="no np nq"><p id="59b5" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y=w1x1+w2x2+c</p></blockquote><p id="f1cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这被称为双变量回归。</p><p id="c7f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果有两个以上的独立变量，即因变量依赖于两个以上的特征，则相应地修改等式。</p><blockquote class="no np nq"><p id="3167" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y=w1x1+w2x2+w3x3+…………..+wkxk+c</p></blockquote><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dd49f5973e8abe22b0cedb58aef025c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*06Qkb60zoIJzn-3v.png"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6836116c05274bb44d9baeb610677e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*7oB9dh3Chv2-0nMv"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0a0f110f8f1d2b9b93ace216ee933243.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*zszYErTOC7bB-ES9.png"/></div></figure><p id="0f8f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这叫做<strong class="la jk">多元</strong>回归。</p><p id="9937" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la jk">线性和多项式</strong></p><p id="0141" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，我们已经看到了线性回归问题，即每个自变量的次数为1。</p><p id="df54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果我们有一个如下所示的非线性曲线，我们就不能画了</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/62515107dd55631475f444aaba1ca28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*vR8ah-zYTBZJArenaz5HXw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">非线性曲线</p></figure><p id="5dbd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最佳拟合线，如果我们使用线性回归。在这种情况下，我们必须选择多项式回归。多项式回归将要素或独立变量提升到幂，并创建多项式方程而不是线性方程。它被给出为:</p><blockquote class="no np nq"><p id="a653" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y=w1x1 +w2x2 + w3x3 +w4x4⁴ +………..+wkxk^k +c</p></blockquote><p id="c2c0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这里的w1，w2，w3 …..wk是模型的参数，幂是模型的超参数。所以，多项式的次数也是一个超参数。</p><p id="9e3d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设y=ut+1/2gt是一个给定的方程，u和1/2g是模型系数或参数。</p><p id="938b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这被称为<strong class="la jk">多项式回归</strong>。</p><h2 id="76a5" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">里脊回归</h2><p id="f3db" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">让我们考虑一种情况:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9e4a66ca8c356789947334afb11921fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*TM7TfkBDVm_YG7kIkO2uVQ.png"/></div></figure><p id="e278" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这里的红点表示训练集的点，蓝点表示测试集的点。我们可以看到，如果我们使用红点进行训练，我们将获得红线(1)，作为回归直线，但是非常明显的是，线(1)将具有非常差的泛化能力，并且不会在测试数据上令人满意地执行。这种情况被认为具有导致过度拟合的高方差。这就是<strong class="la jk">岭回归</strong>发挥作用的地方。它修改了成本函数并增加了正则项。如果我们仔细观察，我们会发现第(2)行是一个更好的概括，并且会执行得更好。线(1)对于以下等式具有非常高的权重和系数值:</p><blockquote class="no np nq"><p id="bd61" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y=w1x1+w2x2+……………..+wkxk。</p></blockquote><p id="e1fc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这导致了过度拟合。岭回归背后的思想是惩罚权重或系数的平方和，以带来正则化效果。比方说，我们的等式是</p><blockquote class="no np nq"><p id="2186" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y_pred=w1x1+w2x2</p></blockquote><p id="a4b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">损失函数变成:</p><blockquote class="no np nq"><p id="8f5e" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">损耗= 1/2m。(Y _ actual—Y _ pred)+λ/2m。(w1 + w2)</p></blockquote><p id="667c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二部分是正则化部分，其集中于惩罚权重或系数，并防止它们达到高值。由于该函数是我们的损失函数，梯度下降也导致对权重的检查，因为权重的增加反过来增加了损失函数。</p><p id="8270" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">λ</strong>参数是通过交叉验证决定的，它是衡量我们在多大程度上专注于调整我们的权重的指标。λ的值越大，正则化程度越大。因此，lambda解决了一个权衡问题。</p><p id="086a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种技术也用于神经网络的L2正则化。</p><h2 id="1f42" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">套索回归</h2><p id="cf80" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">Lasso回归也解决了线性回归中同样的过度拟合问题。区别在于损失函数的修改方式。套索回归惩罚的是权重之和的绝对值，而不是权重的平方。</p><p id="fd81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们的等式是</p><blockquote class="no np nq"><p id="76b4" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y_pred=w1x1+w2x2</p></blockquote><p id="c955" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">损失函数变成:</p><blockquote class="no np nq"><p id="4e21" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">损耗= 1/2m。(Y _ actual—Y _ pred)+λ/2m。(|w1| + |w2|)</p></blockquote><p id="8bcc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果权重值太小，它们几乎等于0。如果w2等于0，w2x2变为0，这表明特征x2对于y的预测不重要</p><p id="1da8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，套索回归被用于特征选择机制。</p><p id="56eb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相同的逻辑用于神经网络的L1正则化。</p><h2 id="3c10" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">分类任务</h2><p id="1ad1" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">任务的类型包括基于一组特征将实体分类到一些给定的类中。逻辑回归用于分类任务。</p><h2 id="04dc" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">为什么不是线性回归？</h2><p id="08a3" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">线性回归通常基于构建最佳拟合线。让我们先看看如何使用线性回归进行分类。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9bd42229291c6d395eac57ddb17e3b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*-7OulkJe5rZvyg_GffY2Jg.png"/></div></figure><p id="a208" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们观察上面的图像，“1”表示一个类，“0”表示另一个类。如果我们得到最佳拟合线和虚线边界(如图所示)，我们就可以很容易地用它来分类。当投影到最佳拟合线上时，蓝点或0级总是小于0.5，这是由虚线提供的判定边界值。相应地，1级或红点给出大于0.4的值。</p><p id="1312" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们来看看问题。</p><p id="f0ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑下面的情况:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/41493004cba9a1e9e8f7f2a17a5b46f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_QW0AZXtMCcZDFZiD-mJA.png"/></div></div></figure><p id="00be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们可以看到由于异常值而导致的最佳拟合线偏移，这可能会在分类中产生错误，从而导致不令人满意的结果。同样，一些点将给出小于0和大于1的y值。因此，使用线性回归，y可以取从-无穷大到+无穷大的值，但是对于分类问题，y的范围必须从0到1。</p><p id="3254" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上几点是线性回归不能用于分类任务的原因。</p><h2 id="f471" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">逻辑回归</h2><p id="8167" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我们知道，</p><blockquote class="no np nq"><p id="4edb" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">Y= w1x1+w2x2+w3x3……..wkxk</p></blockquote><p id="f2e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用作线性回归的方程式。</p><p id="8e94" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，RHS -infinity到+infinity的值。因此，我们必须将LHS变换到从-无穷到+无穷的范围内。</p><p id="9a02" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了实现这一点，我们使用log(Y/1-Y)作为LHS。</p><blockquote class="no np nq"><p id="d7b5" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">log(Y/1-Y)= w1x1+w2x2+w3x3……..wkxk</p><p id="df5e" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">= &gt; y = 1/1+e^-(<strong class="la jk">w1x 1+w2x 2+w3x 3……..wkxk) </strong></p><p id="0167" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">=&gt; Y= 1/(1+e^-transpose(theta).x)</p><p id="a7be" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">其中，转置(θ)。x=( <strong class="la jk"> w1x1+w2x2+w3x3……..wkxk) </strong></p></blockquote><p id="fa8b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，</p><blockquote class="my"><p id="1bd0" class="mz na jj bd nb nc nd ne nf ng nh lt dk translated">Y=1/(1+e^(-转置(θ)。x))</p></blockquote><p id="4b45" class="pw-post-body-paragraph ky kz jj la b lb ni kk ld le nj kn lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">也称为Sigmoid函数，范围从0到1。θ描述系数向量或矩阵的权重，x是特征矩阵。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div></figure><p id="7f1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图显示了Sigmoid函数。0.5是函数的边界，不取小于0大于1的值。</p><p id="57b2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果sigmoid函数由Y轴上的h(x)给出，则是g的函数(transpose(theta)。X)在X轴上，那么，</p><blockquote class="my"><p id="abc1" class="mz na jj bd nb nc nd ne nf ng nh lt dk translated">h(x)=g(转置(θ)。x)</p></blockquote><p id="677a" class="pw-post-body-paragraph ky kz jj la b lb ni kk ld le nj kn lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">我们可以得出以下结论，</p><blockquote class="no np nq"><p id="45b4" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">h(x)&gt;0.5</p><p id="d5a5" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">=&gt;g(转置(θ)。x)&gt;0.5</p><p id="d1f0" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">= &gt;转置(θ)。x&gt;0</p></blockquote><p id="668a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再说一遍，</p><blockquote class="no np nq"><p id="4979" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">h(x) &lt;0.5</p><p id="3c8f" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">=&gt; g(转置(θ)。x) &lt;0.5</p><p id="5749" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">=&gt;转置(theta)。x &lt;0</p></blockquote><p id="2b39" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">So, if <em class="nr">转置(theta)。x，即</em> <strong class="la jk"> w1x1+w2x2+w3x3……..wkxk &gt; 0如果w1x1+w2x2+w3x3，则实体被分类为1类..wkxk &lt; 0，实体归类为0类。</strong></p><p id="9c69" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策边界如下所示:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2ad92f71315405198d67896ca719b52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*n8DAspyo-7fh3OuB6CORjw.png"/></div></figure><p id="1d27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">绿线表示决策边界，等式表示该线的等式。现在，从线性代数中，我们知道，如果我们选择一个点(a，b，c，d)并将其拟合到方程中，如果它在线上，结果将大于0，如果它在线下，结果将小于0。这个类比和我们上面看到的非常相似，class 1 if <strong class="la jk"> w1x1+w2x2+w3x3……..wkxk &gt; 0 </strong>和class 0 if <strong class="la jk"> w1x1+w2x2+w3x3……..wkxk &lt; 0。</strong></p><p id="00c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是逻辑回归的工作原理。</p><h2 id="c5b1" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">损失函数</h2><p id="fdd3" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">在线性回归的情况下，我们使用MSE或均方差。MSE由下式给出:</p><blockquote class="no np nq"><p id="9212" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">MSE =(Y _ actual-h(x))</p><p id="4bc1" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">h(x)= <strong class="la jk"> w1x1+w2x2+w3x3……..wkxk </strong></p></blockquote><p id="f7d4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种情况下，MSE是一个抛物线函数，并且是一个具有明显最小值的纯凸函数。但是在这种情况下，</p><blockquote class="no np nq"><p id="4c09" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">h(x)= 1/(1+e^-(<strong class="la jk">w1x 1+w2x 2+w3x 3……..wkxk)) </strong></p></blockquote><p id="70a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，成本函数的修改扭曲了它的凸性，并且现在有多个最小值。除此之外，如果我们使用MSE，误差的大小将非常小，最大值(1–0)= 1，因为Y可以取的最大值是1，最小值是0，惩罚也是如此。</p><p id="54ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于该区域，我们使用对数损失或二元交叉熵进行逻辑回归。</p><p id="dd41" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">比如说，Y_pred=h(x)，那么，<strong class="la jk">成本(h(x)，Y_actual)= </strong></p><blockquote class="no np nq"><p id="8c42" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">—Y _ actual = 1时的log(h(x))</p><p id="1e22" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">-Y _ actual = 0时的log(1 — h(x))</p></blockquote><p id="0741" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将两者结合起来，该方程被构造为:</p><blockquote class="my"><p id="ce71" class="mz na jj bd nb nc nd ne nf ng nh lt dk translated">损失=—Y _实际。log(h(x))—(1—Y _ actual . log(1—h(x)))</p></blockquote><p id="bf9b" class="pw-post-body-paragraph ky kz jj la b lb ni kk ld le nj kn lg lh nk lj lk ll nl ln lo lp nm lr ls lt im bi translated">如果Y_actual=1，第一部分给出误差，否则第二部分给出误差。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/0bf96aaa9e4eadbdc13290877afe9579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GZiV3ph20z0N9QSwQTHKqg.png"/></div></div></figure><p id="d5d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图显示了对数损失或二元交叉熵。如果实际标签为0，而预测标签向1移动，则损失函数接近无穷大，反之亦然。所以，损失多了，罚款也多了。</p><h2 id="fca9" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">逻辑回归的类型</h2><p id="925c" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">有三种逻辑回归，</p><ol class=""><li id="0069" class="nv nw jj la b lb lc le lf lh oo ll op lp oq lt oa ob oc od bi translated">二项式回归</li><li id="5662" class="nv nw jj la b lb or le os lh ot ll ou lp ov lt oa ob oc od bi translated">多项式回归</li><li id="b387" class="nv nw jj la b lb or le os lh ot ll ou lp ov lt oa ob oc od bi translated">有序回归</li></ol><p id="468c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">定义在开头的图表中给出。我们将讨论二项式和多项式逻辑回归。</p><p id="8d8a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">二项式回归是对两个类别进行分类，比如猫和狗。它的行为方式与上面讨论的方式相同。</p><h2 id="3d18" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">多项式逻辑回归或多类分类</h2><p id="9e34" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">在这种情况下，存在多个类别，一个实体可以被分类为。例如，有四个类，猫、狗和狮子。图像可以被分类为这三类中的任何一类。这是多项式分类的一个例子。需要注意的一点是，这三个类是相互独立的。</p><p id="9e05" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了解决这些问题，我们主要是将多项逻辑回归转换为二项式回归。</p><p id="ab67" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有两种方法:</p><ol class=""><li id="5805" class="nv nw jj la b lb lc le lf lh oo ll op lp oq lt oa ob oc od bi translated"><strong class="la jk">简单方法:</strong>该方法为K个单独的类中的每一个创建K个单独的逻辑回归模型。每个模型都有一个sigmoid输出节点，表示该特定类的输出。对于我们的模型，将有3个乙状结肠节点，第一个用于猫，第二个用于狗，等等。现在，第一个节点给出图像是猫P_c的概率，第二个节点给出图像是P_d的概率，类似地，我们得到狮子的P_l。现在，比较分数以获得最终答案。如果P_c在所有3个中具有最大值，则图像被预测为猫。</li><li id="c245" class="nv nw jj la b lb or le os lh ot ll ou lp ov lt oa ob oc od bi translated"><strong class="la jk">同步方法:</strong>该方法创建K-1个独立的逻辑回归。每一个概率都被认为是一组独立的事件，即节点不是决定一幅图像是猫还是狮子(0/1)，而是决定这幅图像是猫还是狮子(A/B)。</li></ol><p id="35bc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们已经看到了逻辑回归:</p><blockquote class="no np nq"><p id="1968" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">log(Y/1-Y)= w1x1+w2x2+w3x3……..wkxk</p></blockquote><p id="73c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果Y = &gt; 0.5类else类所以，Y可以说P(A)，1-Y等价于P(C)。这被称为<strong class="la jk">赔率。</strong>因此，对于第一个模型:</p><blockquote class="no np nq"><p id="4e86" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">log(P(A)/P(C))= w1 _ 1x1 _ 1+w2 _ 1x 2 _ 1+w3 _ 1x 3 _ 1……..wk_1xk_1+b_1</p><p id="44e6" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">比如说，R1 = w1 _ 1x 1 _ 1+w2 _ 1x 2 _ 1+w3 _ 1x 3 _ 1……..wk_1xk_1+b_1</p><p id="8f8e" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">P(A)/P(C)= exp(R _ 1)———1</p></blockquote><p id="cb9c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再次对于第二个模型，</p><blockquote class="no np nq"><p id="fe94" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">log(P(B)/P(C))= w1 _ 2 x1 _ 2+w2 _ 2 x2 _ 2+w3 _ 2 x3 _ 2……..wk_2xk_2 + b_2</p><p id="c19e" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">比如说，R _ 2 = w1 _ 2 x1 _ 2+w2 _ 2 x2 _ 2+w3 _ 2 x3 _ 2……..wk_2xk_2 + b_2</p><p id="4ae6" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">P(B)/P(C)= exp(R _ 2)———2</p></blockquote><p id="c557" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，根据上面的等式1和2:</p><blockquote class="no np nq"><p id="27b3" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">P(A)=P(C)*exp(R_1)</p><p id="87f5" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">P(B)=P(C)*exp(R_2)</p></blockquote><p id="dd13" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，<strong class="la jk"> P(A)+P(B) +P(C) =1 </strong></p><p id="62db" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，P(C)* exp(R1)+P(C)* exp(R2)+P(C)= 1</p><blockquote class="no np nq"><p id="a217" class="ky kz nr la b lb lc kk ld le lf kn lg ns li lj lk nt lm ln lo nu lq lr ls lt im bi translated">p(C)= 1/(1+exp(R1)+exp(R2))</p></blockquote><p id="5510" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们可以将它们联系起来。</p><h2 id="75e9" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">Softmax回归</h2><p id="4537" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">Softmax回归由下式给出:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/40630945bfcddce3b360a5ff11cd5d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*pT268Ce1hzaZbrfh7c-7kw.png"/></div></figure><p id="0756" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">softmax图层用作多类分类的最终图层或输出图层。该函数是一种指数方法。所有类别的概率之和等于1。最大可能类作为输出给出。</p><h2 id="bf12" class="lz ma jj bd mb mc md dn me mf mg dp mh lh mi mj mk ll ml mm mn lp mo mp mq mr bi translated">结论</h2><p id="e9d6" class="pw-post-body-paragraph ky kz jj la b lb ms kk ld le mt kn lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">在本文中，我们已经讨论了与回归相关的所有概念。希望这有所帮助。</p></div></div>    
</body>
</html>