<html>
<head>
<title>A Gentle Introduction to Web Scraping with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python温和地介绍Web抓取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-web-scraping-with-python-b914a64b2fb8?source=collection_archive---------20-----------------------#2020-10-30">https://towardsdatascience.com/a-gentle-introduction-to-web-scraping-with-python-b914a64b2fb8?source=collection_archive---------20-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="804f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用几行代码编写你的第一个scraper</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0a44e51276e0ddcfd2816d50a36dad8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*62eoqMDR4SfQ0Crz"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@pankajpatel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">潘卡杰·帕特尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="c8d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据科学只有在有数据的情况下才有可能，而在现实世界中，数据通常不会在一个<em class="ls">里等着你。csv </em>文件。你必须去追求它。这就是为什么网络抓取对你的数据科学工具箱非常重要。</p><p id="a546" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是如果你是一个完全的初学者，刮擦看起来有点复杂。如果你正在寻找开始抓取，并想知道如何写你的第一个网页抓取工具在一个简单，快速的方式，这篇文章是给你的。</p><p id="750a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理解这篇文章不需要以前的刮痧知识。我只是假设你知道什么是网络抓取，并且你也知道一些Python的基础知识。</p><p id="9981" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将以quotes.toscrape.com网站为例。这个网站包含数百个著名的报价，他们的作者，也有一些标签来描述每一个报价。这是我们要收集的信息。还有，从网站名称可以推断出是为刮痧而做的。但并不是互联网上的每个页面都是如此，所以在你开始抓取之前，确保你抓取的网站允许你这样做。</p><h1 id="f19d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">代码</h1><p id="006a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">对于我们将在本文中编写的scraper，我们需要三个库:</p><ul class=""><li id="2df0" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><em class="ls">熊猫</em>，用于数据操作。如果你知道一些用于数据分析的Python，你可能已经熟悉它了；</li><li id="2e9a" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><em class="ls"> Urllib </em>，打开并阅读页面；</li><li id="c6a0" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><em class="ls"> BeautifulSoup </em>，解析HTML，使得提取数据更加容易。</li></ul><p id="9309" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你还没有安装，BeautifulSoup 是你唯一需要手动安装的。如果您使用<em class="ls"> pip </em>，只需运行以下命令:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="35b2" class="nj lu iq nf b gy nk nl l nm nn">pip install beautifulsoup4</span></pre><p id="06b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装好所有东西后，我们可以导入，用<em class="ls"> urllib </em>获取URL，用<em class="ls"> BeautifulSoup </em>解析源代码。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="6373" class="nj lu iq nf b gy nk nl l nm nn">import pandas as pd<br/>from urllib.request import urlopen<br/>from bs4 import BeautifulSoup</span><span id="9291" class="nj lu iq nf b gy no nl l nm nn">url = 'https://quotes.toscrape.com'<br/>page = urlopen(url)<br/>soup = BeautifulSoup(page, 'html.parser')</span></pre><h2 id="a9ed" class="nj lu iq bd lv np nq dn lz nr ns dp md lf nt nu mf lj nv nw mh ln nx ny mj nz bi translated">追寻数据</h2><p id="b3cb" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">因此，如果你在网站上按下键盘上的F12键，你会看到它的源代码。我鼓励你花些时间去理解它。您可以在Windows上按下<em class="ls"> Ctrl + shift + C </em>来检查页面上的每个元素。</p><p id="b385" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这样做的时候，注意引号存储在带有名为“quote”的<em class="ls">类</em>的<code class="fe oa ob oc nf b">div</code>标签中，就像下面的代码一样。我们需要做的就是抓住这些元素。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5367" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择元素一点也不难。一切都在<code class="fe oa ob oc nf b">soup</code>里面。例如，如果这里的目标是抓取页面的标题，正如您可能已经看到的那样，它存储在<code class="fe oa ob oc nf b">title </code>标签中，我们所要做的就是:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ec40" class="nj lu iq nf b gy nk nl l nm nn">print(soup.title)</span></pre><p id="f4b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出将是:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="55e5" class="nj lu iq nf b gy nk nl l nm nn">&lt;title&gt;Quotes to Scrape&lt;/title&gt;</span></pre><p id="f173" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了只检索文本，我们只需要指定这就是我们想要的:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b317" class="nj lu iq nf b gy nk nl l nm nn">print(soup.title.text)</span></pre><p id="9027" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，由于源代码中有许多<code class="fe oa ob oc nf b">div </code>标签，我们不能使用这种方法。相反，我们将使用<code class="fe oa ob oc nf b">find_all </code>方法。这个方法检索页面上所有符合我们规范的元素。这里我们使用标记名和类来检索所有的引号:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="083b" class="nj lu iq nf b gy nk nl l nm nn">quotes = soup.find_all('div', class_='quote')</span></pre><p id="8c17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一行的另一种写法是:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="4516" class="nj lu iq nf b gy nk nl l nm nn">quotes = soup.find_all('div', {'class': 'quote'})</span></pre><p id="2c0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用这种语法，我们甚至可以指定更多要过滤的类:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="9140" class="nj lu iq nf b gy nk nl l nm nn">quotes = soup.find_all('div', {'class': {'quote', 'tags'}})</span></pre><p id="b158" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以使用<code class="fe oa ob oc nf b">limit </code>参数来限制要检索的元素数量。有很多不同的可能性。确保检查<a class="ae kv" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="4c7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好了，我们现在有了所有的报价。如果您在页面源代码中打开这些元素之一，您将看到以下内容:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5298" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里存储了我们需要的所有信息。我们只要抓住它。我们将抓取引用本身，即在带有类“text”的<code class="fe oa ob oc nf b">span </code>标签中；作者，那是在一个<code class="fe oa ob oc nf b">small </code>标签中加上“作者”一类的；以及在具有类“tag”的一个<code class="fe oa ob oc nf b">div </code>标签内的几个具有类“tag”的<code class="fe oa ob oc nf b">a </code>标签中的标签。</p><p id="abfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们遍历<code class="fe oa ob oc nf b">quotes </code>并获取它们中每一个的信息。首先是文本和作者:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="c032" class="nj lu iq nf b gy nk nl l nm nn">for quote in quotes:<br/>    text = quote.find('span', class_='text').text<br/>    author = quote.find('small', class_='author').text</span></pre><p id="2c51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，仍然在循环内部，我们将使用<code class="fe oa ob oc nf b">find</code>方法获取类“标签”,然后使用<code class="fe oa ob oc nf b">find_all </code>获取内部的<code class="fe oa ob oc nf b">a </code>标签。这些方法基本上做同样的工作，除了<code class="fe oa ob oc nf b">find </code>只返回下一个符合你的规范的元素。</p><p id="5e85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将把每个标签中的文本追加到一个列表中:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2bb5" class="nj lu iq nf b gy nk nl l nm nn">tags = quote.find('div', class_='tags').find_all('a')<br/><br/>tags_list = []<br/>for tag in tags:<br/>    tags_list.append(tag.text)</span></pre><h2 id="b654" class="nj lu iq bd lv np nq dn lz nr ns dp md lf nt nu mf lj nv nw mh ln nx ny mj nz bi translated">存储数据</h2><p id="49b3" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">抓取工作已经完成，但是我想你并不是只对用<code class="fe oa ob oc nf b">print</code>在控制台中显示数据感兴趣，而是希望数据存储在一个文件中，以便以后使用。</p><p id="57c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有几种方法可以做到这一点。最简单的方法是将每个报价的数据放在一个列表中，并将所有这些列表附加到另一个列表中。然后我们可以很容易地将列表转换成数据帧，并导出为<em class="ls">。csv </em>文件。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="19a7" class="nj lu iq nf b gy nk nl l nm nn">         single_quote = [text, author, tags_list]<br/>         all_quotes.append(single_quote)</span><span id="cf2a" class="nj lu iq nf b gy no nl l nm nn"># Outside the loop<br/>df = pd.DataFrame(all_quotes, columns=['quote', 'author', 'tags'])<br/>df.to_csv('quotes.csv', index=False)</span></pre><h2 id="8fa0" class="nj lu iq bd lv np nq dn lz nr ns dp md lf nt nu mf lj nv nw mh ln nx ny mj nz bi translated">获取更多数据</h2><p id="b729" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">虽然每一页只包含十个报价，我想你会想刮更多。要获得更多报价，我们需要进入下一页。同样，根据你抓取的网站，有很多方法可以做到这一点。</p><p id="c7ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这种情况，我们可以将URL改为“https://quotes.toscrape.com/page/1/”，而不是“https://quotes . toscrape . com ”,并在另一个<code class="fe oa ob oc nf b">for</code>中更改页面的编号。</p><p id="fc08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个例子中，我们抓取了前十页。这是该任务的完整代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="49be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">25行代码！这再简单不过了。易于理解、易于编写、实用且有效。</p><p id="887c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了向你证明这是可行的，这里是前一百个引用的数据:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/2dffe1e4a6349fc3b8d3763e55a9e6e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9Mj15y0YN9MxCyxZjBzYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="90fb" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">更多挑战和更多工具</h1><p id="f649" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">不幸的是，不全是阳光和彩虹。回想一下<em class="ls">quotes.toscrape.com</em>是一个网站<strong class="ky ir">被刮</strong>。所以从定义上来说刮擦它是容易的。</p><p id="443b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你开始抓取真实的、更复杂的网站时，你会遇到新的挑战。你必须让你的代码防错，这样它才不会在抓取的成千上万的页面中崩溃。您必须处理BeautifulSoup无法解析的JavaScript渲染页面，甚至还要处理根据用户动作改变页面的交互式内容。</p><p id="f6eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，您还会遇到处理这些挑战的新工具。熟悉<a class="ae kv" href="https://requests.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">请求</a>库很重要，这是一个比<em class="ls"> urllib </em>更强大的工具；<a class="ae kv" href="https://selenium-python.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> Selenium </a>是JavaScript渲染页面的一个很好的选择，因为它实际上打开了自动浏览器，可以抓取所有内容并与页面进行交互；你也可以使用代理提供商，比如<a class="ae kv" href="https://infatica.io/" rel="noopener ugc nofollow" target="_blank"> Infatica </a>，来保护你和你的连接，避免你的IP被屏蔽。</p><p id="2123" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着你目标的发展，确保你的工具也在发展！</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="cb01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢这本书，它可能会有用。如果你有问题，有建议，或者只是想保持联系，请随时通过<a class="ae kv" href="https://twitter.com/_otavioss" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae kv" href="https://github.com/otavio-s-s" rel="noopener ugc nofollow" target="_blank"> GitHub </a>，或者<a class="ae kv" href="https://www.linkedin.com/in/otavioss28/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p></div></div>    
</body>
</html>