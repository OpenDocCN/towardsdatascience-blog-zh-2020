<html>
<head>
<title>NLP with CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带CNN的NLP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e?source=collection_archive---------8-----------------------#2020-10-13">https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e?source=collection_archive---------8-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="305f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步的解释，以及该架构的Keras实现。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fff293357945e7f1d58d1fca7db08cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BRLw4lsANPEfGgimG3YVQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Convolutional_neural_network</a></p></figure><p id="a4f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积神经网络(CNN)是图像处理和图像识别中最广泛使用的深度学习架构。鉴于他们在视觉领域的至高无上，尝试在机器学习的不同领域实现是很自然的。在本文中，我将尝试从自然语言处理的角度解释有关CNN的重要术语，还将提供一个简短的Keras实现和代码解释。</p><p id="1605" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">滑动或卷积预定数据窗口的概念是CNN如此命名背后的核心思想。这个概念的一个例子如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/4e105b0bbf67e5e5098d503d65099da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXaa7uiOpqQZPA-Lmg62cA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fb57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里首先要注意的是将每个单词(记号)表示为三维单词向量的方法。然后，一个3×3的权重矩阵在句子中水平滑动一步(也称为步幅),一次捕获三个单词。这个权重矩阵被称为过滤器；每个滤波器也由一个激活函数组成，类似于前馈神经网络中使用的那些。由于一些数学性质，激活函数ReLU(校正线性单元)主要用于细胞神经网络和深度神经网络。回到图像分类，这些滤波器背后的一般直觉是，每个滤波器可以检测图像的不同特征，滤波器越深，就越有可能捕捉到更复杂的细节，例如，Convnet中的第一批滤波器将检测边缘和线条等简单特征，但最后面的特征可能能够检测某些动物类型。所有这些都是在没有对任何过滤器进行硬编码的情况下完成的。反向传播将确保从数据中学习这些滤波器的权重。<br/>下一个重要步骤是计算输出(卷积特征)。例如，下面我们将考虑一个5*5的图像和一个3*3的过滤器(在处理CNN时，您将主要使用正方形矩阵)。当每个过滤器在数据窗口上滑动一个步长时，通过对元素乘法求和来计算输出图层，每个像素乘以其在过滤器中的相应权重。以下示例说明了如何计算输出图层中的第一个像元；图像中的红色数字代表过滤器中的权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/7d48b692d5507c0f03a87101286f62b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBtZgyBC1oSuqs2SswwwbA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3ffb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算如下:(1∫2)+(1∫1)+(1∫0)+(0∫1)+(2∫0)+(0∫1)+(0∫0)+(2∫1)+(0∫4)= 5</p><p id="0b47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有激活功能的python代码应该是:</p><p id="d5a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">z0 = max(sum(x * w)，0)</p><p id="225f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在2D滤波器的情况下，可以使用以下公式计算输出层的大小:</p><p id="21c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lu"> (N-F)/S +1 </em></p><p id="6ffd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">N =图像的大小，F =滤波器的大小，S =步幅(在我们的例子中为1)</p><p id="42af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当应用于文本时，您将使用在一维窗口中水平滑动3步的过滤器:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/dd8d7b42b26e58d1a27c02915ed566e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*3GOtm_Z9ekXOzKgjTfGlGg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="04a7" class="lw lx iq bd ly lz ma dn mb mc md dp me lf mf mg mh lj mi mj mk ln ml mm mn mo bi translated"><strong class="ak">填充</strong></h2><p id="7a98" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">最后两个示例产生的输出大小小于输入的大小。也不难想象过滤器不完全适合给定数量幻灯片的矩阵的情况。为了应对这些复杂情况，可以通过两种方式使用衬垫:</p><ol class=""><li id="278d" class="mu mv iq ky b kz la lc ld lf mw lj mx ln my lr mz na nb nc bi translated">用零向量填充外部边缘(零填充)</li><li id="393b" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">忽略不适合过滤器的矩阵部分(有效填充)</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/622f18ab0bf7c792f2a4af2b012f7673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*hNaTf12h3kuAJ22Ian63mA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="5af8" class="lw lx iq bd ly lz ma dn mb mc md dp me lf mf mg mh lj mi mj mk ln ml mm mn mo bi translated"><strong class="ak">统筹</strong></h2><p id="d0bf" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">池化相当于CNN中的降维。中心思想是，我们必须将输出层划分为子部分，并计算最能代表输出的值。之所以如此有效，是因为它有助于算法学习数据的高阶表示，同时减少参数的数量。联营的类型:</p><ol class=""><li id="7446" class="mu mv iq ky b kz la lc ld lf mw lj mx ln my lr mz na nb nc bi translated">总和池</li><li id="2a35" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">最大池化</li><li id="dd7b" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">平均池</li></ol><p id="7187" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是最大池的一个示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/74bcf239d4c97a1103a530c19971e849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8O66et0mI-6X4H8dtLFSJQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="3baf" class="lw lx iq bd ly lz ma dn mb mc md dp me lf mf mg mh lj mi mj mk ln ml mm mn mo bi translated">全连接层</h2><p id="933d" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">末端的全连接层接收来自先前池化和卷积层的输入，然后执行分类任务。在我们的例子中，我们将把300个单词的表征窗口分类为1-积极情绪。0-负面情绪。全连接层中的最后一个神经元将250个神经元的加权平均值作为sigmoid函数(返回(0，1)之间的值)</p><h2 id="7a7e" class="lw lx iq bd ly lz ma dn mb mc md dp me lf mf mg mh lj mi mj mk ln ml mm mn mo bi translated">Keras实施</h2><p id="8307" class="pw-post-body-paragraph kw kx iq ky b kz mp jr lb lc mq ju le lf mr lh li lj ms ll lm ln mt lp lq lr ij bi translated">在这一节中，我们将尽量保持代码对于NLP用例的通用性。为了简单起见，我们将不深入数据预处理的细节，但是一般的过程是对数据进行标记化和矢量化。在我们的示例中，使用了word2vec嵌入，每个标记表示为一个300维的单词向量。我们的数据也被填充，每个句子包含400个标记，长句在400个标记后被删除，而短句用零填充。每个句子的最终尺寸是300*400。然后我们把数据分成x_train和x _ test我们不会在这个项目中使用验证数据集。现在我们已经准备好数据，我们可以定义一些超参数。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="217f" class="lw lx iq nl b gy np nq l nr ns">##hyper parameters<br/>batch_size = 32<br/>embedding_dims = 300 #Length of the token vectors<br/>filters = 250 #number of filters in your Convnet<br/>kernel_size = 3 # a window size of 3 tokens<br/>hidden_dims = 250 #number of neurons at the normal feedforward NN<br/>epochs = 2</span></pre><p id="afa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以开始使用Keras库构建模型了。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="4a72" class="lw lx iq nl b gy np nq l nr ns">model = Sequential()<br/>model.add(Conv1D(filters,kernel_size,padding = 'valid' , activation = 'relu',strides = 1 , input_shape = (maxlen,embedding_dims)))</span></pre><p id="755d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们知道填充是有效的，这意味着我们不会保持输入的大小，得到的卷积矩阵的大小将是100*1。在两个窗口中取最大值的最大池层。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="c8d5" class="lw lx iq nl b gy np nq l nr ns">model.add(GlobalMaxPooling1D())<br/>#GlobalMaxPooling1D(n) default = 2.</span></pre><p id="3055" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们以0.2的丢失率添加完全连接的层(我们用它来应对过度拟合)。最后，输出神经元将基于sigmoid激活函数触发。Keras会把低于0.5的归为0，高于0.5的归为1</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="bc7b" class="lw lx iq nl b gy np nq l nr ns">model.add(Dense(hidden_dims))<br/>model.add(Dropout(0.2))<br/>model.add(Activation('relu'))<br/>model.add(Dense(1))<br/>model.add(Activation('sigmoid'))</span></pre><p id="ba80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一步是编译和拟合模型。</p><pre class="kg kh ki kj gt nk nl nm nn aw no bi"><span id="4d6d" class="lw lx iq nl b gy np nq l nr ns">model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])<br/>model.fit(x_train,y_train,batch_size = batch_size,epochs = epochs , validation_data = (x_test,y_test))</span></pre><p id="7da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你可以坐下来看你的模型训练了。我们能够使用斯坦福大学60%的训练数据达到90%的准确率。你可以在本书的第7章找到更多细节:自然语言处理在行动。</p><h2 id="3825" class="lw lx iq bd ly lz ma dn mb mc md dp me lf mf mg mh lj mi mj mk ln ml mm mn mo bi translated">摘要</h2><ol class=""><li id="abca" class="mu mv iq ky b kz mp lc mq lf nt lj nu ln nv lr mz na nb nc bi translated">在自然语言处理中，细胞神经网络可以用于不同的分类任务。</li><li id="002a" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">卷积是在更大的输入数据上滑动的窗口，重点是输入矩阵的子集。</li><li id="0fa0" class="mu mv iq ky b kz nd lc ne lf nf lj ng ln nh lr mz na nb nc bi translated">在正确的维度上获取数据对于任何学习算法都是极其重要的。</li></ol></div></div>    
</body>
</html>