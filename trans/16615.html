<html>
<head>
<title>Using Neo4j with PySpark on Databricks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在数据块上使用Neo4j和PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-neo4j-with-pyspark-on-databricks-eb3d127f2245?source=collection_archive---------16-----------------------#2020-11-16">https://towardsdatascience.com/using-neo4j-with-pyspark-on-databricks-eb3d127f2245?source=collection_archive---------16-----------------------#2020-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8d55" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="82e2" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">释放Spark和图形数据库携手工作的全部潜力</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/61e9e7c0620da718b13c91fd48b8e7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGEPTIwkwpPHEO16L_b5ww.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@fernanddecanne" rel="noopener ugc nofollow" target="_blank"> Fernand De Canne </a>在<a class="ae le" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ee6c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">随着最近发布的利用Spark数据源API的Apache Spark官方Neo4j连接器，在Apache Spark环境中查询Neo4j数据的方式发生了根本性的变化。除了这一变化，<a class="ae le" href="https://neo4j.com/developer/apache-spark/" rel="noopener ugc nofollow" target="_blank">以前的Neo4j Spark连接器被标记为不推荐使用</a>。在本文中，我想分享一个更新的<strong class="lh ja">端到端工作流，该工作流设置Neo4j和Spark </strong>的完全互连配对，该配对利用了新连接器的功能。</p><p id="7c78" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个过程中，我们将首先使用Azure虚拟机设置一个Neo4j云实例。之后，我们将设置一个运行Spark的<a class="ae le" href="https://azure.microsoft.com/en-us/services/databricks/" rel="noopener ugc nofollow" target="_blank"> Azure Databricks </a>实例，然后使用Apache Spark的新<a class="ae le" href="https://github.com/neo4j-contrib/neo4j-spark-connector" rel="noopener ugc nofollow" target="_blank"> Neo4j连接器</a>在两个资源之间建立连接。如果您已经有了一个运行良好的Neo4j或Databricks实例，那么您当然可以跳过相应的步骤。但是，请注意每个步骤顶部的兼容性信息。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="c909" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">步骤1:设置Neo4j云实例</h1><p id="9da7" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">首先，简单说一下兼容性:我们将使用的连接器支持Neo4j及更高版本。<strong class="lh ja">不支持3.5之前的版本</strong>。但是，它支持Neo4j企业和社区以及单实例虚拟机、因果集群和Neo4j Aura。本文将关注单实例虚拟机的工作流。</p><p id="4f39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为我们的Neo4j实例，我们将使用正式的Neo4j企业虚拟机映像。最新版本在Azure marketplace中被列为<em class="nf"> Neo4j Enterprise VM版本4.1 </em>。如果你没有企业许可证，有Websoft9提供的社区版本的图片以及Bitnami 提供的容器图片:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/2770a533de540e392266e6fe9d8b4b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1VyQPcm0OqPNglKKo2W6Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Azure marketplace上N <strong class="bd nh"> eo4j </strong>的搜索结果(作者截图)</p></figure><p id="894a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虚拟机部署后，导航到它的Networking选项卡，确保它的端口设置是正确的:为了从Spark查询它，一个<a class="ae le" href="https://en.wikipedia.org/wiki/Bolt_(network_protocol)" rel="noopener ugc nofollow" target="_blank"> <em class="nf">螺栓</em>端口</a>必须允许入站流量。默认情况下，这是端口7687。此外，我们将使用Neo4j Web接口来填充数据库，为此我们需要一个开放的<em class="nf"> HTTP </em>或<em class="nf"> HTTPS </em>端口。默认情况下，它们映射到端口号7474和7473。如果缺少这些端口规则中的任何一个，点击<em class="nf">添加入站端口规则</em>按钮添加它们。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/7e569c69ea293af895c70e9139872070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-gmZD1BAvK2QIP_ZwnD3g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Neo4j实例的入站端口设置(作者截图)</p></figure><h2 id="974f" class="nj mj iq bd mk nk nl dn mo nm nn dp ms lo no np mu ls nq nr mw lw ns nt my iw bi translated">用数据填充Neo4j实例</h2><p id="bb52" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">验证端口设置后，从其概述选项卡中获取虚拟机的公共IP，并通过导航到<em class="nf"> http://YOUR。VM.IP.ADDRESS:7474 </em>。对于新启动的虚拟机，Neo4j可能需要几分钟才能启动并接受入站连接。如果您使用VPN或代理，请确保对它们进行了相应的配置，否则您可能会收到“<em class="nf">此站点无法访问</em>”错误。</p><p id="49a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果所有设置都正确，将弹出Neo4j浏览器Web UI:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/64175049b517f79d111d4941705c0652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNDH1FfnfZyFxCxqhi8DVQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Neo4j Web UI(浏览器)(作者截图)</p></figure><p id="f1d7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">系统将提示您输入登录凭据。输入登录数据库的默认用户名和密码:</p><ul class=""><li id="066c" class="nv nw iq lh b li lj ll lm lo nx ls ny lw nz ma oa ob oc od bi translated">用户名:<em class="nf"> neo4j </em></li><li id="16db" class="nv nw iq lh b li oe ll of lo og ls oh lw oi ma oa ob oc od bi translated">密码:<em class="nf"> neo4j </em></li></ul><p id="5e3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一次访问时，出于安全原因，您必须立即更改默认密码。选择一个可靠的密码并登录后，就可以填充数据库了。在本指南中，我们将使用Cypher查询语言通过在命令行中键入以下内容来创建一个简单的虚拟数据集:</p><pre class="kp kq kr ks gt oj ok ol om aw on bi"><span id="c8a0" class="nj mj iq ok b gy oo op l oq or">UNWIND range(1,100) as id<br/>CREATE (p:Person {id:id}) WITH collect(p) as people<br/>UNWIND people as p1<br/>UNWIND range(1,10) as friend<br/>WITH p1, people[(p1.id + friend) % size(people)] as p2<br/>CREATE (p1)-[:KNOWS {years: abs(p2.id - p2.id)}]-&gt;(p2)</span></pre><p id="594c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，将创建一个类似于下面的数据结构，其中每个<em class="nf"> Person </em>节点都用一个ID标记，每个<em class="nf"> KNOWS </em>关系都有一个附加的(任意的)属性<em class="nf"> years </em>，它描述了双方认识多久了:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a2a7d85a19ebab9cdf9419ce5d98533a.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*cyBhLenPTBZNlgES3ne0DQ.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">结果数据结构，限于5个节点(作者截图)</p></figure><p id="8a87" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">就是这样！我们的Neo4j图形数据库已经启动并运行，迫不及待地想从Spark中查询。我们现在将继续设置一个合适的Spark环境。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="96c6" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated"><em class="ot">步骤2: </em>配置火花环境</h1><p id="4b67" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">再次，关于兼容性的重要说明:在撰写本文时，<a class="ae le" href="https://neo4j.com/developer/spark/overview/" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja"> Neo4j不支持Spark 3.0 </strong> </a>的连接器。因此，我们将不得不退回到Spark 2.4环境，以便与Neo4j通信。</p><p id="d2b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于我们的设置，我们将使用Azure Databricks实例。在Azure marketplace上搜索<em class="nf">数据块</em>并创建一个新资源。除了通常的设置(资源组、名称、位置和定价层)，不需要任何特殊的配置设置。</p><p id="f68d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">部署完成后，通过导航到Azure资源的概述选项卡并单击<em class="nf">启动工作区</em>按钮，打开Databricks工作区。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/27cfbddf905768dfb7fd3128d5fc7c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qKDqHlYwfxmrOgTdpELdFA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(作者截图)</p></figure><p id="82f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，我们需要一个集群。导航到集群选项卡并单击<em class="nf">创建集群</em>按钮以显示集群配置工具:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/4b150b272e6ff1d7bb48b5ed5266267e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PS2ayvCbs0p9HpYflIzwzQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Databricks集群创建屏幕(作者截图)</p></figure><p id="fe71" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">选择集群名称和可用性模式，并根据您的喜好配置工作节点的大小和数量。现在，请记住，我们被迫使用Spark 2设置——幸运的是，Databricks仍然提供各种Spark 2.4.5发行版。确保在<em class="nf"> Databricks运行时版本</em>字段中选择其中之一，例如<strong class="lh ja">运行时版本6.6 </strong>运行<strong class="lh ja"> Spark 2.4.5 </strong>和<strong class="lh ja"> Scala 2.11 </strong>。</p><p id="04ec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">启动您的集群，一切就绪！</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="f67b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated"><em class="ot">步骤3: </em>在Neo4j和Spark之间建立连接</h1><p id="3cd4" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">既然我们现在已经启动并运行了Neo4j和Databricks，那么是时候关注它们之间的联系了。为此，我们需要将Apache Spark的Neo4j连接器添加到我们的Databricks集群中。</p><p id="4ffc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在数据块中，导航到集群选项卡。选择先前创建的集群并访问其库选项:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/3fb558e36cd292f3dec7a79cce74bcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8J69xRdeO4K4MZxVi8AxsA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Databricks集群视图(作者截图)</p></figure><p id="fa46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，通过点击<em class="nf">安装新的</em>按钮，选择Maven并点击<em class="nf">搜索包</em>，为Apache Spark添加Neo4j连接器。键入“neo4j”查看所有可用选项。在撰写本文时，搜索中出现了三个包:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/b8dcf449f63ac2b92390f1b1e7fb39da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXB4CCkzSRKbGeZOYW1rsA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(作者截图)</p></figure><ul class=""><li id="3214" class="nv nw iq lh b li lj ll lm lo nx ls ny lw nz ma oa ob oc od bi translated"><strong class="lh ja"> neo4j-spark-connector </strong>:这是<a class="ae le" href="https://neo4j.com/developer/apache-spark/" rel="noopener ugc nofollow" target="_blank">弃用的连接器版本(2.4) </a>。它不再受到开发者的积极支持。</li><li id="c376" class="nv nw iq lh b li oe ll of lo og ls oh lw oi ma oa ob oc od bi translated"><strong class="lh ja">neo4j-connector-Apache-spark _ 2.11</strong>:这是当前的连接器版本(4.0)，也是我们将要使用的。</li><li id="736c" class="nv nw iq lh b li oe ll of lo og ls oh lw oi ma oa ob oc od bi translated">neo4j-connector-Apache-spark _ 2.12:这也是当前的连接器版本，唯一的区别是它是为Scala版本2.12编写的。因为我们的Databricks环境运行Scala 2.11，所以这个不适合我们的目的。</li></ul><p id="9296" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据开发人员的说法，由于API的不同，将两个连接器分开是必要的:</p><blockquote class="oy oz pa"><p id="6b0b" class="lf lg nf lh b li lj ka lk ll lm kd ln pb lp lq lr pc lt lu lv pd lx ly lz ma ij bi translated">由于API的差异，不同的scala版本需要不同的JAR文件。确保您有适合您的环境的JAR文件。</p></blockquote><h2 id="6f83" class="nj mj iq bd mk nk nl dn mo nm nn dp ms lo no np mu ls nq nr mw lw ns nt my iw bi translated">阅读Neo4j</h2><p id="c668" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">连接器安装完成后，在工作区选项卡中创建一个新的Jupyter笔记本(右击→ <em class="nf">新建</em>)。在插入Neo4j虚拟机的<em class="nf"> IP、用户名</em>和<em class="nf">密码</em>后，通过运行以下命令尝试读取Neo4j数据库的节点数据:</p><pre class="kp kq kr ks gt oj ok ol om aw on bi"><span id="0c9e" class="nj mj iq ok b gy oo op l oq or">df = spark.read.format("org.neo4j.spark.DataSource")\<br/> .option("url", "bolt://<strong class="ok ja"><em class="nf">XXX.XXX.XXX.XXX</em></strong>:7687")\<br/> .option("authentication.type", "basic")\<br/> .option("authentication.basic.username", "<strong class="ok ja"><em class="nf">neo4j"</em></strong>)\<br/> .option("authentication.basic.password", "<strong class="ok ja"><em class="nf">password"</em></strong>)\<br/> .option("labels", "Person")\<br/> .load()<br/>display(df)</span></pre><p id="54fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您收到<em class="nf">无法连接</em>错误，请确保您的Neo4j虚拟机仍在运行，并且其Bolt端口接受入站流量。如果查询返回</p><pre class="kp kq kr ks gt oj ok ol om aw on bi"><span id="3863" class="nj mj iq ok b gy oo op l oq or">java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V</span></pre><p id="9bf1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您很可能为错误的Scala版本安装了连接器——确保您选择的是Scala 2.11版本(见上文)。</p><p id="b544" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果一切都解决了，查询将返回如下所示的数据帧:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/cb062d8be39a806c630c2e34bd8fdb96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*9vD3K_tN_CvHK7Xye7wlXw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">演示查询以数据帧格式返回的结果(作者截图)</p></figure><p id="9651" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为Python的替代，您也可以通过在笔记本单元格顶部添加<em class="nf"> %scala </em> <a class="ae le" href="https://docs.databricks.com/notebooks/notebooks-use.html#mix-languages" rel="noopener ugc nofollow" target="_blank">魔法命令</a>来使用Scala进行查询:</p><pre class="kp kq kr ks gt oj ok ol om aw on bi"><span id="5215" class="nj mj iq ok b gy oo op l oq or">%scala</span><span id="ff76" class="nj mj iq ok b gy pf op l oq or">import org.apache.spark.sql.{SaveMode, SparkSession}</span><span id="f3c5" class="nj mj iq ok b gy pf op l oq or">val spark = SparkSession.builder().getOrCreate()</span><span id="5192" class="nj mj iq ok b gy pf op l oq or">val df = spark.read.format("org.neo4j.spark.DataSource")<br/> .option("url", "bolt://<strong class="ok ja"><em class="nf">XXX.XXX.XXX.XXX</em></strong>:7687")<br/> .option("authentication.type", "basic")<br/> .option("authentication.basic.username", "<strong class="ok ja"><em class="nf">neo4j"</em></strong>)<br/> .option("authentication.basic.password", "<strong class="ok ja"><em class="nf">password"</em></strong>)<br/> .option("labels", "Person")<br/> .load()</span><span id="478e" class="nj mj iq ok b gy pf op l oq or">display(df)</span></pre><h2 id="83b4" class="nj mj iq bd mk nk nl dn mo nm nn dp ms lo no np mu ls nq nr mw lw ns nt my iw bi translated">写入Neo4j</h2><p id="bf3f" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">写入图形数据库的工作方式类似。在下面的查询中，我们将更新原始数据，以包括前两个节点的名称:</p><pre class="kp kq kr ks gt oj ok ol om aw on bi"><span id="0480" class="nj mj iq ok b gy oo op l oq or">df = spark.createDataFrame(<br/> [(1, "John"),(2, "Thomas")],<br/> ["id", "name"]<br/>)</span><span id="155a" class="nj mj iq ok b gy pf op l oq or">df.write.format("org.neo4j.spark.DataSource")\<br/> .option("url”, "bolt://<strong class="ok ja"><em class="nf">XXX.XXX.XXX.XXX</em></strong>:7687")\<br/> .option("authentication.type", "basic")\<br/> .option("authentication.basic.username", "<strong class="ok ja"><em class="nf">neo4j</em></strong>")\<br/> .option("authentication.basic.password", "<strong class="ok ja"><em class="nf">password</em></strong>")\<br/> .option("labels", ":Person")\<br/> .option("node.keys", "id")\<br/> .mode("Overwrite")\<br/> .save()</span></pre><p id="e74d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，我们使用了<em class="nf">“Overwrite”</em>模式，并将<em class="nf"> node.keys </em>选项设置为DataFrame的<em class="nf"> id </em>列，以便将新值附加到现有节点。事实上，再次运行read查询将返回我们之前的结果，该结果用前两个节点的name属性进行了更新:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a6a917b995653cea4813e0868332157d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*4TokODFMWT93J4y68sxilA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">写入数据库后读取查询的结果(作者截图)</p></figure><p id="b37a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">就是这样！🎉您已经成功地建立了一对完全基于云的互联Apache Spark和Neo4j，使您能够充分利用“传统”大数据和图形数据库的潜力。</p><p id="9593" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要获得更多的查询示例和语法概述，请深入阅读官方的<a class="ae le" href="https://neo4j.com/developer/spark/overview/" rel="noopener ugc nofollow" target="_blank"> Neo4j连接器for Apache Spark文档</a>或查看快速指南<a class="ae le" href="https://neo4j.com/developer/spark/reading/" rel="noopener ugc nofollow" target="_blank">阅读</a>和<a class="ae le" href="https://neo4j.com/developer/spark/writing/" rel="noopener ugc nofollow" target="_blank">编写来自/到Neo4j的</a>。其他示例可以在<a class="ae le" href="https://github.com/utnaf/neo4j-connector-apache-spark-notebooks" rel="noopener ugc nofollow" target="_blank"> Zeppelin笔记本示例库</a>中找到。连接器还支持Cypher查询，允许您重用Neo4j桌面/ Web应用程序中的现有查询。密码查询示例可在<a class="ae le" href="https://neo4j.com/developer/cypher/" rel="noopener ugc nofollow" target="_blank">官方密码页面</a>上找到。</p></div></div>    
</body>
</html>