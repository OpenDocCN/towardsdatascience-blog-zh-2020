<html>
<head>
<title>How to Prune Decision Trees to Make the Most Out of Them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何修剪决策树以充分利用它们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-prune-decision-trees-to-make-the-most-out-of-them-3733bd425072?source=collection_archive---------25-----------------------#2020-10-08">https://towardsdatascience.com/how-to-prune-decision-trees-to-make-the-most-out-of-them-3733bd425072?source=collection_archive---------25-----------------------#2020-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3b32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用可视化来解释。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a72af24457d1244ee890513f0c8fa1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aez2GN8yqTYXahpwHxh2XQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加里·本迪格在<a class="ae ky" href="https://unsplash.com/s/photos/tree-pruning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e1bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是受监督的机器学习算法，通过迭代地将数据集划分为更小的部分来工作。划分过程是构建决策树的最关键部分。</p><p id="27ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分区不是随机的。目的是在每次分区时尽可能提高模型的预测性，以便模型不断获得有关数据集的信息。</p><p id="bc67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，下面是一个深度为3的决策树。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/1e23d9f4a35b9efcb93a34432f97184b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJ6uLCx9h97ruaaioBbGQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度为3的决策树(图片由作者提供)</p></figure><p id="de6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个分区基于特征6 (X[6])，并且能够将属于第一类(59)的所有数据实例放在树的右侧。</p><p id="3fae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这清楚地表明，最大化信息增益的分区被优先考虑。信息增益可以用熵或基尼系数来量化。一般来说，信息增益与纯度的增加(或杂质的减少)成正比。</p><p id="167e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">节点的纯度与该节点中不同类的分布成反比。</p><p id="0a51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这样一种情况，您选择一个数据实例并随机标记它。<strong class="lb iu">基尼系数</strong>衡量你的随机标签出错的频率。如果所有数据点都有相同的标签，那么标签总是正确的，基尼系数将为零。另一方面，如果数据点平均分布在多个标签中，随机标签通常是不正确的。因此，基尼系数随着随机性的增加而增加。</p><p id="95a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熵是不确定性或随机性的另一种度量。一个变量的随机性越大，熵就越大。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/0850dbfde5e43ee0fd88e31aebc409ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*KoZ9m5sfXMdyqHer.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">熵vs随机性(图片由作者提供)</p></figure><p id="fbc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择产生更纯节点的分区。因此，当选择一个特征来划分数据集时，决策树算法试图实现:</p><ul class=""><li id="54d4" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">更多的预测</li><li id="278b" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">杂质少</li><li id="d786" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">低熵</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="4d55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树需要仔细调整，以充分利用它们。太深的树可能会导致过度拟合。Scikit-learn提供了几个超参数来控制树的生长。</p><p id="5f7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将看到这些超参数是如何使用scikit-learn的tree模块的plot_tree函数实现的。</p><p id="047b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将使用scikit-learn的datasets模块下的wine数据集。让我们首先导入库并加载数据集。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="1b8b" class="mx my it mt b gy mz na l nb nc">import numpy as np<br/>import pandas as pd</span><span id="6494" class="mx my it mt b gy nd na l nb nc">from sklearn import tree<br/>from sklearn.datasets import load_wine</span><span id="5822" class="mx my it mt b gy nd na l nb nc">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="191d" class="mx my it mt b gy nd na l nb nc">X, y = load_wine(return_X_y=True)</span></pre><p id="f702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集包括属于3个不同类别的178个观察值。有13个特征描述了观察结果。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8550" class="mx my it mt b gy mz na l nb nc">X.shape<br/>(178,13)</span><span id="9450" class="mx my it mt b gy nd na l nb nc">np.unique(y, return_counts=True)<br/>(array([0, 1, 2]), array([59, 71, 48]))</span></pre></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="09f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以开始用不同的超参数值构建决策树。最明显的一个是max_depth，它在指定的深度级别停止树的生长。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f4b5" class="mx my it mt b gy mz na l nb nc">clf = tree.DecisionTreeClassifier(criterion='gini', max_depth=2)\<br/>.fit(X, y)</span><span id="85f6" class="mx my it mt b gy nd na l nb nc">plt.figure(figsize=(16,8))<br/>tree.plot_tree(clf, filled=True, fontsize=16)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/1c4c14e9b50264f49e0cd6e8eea8c23d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUwwlOs0eekMmn2EQJdXRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度为2的决策树(图片由作者提供)</p></figure><p id="8bbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于Gini杂质选择分区，并且树的深度是2。Max_depth提供了一种简单的方法来控制树的增长，这在更复杂的情况下可能是不够的。</p><p id="f5b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">min_samples_split指定节点中要进一步分割的最小样本数。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a621" class="mx my it mt b gy mz na l nb nc">clf = tree.DecisionTreeClassifier(criterion='gini', min_samples_split=10).fit(X, y)</span><span id="ccd1" class="mx my it mt b gy nd na l nb nc">plt.figure(figsize=(20,8))<br/>tree.plot_tree(clf, filled=True, fontsize=16)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/4a3d58c56e4bea060d074105011321c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*wbQEfbMHvqXGKXXyz5AHUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">min_samples_split=10(图片由作者提供)</p></figure><p id="2ffb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只放了一部分可视化，我们可以看到当前超参数的效果。右边的节点没有进一步分割，因为其中只有5个样本。如果没有min_samples_split=10，它将被进一步拆分如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0e11f7fac3adced4ebe6d5b77b4acf0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*mGVKLyWvFSfeOA2yEWxdPw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">控制树生长的另一个超参数是min _ infinity _ decrease，它设置杂质减少的阈值以考虑分区。这是一个比最大深度更好的方法，因为它考虑了分区的质量。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9e86" class="mx my it mt b gy mz na l nb nc">clf = tree.DecisionTreeClassifier(criterion='gini', min_impurity_decrease=0.1).fit(X, y)</span><span id="97a1" class="mx my it mt b gy nd na l nb nc">plt.figure(figsize=(10, 6))<br/>tree.plot_tree(clf, filled=True, fontsize=16)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8b8d9cd957acdb04bc3a869aa9e1946d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*jr6HnNq8LFqwRjxAOlYy8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">min _杂质_减少=0.1(图片由作者提供)</p></figure><p id="cb1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有隔板都实现了超过0.1的杂质减少。在设置这个值时，我们还应该考虑标准，因为基尼系数杂质和熵具有不同的值。</p><p id="ab74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">max_leaf_nodes也可以用来控制树的生长。它限制了决策树可以拥有的叶节点的数量。叶节点是决策树末端的节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b63cb18921eec5c6dfa88d2431626448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*H4Y46V5NgeEtBv20.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树的结构(图片由作者提供)</p></figure><p id="7abb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该树以最佳优先的方式不断增长，直到达到最大数量的叶节点。基于杂质的减少来选择最佳分区。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e2d6" class="mx my it mt b gy mz na l nb nc">clf = tree.DecisionTreeClassifier(criterion='gini', max_leaf_nodes=5).fit(X, y)</span><span id="d40c" class="mx my it mt b gy nd na l nb nc">plt.figure(figsize=(14, 8))<br/>tree.plot_tree(clf, filled=True, fontsize=16)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/887dee3cc37775686a43fea5bceca932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y3nDeT4Z2N3jcvBjtE9atg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">max_leaf_nodes=5(图片由作者提供)</p></figure></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="24c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经介绍了5种不同的超参数，它们可以用来控制树木的生长。</p><ul class=""><li id="402a" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">标准</li><li id="d013" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">最大深度</li><li id="b93f" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">最小_样本_分割</li><li id="b527" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">最小_杂质_减少</li><li id="1bf1" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">Max_leaf_nodes</li></ul><p id="0b00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对它们逐一进行了调整，以查看各自的效果。然而，在现实生活中，这些超参数需要一起调整，以创建一个稳健和准确的模型。</p><p id="fa18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们过多地种植一棵树，我们很可能会有一个过度适应的模型。当一棵树与训练集拟合得太好时，就会发生过度拟合。在训练集和测试集上具有非常不同的精确度是过度拟合的强烈指示。在这种情况下，我们应该控制树的生长以获得一个通用的模型。</p><p id="ce2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>