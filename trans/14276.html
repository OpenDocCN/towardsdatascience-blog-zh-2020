<html>
<head>
<title>Cross-Entropy Loss Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e?source=collection_archive---------0-----------------------#2020-10-02">https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e?source=collection_archive---------0-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/634301c18947a358742b789768dafa7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3dVObeW1urXBpQ5v9j-qA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://unsplash.com/@fatosi?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">法托斯Bytyqi </a>在<a class="ae kf" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4642" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当处理机器学习或深度学习问题时，损失/成本函数用于在训练期间优化模型。目标几乎总是最小化损失函数。损失越低，模型越好。交叉熵损失是最重要的代价函数。它用于优化分类模型。交叉熵的理解依赖于对Softmax激活函数的理解。我在下面放了另一篇文章来讨论这个先决条件</p><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/softmax-activation-function-how-it-actually-works-d292d335bd78"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">Softmax激活功能——实际工作原理</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">Softmax是放置在深度学习网络末端的函数，用于将logits转换为分类概率。</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv jz lh"/></div></div></a></div></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="f79c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑一个<code class="fe md me mf mg b">4</code>级分类任务，其中图像被分类为狗、猫、马或猎豹。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/558593edd95e6d5de751cff7beba76b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KvygqiInUpBzpknb-KVKJw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">输入图片来源:Victor Grabarczyk 在<a class="ae kf" href="https://unsplash.com/s/photos/dog?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。作者图解。</p></figure><p id="80cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上图中，Softmax将logits转换为概率。交叉熵的目的是获取输出概率(P)并测量与真值的距离(如下图所示)。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/c6c3369d2b31f1a8113c2bf3de37e5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*rcvGMOuWLMpnNvJ3Oj7fPA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">交叉熵(L)(来源:作者)。</p></figure><p id="d854" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于上面的例子，对于类别<code class="fe md me mf mg b">dog</code>的期望输出是<code class="fe md me mf mg b">[1,0,0,0]</code>，但是模型输出是<code class="fe md me mf mg b">[0.775, 0.116, 0.039, 0.070]</code>。</p><p id="5131" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标是使模型输出尽可能接近期望的输出(真值)。在模型训练期间，模型权重被相应地迭代调整，目的是最小化交叉熵损失。调整权重的过程就是定义<em class="mn">模型训练</em>的过程，随着模型不断训练并且损失最小化，我们说模型正在<em class="mn">学习</em>。</p><p id="c077" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">交叉熵的概念可以追溯到信息论领域，克劳德·香农在1948年引入了熵的概念。在深入交叉熵代价函数之前，让我们先介绍一下熵。</p><h1 id="e287" class="mo mp it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">熵</h1><p id="aca2" class="pw-post-body-paragraph kg kh it ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">随机变量X的熵是该变量可能结果中固有的不确定性水平。</p><p id="6c8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<code class="fe md me mf mg b">p(x)</code>——概率分布和随机变量X，熵定义如下</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/db05a3be5614c5e411f4797d4061d844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQ4Y_y-sUbD9TSJaAd0g5A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式1:熵的定义。注对数以2为基数计算。</p></figure><p id="5335" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">负号原因:</strong> <code class="fe md me mf mg b">log(p(x))&lt;0</code>为<code class="fe md me mf mg b">(0,1)</code>中的所有<code class="fe md me mf mg b">p(x)</code>。p(x)是一个概率分布，因此值必须在0和1之间。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/8d91f56af229c1791f0f00ff72e667eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tee-iLsjN-GRT9WervsaMA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">log(x)的图。对于0到1之间的x值，log(x) &lt;0 (is negative). (Source: Author).</p></figure><p id="1dc8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">The greater the value of entropy, 【T4】  , the greater the uncertainty for probability distribution and the smaller the value the less the uncertainty.</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="8ad3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">示例</strong></p><p id="06f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑以下三种形状的“容器”:三角形和圆形</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/0a5f2058b878c093f79a965c2ce40e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0sUNLjyoQmlXYwTtgN6sQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">3个三角形和圆形容器。(来源:作者)。</p></figure><p id="1839" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">容器1: </strong>拣出三角形的概率是26/30，拣出圆形的概率是4/30。由于这个原因，选择一种形状和/或不选择另一种形状的概率更确定。</p><p id="7737" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">容器2: </strong>选择三角形的概率为14/30，否则为16/30。几乎有50–50%的机会选择任何特定的形状。选择给定形状的确定性比1中的低。</p><p id="28ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">容器3: </strong>从容器3中选取的形状极有可能是圆形。选择圆形的概率是29/30，选择三角形的概率是1/30。很有可能选择的形状是圆形。</p><p id="f415" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们计算熵，这样我们就能确定我们的断言，即选择一个给定的形状是确定的。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/68e63d69f655034c1798ff388277f54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*XnFRwxexIZJrDrQjB1TaxA.png"/></div></figure><p id="7a51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如所料，第一个和第三个容器的熵小于第二个容器。这是因为在容器1和3中选择给定形状的概率比在容器2中更确定。我们现在可以继续讨论交叉熵损失函数。</p><h1 id="73cc" class="mo mp it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">交叉熵损失函数</h1><p id="ff1c" class="pw-post-body-paragraph kg kh it ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">又称<strong class="ki iu">对数损失</strong>、<strong class="ki iu">对数损失</strong>或<strong class="ki iu">逻辑损失</strong>。将每个预测类别概率与实际类别期望输出0或1进行比较，并计算得分/损失，该得分/损失基于该概率与实际期望值的差距来惩罚该概率。惩罚本质上是对数的，对于接近1的大差异产生大的分数，对于趋向于0的小差异产生小的分数。</p><p id="2a99" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当在训练期间调整模型权重时，使用交叉熵损失。目标是最小化损失，即损失越小，模型越好。完美模型的交叉熵损失为0。</p><p id="be5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">交叉熵被定义为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/0dd2c28657d4943a215f7bb9f111a1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WRlyVw_sQNiPDPYAIXf9A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式2:交叉熵的数学定义。请注意，对数是以2为基数计算的，与ln()相同。</p></figure><h1 id="6808" class="mo mp it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">二元交叉熵损失</h1><p id="894c" class="pw-post-body-paragraph kg kh it ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">对于二进制分类(具有两个类别0和1的分类任务)，我们将二进制交叉熵定义为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/cec2a330c2674c445c0ba534141c9b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60s9Kiwpm-QZBh0F1NK9eg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式3:数学二元交叉熵。</p></figure><p id="1245" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">二进制交叉熵通常被计算为所有数据示例的平均交叉熵，即，</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/7d7c37fcb6d4f9aded2bccc7b392ca6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LTGc4T0NKn0b8YUAqzRTMg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式4</p></figure><p id="1072" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">例子</strong></p><p id="1bda" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑具有以下软最大概率(S)和标签(T)的分类问题。目标是在给定这些信息的情况下计算交叉熵损失。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/c6c3369d2b31f1a8113c2bf3de37e5ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*rcvGMOuWLMpnNvJ3Oj7fPA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Logits)和具有分类交叉熵损失函数的一键编码真值标签(T ),用于测量预测概率和真值标签之间的“距离”。(来源:作者)</p></figure><p id="0c3d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分类交叉熵的计算如下</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/57c3536bcc7b2966f7ba9dba27f75c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-pih_yOYXuEYwimEj7HfQ.png"/></div></div></figure><p id="589f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Softmax是连续可微函数。这使得计算损失函数相对于神经网络中每个权重的导数成为可能。该属性允许模型相应地调整权重以最小化损失函数(模型输出接近真实值)。</p><p id="26b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设在模型训练的一些迭代之后，模型输出以下逻辑向量</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/49d38ec4fc6fb8443d67aa50f2a79f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*MdS4M50j9Cn9GVdO-tdoXg.png"/></div></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/ab1e0799f6cc631b746159e08670bc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWibqonxKWdqQrU11ctqqA.png"/></div></div></figure><p id="8ba8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe md me mf mg b">0.095</code>小于前次损失，即<code class="fe md me mf mg b">0.3677</code>暗示模型正在学习。优化过程(调整权重以使输出接近真实值)一直持续到训练结束。</p><p id="78b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Keras提供了以下交叉熵损失函数:二元、分类、稀疏分类交叉熵损失函数。</p><h1 id="b438" class="mo mp it bd mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl bi translated">范畴交叉熵和稀疏范畴交叉熵</h1><p id="1785" class="pw-post-body-paragraph kg kh it ki b kj nm kl km kn nn kp kq kr no kt ku kv np kx ky kz nq lb lc ld im bi translated">类别交叉熵和稀疏类别交叉熵都具有等式2中定义的相同损失函数。两者之间唯一的区别是如何定义真理标签。</p><ul class=""><li id="88fd" class="oa ob it ki b kj kk kn ko kr oc kv od kz oe ld of og oh oi bi translated">当真实标签被一热编码时，使用分类交叉熵，例如，我们对于3类分类问题<code class="fe md me mf mg b">[1,0,0]</code>、<code class="fe md me mf mg b">[0,1,0]</code>和<code class="fe md me mf mg b">[0,0,1].</code>具有以下真实值</li><li id="ca6b" class="oa ob it ki b kj oj kn ok kr ol kv om kz on ld of og oh oi bi translated">在稀疏分类交叉熵中，真值标签是整数编码的，例如，3类问题的<code class="fe md me mf mg b">[1]</code>、<code class="fe md me mf mg b">[2]</code>和<code class="fe md me mf mg b">[3]</code>。</li></ul><p id="935b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇文章能帮助你更清楚地理解交叉熵损失函数。</p><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/softmax-activation-function-how-it-actually-works-d292d335bd78"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">Softmax激活功能——实际工作原理</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">Softmax是放置在深度学习网络末端的函数，用于将logits转换为分类概率。</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv jz lh"/></div></div></a></div><p id="4898" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读:-)</p></div></div>    
</body>
</html>