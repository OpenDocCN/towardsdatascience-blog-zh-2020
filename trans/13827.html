<html>
<head>
<title>4 Pre-Trained CNN Models to Use for Computer Vision with Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4个预训练的CNN模型，用于具有迁移学习的计算机视觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc?source=collection_archive---------0-----------------------#2020-09-23">https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc?source=collection_archive---------0-----------------------#2020-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1216" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习案例研究</h2><div class=""/><div class=""><h2 id="9ee1" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用最先进的预训练神经网络模型，通过迁移学习解决计算机视觉问题</h2></div><p id="b3eb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我们开始之前，如果你正在阅读这篇文章，我确信我们有着相似的兴趣，并且现在/将来会从事相似的行业。那么我们就通过 <a class="ae lo" href="https://linkedin.com/in/orhangaziyalcin/" rel="noopener ugc nofollow" target="_blank"> <em class="ln"> Linkedin </em> </a> <em class="ln">来连线吧！请不要犹豫发送联系请求！</em><a class="ae lo" href="https://linkedin.com/in/orhangaziyalcin/" rel="noopener ugc nofollow" target="_blank"><em class="ln">Orhan g . yaln—Linkedin</em></a></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/c5d18380a9fa0a75ae157005bfaceaa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QoqNAg2t6lF8Q6WWA6AbOg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图一。迁移学习如何工作(图片由作者提供)</p></figure><h2 id="fba8" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">如果你一直在尝试建立高精度的机器学习模型；但从未尝试过迁移学习，这篇文章将改变你的生活。至少，它做到了我的！</h2><p id="0ffe" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">我们大多数人已经尝试了几个机器学习教程来掌握神经网络的基础知识。这些教程非常有助于理解人工神经网络的基础知识，例如<a class="ae lo" rel="noopener" target="_blank" href="/using-recurrent-neural-networks-to-predict-bitcoin-btc-prices-c4ff70f9f3e4">递归神经网络</a>、<a class="ae lo" rel="noopener" target="_blank" href="/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d">卷积神经网络</a>、<a class="ae lo" rel="noopener" target="_blank" href="/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b"> GANs </a>和<a class="ae lo" rel="noopener" target="_blank" href="/image-noise-reduction-in-10-minutes-with-convolutional-autoencoders-d16219d2956a">自动编码器</a>。但是，它们的主要功能是让您为现实世界的实现做好准备。</p><p id="26cb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，如果你计划建立一个利用深度学习的人工智能系统，你要么(I)必须有非常大的培训预算和优秀的人工智能研究人员供你使用，要么(ii)必须从迁移学习中受益。</p><h1 id="8a58" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">什么是迁移学习？</h1><p id="ec13" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">迁移学习是机器学习和人工智能的一个分支，旨在将从一个任务(源任务)获得的知识应用到一个不同但相似的任务(目标任务)。</p><p id="d83a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，在学习维基百科文本分类时获得的知识可以用来解决法律文本分类问题。另一个例子是使用在学习分类汽车时获得的知识来识别天空中的鸟。正如你所看到的，这些例子之间是有联系的。我们没有在鸟类检测上使用文本分类模型。</p><blockquote class="nn"><p id="c36b" class="no np it bd nq nr ns nt nu nv nw lm dk translated"><a class="ae lo" href="https://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf" rel="noopener ugc nofollow" target="_blank">T3】</a></p></blockquote><p id="b052" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la oa lc ld le ob lg lh li oc lk ll lm im bi translated">总之，迁移学习是一个<strong class="kt jd"> <em class="ln">让你不必重新发明轮子</em> </strong>并帮助你在很短的时间内构建人工智能应用的领域。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi od"><img src="../Images/16f56fb0fe9253ccf64a0ecce53dd85c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoPYgWAHNGJ3F_8LuNE7dg.jpeg"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图二。不要重新发明轮子，转移现有的知识(照片由乔恩·卡塔赫纳<a class="ae lo" href="https://unsplash.com/@cartega?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">在</a><a class="ae lo" href="https://unsplash.com/s/photos/wheel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure><h1 id="50bb" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">迁移学习的历史</h1><p id="0ea3" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">为了展示迁移学习的力量，我们可以引用吴恩达的话:</p><blockquote class="nn"><p id="9022" class="no np it bd nq nr ns nt nu nv nw lm dk translated"><a class="ae lo" href="https://www.youtube.com/watch?v=wjqaz6m42wU" rel="noopener ugc nofollow" target="_blank">迁移学习将是继监督学习之后，机器学习商业成功的下一个驱动力。</a></p></blockquote><p id="e724" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la oa lc ld le ob lg lh li oc lk ll lm im bi translated">迁移学习的历史可以追溯到1993年。在她的论文<a class="ae lo" href="http://papers.neurips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">中，Lorien Pratt打开了潘多拉的盒子，向世界展示了迁移学习的潜力。1997年7月，《机器学习》杂志</a><a class="ae lo" href="https://www.springer.com/journal/10994" rel="noopener ugc nofollow" target="_blank">发表了一篇</a><a class="ae lo" href="https://link.springer.com/journal/10994/28/1/page/1" rel="noopener ugc nofollow" target="_blank">专刊</a>进行转移学习的论文。随着该领域的发展，多任务学习等相关主题也被纳入迁移学习领域。<a class="ae lo" href="https://www.springer.com/gp/book/9780792380474" rel="noopener ugc nofollow" target="_blank">学会学习</a>是这方面的先驱书籍之一。今天，迁移学习是科技企业家构建新的人工智能解决方案和研究人员推动机器学习前沿的强大来源。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oe"><img src="../Images/79b05b922b7027b7764261152575a001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rtnhwKSlFBwIareIJJS7g.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图3。吴恩达对机器学习子领域商业成功的期望(图片由作者提供)</p></figure><h1 id="1f4d" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">迁移学习是如何工作的？</h1><p id="047f" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">实现迁移学习有三个要求:</p><ul class=""><li id="a8c4" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated">由第三方开发开源预训练模型</li><li id="ba82" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">改变模型的用途</li><li id="4add" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">针对问题的微调</li></ul><h2 id="8b52" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">开发一个开源的预训练模型</h2><p id="ecdd" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">预训练模型是由其他人创建和训练的模型，用于解决与我们类似的问题。在实践中，有人几乎总是一个科技巨头或一群明星研究员。他们通常选择一个非常大的数据集作为他们的基础数据集，如<a class="ae lo" href="http://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>或<a class="ae lo" href="https://dumps.wikimedia.org/" rel="noopener ugc nofollow" target="_blank">维基百科语料库</a>。然后，他们创建一个大型神经网络(例如，VGG19有<a class="ae lo" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank"> 143，667，240个参数</a>)来解决一个特定的问题(例如，这个问题是VGG19的<a class="ae lo" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank">图像分类</a>)。当然，这种预先训练好的模型必须公开，以便我们可以利用这些模型并重新调整它们的用途。</p><h2 id="1204" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">改变模型的用途</h2><p id="1287" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">在得到这些预先训练好的模型后，我们重新利用学到的知识，包括层次、特征、权重和偏差。有几种方法可以将预先训练好的模型加载到我们的环境中。最终，它只是一个包含相关信息的文件/文件夹。然而，深度学习库已经托管了许多这些预训练的模型，这使得它们更容易访问和方便:</p><ul class=""><li id="5d89" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated"><a class="ae lo" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a></li><li id="2596" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae lo" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank"> Keras应用</a></li><li id="2dfd" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae lo" href="https://pytorch.org/hub/" rel="noopener ugc nofollow" target="_blank"> PyTorch轮毂</a></li></ul><p id="2d1a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">您可以使用上面的来源之一来加载已训练的模型。它通常带有所有的层和权重，你可以随意编辑网络。</p><h2 id="fffd" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">针对问题的微调</h2><p id="9442" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">虽然目前的模型可能对我们的问题有效。由于以下两个原因，对预训练模型进行微调通常更好:</p><ul class=""><li id="51da" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated">这样我们可以达到更高的精确度；</li><li id="ed39" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">我们的微调模型可以以正确的格式生成输出。</li></ul><p id="3232" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一般来说，在神经网络中，底层和中间层通常表示一般特征，而顶层表示特定问题的特征。由于我们的新问题不同于原始问题，我们倾向于放弃顶层。通过针对我们的问题添加特定的层，我们可以实现更高的准确性。</p><p id="6805" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">放下顶层之后，我们需要放置我们自己的层，这样我们就可以得到我们想要的输出。例如，用ImageNet训练的模型可以分类多达1000个对象。如果我们试图对手写数字进行分类(例如，<a class="ae lo" rel="noopener" target="_blank" href="/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d"> MNIST分类</a>)，最终得到只有10个神经元的最终层可能会更好。</p><p id="98c9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在我们将自定义层添加到预训练模型后，我们可以使用特殊的损失函数和优化器对其进行配置，并通过额外的训练进行微调。</p><blockquote class="ot ou ov"><p id="3402" class="kr ks ln kt b ku kv kd kw kx ky kg kz ow lb lc ld ox lf lg lh oy lj lk ll lm im bi translated">要获得快速转移学习教程，您可以访问下面的帖子:</p></blockquote><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/fast-neural-style-transfer-in-5-minutes-with-tensorflow-hub-magenta-110b60431dcc"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">TensorFlow Hub &amp; Magenta在5分钟内实现快速神经风格转换</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">利用Magenta的任意图像风格化网络和深度学习，将梵高的独特风格转移到照片中</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq lz pc"/></div></div></a></div><h1 id="0360" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">计算机视觉的4个预训练模型</h1><p id="66ae" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">下面是四个预先训练好的网络，您可以使用它们来完成计算机视觉任务，例如图像生成、神经类型转移、图像分类、图像字幕、异常检测等:</p><ul class=""><li id="67ce" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated">VGG19</li><li id="ecd6" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">第三版(谷歌网)</li><li id="178a" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">ResNet50</li><li id="9b73" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">效率网</li></ul><p id="875f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们一个一个地深入研究它们。</p><h2 id="96a5" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">VGG-19</h2><p id="fe26" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">VGG是一个深度为19层的卷积神经网络。它是由牛津大学的卡伦·西蒙扬和安德鲁·齐泽曼在2014年构建和训练的，你可以访问他们在2015年发表的论文<a class="ae lo" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank">大规模图像识别的超深度卷积网络</a>中的所有信息。VGG-19网络也使用来自ImageNet数据库的超过一百万幅图像进行训练。当然，您可以导入带有ImageNet训练权重的模型。这个预先训练好的网络可以对多达1000个对象进行分类。该网络在224×224像素的彩色图像上被训练。以下是关于其尺寸和性能的简要信息:</p><ul class=""><li id="7936" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated"><strong class="kt jd">大小:</strong> 549 MB</li><li id="ec5f" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd"> Top-1: </strong>准确率:71.3%</li><li id="f42d" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">前五名:</strong>准确率:90.0%</li><li id="6796" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">参数个数:</strong>143667240</li><li id="9079" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">深度:</strong> 26</li></ul><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/3e6edacce9c4bb1ad9f0d72bb49d2d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*U6_b8aNUtOgdkRlP3b9Vpg.jpeg"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图4。VGG-19网络图(图由<a class="ae lo" href="https://www.researchgate.net/profile/Clifford_Yang" rel="noopener ugc nofollow" target="_blank"> Clifford K. Yang </a>和<a class="ae lo" href="https://www.researchgate.net/profile/Yufeng_Zheng" rel="noopener ugc nofollow" target="_blank"> Yufeng Zheng </a>在<a class="ae lo" href="https://www.researchgate.net/publication/325137356_Breast_cancer_screening_using_convolutional_neural_network_and_follow-up_digital_mammography" rel="noopener ugc nofollow" target="_blank"> ResearchGate </a>上提供)</p></figure><h2 id="e48c" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">第三版(谷歌网)</h2><p id="7285" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">Inceptionv3是一个具有50层深度的卷积神经网络。它是由谷歌构建和训练的，你可以访问论文上的所有信息，标题是“<a class="ae lo" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">用卷积深入</a>”。带有ImageNet权重的预训练版本的Inceptionv3可以对多达1000个对象进行分类。该网络的图像输入大小为299x299像素，大于VGG19网络。虽然VGG19在2014年的ImageNet比赛中获得亚军，但《盗梦空间》却获得了冠军。Inceptionv3特性的简要总结如下:</p><ul class=""><li id="2d37" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated"><strong class="kt jd">大小:</strong> 92 MB</li><li id="05a3" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd"> Top-1: </strong>准确率:77.9%</li><li id="ac29" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">前五名:</strong>准确率:93.7%</li><li id="2a2b" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">参数个数:</strong>23851784</li><li id="d509" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">深度:</strong> 159</li></ul><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/602c6e504eb76d6ae2ec63ec3e653568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*BCLnBA5cbvnjnUMHGemiTQ.jpeg"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图5。Inceptionv3网络图(图由<a class="ae lo" href="https://www.researchgate.net/profile/Masoud_Mahdianpari" rel="noopener ugc nofollow" target="_blank"> Masoud Mahdianpari </a>、<a class="ae lo" href="https://www.researchgate.net/profile/Bahram_Salehi?_sg%5B0%5D=XVPUbjpBrkCmamnUeM5V3Bag4mOrrNkwZxuERGs67V10CIeThkt8REeWTKRve6zgW1bXs84.XLl-e8MR_C_AQFQqx6r3SRyac-xCqMH-eNwXtOuBwDdMW063NsTbPLBiIzp-hbbzf1NsQ6PhRtdUheI7cpee8g&amp;_sg%5B1%5D=q4Cr5rl6btOnicAanokPQKqAzx9b0Ma0OMc6ppXeTrfyZ8Vc0pf0N4vMnxfowGYQpwusZN8.tIFdpj6W4XluchmSqfN1dI38dsRgFHQXBFNYg8y2ckCFavRwOYgnska6Cotvl2q0WDtN0dXllrEokDp8zAngPg" rel="noopener ugc nofollow" target="_blank"> Bahram Salehi </a>和<a class="ae lo" href="https://www.researchgate.net/scientific-contributions/2144549279-Mohammad-Rezaee?_sg%5B0%5D=XVPUbjpBrkCmamnUeM5V3Bag4mOrrNkwZxuERGs67V10CIeThkt8REeWTKRve6zgW1bXs84.XLl-e8MR_C_AQFQqx6r3SRyac-xCqMH-eNwXtOuBwDdMW063NsTbPLBiIzp-hbbzf1NsQ6PhRtdUheI7cpee8g&amp;_sg%5B1%5D=q4Cr5rl6btOnicAanokPQKqAzx9b0Ma0OMc6ppXeTrfyZ8Vc0pf0N4vMnxfowGYQpwusZN8.tIFdpj6W4XluchmSqfN1dI38dsRgFHQXBFNYg8y2ckCFavRwOYgnska6Cotvl2q0WDtN0dXllrEokDp8zAngPg" rel="noopener ugc nofollow" target="_blank"> Mohammad Rezaee </a>在<a class="ae lo" href="https://www.researchgate.net/publication/326421398_Very_Deep_Convolutional_Neural_Networks_for_Complex_Land_Cover_Mapping_Using_Multispectral_Remote_Sensing_Imagery" rel="noopener ugc nofollow" target="_blank"> ResearchGate </a>上提供)</p></figure><h2 id="7bcd" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">ResNet50(剩余网络)</h2><p id="baca" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">ResNet50是一个深度为50层的卷积神经网络。它是由微软在2015年建立和训练的，你可以在他们题为<a class="ae lo" href="http://Deep Residual Learning for Image Recognition" rel="noopener ugc nofollow" target="_blank">图像识别的深度剩余学习</a>的论文中访问模型性能结果。该模型还在来自ImageNet数据库的超过100万幅图像上进行训练。就像VGG-19一样，它可以分类多达1000个物体，网络是在224x224像素的彩色图像上训练的。以下是关于其尺寸和性能的简要信息:</p><ul class=""><li id="c000" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated"><strong class="kt jd">大小:</strong> 98 MB</li><li id="666b" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd"> Top-1: </strong>准确率:74.9%</li><li id="efec" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">前五名:</strong>准确率:92.1%</li><li id="181d" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">参数个数:</strong>25636712</li></ul><p id="bdc5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果您将ResNet50与VGG19进行比较，您会发现ResNet50实际上优于VGG19，尽管它的复杂性更低。ResNet50经过多次改进，您还可以获得更新的版本，如<a class="ae lo" href="https://keras.io/api/applications/resnet/#resnet101-function" rel="noopener ugc nofollow" target="_blank"> ResNet101 </a>、<a class="ae lo" href="https://keras.io/api/applications/resnet/#resnet152-function" rel="noopener ugc nofollow" target="_blank"> ResNet152 </a>、<a class="ae lo" href="https://keras.io/api/applications/resnet/#resnet50v2-function" rel="noopener ugc nofollow" target="_blank"> ResNet50V2 </a>、<a class="ae lo" href="https://keras.io/api/applications/resnet/#resnet101v2-function" rel="noopener ugc nofollow" target="_blank"> ResNet101V2 </a>、<a class="ae lo" href="https://keras.io/api/applications/resnet/#resnet152v2-function" rel="noopener ugc nofollow" target="_blank"> ResNet152V2 </a>。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/d14daf12ef7d80cb9d38861bf0e15524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1rlrAL0ZhUorlQRbGKQxmQ.jpeg"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图6。ResNet50网络的图解(图由<a class="ae lo" href="https://www.researchgate.net/profile/Masoud_Mahdianpari" rel="noopener ugc nofollow" target="_blank">马苏德·马赫迪安帕里</a>、<a class="ae lo" href="https://www.researchgate.net/profile/Bahram_Salehi?_sg%5B0%5D=XVPUbjpBrkCmamnUeM5V3Bag4mOrrNkwZxuERGs67V10CIeThkt8REeWTKRve6zgW1bXs84.XLl-e8MR_C_AQFQqx6r3SRyac-xCqMH-eNwXtOuBwDdMW063NsTbPLBiIzp-hbbzf1NsQ6PhRtdUheI7cpee8g&amp;_sg%5B1%5D=q4Cr5rl6btOnicAanokPQKqAzx9b0Ma0OMc6ppXeTrfyZ8Vc0pf0N4vMnxfowGYQpwusZN8.tIFdpj6W4XluchmSqfN1dI38dsRgFHQXBFNYg8y2ckCFavRwOYgnska6Cotvl2q0WDtN0dXllrEokDp8zAngPg" rel="noopener ugc nofollow" target="_blank">巴赫拉姆·萨利希</a>和<a class="ae lo" href="https://www.researchgate.net/scientific-contributions/2144549279-Mohammad-Rezaee?_sg%5B0%5D=XVPUbjpBrkCmamnUeM5V3Bag4mOrrNkwZxuERGs67V10CIeThkt8REeWTKRve6zgW1bXs84.XLl-e8MR_C_AQFQqx6r3SRyac-xCqMH-eNwXtOuBwDdMW063NsTbPLBiIzp-hbbzf1NsQ6PhRtdUheI7cpee8g&amp;_sg%5B1%5D=q4Cr5rl6btOnicAanokPQKqAzx9b0Ma0OMc6ppXeTrfyZ8Vc0pf0N4vMnxfowGYQpwusZN8.tIFdpj6W4XluchmSqfN1dI38dsRgFHQXBFNYg8y2ckCFavRwOYgnska6Cotvl2q0WDtN0dXllrEokDp8zAngPg" rel="noopener ugc nofollow" target="_blank">穆罕默德·雷扎伊</a>在<a class="ae lo" href="https://www.researchgate.net/publication/326421398_Very_Deep_Convolutional_Neural_Networks_for_Complex_Land_Cover_Mapping_Using_Multispectral_Remote_Sensing_Imagery" rel="noopener ugc nofollow" target="_blank">研究之门</a>上提供)</p></figure><h2 id="29d3" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">效率网</h2><p id="a4e3" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">EfficientNet是一个最先进的卷积神经网络，由谷歌在2019年通过论文“<a class="ae lo" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet:反思卷积神经网络的模型缩放</a>”进行训练并向公众发布。EfficientNet (B0到B7)有8个备选实现，即使是最简单的一个，EfficientNetB0也很出色。它拥有530万个参数，实现了77.1%的顶级精度性能。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/fcfa26ab3ce249b3db7fc6dda187c42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*EMgyZrvDJQec6R6_8_mNzA.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图7。EfficientNet模型大小与ImageNet精度(由<a class="ae lo" href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Mingxing Tan </a>和<a class="ae lo" href="https://scholar.google.com/citations?user=vfT6-XIAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Quoc V. Le </a>在<a class="ae lo" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a>上绘制)</p></figure><p id="0395" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">EfficientNetB0功能的简要概述如下:</p><ul class=""><li id="d768" class="of og it kt b ku kv kx ky la oh le oi li oj lm ok ol om on bi translated"><strong class="kt jd">大小:</strong> 29 MB</li><li id="0dcf" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">前1名:</strong>准确率:77.1%</li><li id="a812" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated">前五名:准确率:93.3%</li><li id="317f" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">参数个数:</strong>~ 530万</li><li id="ee20" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><strong class="kt jd">深度:</strong> 159</li></ul><h2 id="037b" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">计算机视觉问题的其他预训练模型</h2><p id="ce5a" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">我们列出了四个一流的获奖卷积神经网络模型。然而，还有许多其他模式可用于迁移学习。以下是对这些模型的基准分析，这些模型都可以在<a class="ae lo" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank"> Keras应用</a>中找到。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="pu pv l"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">表1。预训练CNN模型的基准分析(作者列表)</p></figure><h1 id="f847" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">结论</h1><p id="73e9" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">在一个我们可以轻松获得最先进的神经网络模型的世界里，试图用有限的资源建立自己的模型就像试图重新发明轮子一样。这毫无意义。</p><p id="e0e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">相反，尝试使用这些训练模型，考虑到您特定的计算机视觉任务，在顶部添加几个新层，然后进行训练。结果会比你从零开始建立的模型更成功。</p><h1 id="bb84" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki nj kj mq kl nk km mt ko nl kp mw nm bi translated">订阅邮件列表获取完整代码</h1><p id="74a8" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">如果你想在Google Colab上获得完整的代码，并获得我的最新内容，请订阅邮件列表:✉️</p><blockquote class="nn"><p id="54ff" class="no np it bd nq nr ns nt nu nv nw lm dk translated"><a class="ae lo" href="http://eepurl.com/hd6Xfv" rel="noopener ugc nofollow" target="_blank">现在就订阅</a></p></blockquote><h1 id="a2dc" class="nc mg it bd mh nd ne nf mk ng nh ni mn ki pw kj mq kl px km mt ko py kp mw nm bi translated">喜欢这篇文章吗？</h1><p id="8a88" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">如果你喜欢这篇文章，可以考虑看看我的其他类似文章:</p><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">使用MNIST数据集在10分钟内完成图像分类</h2><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pz l pn po pp pl pq lz pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/tensorflow-and-vgg19-can-help-you-convert-your-photos-into-beautiful-pop-art-pieces-c1abe87e7e01"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">TensorFlow和VGG19可以帮助您将照片转换成美丽的波普艺术作品</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">神经风格转移基于安迪沃霍尔的门罗双联画与预训练的计算机视觉网络VGG19，转移…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="qa l pn po pp pl pq lz pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">利用生成性对抗网络在10分钟内生成图像</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">使用无监督深度学习生成手写数字与深度卷积甘斯使用张量流和…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="qb l pn po pp pl pq lz pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jd gy z fp ph fr fs pi fu fw jc bi translated">伯特和拥抱脸10分钟情感分析</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">学习预训练的自然语言处理模型的基础，伯特，并建立一个使用IMDB电影评论的情感分类器…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="qc l pn po pp pl pq lz pc"/></div></div></a></div></div></div>    
</body>
</html>