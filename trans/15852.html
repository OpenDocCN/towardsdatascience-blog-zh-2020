<html>
<head>
<title>Deep Learning in Java</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Java中的深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-in-java-d9b54ae1423a?source=collection_archive---------10-----------------------#2020-11-01">https://towardsdatascience.com/deep-learning-in-java-d9b54ae1423a?source=collection_archive---------10-----------------------#2020-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e47992d22ee803169a09d71d65858ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pHOrBh5VWH3WLKzf"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@urielsc26?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌列尔SC </a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="3744" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">DeepJavaLibrary简介:一个开源的、与引擎无关的、用于训练和推理的深度学习Java库</h2></div><p id="99fc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">长久以来，Java一直是企业中<a class="ae jd" href="https://www.cloudfoundry.org/wp-content/uploads/Developer-Language-Report_FINAL.pdf" rel="noopener ugc nofollow" target="_blank">最受欢迎的编程语言之一，拥有一个庞大的库、框架和开发人员社区的生态系统。然而，Java为深度学习应用程序提供的选项非常有限。目前，大多数深度学习模型都是用Python编写和训练的。这为想要进入这一领域的Java开发人员带来了额外的进入壁垒，因为他们必须学习一种新的编程语言和复杂的深度学习领域。<br/> <br/>为了降低Java开发者进入深度学习的门槛，AWS构建了</a><a class="ae jd" href="https://djl.ai" rel="noopener ugc nofollow" target="_blank"> Deep Java Library (DJL) </a>，这是一个开源的Java深度学习框架，通过支持任何深度学习引擎，如Apache MXNet、PyTorch或TensorFlow，在Java中原生运行训练和推理，为Java开发者搭建桥梁。它还包含一个强大的ModelZoo设计，允许您管理经过训练的模型，并在一行代码中加载它们。内置的ModelZoo目前支持来自GluonCV、HuggingFace、TorchHub和Keras的70多个预训练和随时可用的模型。如果你是一名Java开发人员，并且对探索深度学习感兴趣，<a class="ae jd" href="https://djl.ai/" rel="noopener ugc nofollow" target="_blank"> Deep Java Library (DJL) </a>是一个很好的起点。<br/> <br/>在本教程中，我们将通过在流行的MNIST数据集上训练一个简单模型来演示DJL的训练能力。</p><h1 id="309a" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">什么是深度学习？</h1><p id="c799" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">机器学习是通过使用各种统计技术，让计算机从数据中学习给定任务的规范的过程。这种学习任务特征的能力允许计算机执行复杂的任务，例如检测图像中的对象，这些任务通常被认为超出了计算机的范围，因为很难为每种可能的情况提供准确的规范。<br/> <br/>深度学习是基于人工神经网络的机器学习分支。人工神经网络是一种受人脑启发的编程范式，它帮助计算机基于观察数据学习和执行任务。深度学习是一组强大的技术，可以用来帮助训练大型人工神经网络来执行复杂的任务。深度学习技术已被证明在解决复杂任务方面非常有效，如对象检测、动作识别、机器翻译、自然语言理解等。</p><h1 id="c9eb" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">和DJL一起训练MNIST</h1><h1 id="36cb" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">设置项目</h1><p id="cb6e" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">您可以在gradle项目中使用以下配置来导入所需的依赖项。在这个例子中，我们使用了包含DJL项目核心API的<code class="fe mo mp mq mr b">api</code>包，以及包含DJL一些基本数据集的<code class="fe mo mp mq mr b">basicdataset</code>包。因为我们用MXNet引擎训练，我们也将导入<code class="fe mo mp mq mr b">mxnet-engine</code>包和<code class="fe mo mp mq mr b">mxnet-native-auto</code>包。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="7e24" class="na ls jg mr b gy nb nc l nd ne">plugins {<br/>    id 'java'<br/>}<br/>repositories {                           <br/>    jcenter()<br/>}<br/>dependencies {<br/>    implementation "ai.djl:api:0.8.0"<br/>    implementation<!-- --> "ai.djl:basicdataset:0.8.0"<br/>    // MXNet<br/>    runtimeOnly "ai.djl.mxnet:mxnet-engine:0.8.0"<br/>    runtimeOnly "ai.djl.mxnet:mxnet-native-auto:1.7.0-backport"<br/>}</span></pre><h1 id="2902" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">NDArray和NDManager</h1><p id="ce79" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">NDArray是DJL所有数学计算的核心数据结构。NDArray表示一个多维、固定大小的同构数组。NDArray的行为类似于python程序numpy。<br/> <br/> <a class="ae jd" href="https://javadoc.djl.ai/api/0.4.0/ai/djl/ndarray/NDManager.html" rel="noopener ugc nofollow" target="_blank"> NDManager </a>是<a class="ae jd" href="https://javadoc.djl.ai/api/0.4.0/ai/djl/ndarray/NDArray.html" rel="noopener ugc nofollow" target="_blank"> NDArray </a>的管理者。NDManager管理NDArray的生命周期，是DJL内存管理的重要组成部分。一旦NDManager关闭，由NDManager实例创建的每个NDArray都将关闭。NDManager和NDArray都扩展了<code class="fe mo mp mq mr b">AutoCloseable</code>。为了更好的了解和理解NDArray和NDManager的用法，请看<a class="ae jd" rel="noopener" target="_blank" href="/ndarray-a-java-based-n-dim-array-toolkit-60b4035b10b8">这篇博文</a>。</p><h1 id="98ae" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">模型</h1><p id="c7fe" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在DJL，训练和推理以<code class="fe mo mp mq mr b">Model</code>开始。在本文中，我们将集中讨论培训过程。为了开始训练过程，我们创建了一个<code class="fe mo mp mq mr b">Model</code>类的新实例。<code class="fe mo mp mq mr b">Model</code>类也扩展了<code class="fe mo mp mq mr b">AutoCloseable</code>。所以，它是用try-with-resources创建的。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="4e12" class="na ls jg mr b gy nb nc l nd ne">try (Model model = Model.newInstance()) {<br/>    ...<br/>    // training process takes place here<br/>    ...<br/>}</span></pre><h1 id="b707" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">准备数据</h1><p id="b52c" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">MNIST数据库(改进的国家标准和技术研究所数据库)是一个手写数字的大型数据库，通常用于训练各种图像处理系统。<code class="fe mo mp mq mr b">MNIST</code>数据集在DJL随处可见。来自DJL MNIST数据集的单个图像的形状为(28，28)。如果您希望在自己的数据集上训练模型，您可以按照这里的说明<a class="ae jd" href="https://github.com/awslabs/djl/blob/4e2cd53cda13d01749fe739a384f8e8e1dbc9ef6/docs/development/how_to_use_dataset.md#how-to-create-your-own-dataset" rel="noopener ugc nofollow" target="_blank">添加自己的数据集。</a></p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/7ef0525c5cfda787ee45508a0a856450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXxaNOjhIYTTdiWJZ_1L1A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来自<a class="ae jd" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>、<em class="ng">的随机图像集合，由克尔森·瓦斯特</em>拍摄</p></figure><p id="f3cd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练您的模型，您首先需要加载数据集。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="091f" class="na ls jg mr b gy nb nc l nd ne">int batchSize = 32;<br/>Mnist trainingDataset = Mnist.builder()<br/>        .optUsage(Usage.TRAIN)<br/>        .setSampling(batchSize, true)<br/>        .build();<br/>Mnist validationDataset = Mnist.builder()<br/>        .optUsage(Usage.TEST)<br/>        .setSampling(batchSize, true)<br/>        .build();</span></pre><p id="3d12" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这段代码创建了训练和验证数据集。数据集还被配置为对数据集进行随机采样。对数据集进行了进一步的配置，如对图像应用变换，或限制数据集的大小。</p><h1 id="1033" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">构建模型(构建模块)</h1><p id="d015" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">一旦数据准备好了，你需要构建你想要训练的神经网络。在DJL，神经网络用一个<code class="fe mo mp mq mr b">Block</code>来表示。块是形成神经网络的可组合函数。它们可以代表单个操作、神经网络的一部分，甚至整个神经网络。一个<code class="fe mo mp mq mr b">Block</code>可以有参数和子块。在训练过程中，参数被更新，并且子块也被训练。这也递归地更新了它所有子节点的参数。<br/> <br/>在构建这些块函数时，最简单的方法就是使用composition。积木可以由其他积木组合而成。我们将包含块称为父块，将子块称为子块。<br/> <br/>我们提供了几个助手，让构建通用的块组合结构变得容易。<code class="fe mo mp mq mr b">SequentialBlock</code>是一个容器块，它的子块形成了一个块链，每个子块将其输出提供给序列中的下一个子块。ParallelBlock是一个容器块，其子块并行执行，块的输出根据指定的组合函数进行组合。block是一个具有操作功能的模块，必须由用户指定。</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/bf812cba091689db58e9eb5c8a53f475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3faDmakd9sWvWGhlCtSMQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">可用于构建神经网络的模块类型(非穷举)，<em class="ng">keer than Vasist的图像</em></p></figure><p id="df0a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将建立一个简单的MLP(多层感知器)。多层感知器(MLP)是一种前馈人工神经网络，它从一组输入生成一组输出。MLP的特征在于若干层输入结点在输入层和输出层之间连接成一个有向图。它可以通过在一个<code class="fe mo mp mq mr b">SequentialBlock</code>内使用多个<code class="fe mo mp mq mr b">LinearBlock</code>来构建。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="67c5" class="na ls jg mr b gy nb nc l nd ne">int input = 768;<br/>int output = 10;<br/>int[] hidden = new int[] {128, 64};<br/>SequentialBlock sequentialBlock = new SequentialBlock();<br/>sequentialBlock.add(Blocks.batchFlattenBlock(input));<br/>for (int hiddenSize : hidden) {<br/>    sequentialBlock.add(Linear.builder().setUnits(hiddenSize).build());<br/>    sequentialBlock.add(activation);<br/>}<br/>sequentialBlock.add(Linear.builder().setUnits(output).build());</span></pre><p id="5153" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">DJL还提供了一个预制的<code class="fe mo mp mq mr b">Mlp</code>模块，我们可以直接使用。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="427d" class="na ls jg mr b gy nb nc l nd ne">Block block = new Mlp(<br/>        Mnist.IMAGE_HEIGHT * Mnist.IMAGE_WIDTH,<br/>        Mnist.NUM_CLASSES,<br/>        new int[] {128, 64});</span></pre><h1 id="9393" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">培养</h1><p id="7763" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在，您已经创建了模型的一个新实例，准备了数据集，并构造了一个块，您可以开始训练了。在深度学习中，训练包括以下步骤:</p><ul class=""><li id="9055" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated">初始化:该步骤初始化模块，并根据指定的<code class="fe mo mp mq mr b">Initializer</code>方案创建相应的参数。</li><li id="3286" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">Forward:该步骤执行由<code class="fe mo mp mq mr b">Block</code>表示的计算，并生成输出。</li><li id="f70e" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">损失计算:在这一步中，通过将指定的<code class="fe mo mp mq mr b">Loss</code>函数应用于输出和提供的标签来计算损失。</li><li id="2447" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">向后:在此步骤中，使用损失，并沿神经网络反向传播梯度。</li><li id="4c3f" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated">步骤:在该步骤中，根据指定的<code class="fe mo mp mq mr b">Optimizer</code>更新程序块的参数值。</li></ul><p id="5e5f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，DJL通过<code class="fe mo mp mq mr b">Trainer</code>抽象了所有这些步骤。可以通过指定<code class="fe mo mp mq mr b">Initializer</code>、<code class="fe mo mp mq mr b">Loss</code>、<code class="fe mo mp mq mr b">Optimizer</code>等训练配置来创建<code class="fe mo mp mq mr b">Trainer</code>。使用<code class="fe mo mp mq mr b">TrainingConfig</code>可以设置这些配置和更多配置。可以设置的其他一些配置有:</p><ul class=""><li id="86f0" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><code class="fe mo mp mq mr b">Device</code> -必须进行培训的设备</li><li id="8beb" class="ni nj jg kx b ky nr lb ns le nt li nu lm nv lq nn no np nq bi translated"><code class="fe mo mp mq mr b">TrainingListeners</code> -监听器监听训练过程中的各个阶段，并执行特定的功能，如记录和评估。用户可以根据需要实现自定义<code class="fe mo mp mq mr b">TrainingListener</code>。</li></ul><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="acd0" class="na ls jg mr b gy nb nc l nd ne">DefaultTrainingConfig config = new DefaultTrainingConfig(Loss.softmaxCrossEntropyLoss())<br/>                .addEvaluator(new Accuracy())<br/>                .optDevices(Device.getDevices(arguments.getMaxGpus()))<br/>                .addTrainingListeners(TrainingListener.Defaults.logging(arguments.getOutputDir()));<br/>try (Trainer trainer = model.newTrainer(config)){<br/>    // training happens here<br/>}</span></pre><p id="1d3b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦创建了训练器，必须用输入的<code class="fe mo mp mq mr b">Shape</code>对其进行初始化。然后，你调用<code class="fe mo mp mq mr b">fit()</code>方法开始训练。<code class="fe mo mp mq mr b">fit()</code>方法对数据集上的模型进行指定数量的历元训练，运行验证，并将模型保存在文件系统的指定目录中。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="7ec3" class="na ls jg mr b gy nb nc l nd ne">/*<br/>* MNIST is 28x28 grayscale image and pre processed into 28 * 28 NDArray.<br/>* 1st axis is batch axis, we can use 1 for initialization.<br/>*/<br/>Shape inputShape = new Shape(1, Mnist.IMAGE_HEIGHT * Mnist.IMAGE_WIDTH);<br/>int numEpoch = 5;<br/>String outputDir = "/build/model";</span><span id="cbcc" class="na ls jg mr b gy nw nc l nd ne">// initialize trainer with proper input shape<br/>trainer.initialize(inputShape);</span><span id="1589" class="na ls jg mr b gy nw nc l nd ne">TrainingUtils.fit(trainer, numEpoch, trainingSet, validateSet, outputDir, "mlp");</span></pre><p id="394b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就是这样。恭喜你！你已经使用DJL训练了你的第一个深度学习模型！您可以在控制台上监视训练过程，也可以监视监听器的实现。如果您使用默认侦听器，您的输出应该类似于以下内容。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="9abb" class="na ls jg mr b gy nb nc l nd ne">[INFO ] - Downloading libmxnet.dylib ...<br/>[INFO ] - Training on: cpu().<br/>[INFO ] - Load MXNet Engine Version 1.7.0 in 0.131 ms.<br/>Training:    100% |████████████████████████████████████████| Accuracy: 0.93, SoftmaxCrossEntropyLoss: 0.24, speed: 1235.20 items/sec<br/>Validating:  100% |████████████████████████████████████████|<br/>[INFO ] - Epoch 1 finished.<br/>[INFO ] - Train: Accuracy: 0.93, SoftmaxCrossEntropyLoss: 0.24<br/>[INFO ] - Validate: Accuracy: 0.95, SoftmaxCrossEntropyLoss: 0.14<br/>Training:    100% |████████████████████████████████████████| Accuracy: 0.97, SoftmaxCrossEntropyLoss: 0.10, speed: 2851.06 items/sec<br/>Validating:  100% |████████████████████████████████████████|<br/>[INFO ] - Epoch 2 finished.NG [1m 41s]<br/>[INFO ] - Train: Accuracy: 0.97, SoftmaxCrossEntropyLoss: 0.10<br/>[INFO ] - Validate: Accuracy: 0.97, SoftmaxCrossEntropyLoss: 0.09<br/>[INFO ] - train P50: 12.756 ms, P90: 21.044 ms<br/>[INFO ] - forward P50: 0.375 ms, P90: 0.607 ms<br/>[INFO ] - training-metrics P50: 0.021 ms, P90: 0.034 ms<br/>[INFO ] - backward P50: 0.608 ms, P90: 0.973 ms<br/>[INFO ] - step P50: 0.543 ms, P90: 0.869 ms<br/>[INFO ] - epoch P50: 35.989 s, P90: 35.989 s</span></pre><p id="0265" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦训练完成，我们就可以使用训练好的模型进行推理来获得预测。您可以使用您的model jupyter笔记本按照<a class="ae jd" href="http://docs.djl.ai/jupyter/tutorial/03_image_classification_with_your_model.html" rel="noopener ugc nofollow" target="_blank">推理，在保存的模型上运行推理。您也可以按照</a><a class="ae jd" href="https://github.com/awslabs/djl/blob/master/examples/docs/train_mnist_mlp.md" rel="noopener ugc nofollow" target="_blank">使用多层感知器(MLP)模型</a>训练手写数字识别中的说明直接运行完整的代码。</p><h1 id="92d6" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">摘要</h1><p id="0986" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在本文中，我们向您介绍了深度学习，并使用DJL完成了一个简单的训练示例。虽然这个例子是一个简单的例子，但DJL为更复杂的深度学习模型提供了相同的抽象。<br/> <br/>关注我们的<a class="ae jd" href="https://github.com/awslabs/djl/tree/master/docs" rel="noopener ugc nofollow" target="_blank"> GitHub </a>、<a class="ae jd" href="https://github.com/aws-samples/djl-demo" rel="noopener ugc nofollow" target="_blank">演示库</a>、<a class="ae jd" href="https://join.slack.com/t/deepjavalibrary/shared_invite/zt-ar91gjkz-qbXhr1l~LFGEIEeGBibT7w" rel="noopener ugc nofollow" target="_blank"> Slack channel </a>和<a class="ae jd" href="https://twitter.com/deepjavalibrary" rel="noopener ugc nofollow" target="_blank"> Twitter </a>获取更多关于DJL的文档和示例！</p></div></div>    
</body>
</html>