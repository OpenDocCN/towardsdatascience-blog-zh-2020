<html>
<head>
<title>Use Pre-trained Word Embedding to detect real disaster tweets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用预先训练的单词嵌入来检测真实的灾难推文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead?source=collection_archive---------8-----------------------#2020-11-13">https://towardsdatascience.com/pre-trained-word-embedding-for-text-classification-end2end-approach-5fbf5cd8aead?source=collection_archive---------8-----------------------#2020-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="68f7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">端-2-端方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/17885747dcd8289eff7a083a38ab99d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UxBhafia56JZbSb5s2eEaw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/</a></p></figure><p id="863c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将经历整个文本分类流程，尤其是数据预处理步骤，我们将使用一个<a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>预先训练的单词嵌入。<br/>文本特征处理比线性或分类特征稍微复杂一点。事实上，机器学习算法更多的是关于标量和向量，而不是字符或单词。因此，我们必须将文本输入转换成标量，而<strong class="ky ir"> keystone </strong> 🗝元素在于<strong class="ky ir">如何找出输入单词</strong>的最佳表示。这是自然语言处理背后的主要思想</p><p id="ecc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用一个名为<a class="ae kv" href="https://www.kaggle.com/c/nlp-getting-started/data" rel="noopener ugc nofollow" target="_blank">的Kaggle竞赛的数据集，真实与否？灾难推文NLP</a>。这项任务在于预测一条推文是否是关于一场真正的灾难。为了解决这个文本分类任务，我们将使用单词嵌入变换，然后是递归深度学习模型。其他不太复杂但仍然有效的解决方案也是可能的，比如结合tf-idf编码和朴素贝叶斯分类器(查看我上一篇<a class="ae kv" href="https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de" rel="noopener">帖子</a>)。</p><p id="fe03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我将包括一些方便的Python代码，可以在其他NLP任务中重现。整个源代码可以在这个<a class="ae kv" href="https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932" rel="noopener ugc nofollow" target="_blank"> kaggle笔记本</a>中获得。</p><h1 id="8d74" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">简介:</h1><p id="e582" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">LSTM或CNN等模型在捕捉词序和它们之间的语义关系方面更有效，这通常对文本的意义至关重要:来自我们数据集的一个样本被标记为真正的灾难:</p><blockquote class="mp mq mr"><p id="d452" class="kw kx ms ky b kz la jr lb lc ld ju le mt lg lh li mu lk ll lm mv lo lp lq lr ij bi translated">#RockyFire更新= &gt;加州高速公路。20个双向关闭，由于莱克县火灾-# CAfire #野火'</p></blockquote><p id="7985" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显，单词顺序在上面的例子中很重要。</p><p id="8574" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，我们需要将输入文本转换成机器可读的格式。它存在许多技术，如</p><ul class=""><li id="31c0" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated"><strong class="ky ir"> one-hot encoding </strong>:每个序列文本输入在d维空间中表示，其中d是数据集词汇的大小。如果每个术语出现在文档中，则该术语将得到1，否则将得到0。对于大型语料库，词汇表将大约有数万个标记，这使得一次性向量非常稀疏和低效。</li><li id="ab61" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><strong class="ky ir"> TF-IDF编码</strong>:单词被映射成使用TF-IDF度量生成的数字。该平台集成了快速算法，使得保持<strong class="ky ir">所有</strong>单元和二元tf-idf编码成为可能，而无需应用降维</li><li id="60b3" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><strong class="ky ir">单词嵌入变换</strong>:单词被投影到一个密集的向量空间，在这个空间中，单词之间的语义距离被保留:(见下图):</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/2b1cb181ae849eeb448757f7ccfab22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70XQn1EDy3m3LV9S_lLE0Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/images/linear-relationships . SVG</a></p></figure><p id="45c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是预训练单词嵌入？</strong></p><p id="f525" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入是表示一个单词(或一个符号)的密集向量。默认情况下，嵌入向量是<strong class="ky ir">随机</strong>初始化的，然后将在训练阶段逐渐改进，在每个反向传播步骤使用梯度下降算法，以便相似的单词或相同词汇域中的单词或具有共同词干的单词…将在新向量空间中的距离方面以<strong class="ky ir">接近</strong>结束；(见下图):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/ad2cd106b2723a4ae5972338196ea829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43-X3p4qhoVDyercgFxhew.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者:Zeineb Ghrib</p></figure><p id="c860" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预训练单词嵌入是<strong class="ky ir">迁移学习的一个例子。</strong>其背后的主要思想是使用已经在大型数据集上训练过的公共嵌入。具体来说，我们将<strong class="ky ir">将这些预训练的嵌入设置为初始化权重</strong>，而不是随机初始化我们的神经网络权重。这个技巧有助于加速训练和提高NLP模型的性能。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="7935" class="ls lt iq bd lu lv nt lx ly lz nu mb mc jw nv jx me jz nw ka mg kc nx kd mi mj bi translated">步骤0:导入和设置:</h1><p id="e4f6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，让我们导入所需的库和工具，它们将帮助我们执行NLP处理和</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="c367" class="od lt iq nz b gy oe of l og oh">import pandas as pd<br/>import numpy as np<br/>from nltk.corpus import stopwords<br/>from nltk.util import ngrams<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from collections import defaultdict<br/>from collections import  Counter<br/>stop=set(stopwords.words('english'))<br/>import re<br/>from nltk.tokenize import word_tokenize<br/>import gensim<br/>import string<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from tqdm import tqdm<br/>from keras.models import Sequential<br/>from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D<br/>from keras.initializers import Constant<br/>from sklearn.model_selection import train_test_split<br/>from keras.optimizers import Adam</span></pre></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="1802" class="ls lt iq bd lu lv nt lx ly lz nu mb mc jw nv jx me jz nw ka mg kc nx kd mi mj bi translated">第一步:文本清理:🧹</h1><p id="acc0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">不考虑EDA步骤可以带出未清理的元素并帮助我们自定义清理代码，我们可以应用一些在tweeters中反复出现的基本数据清理，如删除标点符号，html标签URL和表情符号，拼写纠正，..</p><p id="e599" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是一段python代码，可以在其他类似的用例中重现😉</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="e7a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将数据集拆分为:</p><ul class=""><li id="53d9" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">一个<strong class="ky ir">训练数据集</strong>(训练数据集的80%)</li><li id="ecbd" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">一个<strong class="ky ir">验证数据集</strong>:剩余20%的训练数据集将用于验证每个时期的模型性能</li><li id="2920" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><strong class="ky ir">测试数据集</strong>(此处可选) :由kaggle提供，用于进行预测</li></ul><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="2b85" class="od lt iq nz b gy oe of l og oh">train = df[~df['target'].isna()]<br/>X_train, X_val, y_train, y_val = train_test_split(train, train['target'], test_size=0.2, random_state=42)</span></pre><h1 id="f37c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第二步:文本预处理🤖</h1><p id="3dee" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如前所述，机器学习算法将数字作为输入，而不是文本，这意味着我们需要将文本转换为数字向量。<br/>我们进行如下操作:</p><h2 id="01cd" class="od lt iq bd lu ok ol dn ly om on dp mc lf oo op me lj oq or mg ln os ot mi ou bi translated">1.标记化</h2><p id="b650" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">它包括将文本划分为单词或更小的子文本，允许我们确定数据集的“词汇”(数据中存在的一组唯一标记)。通常我们使用单词级表示。对于我们的例子，我们将使用NLTK <code class="fe ov ow ox nz b">Tokenizer()</code></p><h2 id="fb7a" class="od lt iq bd lu ok ol dn ly om on dp mc lf oo op me lj oq or mg ln os ot mi ou bi translated">2.单词索引:</h2><p id="6acc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">基于词频构建一个词汇索引映射器:索引将与整个数据集中的词频成反比。最频繁的世界的索引=1..每个单词都会有一个唯一的索引。</p><p id="3e0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个步骤分解如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b262" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于NLTK标记器的一些解释:</p><ol class=""><li id="1fdf" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr oy nc nd ne bi translated"><code class="fe ov ow ox nz b">fit_on_texts()</code>方法🤖:它根据词频创建词汇索引。<br/>例:"<em class="ms">外壳中的幽灵</em>"会生成word _ index[" the "]= 1；word_index["ghost"] = 2..<br/> - &gt;所以每个单词都得到一个唯一的整数值。从1开始(0保留用于填充)，单词越频繁，对应的索引越低。<br/> (PS往往前几个是停用词，因为出现很多但是建议在数据清理的时候去掉)。</li><li id="7573" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr oy nc nd ne bi translated"><code class="fe ov ow ox nz b">textes_to_sequences()</code>法📟:将每个文本转换为整数序列:每个单词都映射到word_index字典中的索引。</li><li id="ef41" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr oy nc nd ne bi translated"><code class="fe ov ow ox nz b">pad_sequences()</code>方法🎞:为了使输出的形状标准化，我们定义了一个唯一的向量长度(在我们的例子中<code class="fe ov ow ox nz b">MAX_SEQUENCE_LENGTH</code>将其固定为50):任何更长的序列都将被截断，任何更短的序列都将用0填充。</li></ol><h1 id="16a4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">步骤3:构建嵌入矩阵🧱</h1><p id="3304" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，我们将从官方网站下载<a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套预训练嵌入</a>(由于一些技术限制，我必须通过代码下载:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="ebea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将创建一个嵌入矩阵，将每个单词索引映射到其对应的嵌入向量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/0a48cee64090ca15ee16daeac7bf414b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hRzoJM0nrU7vmBM8.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://developers.google.com/machine-learning/guides/text-classification/images/EmbeddingLayer.png" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/guides/text-classification/images/embedding layer . png</a></p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="09a9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">步骤4:创建和训练模型:</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/eae8988126bfa8fb2b44bbf1885fc073.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/0*h_Dekage-RNPARya.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://emojipedia.org/whatsapp/2.20.198.15/robot/" rel="noopener ugc nofollow" target="_blank"> whatsapp机器人</a></p></figure><p id="ef88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用顺序keras模型创建一个递归神经网络，该模型将包含:</p><ol class=""><li id="996b" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr oy nc nd ne bi translated">以嵌入矩阵为初始权重的<strong class="ky ir">嵌入层</strong></li><li id="38d2" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr oy nc nd ne bi translated">一个<strong class="ky ir">脱落层</strong>以避免过度拟合(查看这篇关于神经网络中脱落层及其效用的优秀<a class="ae kv" href="https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/#:~:text=Dropout%20is%20a%20regularization%20method,overfitting%20and%20improving%20model%20performance." rel="noopener ugc nofollow" target="_blank">帖子</a></li><li id="a71e" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr oy nc nd ne bi translated">一个<strong class="ky ir"> LSTM层</strong>:包括长短期存储单元</li><li id="d849" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr oy nc nd ne bi translated">使用<em class="ms">二元交叉熵</em>损失函数的<strong class="ky ir">激活层</strong></li></ol><p id="c6ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想要计算我们的二元keras分类器模型的准确度、精确度、召回率和F1分数，我们必须手动计算它们，因为自<a class="ae kv" href="https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes" rel="noopener ugc nofollow" target="_blank"> 2.0版本</a>以来，Keras不支持这些指标。</p><p id="aa5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(解决方案来自<a class="ae kv" href="https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="4863" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在编译和训练模型:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="5f71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要获得验证性能结果，使用<code class="fe ov ow ox nz b">evaluate()</code>方法:</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="1a3b" class="od lt iq nz b gy oe of l og oh">loss, accuracy, f1_score, precision, recall = model.evaluate(tokenized_val, y_val, verbose=0)</span></pre><p id="3372" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们检查结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/f0e8187080ead2c6b3669b832c40090d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjaI_OrZsI2VNg2BJSlCxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由Zeineb Ghrib从<a class="ae kv" href="https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932" rel="noopener ugc nofollow" target="_blank">这里</a></p></figure><p id="d1db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些结果似乎相当不错，但当然可以通过微调神经网络超参数或使用auto-ml工具(如<a class="ae kv" href="https://cloud.prevision.io/" rel="noopener ugc nofollow" target="_blank"> prevision </a>)来增强，除了wor2vec之外，这些工具还应用了许多其他转换，如ngram令牌化、tf-idf或更先进的技术(如<a class="ae kv" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank"> BERT </a> transformers)。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="86ee" class="ls lt iq bd lu lv nt lx ly lz nu mb mc jw nv jx me jz nw ka mg kc nx kd mi mj bi translated">结论:</h1><p id="31a4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这篇文章中，我一步一步地向您展示了如何从Glove预训练的单词嵌入应用wor2vec变换，以及如何使用它来训练一个递归神经网络。请注意，该方法和代码可以在其他类似的用例中重用。整体源代码可以在这个<a class="ae kv" href="https://www.kaggle.com/schopenhacker75/eda-text-cleaning-glove?scriptVersionId=46794932" rel="noopener ugc nofollow" target="_blank"> kaggle笔记本</a>中找到。<br/>我还在同一个数据集上应用了完全不同的方法:我使用了tf-idf朴素贝叶斯分类器，如果你想获得更多信息，请访问<a class="ae kv" href="https://medium.com/prevision-io/automated-nlp-with-prevision-io-part1-naive-bayes-classifier-475fa8bd73de" rel="noopener">我的上一篇文章</a>。</p><p id="31ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我打算写一篇关于如何使用名为Bert的突破性算法的文章，并将其与其他NLP算法进行比较</p><p id="bff1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您阅读我的帖子🤗！！如果您有任何问题，可以在<a class="ae kv" href="https://cloud.prevision.io/" rel="noopener ugc nofollow" target="_blank"> prevision cloud instance </a>的聊天会话中找到我，或者发送电子邮件至:zeineb.ghrib@prevision.io</p></div></div>    
</body>
</html>