<html>
<head>
<title>Text analysis basics in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的文本分析基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5?source=collection_archive---------2-----------------------#2020-10-20">https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5?source=collection_archive---------2-----------------------#2020-10-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ea30" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="d736" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">二元/三元模型、情感分析和主题建模</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e52c46917250424418077eea2f328156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bj31A3JBbGQPwXWYZtgrcA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源:<a class="ae lh" href="https://unsplash.com/photos/uGP_6CAD-14" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/uGP_6CAD-14</a></p></figure><p id="6248" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文讨论Python中最基本的文本分析工具。我们不打算进入花哨的NLP模型。只是最基本的。有时候你需要的只是基础:)</p><p id="fec9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们先来获取一些文本数据。这里有一份我写的课程评论清单。我们可以用这些数据做什么？想到的第一个问题是，我们能分辨哪些评论是正面的，哪些是负面的吗？我们可以对这些评论做一些情感分析吗？</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="99b0" class="mj mk it mf b gy ml mm l mn mo">corpus = [<br/>'Great course. Love the professor.',<br/>'Great content. Textbook was great',<br/>'This course has very hard assignments. Great content.',<br/>'Love the professor.',<br/>'Hard assignments though',<br/>'Hard to understand.'<br/>]</span></pre><h1 id="4cf7" class="mp mk it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">情感分析</h1><p id="0a86" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">太好了，我们来看看整体的情绪分析。我喜欢使用熊猫数据框。因此，让我们从列表中创建一个熊猫数据框。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="3731" class="mj mk it mf b gy ml mm l mn mo">import pandas as pd<br/>df = pd.DataFrame(corpus)<br/>df.columns = ['reviews']</span></pre><p id="f2d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们安装库<em class="nl"> textblob </em> ( <code class="fe nm nn no mf b">conda install textblob -c conda-forge</code>)并导入库。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="3e31" class="mj mk it mf b gy ml mm l mn mo">from textblob import TextBlob<br/>df['polarity'] = df['reviews'].apply(lambda x: TextBlob(x).polarity)<br/>df['subjective'] = df['reviews'].apply(lambda x: TextBlob(x).subjectivity)</span></pre><p id="0460" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们可以通过<code class="fe nm nn no mf b">polarity</code>函数计算情感。<code class="fe nm nn no mf b">polarity</code>范围从-1到1，其中-1为负，1为正。TextBlob还可以使用<code class="fe nm nn no mf b">subjectivity</code>函数来计算<code class="fe nm nn no mf b">subjectivity</code>，范围从0到1，0表示客观，1表示主观。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/89fe9222b2fa46091ac3d121d52bd720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYhLelhxEW1VKTZ0NzpjYA.png"/></div></div></figure><h1 id="057c" class="mp mk it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">二元/三元模型的情感分析</h1><p id="721b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">接下来，我们可以探讨一些单词联想。n元语法分析通常用于查看哪些单词经常一起出现。我经常喜欢研究两个词或三个词的组合，即二元模型/三元模型。</p><blockquote class="nq nr ns"><p id="fa9d" class="li lj nl lk b ll lm kd ln lo lp kg lq nt ls lt lu nu lw lx ly nv ma mb mc md im bi translated">一个<strong class="lk jd"> <em class="it"> n </em> -gram </strong>是来自给定文本或语音样本的<em class="it"> n </em>项的连续序列。</p></blockquote><p id="fd05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在文本分析中，过滤掉一些停用词通常是一种好的做法，这些停用词是最常见的词，但在句子中没有重要的上下文意义(例如，“a”、“the”、“and”、“but”等)。nltk为我们提供了一个停用词的列表。我们还可以在列表中添加定制的停用词。比如这里我们加了“虽然”这个词。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="28ef" class="mj mk it mf b gy ml mm l mn mo">from nltk.corpus import stopwords<br/>stoplist = stopwords.words('english') + ['though']</span></pre><p id="0eda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们可以删除停用词，使用一些二元/三元模型。函数<code class="fe nm nn no mf b">CountVectorizer</code>“将文本文档的集合转换成令牌计数的矩阵”。<code class="fe nm nn no mf b">stop_words</code>参数有一个内置选项“英语”。但是我们也可以使用用户定义的停用词，就像我在这里展示的。<code class="fe nm nn no mf b">ngram_range</code>参数定义了我们对哪些n元语法感兴趣——2表示二元语法，3表示三元语法。另一个值得一提的参数是<code class="fe nm nn no mf b">lowercase</code>，它有一个默认值<em class="nl"> True </em>，为我们自动将所有字符转换成小写。现在用下面的代码，我们可以得到所有的二元/三元模型，并按频率排序。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="f319" class="mj mk it mf b gy ml mm l mn mo">from sklearn.feature_extraction.text import CountVectorizer<br/>c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))<br/># matrix of ngrams<br/>ngrams = c_vec.fit_transform(df['reviews'])<br/># count frequency of ngrams<br/>count_values = ngrams.toarray().sum(axis=0)<br/># list of ngrams<br/>vocab = c_vec.vocabulary_</span><span id="04c3" class="mj mk it mf b gy nw mm l mn mo">df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)<br/>            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/19b816a4ea9adec9920f77cbcf77d038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNv_Sq5pwO6uIBo5UmXXFw.png"/></div></div></figure><p id="cd1e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">类似于之前的情感分析，我们可以计算每个二元/三元模型的极性和主观性。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="64e7" class="mj mk it mf b gy ml mm l mn mo">df_ngram['polarity'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).polarity)<br/>df_ngram['subjective'] = df_ngram['bigram/trigram'].apply(lambda x: TextBlob(x).subjectivity)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/3b7d57b7d944c53328efe7c2b89a7ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvqRmif_ejHYCtdTLJMenA.png"/></div></div></figure><h1 id="8fcd" class="mp mk it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">主题建模</h1><p id="b80f" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们也可以用文本数据做一些主题建模。有两种方法可以做到这一点:NMF模型和LDA模型。接下来我们将展示使用这两种方法的例子。</p><h2 id="9d28" class="mj mk it bd mq nz oa dn mu ob oc dp my lr od oe na lv of og nc lz oh oi ne iz bi translated">NMF模型</h2><p id="775e" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">非负矩阵分解(NMF)是一种矩阵分解方法，将一个矩阵分解为非负元素W和H的乘积。默认方法优化原始矩阵和WH之间的距离，即Frobenius范数。下面是一个例子，我们使用NMF产生3个主题，我们在每个主题中显示了3个二元模型/三元模型。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="c73e" class="mj mk it mf b gy ml mm l mn mo">Source: <a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html</a></span><span id="58e7" class="mj mk it mf b gy nw mm l mn mo">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.decomposition import NMF<br/>from sklearn.pipeline import make_pipeline</span><span id="6bc4" class="mj mk it mf b gy nw mm l mn mo">tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))<br/>nmf = NMF(n_components=3)<br/>pipe = make_pipeline(tfidf_vectorizer, nmf)<br/>pipe.fit(df['reviews'])</span><span id="56a8" class="mj mk it mf b gy nw mm l mn mo">def print_top_words(model, feature_names, n_top_words):<br/>    for topic_idx, topic in enumerate(model.components_):<br/>        message = "Topic #%d: " % topic_idx<br/>        message += ", ".join([feature_names[i]<br/>                             for i in topic.argsort()[:-n_top_words - 1:-1]])<br/>        print(message)<br/>    print()</span><span id="cad0" class="mj mk it mf b gy nw mm l mn mo">print_top_words(nmf, tfidf_vectorizer.get_feature_names(), n_top_words=3)</span></pre><p id="c113" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是结果。看起来主题0是关于教授和课程的；题目1是关于作业，题目3是关于课本。请注意，我们不知道这里的最佳主题数量是多少。我们用3只是因为我们的样本量很小。在实践中，您可能需要进行网格搜索来找到最佳的主题数量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/9ff833896fe7f11cd5b441ad14bfa8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkEP9qceKwJMVtsU0USzmw.png"/></div></div></figure><h2 id="34ab" class="mj mk it bd mq nz oa dn mu ob oc dp my lr od oe na lv of og nc lz oh oi ne iz bi translated">LDA模型</h2><blockquote class="nq nr ns"><p id="768f" class="li lj nl lk b ll lm kd ln lo lp kg lq nt ls lt lu nu lw lx ly nv ma mb mc md im bi translated">潜在狄利克雷分配是用于诸如文本语料库的离散数据集集合的生成概率模型。它也是一个主题模型，用于从文档集合中发现抽象主题。</p></blockquote><p id="4c22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的例子中，我们使用函数<code class="fe nm nn no mf b"><strong class="lk jd"><em class="nl">LatentDirichletAllocation</em></strong></code>，它“实现了在线变分贝叶斯算法，并支持在线和批量更新方法”。这里我们展示了一个学习方法被设置为默认值“在线”的例子。</p><pre class="ks kt ku kv gt me mf mg mh aw mi bi"><span id="ca62" class="mj mk it mf b gy ml mm l mn mo">Source: <a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html</a></span><span id="54c3" class="mj mk it mf b gy nw mm l mn mo">from sklearn.decomposition import LatentDirichletAllocation<br/>tfidf_vectorizer = TfidfVectorizer(stop_words=stoplist, ngram_range=(2,3))<br/>lda = LatentDirichletAllocation(n_components=3)<br/>pipe = make_pipeline(tfidf_vectorizer, lda)<br/>pipe.fit(df['reviews'])</span><span id="2c24" class="mj mk it mf b gy nw mm l mn mo">def print_top_words(model, feature_names, n_top_words):<br/>    for topic_idx, topic in enumerate(model.components_):<br/>        message = "Topic #%d: " % topic_idx<br/>        message += ", ".join([feature_names[i]<br/>                             for i in topic.argsort()[:-n_top_words - 1:-1]])<br/>        print(message)<br/>    print()</span><span id="cb0e" class="mj mk it mf b gy nw mm l mn mo">print_top_words(lda, tfidf_vectorizer.get_feature_names(), n_top_words=3)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/ea045400dadeebfedeb3478dcf904001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oO_CSq8oAX5Y-0tVTAzHA.png"/></div></div></figure><p id="33d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在你知道如何用Python做一些基本的文本分析了。出于演示目的，我们的示例具有非常有限的数据大小。现实世界中的文本分析将更具挑战性和趣味性。希望你喜欢这篇文章。谢谢！</p><h1 id="582f" class="mp mk it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">参考</h1><p id="1857" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/applications/plot _ topics _ extraction _ with _ NMF _ LDA . html</a></p><p id="5282" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ extraction . text . count vectorizer . html</a></p><p id="7b6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://stackoverflow.com/questions/11763613/python-list-of-ngrams-with-frequencies/11834518" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/11763613/python-list-of-ngrams-with-frequency/11834518</a></p></div></div>    
</body>
</html>