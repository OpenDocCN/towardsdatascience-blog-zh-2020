<html>
<head>
<title>Distilling BERT Using an Unlabeled Question-Answering Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用无标签问答数据集提取BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distilling-bert-using-unlabeled-qa-dataset-4670085cc18?source=collection_archive---------29-----------------------#2020-10-21">https://towardsdatascience.com/distilling-bert-using-unlabeled-qa-dataset-4670085cc18?source=collection_archive---------29-----------------------#2020-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3357" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何利用知识提炼将未标记数据用于问答任务</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b74cc773180acbe5afcb6a6ee7a81b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cUOizjT_nme67cbk"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@alfonsmc10?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿尔方斯·莫拉莱斯</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="e813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据标注过程相当复杂，尤其是机器阅读理解(问答)等任务。在这篇文章中，我想描述一种技术，我们使用有限的标记数据来使问答模型适应特定的领域——T4知识蒸馏T5。事实证明，我们不仅可以使用它来“压缩”模型，还可以利用域内未标记的数据。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="65c0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">问答和小队📖</h1><p id="ee8b" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">最简单的问答系统形式之一是机器阅读理解(MRC)。这里的任务是在提供的文档中找到一个问题的简短答案。MRC最流行的基准是<strong class="ky ir">斯坦福问答数据集(SQuAD) </strong> [1]。它包含100，000个问答对和53，775个无法回答的问题，这些问题来自维基百科的热门文章的23，215个段落。每个问题的答案都是来自相应阅读文章的一段文字(一个跨度)。对于无法回答的问题，系统应确定该段落何时不支持任何答案，并放弃回答。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/0df47b95651f764386dc51c77e8d596b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dUVzAlvWgW2KxD7fOEscQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">2.0班中可回答和不可回答问题的示例</p></figure><p id="f8cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有其他可用的问答数据集，如Natural Questions和MS MARCO，但SQuAD 2.0是使用最多的数据集之一，也是我们项目的起点。</p><h1 id="5fa1" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">评估预训练模型👩🏻‍🔬</h1><p id="79f9" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">最近，我们为一个照片&amp;摄像机在线商店开发了一个<a class="ae kv" href="https://blog.griddynamics.com/question-answering-system-using-bert/" rel="noopener ugc nofollow" target="_blank">问题回答系统，在那里我们训练了一个机器阅读理解模型。在我们的项目中，我们测试了各种预先训练的问答模型(感谢🤗拥抱脸)，并发现在SQuAD 2.0上训练的ALBERT-xxlarge<strong class="ky ir"/>【2】<strong class="ky ir"/>在我们的域上显示了有希望的结果:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/7363168eb0a5ed0b2b45ad615a8a5aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bsyG8hFOUgVSY1SDQb_p0A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用在SQuAD 2.0 <em class="nd">(图片由作者提供)</em>上预先训练的模型的测试数据集的结果</p></figure><p id="bf94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是这个模型太慢了，我们不能在生产中使用它。在这种情况下，常用的方法之一是使用<em class="ne">知识蒸馏</em>。</p><h1 id="50c1" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">知识升华⚗️</h1><p id="2349" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">当我们想要训练更快或更小的模型时，知识蒸馏[3]通常被用作模型压缩技术。在这个过程中，我们使用我们主要的较大模型(<em class="ne">老师</em>)的输出概率来训练一个较小的模型(<em class="ne">学生</em>)，因此一个学生开始“模仿”老师的行为。基于比较产出分布的损失比仅基于硬目标的损失要丰富得多。这个想法是，通过使用这样的软标签，我们可以从教师模型中转移一些“黑暗知识”。此外，从软标签中学习可以防止模型对其预测过于确定，这类似于标签平滑[4]技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/dfca7664a6f73eb1fcd202d9b493963e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EP0DKzoSvBw6U3J74IeG5w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nd">知识蒸馏(图片作者)</em></p></figure><p id="cdb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">知识提炼是一项非常漂亮的技术，效果出奇地好，对变形金刚尤其有用——更大的模型往往显示出更好的结果，但很难将如此大的模型投入生产。</p><h1 id="45fb" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">使用未标记数据的蒸馏🔮</h1><p id="3058" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在SQuAD上提取或训练的模型在我们的数据集上没有显示出竞争结果。小队数据集与我们的领域没有重叠，所以这样的提取效果不好。似乎在SQuAD上训练的较大模型在域外数据上工作得更好。为了使提炼过程有效，我们需要使用来自我们领域的数据集。除了我们的小标签数据集，我们还有大约<strong class="ky ir"> 30，000个未标签的(没有突出显示的答案)问题-文档对</strong>，我们考虑如何使用它们。</p><p id="d08c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们蒸馏需要什么？一个老师和一个带标签的数据集。ALBERT-xxlarge可以当老师模特。对于我们的30K示例，我们没有标签，但是我们可以删除使用标签的部分损失吗？当然，在没有接地标签的知识提炼过程中，我们会继承更多老师的错误。但是目前我们没有比ALBERT-xxlarge更好的模型，所以即使用更小的模型得到类似的结果对我们也是有用的。因此，我们试图仅使用未标记的30K个示例，从ALBERT-xxlarge到ALBERT-base提取知识。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/10c50c930de9fa7a00d78fb985b36903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eFYCPFxPfcRKZRUlw5PsRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nd">知识蒸馏</em>透地标签<em class="nd">(图片由作者提供)</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/8164b49d40c2d7f0e5e711c1ac3df5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gefbydcuXIM5QpSrdZ5EkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用在SQuAD 2.0 <em class="nd">(图片由作者提供)</em>上预先训练的模型的测试数据集的结果</p></figure><p id="a1a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，我们用F1/EM接近它的老师得到了ALBERT-base，并且我们没有使用任何标记的数据进行训练。当然，这并不意味着我们不再需要标记的数据，分数仍然远不理想，我们也继承了老师的错误，所以添加标记的数据可能会改善这个训练程序。</p><h1 id="3edd" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">蒸馏作为预培训🏋🏻‍♂️</h1><p id="1662" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们还可以考虑将蒸馏作为额外的预训练步骤，以便在使用标记数据时实现更好的样品效率。在下面，您可以看到，在我们的例子中，蒸馏有助于进一步微调带标签的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/6207a9ef0f711ad9d06adc4f6125faf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*auW5dHs7xaV3ZtUR9cXxxg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用在SQuAD 2.0上预训练并在域内标记数据上微调的模型的测试数据集上的结果(~1000个标记示例)<em class="nd">(图片由作者提供)</em></p></figure><h1 id="256a" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">自蒸馏👯‍♀️</h1><p id="1f74" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">你的老师越好，你就能培养出越好的学生。因此，一个主要的方向可以是改进教师模式。除了使用我们的标记数据集，当我们对学生和教师使用相同的模型架构时，还有另一种令人兴奋的技术— <strong class="ky ir">自我升华</strong>。自我升华让我们训练出一个比老师表现更好的学生模型。这听起来可能很奇怪，但是因为学生从老师没有看到的数据中更新其权重，这可以导致学生(可比大小)在来自该分布的数据中表现稍好。当我们对ALBERT-xxlarge应用自蒸馏，然后使用它作为我们的教师之一来进一步蒸馏到一个更小的模型时，我们的实验也再现了这种行为。</p><h1 id="10a8" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">合奏👯‍♀️️️👯‍♂️</h1><p id="e7aa" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">当然，提炼训练程序允许我们有效地使用教师模型的集合，并将它们提炼为一个更小的模型。结合所有这些方法(见下文)和领域自适应语言预训练，我们能够使用有限数量的标记示例获得良好的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/01cb6b0c82eb73211d6621c9755e936b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DW-rvfOeEbqjEeUarKB8dg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nd">我们培训管道的所有步骤</em> <em class="nd">(图片由作者提供)</em></p></figure><h1 id="95ad" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">等等，我们的数据真的没有标签吗？🤔</h1><p id="7f90" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">值得一提的是，即使我们没有给任何数据贴上蒸馏的标签，我们仍然有疑问，但情况并非总是如此。与文本分类和NER等NLP任务相比，问答模型的输入由一个问题和一个文档组成。尽管问题是一个输入，但它可以被视为数据标注过程的一部分(你必须写一个问题，而不只是突出显示一个答案)。这样，真正无标签的QA数据集是当我们只有没有问题的文档时。</p><p id="4f1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，即使我们有问题，我们也没有手动将每个问题准备到一个特定的文档中。我们使用预先训练的<a class="ae kv" href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html" rel="noopener ugc nofollow" target="_blank"> USE-QA </a>模型收集了这30，000个与一组独立文档的问题相匹配的QA对。通过这种方式，我们可以从预先训练的模型开始，并在生产中进一步改进模型。在我们开始收集与我们的系统交互的用户提出的真实问题后，我们可以以同样的方式找到这些问题的候选文档，并使用这个数据集进行知识提炼，而无需标记许多示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/0bfb8cb5a32f567490504661458bfa78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRso4Dhu9HlkIxUnFG9o_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用从已部署系统收集的问题<em class="nd">(图片由作者提供)</em></p></figure><h1 id="932d" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">🛠实验装置</h1><p id="8627" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们使用了来自的<a class="ae kv" href="https://github.com/huggingface/transformers/tree/master/examples/distillation" rel="noopener ugc nofollow" target="_blank">示例</a>🤗拥抱面部变形金刚进行知识提炼，1x NVIDIA 2080TI用于我们所有的实验。<a class="ae kv" href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255" rel="noopener">梯度累积</a>允许我们使用更大的模型，如ALBERT-xxlarge和RoBERTa-large，以及混合精度(fp16)来更快地训练模型。为了在一个2080TI上对ALBERT-xxlarge进行自我蒸馏，我们首先预计算了老师的预测(软标签)。</p><h1 id="e4b6" class="lz ma iq bd mb mc mx me mf mg my mi mj jw mz jx ml jz na ka mn kc nb kd mp mq bi translated">结论</h1><p id="2d14" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">知识提炼是一种方便的技术，它允许我们“压缩”巨大的变压器模型，以便在生产中使用它们。我们还可以以对未标记数据进行预训练的形式使用它，这有助于在对标记样本进行微调时提高样本效率。这对于机器阅读理解等任务尤其有用，在这些任务中，标记数据的过程相当复杂。使用未标记的数据进行提炼并不是一个新颖的想法，它已经在计算机视觉(<a class="ae kv" href="https://arxiv.org/abs/2006.10029" rel="noopener ugc nofollow" target="_blank"> SimCLRv2 </a>，<a class="ae kv" href="https://arxiv.org/abs/1911.04252" rel="noopener ugc nofollow" target="_blank">嘈杂的学生</a>)和NLP ( <a class="ae kv" href="https://arxiv.org/abs/1908.08962" rel="noopener ugc nofollow" target="_blank">阅读能力强的学生学得更好</a>)中显示出良好的结果。</p><p id="0631" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您想了解更多关于我们构建问答系统的旅程，请查看我们更详细的帖子，其中我们描述了我们如何收集和标记数据，微调模型，以及应用各种技术，如知识提取和修剪:</p><div class="nk nl gp gr nm nn"><a href="https://blog.griddynamics.com/question-answering-system-using-bert/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd ir gy z fp ns fr fs nt fu fw ip bi translated">我们如何使用BERT为网上商店构建问答系统</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">在这篇博文中，我们描述了我们基于Transformer模型构建问答系统的经验，比如…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">blog.griddynamics.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob kp nn"/></div></div></a></div></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="62af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要了解更多关于知识提炼和其他压缩方法的信息，我推荐以下资源:</p><ul class=""><li id="eab5" class="oc od iq ky b kz la lc ld lf oe lj of ln og lr oh oi oj ok bi translated"><a class="ae kv" href="https://neptune.ai/blog/knowledge-distillation" rel="noopener ugc nofollow" target="_blank">知识蒸馏:原理、算法、应用</a> (Neptune.ai)</li><li id="1607" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://blog.rasa.com/compressing-bert-for-faster-prediction-2/" rel="noopener ugc nofollow" target="_blank">伯特压缩技术(Rasa)概述</a></li><li id="8607" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://www.pragmatic.ml/a-survey-of-methods-for-model-compression-in-nlp" rel="noopener ugc nofollow" target="_blank">自然语言处理中模型压缩方法综述</a></li><li id="6ce2" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://medium.com/huggingface/distilbert-8cf3380435b5" rel="noopener">更小、更快、更便宜、更轻:推出蒸馏伯特，伯特的蒸馏版本</a></li><li id="1f8f" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/1908.08962" rel="noopener ugc nofollow" target="_blank">博览群书的学生学得更好:论预训练紧凑模型的重要性</a></li></ul><p id="51d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下资源将帮助您深入机器阅读理解任务:</p><ul class=""><li id="124a" class="oc od iq ky b kz la lc ld lf oe lj of ln og lr oh oi oj ok bi translated"><a class="ae kv" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture10-QA.pdf" rel="noopener ugc nofollow" target="_blank">问答架构(CS224N斯坦福讲座)</a></li><li id="5895" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/2006.11880" rel="noopener ugc nofollow" target="_blank">关于机器阅读理解的调查:任务、评估标准和基准数据集</a></li><li id="aba1" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/1901.11373" rel="noopener ugc nofollow" target="_blank">学习和评估一般语言智能(小组、样本效率、概括)</a></li><li id="c5b1" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/1910.06431" rel="noopener ugc nofollow" target="_blank">在回答问题时提高伯特的注意力</a></li><li id="adc9" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/1909.04925" rel="noopener ugc nofollow" target="_blank">BERT如何回答问题？变压器表示的分层分析</a></li><li id="674c" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/2004.03490" rel="noopener ugc nofollow" target="_blank">模型从问答数据集中学到了什么？</a></li><li id="3638" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/2008.02637" rel="noopener ugc nofollow" target="_blank">开放领域问答数据集中的问答测试训练重叠</a></li><li id="d455" class="oc od iq ky b kz ol lc om lf on lj oo ln op lr oh oi oj ok bi translated"><a class="ae kv" href="https://arxiv.org/abs/1809.10735" rel="noopener ugc nofollow" target="_blank">CoQA、SQuAD 2.0和QuAC的定性比较</a></li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="924d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] Pranav Rajpurkar，Robin Jia，<a class="ae kv" href="https://arxiv.org/abs/1806.03822" rel="noopener ugc nofollow" target="_blank"> Percy Liang，知道你不知道的:对小队的无法回答的问题</a> (2018)，ACL 2018</p><p id="c0f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]·兰，陈明达，塞巴斯蒂安·古德曼，凯文·金佩尔，皮尤什·夏尔马，拉杜·索里库特，<a class="ae kv" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">阿尔伯特:一个用于语言表征自我监督学习的Lite BERT</a>(2019)</p><p id="38c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] Geoffrey Hinton，Oriol Vinyals，Jeff Dean，<a class="ae kv" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a> (2015)，NIPS 2014深度学习研讨会</p><p id="1bfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] Rafael Müller，Simon Kornblith，Geoffrey Hinton，<a class="ae kv" href="https://arxiv.org/abs/1906.02629" rel="noopener ugc nofollow" target="_blank">标签平滑在什么情况下有帮助？</a> (2019)，NeurIPS 2019</p></div></div>    
</body>
</html>