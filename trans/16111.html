<html>
<head>
<title>How Transformers Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器如何工作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-transformers-work-6cb4629506df?source=collection_archive---------10-----------------------#2020-11-06">https://towardsdatascience.com/how-transformers-work-6cb4629506df?source=collection_archive---------10-----------------------#2020-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a08a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解前沿自然语言处理的直观可视化指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f28b99a0dcc8c5e9cc7f5905f54b2b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KljjfahOg0g-4Mfb"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Johannes Plenio 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7bdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> G </span> PT-3、BERT、XLNet，所有这些都是自然语言处理(NLP)的当前艺术状态——并且它们都使用一种称为转换器的特殊架构组件。</p><p id="49e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">毫无疑问，《变形金刚》是过去十年人工智能领域最大的发展之一——让我们越来越接近人类不可想象的未来。</p><p id="5ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管如此，尽管它们很受欢迎，但变形金刚一开始看起来可能会令人困惑。部分原因似乎是因为许多对transformer的“介绍”忽略了与理解transformer模型架构本身同样重要的上下文。</p><p id="926f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果不先了解注意力和注意力从何而来，怎么能指望任何人掌握变形金刚的概念呢？这是不可能的。</p><p id="f88d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我写这篇文章的动机。我们将通过变压器模型直观地探索信息流，直观地掌握这些系统的机制——触及魔法背后的数学。</p><p id="4dd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将涵盖:</p><pre class="kj kk kl km gt me mf mg mh aw mi bi"><span id="682b" class="mj mk it mf b gy ml mm l mn mo"><strong class="mf iu">Attention</strong><br/>- Attention vs Recurrence</span><span id="4558" class="mj mk it mf b gy mp mm l mn mo"><strong class="mf iu">Positional Encoding</strong></span><span id="52e2" class="mj mk it mf b gy mp mm l mn mo"><strong class="mf iu">Self-Attention</strong></span><span id="d08d" class="mj mk it mf b gy mp mm l mn mo"><strong class="mf iu">Multi-Head Attention</strong></span></pre></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="bea2" class="mx mk it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">注意力</h1><p id="b67c" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">注意机制早于变形金刚，最初用于增强NLP中以前的尖端技术——递归神经网络(RNNs)。</p><p id="7739" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初，这些rnn通过单点联系共享信息，这意味着大多数输入和输出单元之间没有直接连接，从而导致信息瓶颈和远程依赖性的有限性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/9c08341714a1720fa683a6527b7e48a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2RoY3m1ikaq35ysXU-S5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">注意编解码器LSTMs。</p></figure><p id="021d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">添加了两层简单、紧密连接的神经网络，一层用于消耗所有输入值的编码器，另一层用于解码器，解码器会将当前时间步长的隐藏状态传递给注意力操作[1]。</p><p id="ec44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意操作产生一个“对齐”分数，该分数连接到解码器隐藏状态，并且执行softmax操作以产生输出概率。</p><p id="d160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息通过查询、键和值张量在rnn和编码器-解码器之间传递。查询是从解码器隐藏状态传递的，而键值张量是从编码器隐藏状态传递的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d17ef08f242ae2fcb46142c28afb4f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQP1tXf-boH83vGb8WhG_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">英法翻译任务中编码和解码神经元间的注意。图像源[1]。</p></figure><p id="71c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当单词表征被发现具有高相似性(对齐)时，注意力操作会分配一个高分，而那些具有低相似性的单词表征会分配一个低分。</p><p id="0663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种机制产生了一种被称为注意力的行为，即输出层中的单元可以关注输入层中特定的相关单元。</p><h2 id="843c" class="mj mk it bd my nv nw dn nc nx ny dp ng li nz oa ni lm ob oc nk lq od oe nm of bi translated">注意力与复发</h2><p id="36c4" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在变形金刚和注意力之前，我们有RNNs。由于它们的重现，这些网络非常适合语言任务。</p><p id="3023" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种循环允许神经网络记录一个单词是否出现在序列中，并考虑它周围的单词——甚至是它之前或之后很久的单词。正因为如此，人类语言的细微差别才能得到更好的体现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/9946a5946fb5541fe5956206c288d727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZXsdeCUa3htLzpIcDlSmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oh">注意力的架构就是你所需要的</strong>变压器。改编自同一篇论文。</p></figure><p id="73f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这种新的注意力机制被证明非常强大。强大到足以让2017年一篇名为<strong class="lb iu">的论文【注意力是你所需要的全部】</strong>推出一个没有复发的模型，用注意力完全取代它【2】——第一个变形金刚模型。</p><p id="52e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">三种机制使注意力工作而不需要重现——(1)位置编码，(2)多头注意，和(3)自我注意。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="0ac1" class="mx mk it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">位置编码</h1><p id="6e16" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">正如我们提到的，在《变形金刚》之前，NLP的前沿是rnn——特别是使用长短期记忆(LSTM)单元的rnn。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/222c7df82e573a42c362972672dc4cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TuODQ4sL9breilLQcC1LwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM单元的状态作为输入传递给下一个单元。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/55f19b254646c9778cec45a78a82e5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B05Y5Hifc5zr80xjXvN8Kg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信息可以通过单元状态(顶行)以较少的转换传递更远的距离。</p></figure><p id="bd01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，这些网络能够通过将一个单元的输出状态(在<em class="ok"> t-1 </em>)传递给下一个单元的输入状态(在<em class="ok"> t </em>)来考虑单词的顺序。</p><p id="b91f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Transformer模型不像rnn那样是顺序的，因此需要一种新的方法来保存现在缺失的顺序语义，这种语义以前允许我们考虑单词的顺序，而不仅仅是它们的存在。</p><p id="c3bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持单词的位置信息，在进入注意机制之前，将位置编码添加到单词嵌入中。</p><p id="6d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">《注意力就是你所需要的》一文中采用的方法是为嵌入维中的每一维生成不同的正弦函数。</p><p id="e22e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还记得我们之前说过word2vec引入了用50到100维的向量来表示单词的概念吗？这里，在Vaswani等人的论文中，他们使用了相同的概念，但表示一个单词的位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/8e50fe657f116838a65c9ef7a8e1deac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*YqVm4d_OmlE-J17r4i2yIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交替位置编码值。使用字位置<strong class="bd oh"> pos </strong>，嵌入尺寸<strong class="bd oh"> i </strong>，嵌入尺寸数量<strong class="bd oh"> d_model </strong>。</p></figure><p id="5518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，这次不是使用ML模型计算矢量值，而是使用修改后的正弦函数计算矢量值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/53541ca27e5499b02dc3e239e561f10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*guZylOVMGI5W4n68mxxNsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正弦之后是余弦函数。这种正弦-余弦-正弦的交替模式对于嵌入索引中的每个增量持续。</p></figure><p id="e5d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个向量索引被分配一个交替的正弦-余弦-正弦函数(索引0是正弦；索引1是余弦)。接下来，随着索引值从零向d(嵌入维数)增加，正弦函数频率降低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ab1c17d80417d13525f15f2946e8e625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnrzoDFNyeINfDBileb2wA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用总嵌入维数为20的前五个嵌入指数的正弦函数。嵌入索引位置显示在图例中。</p></figure><p id="0861" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样做的最终效果就像一个有很多指针的时钟。我们输入的每一个令牌都是向前一个时间步长。第一根指针(索引0，正弦)开始指向12，并在100个时间步长内围绕时钟旋转八次，以到达序列的终点。</p><p id="4a4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个指针(索引1，余弦)开始指向6，并在100个时间步长内绕时钟旋转略少于8次(比如7.98次)。</p><p id="f5c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种旋转(频率)随着我们增加嵌入维数的指数而减少。在到达最终指针时(A. Vaswani等人论文中的<em class="ok">第512次</em>)，指针将在100个时间步长内从12点移动到12点过0.001秒。</p><p id="0509" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将所有512个时钟指针绘制成正弦曲线，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/137664a41e6e41010f3d8539dc265307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RsogCpu15J2in40EyvINg.png"/></div></div></figure><p id="52af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们将时钟停在任何一个时间步长，位置20，我们对该时间步长的位置编码是时钟指针指向的所有值的向量。</p><p id="493a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将上面同样难以控制的正弦曲线绘制到更容易理解的热图上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/e9130abccab894041d596aacb229950c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_0otFcN2cN-VWJLfiOJzw.png"/></div></div></figure><p id="87e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在较低的嵌入维度中看到较高的频率(左侧)，随着嵌入维度的增加而降低。在第24维附近，频率降低了很多，以至于我们在剩余的(交替的)正弦余弦波中不再看到任何变化。</p><p id="e5c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，这些位置编码被添加到单词嵌入中。</p><p id="02f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要将这两个向量相加，它们需要有相等的维数。Vaswani等人对单词和位置嵌入都使用了512-嵌入向量[2]。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="5b89" class="mx mk it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">自我关注</h1><p id="7a9a" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">以前，我们使用模型输入和输出层来计算注意力——自我注意力只允许我们使用其中的一个。</p><div class="kj kk kl km gt ab cb"><figure class="oo kn op oq or os ot paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7e170255c4e6112d01bf3727ad53ad30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*1Jy-6XkhyhVSjaMdYn7H3A.png"/></div></figure><figure class="oo kn ou oq or os ot paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4a0ea3d9ea166197bd14ac03b6f94e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*x53NB_sRqsiI0DXfG2WJUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ov di ow ox translated">LSTM编解码器模型中的原始注意机制(左)和自我注意机制(右)。</p></figure></div><p id="858a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，之前从输入编码器中提取的<strong class="lb iu"> K </strong> ey和<strong class="lb iu">V</strong>value以及从输出隐藏状态中提取的<strong class="lb iu">Q</strong>uery——我们现在从单个输入嵌入数组中获得所有三个值。</p><p id="ffee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自我注意允许输入相互作用——这意味着我们可以对同一输入中单词之间的“对齐”进行编码。例如:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/84e07f08ae1583941b5dae17a6e40afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lArKjROg3KFuaO_sNZGX5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用两个略有不同的短语自我关注。来源[3]。</p></figure><p id="0112" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一张图片(左)显示，当最后一个单词是“累”时，单词“它”的注意力集中在“动物”上。当最后一个字改成‘宽’的时候，‘它’的注意力就把焦点转移到了‘街’上。</p><p id="7395" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当通过自我注意单元时，单词之间的这种关系被编码到张量中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d17ef08f242ae2fcb46142c28afb4f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQP1tXf-boH83vGb8WhG_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">英法翻译任务中编码和解码神经元间的注意。图像源[1]。</p></figure><p id="8dec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正常注意力只会在输入和输出序列之间执行这种操作——如上面的热图所示。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="26c5" class="mx mk it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">多头注意力</h1><p id="e7da" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">多头关注是第一个变形金刚成功的关键因素。没有它，结果显示它的表现比它的重复出现的前辈更差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/f1b3e9e88f3f826d7724aeb8bdfc2667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOMEPuuHm0iDWP1reEf95w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多头注意是指并行处理多个注意‘头’。这些多个头的输出被连接在一起。</p></figure><p id="3882" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们产生了我们的位置编码单词嵌入张量之后——我们将它们传递给多头注意力单元。“多头”部分意味着我们实际上是在并行运行多个自我关注单元。</p><p id="43fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦多个注意力单元并行处理了我们的张量，结果就会连接在一起，通过线性单元调整大小，并传递到下一步。</p><div class="kj kk kl km gt ab cb"><figure class="oo kn pa oq or os ot paragraph-image"><img src="../Images/d585e138f3d5efd7766cd10ec61fc7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/0*nEG2OGh8Bc4ol3VE.png"/></figure><figure class="oo kn pb oq or os ot paragraph-image"><img src="../Images/a7394c20008398925c324e040b337767.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*wtOJ22fOvG-Fj3W0.png"/><p class="ku kv gj gh gi kw kx bd b be z dk pc di pd ox translated">两个头(左)和八个头(右)的多头注意力示例[5]。</p></figure></div><p id="a244" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，使用多头注意力的力量来自于同时包含几个<em class="ok">代表子空间</em>。这允许一个子空间识别出<strong class="lb iu"> it </strong>和<strong class="lb iu">累</strong>之间的关系，以及另一个子空间识别出<strong class="lb iu"> it </strong>和<strong class="lb iu">狗</strong>之间的关系。</p><p id="8bed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变压器模型中不需要多头关注。BERT是世界上最强大的NLP模型之一，它将单头注意力单元和前馈单元串联起来，但没有利用多头注意力[4]。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="ab56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于变形金刚的介绍就到此为止。我们涵盖了:</p><ul class=""><li id="bc24" class="pe pf it lb b lc ld lf lg li pg lm ph lq pi lu pj pk pl pm bi translated">RNNs和编码器-解码器机制</li><li id="b5a4" class="pe pf it lb b lc pn lf po li pp lm pq lq pr lu pj pk pl pm bi translated">注意编码器-解码器</li><li id="6782" class="pe pf it lb b lc pn lf po li pp lm pq lq pr lu pj pk pl pm bi translated">注意和变形金刚</li><li id="bc59" class="pe pf it lb b lc pn lf po li pp lm pq lq pr lu pj pk pl pm bi translated">位置编码</li><li id="1947" class="pe pf it lb b lc pn lf po li pp lm pq lq pr lu pj pk pl pm bi translated">多头注意力</li><li id="5964" class="pe pf it lb b lc pn lf po li pp lm pq lq pr lu pj pk pl pm bi translated">自我关注</li></ul><p id="3986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都是非常抽象的概念，很难让我们理解。然而，它们构成了NLP中现代标准的基础，并产生了真正的下一级结果，因此值得花时间去研究。</p><p id="b48e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章能够帮助您更好地理解这些概念，以及它们如何在Transformer中结合在一起。如果你想了解更多，我在<a class="ae ky" href="https://www.youtube.com/channel/UCv83tO5cePwHMt1952IVVHw" rel="noopener ugc nofollow" target="_blank"> YouTube上发布了编程/ML教程</a>！</p><p id="f552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有任何问题、想法或建议——通过<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中联系。</p><p id="e9cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="03f8" class="mx mk it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">参考</h1><p id="0eca" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">[1] D. Bahdanau等人，<a class="ae ky" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a> (2015)，ICLR</p><p id="3472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] A. Vaswani等人，<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)，NeurIPS</p><p id="2930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] J. Uszkoreit，<a class="ae ky" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank"> Transformer:一种新颖的用于语言理解的神经网络架构</a> (2017)，谷歌AI博客</p><p id="59e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] J. Devlin等人，<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2019)，ACL</p><p id="71a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] J .阿拉玛，<a class="ae ky" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《图解变形金刚》</a> (2018)，GitHub</p><p id="0653" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] I. Sutskever等人，<a class="ae ky" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">用神经网络进行序列间学习</a> (2014)，NeurIPS</p><p id="111f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] K. Cho等人，<a class="ae ky" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">使用用于统计机器翻译的RNN编码器-解码器学习短语表示</a> (2014)，EMNLP</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="c2ae" class="mj mk it bd my nv nw dn nc nx ny dp ng li nz oa ni lm ob oc nk lq od oe nm of bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》课程NLP的70%折扣</a></h2></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="cf36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想更好地理解过去十年NLP中变形金刚的起源，我会在本文中详细介绍:</p><div class="ps pt gp gr pu pv"><a rel="noopener follow" target="_blank" href="/evolution-of-natural-language-processing-8e4532211cfe"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">自然语言处理的发展</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">对过去十年自然语言处理的直观视觉解释</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">towardsdatascience.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj ks pv"/></div></div></a></div><p id="af10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ok">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>