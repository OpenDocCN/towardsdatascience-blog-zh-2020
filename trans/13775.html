<html>
<head>
<title>NLP: Detecting Spam Messages with TensorFlow (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP:用TensorFlow检测垃圾邮件(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-detecting-spam-messages-with-tensorflow-part-ii-77826c8f1abf?source=collection_archive---------65-----------------------#2020-09-21">https://towardsdatascience.com/nlp-detecting-spam-messages-with-tensorflow-part-ii-77826c8f1abf?source=collection_archive---------65-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="69eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">构建垃圾邮件检测模型时防止过度拟合</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/722645486e370a710a22c85def457623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzMHH5yD_uh6QB8dxYlvVQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:图片来自<a class="ae ky" href="https://pixabay.com/photos/iphone-smartphone-apps-apple-inc-410324/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="0e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的<a class="ae ky" rel="noopener" target="_blank" href="/nlp-detecting-spam-messages-with-tensorflow-b12195b8cf0e">上一篇文章</a>中，TensorFlow中的递归神经网络被用来检测垃圾短信。</p><p id="9523" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是通过以下方式实现的:首先通过标记化、文本排序和填充等方法以正确的方式操纵文本数据，然后最终训练模型来检测消息是垃圾邮件= 1还是不是垃圾邮件= 0。</p><p id="d475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不熟悉NLP，我建议您先阅读我以前的文章——因为这篇文章的细节会更有意义:)</p><h1 id="2d87" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">原始分析</h1><p id="0b68" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">示例中使用的数据集来自<a class="ae ky" href="https://www.kaggle.com/team-ai/spam-text-message-classification" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>(原始作者<a class="ae ky" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/" rel="noopener ugc nofollow" target="_blank"> Almeida和Hidalgo，2011 </a>)。</p><p id="2bf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，递归神经网络是使用TensorFlow作者的原始单词嵌入和情感笔记本构建的——原始笔记本可在此处<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c04_nlp_embeddings_and_sentiment.ipynb" rel="noopener ugc nofollow" target="_blank">获得</a>。</p><p id="10a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管之前的模型被证明可以有效地为垃圾邮件分配更高的概率，但是仍然有改进的空间。</p><p id="2ad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为参考，使用以下神经网络配置:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="1e5f" class="mx lw it mt b gy my mz l na nb">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (None, 100, 16)           16000     <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 1600)              0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 6)                 9606      <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 7         <br/>=================================================================<br/>Total params: 25,613<br/>Trainable params: 25,613<br/>Non-trainable params: 0</span></pre><p id="6d61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，使用了最大长度为100的1000个词汇:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="183b" class="mx lw it mt b gy my mz l na nb">vocab_size = 1000<br/>embedding_dim = 16<br/>max_length = 100<br/>trunc_type='post'<br/>padding_type='post'<br/>oov_tok = "&lt;OOV&gt;"</span></pre><p id="6e62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当训练网络超过30个时期时，我们可以看到，随着验证损失在某个点之后开始增加，网络出现过拟合的迹象，而训练损失继续减少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/0894d42dd0dc3a15b6f027f875b6f265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*27BLeKVxMVf1EEapDgeDKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="4768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明该模型在训练数据的基础上过于一般化，因此该模型在预测现有消息方面表现良好，但在预测新消息方面表现不佳。</p><p id="1bf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们可以看到，虽然该模型在为句子“恭喜您的新iPhone！单击此处领取您的奖品……”，该模型只产生了以下可能是垃圾邮件的消息的<strong class="lb iu"> 37% </strong>概率:“立即参加免费的COVID网络研讨会:立即预订您的会议……”。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="24e3" class="mx lw it mt b gy my mz l na nb">['Greg, can you call me back once you get this?', 'Congrats on your new iPhone! Click here to claim your prize...', 'Really like that new photo of you', 'Did you hear the news today? Terrible what has happened...', 'Attend this free COVID webinar today: Book your session now...']<br/>Greg, can you call me back once you get this?<br/>[9.670509e-06]<br/><br/><br/>Congrats on your new iPhone! Click here to claim your prize...<br/>[0.91056645]<br/><br/><br/>Really like that new photo of you<br/>[3.0444193e-05]<br/><br/><br/>Did you hear the news today? Terrible what has happened...<br/>[0.00360294]<br/><br/><br/>Attend this free COVID webinar today: Book your session now...<br/>[0.37343866]</span></pre><p id="9406" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这方面，模型参数将被稍微调整，以查看模型的拟合度是否可以提高，即<strong class="lb iu">和</strong>训练和验证损失都在减少的情况。</p><h1 id="ec1c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">扁平化与全球平均池</h1><p id="84a4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在最后一个例子中，你会注意到使用了一个<a class="ae ky" href="https://stackoverflow.com/questions/49295311/what-is-the-difference-between-flatten-and-globalaveragepooling2d-in-keras" rel="noopener ugc nofollow" target="_blank">展平</a>层。这样做的目的是为了将多维张量简化为一维张量，以便进行分析。</p><p id="7a9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，全局平均池可以通过对数据进行平均来更好地表示向量，而<a class="ae ky" href="https://github.com/keras-team/keras/issues/8470" rel="noopener ugc nofollow" target="_blank">扁平化</a>可能会导致过度拟合的风险，这取决于数据量。</p><p id="c4d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对此，我们再来训练一下模型。这一次，使用了一个<strong class="lb iu"> GlobalAveragePooling1D() </strong>层来代替Flatten。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a90c" class="mx lw it mt b gy my mz l na nb">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (None, 100, 16)           16000     <br/>_________________________________________________________________<br/>global_average_pooling1d (Gl (None, 16)                0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 6)                 102       <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 7         <br/>=================================================================<br/>Total params: 16,109<br/>Trainable params: 16,109<br/>Non-trainable params: 0</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3ee40688a6888a105e5702bbe0c3630c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*WBglQ73wKVCzJqNOhosX6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="9aa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练时，我们可以看到训练和验证损失都显著下降，这表明模型没有过度拟合。</p><p id="1837" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是尝试检测大量句子中的垃圾邮件的概率:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c201" class="mx lw it mt b gy my mz l na nb">['Greg, can you call me back once you get this?', 'Congrats on your new iPhone! Click here to claim your prize...', 'Really like that new photo of you', 'Did you hear the news today? Terrible what has happened...', 'Attend this free COVID webinar today: Book your session now...']<br/>Greg, can you call me back once you get this?<br/>[0.02485983]<br/><br/><br/>Congrats on your new iPhone! Click here to claim your prize...<br/>[0.9198217]<br/><br/><br/>Really like that new photo of you<br/>[0.00754593]<br/><br/><br/>Did you hear the news today? Terrible what has happened...<br/>[0.00529179]<br/><br/><br/>Attend this free COVID webinar today: Book your session now...<br/>[0.42277685]</span></pre><p id="ba39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于最后一句话，概率增加到42%。然而，不清楚(甚至肉眼也不清楚)免费的COVID网络研讨会上的句子是否会被视为垃圾邮件。这将取决于其他因素，例如文本消息是否被请求、发送者的号码以及其他因素。在这方面，分配的概率似乎是合理的。</p><p id="1c4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，在训练模型时，假设验证损失较低，我们可以更有信心认为每条短信的指示概率是可信的。</p><p id="87e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，使用了最大长度为100的1000个词汇。用30个时期训练，在时期30的验证损失为0.0642，验证准确度为0.9848。假设验证精度已经相当高，那么如果这些参数减少，模型还会产生准确的结果吗？</p><p id="3da5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了测试这一点，让我们将词汇量设置为600，最大长度为60。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/33f5254937fea47a67d62edbb2312cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*38fQuGRRWNeKMa5-HxHzTA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="6aec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，验证准确度为0.9865，验证损失为0.0455。在这种情况下，模型实际上表现得稍好一些，这表明模型中的词汇大小可以在不影响性能的情况下减少。</p><p id="65ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是我们在上面看到的一系列假设消息的更新概率:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a463" class="mx lw it mt b gy my mz l na nb">['Greg, can you call me back once you get this?', 'Congrats on your new iPhone! Click here to claim your prize...', 'Really like that new photo of you', 'Did you hear the news today? Terrible what has happened...', 'Attend this free COVID webinar today: Book your session now...']<br/>Greg, can you call me back once you get this?<br/>[0.01641327]<br/><br/><br/>Congrats on your new iPhone! Click here to claim your prize...<br/>[0.9475718]<br/><br/><br/>Really like that new photo of you<br/>[0.00675001]<br/><br/><br/>Did you hear the news today? Terrible what has happened...<br/>[0.00826639]<br/><br/><br/>Attend this free COVID webinar today: Book your session now...<br/>[0.3014569]</span></pre><h1 id="faf4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="99a8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这个例子是上一个教程使用递归神经网络检测垃圾短信的后续。</p><p id="15bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，我们看到了扁平化和全局平均池在训练模型时的不同之处——在较小的训练集上使用扁平化有过度拟合的风险，这里似乎就是这种情况。此外，我们还看到了正确的模型配置如何在较小的词汇表中产生出色的结果。</p><p id="5b7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow作者(版权2020)在<a class="ae ky" href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c05_nlp_tweaking_the_model.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>提供了这个例子的原始模板。此外，我推荐Udacity提供的深度学习TensorFlow的<a class="ae ky" href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187" rel="noopener ugc nofollow" target="_blank">简介</a>课程，以便更深入地理解这个主题。</p><p id="52a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面例子的GitHub库可以在<a class="ae ky" href="https://github.com/MGCodesandStats/tensorflow-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="9595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nf">免责声明:本文是在“原样”的基础上编写的，没有任何担保。本文旨在提供数据科学概念的概述，不应被解释为任何形式的专业建议。作者与本文中提到的任何一方都没有关系，本文或其调查结果也没有得到同样的认可。</em></p></div></div>    
</body>
</html>