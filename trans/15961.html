<html>
<head>
<title>VGG-16 Transfer Learning in Classifying Log-Mel Spectrogram Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Log-Mel谱图图像分类中的VGG-16迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-in-speech-emotion-recognition-d55b6616ba83?source=collection_archive---------16-----------------------#2020-11-03">https://towardsdatascience.com/transfer-learning-in-speech-emotion-recognition-d55b6616ba83?source=collection_archive---------16-----------------------#2020-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/008557b7af38590e45a77e673ccc4a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8S6iyGmyixahSFiGBkEE3A.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@starburst1977" rel="noopener ugc nofollow" target="_blank">斯文·里德</a>在<a class="ae kf" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="2cd3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为我之前<a class="ae kf" rel="noopener" target="_blank" href="/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690?source=your_stories_page-------------------------------------">帖子</a>的后续，我将对<a class="ae kf" href="https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio" rel="noopener ugc nofollow" target="_blank"> RAVDESS音频数据集</a>应用迁移学习，希望提高模型的准确性。回顾一下，迁移学习是一种深度学习方法，其中已经在一项任务上训练的模型被用作训练类似任务的模型的起点。在DJ Sarkar的这篇文章中，他用例子为理解迁移学习提供了很好的指导。</p><p id="a898" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将首先尝试使用<a class="ae kf" href="https://www.kaggle.com/keras/vgg16/home" rel="noopener ugc nofollow" target="_blank"> VGG-16 </a>预训练模型作为我们数据集的特征提取器，在这里我们冻结预训练模型的卷积块并修改密集层。然后，我们将尝试图像增强的预训练模型。</p><p id="4678" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以让我们开始吧！</p><h1 id="cd03" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">数据准备</h1><p id="a2a7" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在导入必要的库之后，我们必须导入带有标签的训练和测试图像。</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="d0c5" class="mq lf it mm b gy mr ms l mt mu"># For training set only</span><span id="c304" class="mq lf it mm b gy mv ms l mt mu">import glob</span><span id="e478" class="mq lf it mm b gy mv ms l mt mu">angry = glob.glob('/content/drive/My_Drive/train_logmel/angry/*.*')<br/>calm = glob.glob('/content/drive/My_Drive/train_logmel/calm/*.*')<br/>disgust = glob.glob('/content/drive/My_Drive/train_logmel/disgust/*.*')<br/>fearful = glob.glob('/content/drive/My_Drive/train_logmel/fearful/*.*')<br/>happy = glob.glob('/content/drive/My_Drive/train_logmel/happy/*.*')<br/>neutral = glob.glob('/content/drive/My_Drive/train_logmel/neutral/*.*')<br/>sad = glob.glob('/content/drive/My_Drive/train_logmel/sad/*.*')<br/>surprised = glob.glob('/content/drive/My_Drive/train_logmel/surprised/*.*')</span><span id="4da2" class="mq lf it mm b gy mv ms l mt mu">data = []<br/>labels = []</span><span id="76b9" class="mq lf it mm b gy mv ms l mt mu">for i in angry:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Angry')<br/>for i in calm:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Calm')<br/>for i in disgust:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Disgust')<br/>for i in fearful:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Fearful')<br/>for i in happy:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Happy')<br/>for i in neutral:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Neutral')<br/>for i in sad:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Sad')<br/>for i in surprised:   <br/>    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', <br/>    target_size= (224,224))<br/>    image=np.array(image)<br/>    data.append(image)<br/>    labels.append('Surprised')</span><span id="47fb" class="mq lf it mm b gy mv ms l mt mu">train_data = np.array(data)<br/>train_labels = np.array(labels)</span></pre><p id="391c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看训练集中的图像示例:</p><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/32516262a09bbea3f98abc15502cb2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mt6UeXWQ41yUVYZo5JmorA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h2 id="5fc2" class="mq lf it bd lg mx my dn lk mz na dp lo kr nb nc ls kv nd ne lw kz nf ng ma nh bi translated">数据预处理</h2><ol class=""><li id="25da" class="ni nj it ki b kj mc kn md kr nk kv nl kz nm ld nn no np nq bi translated">标准化数据</li></ol><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="bb7c" class="mq lf it mm b gy mr ms l mt mu">X_train = X_train.astype('float32')<br/>X_test = X_test.astype('float32')<br/>X_train /= 255<br/>X_test /= 255</span></pre><p id="8290" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.一键编码目标类</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="01f2" class="mq lf it mm b gy mr ms l mt mu">lb = LabelEncoder()<br/>y_train = np_utils.to_categorical(lb.fit_transform(y_train))<br/>y_test = np_utils.to_categorical(lb.fit_transform(y_test))</span></pre><h1 id="7503" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">VGG-16模型</h1><p id="3f82" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">首先，我们导入VGG16并设置必要的参数:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="ccdd" class="mq lf it mm b gy mr ms l mt mu">from keras.applications import VGG16</span><span id="d5cf" class="mq lf it mm b gy mv ms l mt mu">vgg_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))</span></pre><p id="65e3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">权重= 'imagenet' </strong>:使用预训练的权重，而不是从头开始训练模型</p><p id="bb27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> include_top=False : </strong>我们希望加载没有分类器层的模型，并添加我们自己的模型</p><p id="f4ba" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> input_shape=(224，224，3) </strong>:指定数据集中图像的首选形状</p><p id="a601" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们冻结卷积模块:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="c3ec" class="mq lf it mm b gy mr ms l mt mu">for layer in vgg_model.layers:<br/>layer.trainable = False</span><span id="abc1" class="mq lf it mm b gy mv ms l mt mu"># Make sure you have frozen the correct layers<br/>for i, layer in enumerate(vgg_model.layers):<br/>    print(i, layer.name, layer.trainable)</span></pre><figure class="mh mi mj mk gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ee49285bf2fee72560945142ffaee628.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*WoChDBO4C2rXqSp-Y5SY6Q.png"/></div></figure><p id="7217" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后创建我们的密集层:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="f57d" class="mq lf it mm b gy mr ms l mt mu">x = vgg_model.output<br/>x = Flatten()(x) # Flatten dimensions to for use in FC layers<br/>x = Dense(512, activation='relu')(x)<br/>x = Dropout(0.5)(x) # Dropout layer to reduce overfitting<br/>x = Dense(256, activation='relu')(x)<br/>x = Dense(8, activation='softmax')(x) # Softmax for multiclass<br/>transfer_model = Model(inputs=vgg_model.input, outputs=x)</span></pre><p id="8afc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们编译并拟合模型:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="2f22" class="mq lf it mm b gy mr ms l mt mu">learning_rate= 5e-5<br/>transfer_model.compile(loss="categorical_crossentropy", optimizer=optimizers.Adam(lr=learning_rate), metrics=["accuracy"])</span><span id="5c8a" class="mq lf it mm b gy mv ms l mt mu">history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test))</span></pre><p id="3110" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过50个时代，我们达到了69%的准确率。</p><div class="mh mi mj mk gt ab cb"><figure class="ns ju nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/487d019e8799421f3e372e3f7fb7116f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*gGPUlbKhZUveemeGUsCyfg.png"/></div></figure><figure class="ns ju ny nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/0ce51836329e23e28a5e803a88c29b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3R9DQc2Cjg-Zk1cVHCstnA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk nz di oa ob translated">作者图片</p></figure></div><p id="0db1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这比以前的模型表现稍好，但必须有一种方法来提高模型的准确性！让我们尝试使用VGG-16作为图像增强的特征提取器。在处理小数据集时，图像增强是添加更多训练数据的好方法。</p><h1 id="cf3e" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">图像增强</h1><p id="09f5" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">使用ImageDataGenerator，我们可以增强图像:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="7a64" class="mq lf it mm b gy mr ms l mt mu">train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=40, width_shift_range=0.3, height_shift_range=0.3, shear_range=0.3, horizontal_flip=True, fill_mode="nearest")</span><span id="5bf6" class="mq lf it mm b gy mv ms l mt mu">train_generator = train_datagen.flow(train_data, train_lb, batch_size=1)</span><span id="6974" class="mq lf it mm b gy mv ms l mt mu">val_datagen = ImageDataGenerator(rescale=1./255)</span><span id="f85f" class="mq lf it mm b gy mv ms l mt mu">val_generator = val_datagen.flow(test_data,val_lb,batch_size=1)</span></pre><p id="b9ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将构建我们的深度学习模型，编译该模型，然后拟合它:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="7459" class="mq lf it mm b gy mr ms l mt mu">x = vgg_model.output<br/>x = Flatten()(x) # Flatten dimensions to for use in FC layers<br/>x = Dense(512, activation='relu')(x)<br/>x = Dropout(0.5)(x) # Dropout layer to reduce overfitting<br/>x = Dense(256, activation='relu')(x)<br/>x = Dense(8, activation='softmax')(x) # Softmax for multiclass<br/>transfer_model = Model(inputs=vgg_model.input, outputs=x)</span><span id="026f" class="mq lf it mm b gy mv ms l mt mu">learning_rate= 5e-5<br/>transfer_model.compile(loss="sparse_categorical_crossentropy", optimizer=keras.optimizers.Adam(lr=learning_rate), metrics=["accuracy"])</span><span id="5304" class="mq lf it mm b gy mv ms l mt mu">history = transfer_model.fit_generator(train_generator, validation_data=val_generator, epochs=100, shuffle=True, verbose=1)</span></pre><p id="c098" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过100个时期后，我们获得了78%的准确率</p><div class="mh mi mj mk gt ab cb"><figure class="ns ju oc nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/601f4503f70c83eb7f2ec6cae49b2333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*OWhGgT1eosgCN1QHcY2P9g.png"/></div></figure><figure class="ns ju od nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/3bc56860cbaf82311986bad3ef1e14d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*OehIM9NBgEf69wUeHiFYxg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk oe di of ob translated">作者图片</p></figure></div><p id="63ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，这并不比我们之前的模型表现得更好，当前的模型过度拟合了训练数据。</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="dff8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将研究其他预训练模型(如Inception_V3和Resnet50 ),并探索微调，而不是使用预训练模型作为特征提取器。感谢您的阅读！:)</p></div></div>    
</body>
</html>