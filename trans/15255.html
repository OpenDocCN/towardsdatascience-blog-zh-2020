<html>
<head>
<title>Incremental join using AWS Glue Bookmarks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用AWS粘合书签的增量连接</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/incremental-join-using-aws-glue-bookmarks-ad8fb2b05505?source=collection_archive---------15-----------------------#2020-10-20">https://towardsdatascience.com/incremental-join-using-aws-glue-bookmarks-ad8fb2b05505?source=collection_archive---------15-----------------------#2020-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4c21" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><h1 id="043e" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">问题是</h1><p id="33d9" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最近，我遇到了一个挑战，即在时间戳上将两个时间序列数据集连接在一起，而不要求两个数据集中的相应数据同时到达。例如，一个数据集上个月某一天的数据可能在一周前到达S3，而另一个数据集上个月该天的相应数据可能在昨天到达。这是一个增量连接问题。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/ad46496dd1a749f3a38486dcc214b494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*I9UbDY9wCXBNzBdFy3jZ4g.jpeg"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">由<a class="ae me" href="https://unsplash.com/@shanerounce?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沙恩·朗斯</a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="94ae" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">潜在的解决方案</h1><ul class=""><li id="e042" class="mf mg iq kw b kx ky lb lc lf mh lj mi ln mj lr mk ml mm mn bi translated">也许可以通过在查询之前不连接数据来解决这个问题，但是我想对连接进行预处理，以便可以在任何比例下直接查询连接的数据。</li><li id="3fe4" class="mf mg iq kw b kx mo lb mp lf mq lj mr ln ms lr mk ml mm mn bi translated">您可以在每次管道运行时重新处理整个数据集。对于每天都在不断变大的数据集来说，这不是一个。</li><li id="1742" class="mf mg iq kw b kx mo lb mp lf mq lj mr ln ms lr mk ml mm mn bi translated">也可以手动实现一个系统，在两个表都着陆之前，不处理来自任何一个表的数据。这实际上是重新实现AWS Glue已经提供的一个特性:<a class="ae me" href="https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html" rel="noopener ugc nofollow" target="_blank">书签</a>，我们将在下面利用它。</li></ul><h1 id="3691" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">使用AWS粘合书签和谓词下推</h1><p id="31e7" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AWS粘合书签允许您仅处理自管道之前运行以来到达数据管道的新数据。在上述增量连接问题中，需要处理的相应数据可能已经到达并且已经在流水线的不同运行中被处理，这并没有完全解决问题，因为相应的数据将由书签馈送以在不同的作业中被处理，因此不会被连接。</p><p id="6908" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">我们提出的解决方案利用了AWS Glue的另一个特性，即使用<a class="ae me" href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html" rel="noopener ugc nofollow" target="_blank">谓词下推</a>加载表的子集的能力。我们在Glue ETL工作中完成了以下ETL步骤:</p><pre class="lt lu lv lw gt my mz na nb aw nc bi"><span id="0f93" class="nd jx iq mz b gy ne nf l ng nh"># setup Glue ETL environment</span><span id="ae3f" class="nd jx iq mz b gy ni nf l ng nh">import sys</span><span id="f18d" class="nd jx iq mz b gy ni nf l ng nh">from awsglue.transforms import *</span><span id="f11c" class="nd jx iq mz b gy ni nf l ng nh">from awsglue.utils import getResolvedOptions</span><span id="054b" class="nd jx iq mz b gy ni nf l ng nh">from pyspark.context import SparkContext</span><span id="2f48" class="nd jx iq mz b gy ni nf l ng nh">from awsglue.context import GlueContext</span><span id="c01f" class="nd jx iq mz b gy ni nf l ng nh">from awsglue.job import Job</span><span id="7dc7" class="nd jx iq mz b gy ni nf l ng nh">from pyspark.sql.functions import split, col</span><span id="59e7" class="nd jx iq mz b gy ni nf l ng nh">from awsglue.dynamicframe import DynamicFrame</span><span id="036e" class="nd jx iq mz b gy ni nf l ng nh">## @params: [JOB_NAME]</span><span id="4a31" class="nd jx iq mz b gy ni nf l ng nh">args = getResolvedOptions(sys.argv, ['JOB_NAME'])</span><span id="3702" class="nd jx iq mz b gy ni nf l ng nh">sc = SparkContext()</span><span id="0d1a" class="nd jx iq mz b gy ni nf l ng nh">glueContext = GlueContext(sc)</span><span id="f8a9" class="nd jx iq mz b gy ni nf l ng nh">spark = glueContext.spark_session</span><span id="138f" class="nd jx iq mz b gy ni nf l ng nh">job = Job(glueContext)</span><span id="b906" class="nd jx iq mz b gy ni nf l ng nh">job.init(args['JOB_NAME'], args)</span></pre><p id="ea08" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">使用AWS Glue目录中的Glue书签加载自上次管道运行以来到达的新数据。</p><pre class="lt lu lv lw gt my mz na nb aw nc bi"><span id="09db" class="nd jx iq mz b gy ne nf l ng nh">table1_new = glueContext.create_dynamic_frame.from_catalog(database="db", table_name="table1", transformation_ctx='table1_new')</span><span id="1f9f" class="nd jx iq mz b gy ni nf l ng nh">table2_new = glueContext.create_dynamic_frame.from_catalog(database="db", table_name="table1", transformation_ctx='table2_new')</span></pre><p id="0661" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">在新数据中找到受影响的分区。因为这是时间序列数据，所以按日期时间进行分区是有意义的。将这些分区写入可以在整个数据集上查询的下推谓词。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="0dc4" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">已经构建了谓词字符串，该字符串列出了在一个或两个表中有新数据的每个分区，现在我们可以从源数据表中加载“解锁”该数据。</p><pre class="lt lu lv lw gt my mz na nb aw nc bi"><span id="cf61" class="nd jx iq mz b gy ne nf l ng nh">we use the predicate string we previously built and load the table without using bookmarks</span><span id="e4cb" class="nd jx iq mz b gy ni nf l ng nh">table1_unlock = glueContext.create_dynamic_frame.from_catalog(database="db", table_name="table1", push_down_predicate=table1_predicate)</span><span id="45cb" class="nd jx iq mz b gy ni nf l ng nh">table2_unlock = glueContext.create_dynamic_frame.from_catalog(database="db", table_name="table2", push_down_predicate=table2_predicate)</span></pre><p id="74c6" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">现在，我们可以使用这两个表运行我们想要的任何连接转换。</p><p id="3aef" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">然后，我们可以将这些表写入数据库，在我们的例子中是S3。根据您正在进行的连接转换的类型，我们发现最好在“覆盖”模式下使用Spark API编写器，而不是Glue DynamicFrame编写器，因为我们希望删除以前运行时写入分区的任何旧数据，并只写入新处理的数据。</p><pre class="lt lu lv lw gt my mz na nb aw nc bi"><span id="8bb5" class="nd jx iq mz b gy ne nf l ng nh"># set the overwrite mode to dynamic</span><span id="b3df" class="nd jx iq mz b gy ni nf l ng nh">spark.conf.set("spark.sql.sources.partitionOverwriteMode","dynamic")</span><span id="b254" class="nd jx iq mz b gy ni nf l ng nh">final_df.write.partitionBy(["partition1", "partition2"]).saveAsTable("db.output", format='parquet', mode='overwrite', path='s3://your-s3-path')</span></pre><p id="db51" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">注意，写入Glue目录的PySpark API模式似乎偶尔会导致表在被写入时变得不可用。</p><h1 id="7a0f" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">结论</h1><p id="3516" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将AWS粘合书签与谓词下推结合使用，可以在ETL管道中实现数据的增量连接，而无需每次都重新处理所有数据。选择一个好的分区模式可以确保您的增量连接作业处理接近所需的最小数据量。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="94a7" class="pw-post-body-paragraph ku kv iq kw b kx mt kz la lb mu ld le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated"><em class="ns">原载于</em><a class="ae me" href="https://datamunch.tech/content/posts/incremental-join/" rel="noopener ugc nofollow" target="_blank"><em class="ns">https://data munch . tech</em></a><em class="ns">。</em></p></div></div>    
</body>
</html>