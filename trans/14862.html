<html>
<head>
<title>How important the words in your text data? Tf-Idf answers…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的文本数据中的单词有多重要？Tf-Idf答案…</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-important-are-the-words-in-your-text-data-tf-idf-answers-6fdc733bb066?source=collection_archive---------15-----------------------#2020-10-13">https://towardsdatascience.com/how-important-are-the-words-in-your-text-data-tf-idf-answers-6fdc733bb066?source=collection_archive---------15-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e28467e8dfd31d8e42bc5ef117405cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1cyl1ijMdO-dvjUpNGxQQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="4821" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们人类经常处理文本数据。文本数据主要来源于语言。语言技能是我们在婴儿时期就开始练习的👶。口语大多被转换成音频形式的数据🔉，文本📃或者很少作为图像🔡。从技术角度来看，文本数据可能以不同的形式出现，如新闻文章📰，书籍📚，网页🌐，评论💬、软件代码🖥、计算机系统日志等等。不管是什么形式，这些文本数据都有自己的结构特征，这些特征是由语言(或语法)定义的。此外，在文本数据中存在语义特征(含义),这可以导致识别甚至跨语言的文本片段的相似性。随着科学领域中自然语言处理(<strong class="ke ir">N</strong>T2】L语言处理(<strong class="ke ir"> NLP </strong>)和信息处理(<strong class="ke ir">I</strong>R<strong class="ke ir">R</strong>IR)的发展，文本表示和文本相似性成为潜在的研究领域。文本表示主要用于，</p><ol class=""><li id="236d" class="la lb iq ke b kf kg kj kk kn lc kr ld kv le kz lf lg lh li bi translated">机器翻译—自动将文本从一种语言翻译成另一种语言。</li><li id="2a49" class="la lb iq ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">文档聚类——基于结构和/或语义相似性对文本文档进行分组。</li><li id="e340" class="la lb iq ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">主题检测——识别大型文本语料库的主题。</li><li id="a9dc" class="la lb iq ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">文本摘要。</li><li id="6c3d" class="la lb iq ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">问题回答。</li></ol><p id="1147" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以及更广泛的领域，如信息检索和文档排序。</p><h1 id="bc7f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">代表权问题…</h1><p id="0149" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">即使一些与文本数据相关的用例如文档聚类可以通过考虑数据的文本相似性(通过使用<a class="ae mr" href="https://en.wikipedia.org/wiki/Edit_distance#:~:text=In%20computational%20linguistics%20and%20computer,one%20string%20into%20the%20other." rel="noopener ugc nofollow" target="_blank">编辑距离</a>、<a class="ae mr" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> Jaccard距离</a>等)来实现。)，对于自然语言处理(<strong class="ke ir"> NLP </strong>)和信息检索(IR)中最复杂的用例，文本数据使用向量表示。这是因为大多数机器学习算法都很聪明，可以处理数字数据🧮.例如，尽管图像被视为平面中颜色组合的集合，但默认情况下，它们被表示为包含所有图像信息的数字矩阵，这些信息很容易被计算机处理。但是，在文本数据的情况下，识别有意义的表示并不那么简单。</p><p id="7873" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">过去引入了几种文本表示技术。其中有<a class="ae mr" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">一键编码</a>、<a class="ae mr" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-gram模型</a>、<a class="ae mr" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">词袋模型</a>和<a class="ae mr" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">神经词嵌入</a>。这些技术各有优缺点。更好的做法是挖掘您的用例，并选择最佳选项。重要的是要理解，表示是您将文本数据呈现给算法的方式。因此，您必须注意选择最合适的数据表示技术，以便您的模型看到它应该看到的数据。</p><h1 id="efd6" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">词汇袋👜模型</h1><p id="d42c" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">一种流行的文本表示技术是<strong class="ke ir">B</strong>ag-<strong class="ke ir">o</strong>f-<strong class="ke ir">W</strong>ords(<strong class="ke ir">BoW</strong>)模型。在单词包中，模型文本或文档通过对单词的<em class="ms">包</em>(无序集合)进行建模来表示。这个<em class="ms">单词包</em>不算单词在文中的位置、语法或者结构。它只是统计单词在目标文本中的出现频率，并将这些单词放入一个<em class="ms">包中。</em>词在文本(句子或文档)中出现的频率是用于词袋模型的特征。</p><p id="c3ce" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">让我们看看如何为文本语料库创建一个单词包模型。</p><p id="18a4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">考虑以下文档集(<strong class="ke ir"> D </strong>)(这里，我们将句子作为文档)。我们的目标是生成一个数字表示📊对于这些文档中的每一个。</p><blockquote class="mt mu mv"><p id="7054" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated">鲍勃比其他人都聪明</p><p id="c0ad" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated">约翰喜欢在晚上探索星星</p><p id="85bd" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated">文件三:天空中阳光灿烂</p><p id="2eba" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated"><strong class="ke ir">文件四</strong>:夜空布满了星星</p></blockquote><h2 id="c307" class="mz lp iq bd lq na nb dn lu nc nd dp ly kn ne nf mc kr ng nh mg kv ni nj mk nk bi translated">第一步:清理✂️，预处理数据</h2><p id="eeae" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">在进行表示建模之前，我们应该明白<strong class="ke ir">原始数据</strong>总是<strong class="ke ir">脏的</strong>，我们必须在使用之前清理它们。</p><p id="18b0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，我们将所有的单词都小写，以避免将不同大小写的相同单词识别为不同的单词。例如，语料库中的单词“sky”和“Sky”应该被理解为一个单词。</p><p id="2ddf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">注意</strong>:我们可以使用一些其他的文本前置技术，比如词汇化(将单词转换成它们的基本形式(" shine "、" shining ' "、" shined "-"&gt;" shine ")，去除停用词，比如" is "、" the "和" in "，等等。但是为了简单起见，为了强调<strong class="ke ir"> Tf-Idf </strong>中<strong class="ke ir"> Idf </strong>(稍后将详细讨论)组件的威力，我们只坚持使用小写字母。</p><h2 id="12fd" class="mz lp iq bd lq na nb dn lu nc nd dp ly kn ne nf mc kr ng nh mg kv ni nj mk nk bi translated">第二步:创建单词包</h2><p id="4fd1" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">完成前面提到的预处理步骤后，我们的四个文档将是:</p><blockquote class="mt mu mv"><p id="2ab4" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated">鲍勃比其他人都聪明</p><p id="2d55" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated">约翰喜欢在晚上探索星星</p><p id="f9d9" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated"><strong class="ke ir">文件3 </strong>:天空中阳光灿烂</p><p id="459a" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated"><strong class="ke ir">文件四</strong>:夜空布满星星</p></blockquote><p id="ae9b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，我们可以创建我们的<strong class="ke ir"> <em class="ms">包</em> </strong>👜单词如下。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fb0feeb3fdd83a85745af48980986969.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*1DidZZm5ahUCGmcih7Re0g.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">所选语料库的单词包</p></figure><h1 id="8d30" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">文本表示的Tf-Idf</h1><p id="4f45" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated"><strong class="ke ir">Tf-Idf</strong>(<strong class="ke ir">T</strong>erm<strong class="ke ir">f</strong>frequency—<strong class="ke ir">I</strong>nverse<strong class="ke ir">d</strong>document<strong class="ke ir">f</strong>frequency)是一个词袋模型，它在捕捉文本中最重要的词方面非常强大。<strong class="ke ir"> Tf-Idf </strong>背后的概念可以通过术语频率(<strong class="ke ir"> Tf </strong>)和逆文档频率(<strong class="ke ir"> Idf </strong>)来理解。从<strong class="ke ir"> Tf </strong>和<strong class="ke ir"> Idf </strong>表示生成的集合表示被称为<strong class="ke ir"> Tf-Idf </strong>，它是一种强大的表示文本数据的方式。让我们进入<strong class="ke ir"> Tf-Idf </strong>背后的理论。</p><h2 id="a458" class="mz lp iq bd lq na nb dn lu nc nd dp ly kn ne nf mc kr ng nh mg kv ni nj mk nk bi translated">术语频率(Tf)</h2><p id="6d42" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">顾名思义，术语频率给出了文档中出现的与BoW相关的术语的计数。术语频率是术语<strong class="ke ir"> t </strong>和文档<strong class="ke ir">d</strong>的函数</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/579fc2c6e7dec6868a0ee25418652376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/1*CNB9-Q9meRRvs4X6_3v50A.gif"/></div></figure><p id="1bb6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们现在可以为每个文档生成一个<strong class="ke ir"> Tf </strong>向量。每个<strong class="ke ir"> Tf </strong>向量是<strong class="ke ir">弓</strong>的尺寸(在我们的例子中是19的尺寸)。如果目标文档在包中没有术语，则该维度将变为零，因为上述等式的分子变为零。下表中的文档列表示每个文档的Tf向量。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/879c09a913af9dd33aec688a369b652b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xtJuOwEYe3di67kGOYxYBA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">关于单词包中的术语(t)的文档(d)的Tf表示。</p></figure><p id="aa07" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以看到，这个<strong class="ke ir"> Tf </strong>表示给出了术语在文档中相对于给定的<strong class="ke ir"> BoW </strong>的出现程度。但是这足以决定两个文档之间的相似性吗？让我们看看…考虑一下<strong class="ke ir">文件1 </strong>和<strong class="ke ir">文件2 </strong>的<strong class="ke ir"> Tf </strong>表示。我们可以看到，只有术语<strong class="ke ir">和</strong>在这两个文档中是通用的。因此，我们的向量空间的“<strong class="ke ir"/>”-维度将显示<strong class="ke ir">文档1 </strong>和<strong class="ke ir">文档2 </strong>具有相似度(即使所有其他维度将显示这两个文档不相似)。这是真的吗？实际上不是。它们有完全不同的意思。这是作为文本文档的数字表示的术语频率的缺点。词频并不强调文字在文本中的重要性。这就是Idf 发挥作用的地方。</p><h2 id="b1d7" class="mz lp iq bd lq na nb dn lu nc nd dp ly kn ne nf mc kr ng nh mg kv ni nj mk nk bi translated">反向文档频率(Idf)</h2><p id="996a" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">逆文档频率(<strong class="ke ir"> Idf </strong>)是一个术语相对于文档语料库的唯一性的度量。这里的想法是，出现在语料库的大多数文档中的术语不会向目标文档添加特殊信息。逆文档频率是为您的<strong class="ke ir"> BoW </strong>中的每个术语定义的。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4fa35bfe6437ed4263acba9a6937395d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/1*4Hd4q9JZTPcKLf6r1-FXJA.gif"/></div></figure><p id="219c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> Idf </strong>能够在<strong class="ke ir"> BoW </strong>中强调术语的独特性或重要性。如果一个术语出现在大多数文档中，它就不是唯一的，并且<strong class="ke ir"> Idf </strong>等式的分母会更大。因此，分数的对数将更小，Idf 值也将更小。一个独特的或罕见的单词将出现在较少数量的文档中，并且其<strong class="ke ir"> Idf </strong>值将较大。让我们看看我们的<strong class="ke ir">弓</strong>的Idf值是多少。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6a5f9201506055f9052a6078849cd31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*dbSfsVF99anJlLrZCgku8Q.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">BoW中术语的Idf值</p></figure><p id="007a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以看到，<strong class="ke ir"><em class="ms"/></strong>项的Idf变为0。这是因为它出现在所有文档中，并且没有给目标文档添加独特的含义。同样，术语<strong class="ke ir"> <em class="ms">与</em> </strong>的Idf权重也很低。我们可以看到，像<strong class="ke ir"> <em class="ms"> bob、explore </em> </strong>这样在语料库中只出现在一个文档中的单词，由于其唯一性，具有更高的<strong class="ke ir"> Idf </strong>值。</p><p id="b83d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">请记住，像<strong class="ke ir"> <em class="ms">、【of】、</em>、</strong>这样的术语并没有给文件<strong class="ke ir">增加特定的含义。</strong>尽管我们在这里发现它们是独一无二的，但它们并不存在于现实世界中。为了简单起见，我们选择将这些术语(<strong class="ke ir"> <em class="ms"> the，in，of，to </em> </strong>和<strong class="ke ir"> <em class="ms">是</em> </strong>)保留在<strong class="ke ir"> BoW </strong>中。但是在实际用例中，我们在前置阶段省略了这种<em class="ms">停止字</em>，正如我前面提到的。</p><h1 id="aca7" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">天哪…！那么什么是Tf-Idf呢…</h1><p id="f66b" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">对…我们已经了解了Tf-Idf背后的理论。Tf-Idf无非是<strong class="ke ir"> Tf(t，d) </strong>和<strong class="ke ir"> Idf(t，D) </strong>的乘积。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/19815d161a347bc0def0f14a50b62ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/1*JkJqvGFPBtiW1VEVJcSLew.gif"/></div></figure><p id="8f6f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以通过上面的等式为每个文档生成<strong class="ke ir"> Tf-Idf </strong>矢量表示，如下表所示。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/577ddfc82409eecb4504a0a007091299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyClj-osQtEFzdEw45xaRQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">使用Tf和Idf计算的Tf-Idf表示。</p></figure><p id="86d8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所以，实际上，<strong class="ke ir"> Tf-Idf可以通过对所选语料库</strong>的词唯一性加权的词频率来理解。在目标文档中几乎随处可见而在语料库中的其他文档中找不到的术语在目标文档中具有高度的唯一性，因此在该文档中具有高Tf-Idf权重。</p><h1 id="485e" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">有点文字相似…</h1><p id="5cb1" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">为了理解Tf-Idf作为文本表示的用法，让我们检查一下语料库中文档之间的相似性。这里，我们要考虑<a class="ae mr" href="https://en.wikipedia.org/wiki/Cosine_similarity\" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>。下表显示了每个文档对之间的余弦相似性。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bc3e9d72d435354c29aa48272ca10854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*0UO1bAkr-W9-hH_VNTzj_w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">文档间的余弦相似度。</p></figure><p id="98ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过观察余弦相似性，我们可以观察到<strong class="ke ir">文档1 </strong>和<strong class="ke ir">文档2 </strong>不再相似，因为Idf处理了在每个文档中出现术语<strong class="ke ir"><em class="ms"/></strong>的问题。但是我们可以看到<strong class="ke ir">文档1 </strong>和<strong class="ke ir">文档3 </strong>有0.012的相似度。这是因为它们有一个共同的术语<strong class="ke ir"> <em class="ms">就是</em> </strong>。尽管这些文档之间存在相似性，但Tf-Idf已设法通过<strong class="ke ir"> Idf </strong>加权来降低相似度。<strong class="ke ir">文件1 </strong>和<strong class="ke ir">文件4 </strong>的相似性表现相同。<strong class="ke ir">文档3 </strong>和<strong class="ke ir">文档4 </strong>都是关于<strong class="ke ir"> <em class="ms">天空</em> </strong>的，因此相似度稍高(0.097)。<strong class="ke ir">文件2 </strong>和<strong class="ke ir">文件4 </strong>是关于<strong class="ke ir"> <em class="ms">星星</em> </strong>和<em class="ms">夜晚</em> 的。这种相似性反映为0.1288，这是所有相似性中最高的。</p><p id="d0d1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以清楚地看到，Tf-Idf在将文本数据表示为向量方面非常有用，它有可能强调单词的重要性，并相应地对独特和信息丰富的单词进行加权。</p><p id="8377" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Tf-Idf有几种变体，可以在各个方面对其进行改进。你可以通过跟踪<a class="ae mr" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">这个</a>找到它们。</p><p id="ad1e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我希望你通过阅读这篇文章了解了<strong class="ke ir"> Tf-Idf </strong>的基本结构和用途。</p><h1 id="6cf6" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">Tf-Idf的缺点</h1><p id="ab3f" class="pw-post-body-paragraph kc kd iq ke b kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv mq kx ky kz ij bi translated">尽管Tf-Idf是如此强大的表示文本数据的技术，它仍然是一个<strong class="ke ir"> BoW </strong>模型。因此，Tf-Idf不计算文档中单词的位置。这可能会导致Tf-Idf对您的文本数据给出错误的解释。</p><p id="041f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">举个例子，</p><blockquote class="mt mu mv"><p id="849c" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">文献1 </em> </strong>:鲍勃比爱丽丝聪明</p><p id="7bf7" class="kc kd ms ke b kf kg kh ki kj kk kl km mw ko kp kq mx ks kt ku my kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="iq">文件二</em> </strong>:爱丽丝比鲍勃聪明</p></blockquote><p id="0ba2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">具有相同词频的相同单词集({ 'bob ' '，' is '，' clever ' '，' than ' '，' alice'' })，但它们给出不同的含义。但是，Tf-Idf作为BoW文本表示模型，将为两个文档提供相同的表示。在Tf-Idf模型中可以看到的另一个主要缺点是它消耗了大量的内存和处理能力。</p><p id="4bf0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">有一些文本表示技术，如神经嵌入，在生成向量表示时计算单词的位置。我们将在另一篇文章中讨论它们。</p><p id="f424" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在那之前，祝你阅读愉快...😀</p></div></div>    
</body>
</html>