<html>
<head>
<title>Adaptive Learning Rate: AdaGrad and RMSprop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">适应性学习率:AdaGrad和RMSprop</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaptive-learning-rate-adagrad-and-rmsprop-46a7d547d244?source=collection_archive---------29-----------------------#2020-10-10">https://towardsdatascience.com/adaptive-learning-rate-adagrad-and-rmsprop-46a7d547d244?source=collection_archive---------29-----------------------#2020-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4feb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我之前的文章<a class="ae kl" href="https://medium.com/swlh/gradient-descent-with-momentum-59420f626c8f" rel="noopener">中，我们看到了学习率(η)如何影响收敛。将学习率设置得太高会导致最小值附近的振荡，而设置得太低会减慢收敛速度。梯度下降中的学习速率(η)及其变化(如动量)是一个超参数，需要针对所有特征手动调整。</a></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/c4c347437eb3894d8ceae5d654fd76ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*_2yTRTRx7Gl9ykYP4kVc6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="2f70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们使用上述等式来更新神经网络中的权重时</p><ol class=""><li id="6fcc" class="kz la iq jp b jq jr ju jv jy lb kc lc kg ld kk le lf lg lh bi translated">所有功能的学习速度都是一样的</li><li id="7db8" class="kz la iq jp b jq li ju lj jy lk kc ll kg lm kk le lf lg lh bi translated">学习率在成本空间的所有地方都是一样的</li></ol><h1 id="0503" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak">恒定学习率对收敛的影响</strong></h1><p id="07c7" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">假设，我们试图预测一部电影的成功/评级。我们假设有成千上万个特征，其中一个是“is_director_nolan”。我们的输入空间中的特征“is_director_nolan”将主要为0，因为nolan导演了很少的电影，但是他的出现显著影响了电影的成功/评级。从本质上来说，这个特征是稀疏的，但是由于高信息量，我们不能忽略它。</p><p id="9926" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在神经网络的正向传递过程中，如果迭代(t)时的输入(x)为0，则使用下面的等式，输出变为bias(b)的激活(φ <strong class="jp ir"> ) </strong>。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d3eaae5526f0f3da9f34a471b31efb44.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*WYCteYP8814-4wD7CCgKIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="6e92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在反推w.r.t期间计算的局部梯度该恒定偏差将为1，并且对于该特征，权重更新将非常小(参见第一个等式)</p><p id="0569" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于像“is_director_nolan”这样的稀疏输入特征，只有当输入从0变为1或相反时，才会发生大的权重更新。而密集特征将接收更多更新。因此，对所有特征使用恒定和相同的学习速率不是一个好主意。</p><h1 id="a691" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak">解决方案</strong></h1><p id="9f32" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">从上面我们可以推断出，特征的学习率应该衰减，使得它与该特征的权重更新频率成反比。对于频繁特征，新的学习率应该低，而对于稀疏特征，新的学习率应该高。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f60c2031ac7ee5319beb0fcb2ec3db11.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*v4iUQ2Db5lgBJFicPgcNLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="c0c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">权重的更新频率的代表是在过去的更新中获得的梯度的总和。我们应该取梯度的平方，而不是梯度的绝对值，这样梯度的符号(方向)就无关紧要了。我们只关心梯度频率。这可以使用下面的等式来捕捉。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/a95201945db2131436387913ae72f9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*LELfs240kSwc4fxULYHNVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="53e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">密集要素的该代理值较高，而稀疏要素的该代理值很低。</p><p id="2a94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设𝓥 <strong class="jp ir"> (0) = 0 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/982c2e21379ea5d33c0d8fff53f0c4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*RYbvL6w0ggugDFZFXO0SXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="740b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">新的学习率变为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7c2d058713f72273e6bbf8e4e43a3fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*JhCjGBZZCUVuSS4fHsTyMA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="e8d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意</strong>:我们取𝓥 <strong class="jp ir"> (t) </strong>的平方根，以便保持我们的权重更新方程的维数</p><p id="6ccb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">新的权重更新公式变为</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a8c450774d93fb3ab0dd859b71f3f015.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*mT6G_DZXkf0vQaa77j9yVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="e8bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果𝓥 <strong class="jp ir"> (t) </strong>为零(在稀疏特征的情况下)会发生什么？为了避免分母变成零，我们应该在分母中加一个小值ε(一般是1e -07)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5222cc79b68cd03f9dbcd6d30738586f.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*vmz2cdyIwcgG25qv9PzslA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="99ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的等式是用于<strong class="jp ir"> AdaGrad </strong>(自适应梯度)的权重更新等式。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="362c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每次迭代后，<strong class="jp ir"> AdaGrad </strong>的新学习率以过去梯度的平方和的因子衰减。虽然它解决了我们更新稀疏特征的问题，但同时也引入了一个新的问题。对于密集特征，过去的梯度将是非零的，并且在一些迭代之后，学习率由于分母中所有过去的平方梯度的累积而收缩得太快。</p><p id="4bb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决这个问题，我们可以借鉴基于动量的梯度下降法。不是以相等的比例使用所有过去的梯度，我们将使用过去平方梯度的指数移动平均，以基本上将累积梯度的窗口限制为仅几个最近的梯度。</p><p id="ad26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，更新频率𝓥的新代理<strong class="jp ir"> (t) </strong>变为。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d5b2db1c2abb501b2c1693f17f719e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*X-tHEGSLXjASn60Ckki0xQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="5ea0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的等式是<strong class="jp ir"> RMSprop的权重更新等式。</strong></p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="d7d5" class="ln lo iq bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated"><strong class="ak">额外说明</strong>:</h1><p id="e087" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">在一些文本中，包括AdaGrad 的<a class="ae kl" href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="noopener ugc nofollow" target="_blank">原始研究论文</a>中，你会发现在分母中没有使用整个梯度矩阵，而是使用了累积梯度的对角矩阵。</p><p id="2554" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> AdaGrad </strong>的权重更新规则的矩阵表示。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1251ca3c6cdc4d9515e1a2c5bde4f8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*KoCObn4ykbcZGjKPTTlPsw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">作者图片</em></p></figure><p id="9c2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度矩阵<strong class="jp ir"> G(t) </strong>的平方根和逆矩阵是一个昂贵的运算，对于大的输入空间，这可能变得非常难以计算。<strong class="jp ir"> G(t) </strong>的对角矩阵是最佳近似，因为它忽略了梯度相互作用。<strong class="jp ir"> diag(G(t)) </strong>的平方根和逆可以用O(n)时间复杂度计算。因此，AdaGrad和RMSprop的大多数实现仅使用梯度矩阵<strong class="jp ir"> G(t) </strong>的对角元素。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="1665" class="ln lo iq bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated"><strong class="ak">结论</strong></h1><p id="5a4d" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">学习率是一个重要的超参数，必须针对输入空间中的每个特征进行优化调整，以实现更好的收敛。通过采用自适应学习速率方法，如<strong class="jp ir"> AdaGrad </strong>和<strong class="jp ir"> RMSprop，</strong>，我们让这些优化器通过学习底层数据的特征来调整学习速率。这些优化器给频繁出现的特征以低学习率，给不频繁出现的特征以高学习率，从而收敛得更快。</p></div></div>    
</body>
</html>