<html>
<head>
<title>Probabilistic Linear Regression with Weight Uncertainty</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">权重不确定的概率线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probabilistic-linear-regression-with-weight-uncertainty-a649de11f52b?source=collection_archive---------18-----------------------#2020-10-19">https://towardsdatascience.com/probabilistic-linear-regression-with-weight-uncertainty-a649de11f52b?source=collection_archive---------18-----------------------#2020-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ddb6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用张量流概率实现贝叶斯线性回归预测汽车的MPG</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7696b002b2e7ad1c7ee11aafca472ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5WMUls0eC8OF1sn7nUV4A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@wesleyphotography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">韦斯利·廷吉</a>在<a class="ae ky" href="https://unsplash.com/s/photos/unsymmetry-line?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fe1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归可能是你在学习数据科学和机器学习时遇到的第一种统计方法。所以，我会抓住机会猜测这不是你第一次处理线性回归。因此，在本文中，我想讨论概率线性回归，而不是典型/确定性线性回归。</p><p id="886f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是在此之前，让我们简单地讨论一下确定性线性回归的概念，让我们快速了解本文的主要观点。</p><p id="c7ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归是一种基本的统计方法，用于模拟一个或多个输入变量(或自变量)与一个或多个输出变量(或因变量)之间的线性关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/66be8081964c4cf88761ead1f10b5a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*1vNp4g0aV9BHbOyD3Gn4ew.png"/></div></figure><p id="a5a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上式中，<code class="fe lw lx ly lz b">a</code>称为截距，<code class="fe lw lx ly lz b">b</code>称为斜率。<code class="fe lw lx ly lz b">x</code>是我们的自变量，<code class="fe lw lx ly lz b">y</code>是我们的因变量，也就是我们试图预测的值。</p><p id="981f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要使用梯度下降算法来优化<code class="fe lw lx ly lz b">a </code>和<code class="fe lw lx ly lz b">b</code>的值。然后，我们得到一条回归线，显示自变量和因变量之间的最佳拟合。有了回归线，我们可以用任何给定的<code class="fe lw lx ly lz b">x</code>输入来预测<code class="fe lw lx ly lz b">y</code>的值。这些是典型或确定性线性回归算法通常是如何构建的步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/7ba922e3cba468fe2dd680b690417527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4Ry0s0BRQ2Xss71Cwx9CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确定性线性回归方法中最佳拟合线的典型图</p></figure><p id="cd48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这种确定性的线性回归算法并不能真正说明数据和模型的全部情况。这是为什么呢？</p><p id="54ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，当我们进行线性回归分析时，会出现两种类型的不确定性:</p><ul class=""><li id="992f" class="mb mc it lb b lc ld lf lg li md lm me lq mf lu mg mh mi mj bi translated">随机不确定性，即由数据产生的不确定性。</li><li id="f68d" class="mb mc it lb b lc mk lf ml li mm lm mn lq mo lu mg mh mi mj bi translated">认知不确定性，这是由回归模型产生的不确定性。</li></ul><p id="503c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们阅读这篇文章的时候，我将详细阐述这些不确定性。为了考虑这些不确定性，应使用概率线性回归代替确定性线性回归。</p><p id="37f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论概率线性回归以及它与确定性线性回归的区别。我们将首先看到如何在TensorFlow中构建确定性线性回归，然后我们将继续构建具有TensorFlow概率的概率性线性回归模型。</p><p id="1b8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们从加载我们将在本文中使用的数据集开始。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="0bb4" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">加载和预处理数据</h1><p id="5bdc" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">本文将使用的数据集是<a class="ae ky" href="https://www.kaggle.com/uciml/autompg-dataset" rel="noopener ugc nofollow" target="_blank">汽车的MPG数据集</a>。像往常一样，我们可以用熊猫加载数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/2a77cdd2eb232fc4e49e3eac3cb4da62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnF_A1a534LDiayOnbpS9w.png"/></div></div></figure><p id="4c4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是数据的统计汇总。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/de7269bbe3caea567ef75eccc4ce2755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eegwSdRYfpkJHEY4eheYNA.png"/></div></div></figure><p id="f4d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们可以用下面的代码来看看数据集中变量之间的相关性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/a85795ad9837b393efc76026f1f436e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpeuLnTmCCkRkYo50DOJnw.png"/></div></div></figure><p id="dbb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果我们看看相关性，汽车的每加仑英里数(MPG)和汽车的重量有很强的负相关性。</p><p id="dd61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将做一个简单的线性回归分析，以实现可视化。自变量将是汽车的重量，因变量将是汽车的英里数。</p><p id="5d5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们用Scikit-learn将数据拆分为训练数据和测试数据。拆分数据后，我们现在可以缩放因变量和自变量。这是为了确保这两个变量将在相同的规模，这也将提高我们的线性回归模型的收敛速度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1186" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果我们将训练数据可视化，我们会得到以下可视化结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/319f4b8b230b1b8f90ad563e97865b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hAzoY0n8lIjWp0MyLT3ig.png"/></div></div></figure><p id="7b5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">厉害！接下来，让我们继续用TensorFlow构建确定性线性回归模型。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="f879" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">张量流的确定性线性回归</h1><p id="ac76" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">用TensorFlow建立一个简单的线性回归模型是非常容易的。我们所要做的就是建立一个没有任何激活函数的单一致密层。对于成本函数，通常使用均方误差。在这个例子中，我将使用RMSprop作为优化器，模型将在100个时期内被训练。我们可以用下面几行代码来构建和训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="9fa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们训练了模型之后，让我们看看模型的损失历史来检查损失收敛。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/5f3eb992143bc592de8f57c9bcd4d2a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLvFCE__nTGxabwZSluCWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失v历元数</p></figure><p id="0ffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">损失似乎已经收敛了。现在，如果我们使用训练好的模型来预测测试集，我们可以看到下面的回归线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/7356e9015f09431631042c2337d191c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OqIRNrfuHFvptDn2NNVA1w.png"/></div></div></figure><p id="537c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。我们完了！</p><p id="c795" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我前面提到的，用TensorFlow建立一个简单的线性回归模型是非常容易的。有了回归线，我们现在可以在任何给定的汽车重量输入下近似汽车的MPG。举个例子，假设汽车在特征缩放后的重量是0.64。我们可以通过将此值传递给训练好的模型来获得汽车的MPG的相应值，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/a7880702b2a7fea6482586b5a783adb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzbZfCrliW8vgmsmMhyNtg.png"/></div></div></figure><p id="de5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你可以看到，模型预测汽车的MPG将是0.21。简单地说，对于任何给定的汽车重量，我们得到一个单一的确定性的汽车的MPG值</p><p id="b856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这个产值并不能真正说明全部情况。这里有两点需要注意。首先，我们只有有限的数据点。第二，从线性回归图可以看出，大部分数据点并没有真正位于回归线上。</p><p id="8965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们得到的输出值是0.21，但我们知道实际汽车的MPG并不精确地是0.21。可能略低于这个数字，也可能略高于这个数字。换句话说，有一种不确定性需要考虑进去。这种不确定性被称为任意不确定性。</p><p id="73a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确定性线性回归无法捕捉数据的这种任意不确定性。为了捕捉这种随机的不确定性，可以改为应用概率线性回归。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="cf6a" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated"><strong class="ak">具有张量流概率的概率线性回归</strong></h1><p id="fc1c" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">得益于TensorFlow Probability，建立概率线性回归模型也非常容易。但是，你需要先安装<code class="fe lw lx ly lz b">tensorflow_probability</code>库。您可以使用pip命令进行安装，如下所示:</p><pre class="kj kk kl km gt oc lz od oe aw of bi"><span id="b625" class="og mx it lz b gy oh oi l oj ok">pip install tensorflow_probability</span></pre><p id="c267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">安装这个库的先决条件是需要有TensorFlow版本2.3.0。因此，请确保在安装TensorFlow Probability之前升级您的TensorFlow版本。</p><h2 id="c001" class="og mx it bd my ol om dn nc on oo dp ng li op oq ni lm or os nk lq ot ou nm ov bi translated">建立随机不确定性的概率线性回归模型</h2><p id="2097" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在这一节中，我们将建立一个概率线性回归模型，将随机不确定性考虑在内。</p><p id="8357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型非常类似于确定性线性回归。然而，不是像以前一样只使用一个单一的致密层，我们需要增加一层作为最终层。最后一层将最终输出值从确定性分布转换为概率分布。</p><p id="22c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本例中，我们将创建一个最终图层，将输出值转换为正态分布的概率值。下面是它的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="8377" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，我们在张量流概率层的最后增加了一层。该层将把先前密集层的两个输出(一个用于平均值，一个用于标准偏差)转换成概率值，该概率值正态分布有可训练的平均值(<em class="ow"> loc </em>)和标准偏差(<em class="ow">标度</em>)。</p><p id="f89c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用RMSprop作为优化器，但是如果您愿意，也可以使用其他优化器。对于损失函数，我们需要使用负对数似然。</p><p id="bfed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是为什么我们使用负对数似然作为损失函数呢？</p><h2 id="34a9" class="og mx it bd my ol om dn nc on oo dp ng li op oq ni lm or os nk lq ot ou nm ov bi translated">负对数似然作为成本函数</h2><p id="e072" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">为了使分布符合某些数据，我们需要使用似然函数。利用似然函数，我们尝试根据我们在数据中看到的模式来估计未知参数<em class="ow"> θ </em>(例如，正态分布数据的平均值和标准差)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/6e6030532e9875ac65a817452a70e6cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*DakgXcLr5Ae-SyURirS6XA.png"/></div></figure><p id="a293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的概率回归模型中，优化器的工作是找到未知参数的最大似然估计。换句话说，模型被训练以从我们的数据中找到给定模式的最可能的参数值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c5d4dd59f005c9c6d175abb1f9e4fd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*SW7AU1lwJSCCS_dMbMlaIg.png"/></div></figure><p id="f184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大化似然估计与最小化负对数似然是一样的。在优化领域，目标通常是最小化成本，而不是最大化成本。这就是为什么我们使用负对数似然作为我们的成本函数。</p><p id="8d73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是负对数似然作为我们的自定义损失函数的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><h2 id="7f8d" class="og mx it bd my ol om dn nc on oo dp ng li op oq ni lm or os nk lq ot ou nm ov bi translated">任意不确定性概率线性回归模型的训练和预测结果</h2><p id="20cc" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">既然我们已经构建了模型并定义了优化器和损失函数，让我们编译和训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="a327" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以从训练好的模型中抽取样本。我们可以用下面的代码来可视化测试集和从模型中生成的样本之间的比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/3a4697a701184746ad8803c37251fa3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rDazjP1LIGUjV4EmvDc6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测试数据v从概率线性回归模型生成的样本</p></figure><p id="b9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的可视化中可以看出，对于任何给定的输入值，模型都不会返回确定性的值。相反，它将返回一个分布，并根据该分布绘制一个样本。</p><p id="b12d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果将测试集的数据点(蓝点)与训练模型预测的数据点(绿点)进行比较，您可能会认为绿点与蓝点来自相同的分布。</p><p id="2ae6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，在给定训练集中的数据的情况下，我们还可以可视化由训练模型生成的分布的均值和标准差。我们可以通过应用下面的代码来做到这一点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/fec803f4fe5d5db8e5f5750020e9b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1zcavdTYsK3LDzMJB7l4g.png"/></div></div></figure><p id="7400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，概率线性回归模型给我们的不仅仅是回归线。它还给出数据标准偏差的近似值。可以看出，测试集的大约95%的数据点位于两个标准偏差内。</p><h2 id="741e" class="og mx it bd my ol om dn nc on oo dp ng li op oq ni lm or os nk lq ot ou nm ov bi translated">建立随机和认知不确定性的概率线性回归模型</h2><p id="c7d3" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">到目前为止，我们已经建立了一个概率回归模型，它考虑了来自数据的不确定性，或者我们称之为随机不确定性。</p><p id="4bd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在现实中，我们还需要处理来自回归模型本身的不确定性。由于数据的不完善，回归参数的权重或斜率也存在不确定性。这种不确定性被称为认知不确定性。</p><p id="ea50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们建立的概率模型只考虑了一个确定性权重。正如您从可视化中看到的，该模型仅生成一条回归线，通常这并不完全准确。</p><p id="5868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一节中，我们将改进我们的概率回归模型，将任意的和认知的不确定性都考虑在内。我们可以使用贝叶斯观点来引入回归权重的不确定性。</p><p id="d995" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，在我们看到数据之前，我们需要定义我们对体重分布的先验信念。通常，我们不知道会发生什么，对吗？为了简单起见，让我们假设权重的分布是正态分布，平均值等于0，标准差等于1。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="8287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们硬编码了均值和标准差，这种先验信念是不可训练的。</p><p id="ebd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要定义回归权重的后验分布。后验分布显示了在看到数据中的模式后，我们的信念是如何改变的。因此，这个后验分布中的参数是可训练的。下面是定义后验分布的代码实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="fd14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在的问题是，上面后验函数中定义的这个<code class="fe lw lx ly lz b">VariableLayers</code>是什么？这个可变层背后的想法是，我们试图近似真实的后验分布。通常，不可能得到真实的后验分布，因此我们需要近似它。</p><p id="e948" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在定义了先验和后验函数之后，现在我们可以建立权重不确定的概率线性回归模型。下面是它的代码实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="efcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能注意到了，这个模型和之前的概率回归模型唯一的区别只是第一层。我们使用<code class="fe lw lx ly lz b">DenseVariational</code>层，而不是普通的密集层。在这一层，我们将先验和后验函数作为自变量传递。第二层和前面的模型一模一样。</p><h2 id="cb7f" class="og mx it bd my ol om dn nc on oo dp ng li op oq ni lm or os nk lq ot ou nm ov bi translated">随机和认知不确定性的概率线性回归模型的训练和预测结果</h2><p id="e3d2" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在是我们编译和训练模型的时候了。</p><p id="691f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化器和成本函数仍然与之前的模型相同。我们使用RMSprop作为优化器，使用负对数似然作为成本函数。我们来编译训练或者建模。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="73df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候让我们可视化回归模型的权重或斜率不确定性了。下面是可视化结果的代码实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/f20f2326d2c2095086f94e40da5322b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQOfoVDFcWQH0dJblKoPTA.png"/></div></div></figure><p id="efb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的可视化中，您可以看到线性线(平均值)以及由训练模型的后验分布生成的标准偏差在每次迭代中都是不同的。所有这些线都是拟合测试集中数据点的合理解决方案。然而，由于认知的不确定性，我们不知道哪条线是最好的。</p><p id="7552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，我们拥有的数据点越多，我们将看到的回归线的不确定性就越小。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="bab6" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">最后的想法</h1><p id="1985" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">就是这样！现在，您已经看到了概率线性回归与确定性线性回归的不同之处。使用概率线性回归，可以考虑由数据(任意的)和回归模型(认知的)产生的两种不确定性。</p><p id="de26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要建立一个深度学习模型，其中不准确的预测会导致非常严重的负面后果，例如在自动驾驶和医疗诊断领域，考虑这些不确定性是非常重要的。</p><p id="4c35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，当我们有更多的数据点时，模型的认知不确定性会降低。</p></div></div>    
</body>
</html>