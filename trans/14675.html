<html>
<head>
<title>GANs: Leveraging Technology for a Better Tomorrow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANs:利用技术创造更美好的明天</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gans-leveraging-technology-for-a-better-tomorrow-ea192087b4e4?source=collection_archive---------50-----------------------#2020-10-09">https://towardsdatascience.com/gans-leveraging-technology-for-a-better-tomorrow-ea192087b4e4?source=collection_archive---------50-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ae52" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">下面的文章讨论了GAN的基础知识以及它的一些变种。</h2></div><p id="acaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当有人只是把自己的脸实时换成一些已知人物的脸，是不是很神奇？或者，当你看到一幅时尚的肖像画仅仅通过提供一个小的布局就用电脑制作出来时，你会变得非常好奇和兴奋吗？这就是我们在这篇文章中要揭示的。所有这些很酷的想法大多是在机器学习中的一个现代想法的帮助下实现的，即<strong class="kh ir"> GANs </strong>。</p><blockquote class="lb lc ld"><p id="695b" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">注意:上述想法也可以通过有效使用计算机图形包来实现，这超出了本文的范围，但是我在参考资料部分添加了一些链接。</p></blockquote><p id="6a3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GANs或<strong class="kh ir">生成对抗网络</strong>是一类能够生成与真实图像相似的图像的网络。如果你熟悉<strong class="kh ir">风格转移</strong>的概念，那么GANs的想法对你来说并不陌生。然而，你可以从参考部分的链接中得到一个很好的风格转换的复习。GANs有各种各样的应用，像图像-图像翻译，文本-图像翻译，实时视频模拟，生成卡通人物等等。我认为GANs也可以证明自己有用的一个想法是对数据进行上采样。(仔细想想！).</p><p id="d236" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，我参加了一个由IEEE主办的技术写作比赛，该比赛旨在写任何技术的未来的潜在和有益的用例，因此我写下了关于GANs的内容。你可以在参考资料部分找到这篇文章的PDF文档！</p><h1 id="5e41" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">目录:</h1><ol class=""><li id="3ef5" class="ma mb iq kh b ki mc kl md ko me ks mf kw mg la mh mi mj mk bi translated"><a class="ae ml" href="#bbe4" rel="noopener ugc nofollow">定义</a></li><li id="dae5" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la mh mi mj mk bi translated"><a class="ae ml" href="#fa82" rel="noopener ugc nofollow">GANs的当前使用案例</a></li><li id="4600" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la mh mi mj mk bi translated"><a class="ae ml" href="#5566" rel="noopener ugc nofollow">甘的种类</a></li><li id="238e" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la mh mi mj mk bi translated"><a class="ae ml" href="#dd10" rel="noopener ugc nofollow">结论</a></li><li id="2d88" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la mh mi mj mk bi translated"><a class="ae ml" href="#8710" rel="noopener ugc nofollow">参考文献</a></li></ol><h2 id="bbe4" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">1.定义:</h2><p id="a454" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">基本上如上所述，GANs或生成对抗网络是我们生成与真实世界图像相似的图像的网络。但是等等！到底是谁生成了这些图像？图像生成怎么可能呢？模型是如何训练的？模型如何学习映射来生成图像？我们不用卷积网络吗？你会在文章中找到所有这些问题的答案，所以请仔细阅读。</p><p id="0b6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特定的GAN包含两个不同的网络。其中一个被称为<strong class="kh ir">发生器网络</strong>，另一个被称为<strong class="kh ir">鉴别器网络</strong>。生成器模型(作为艺术家)试图生成与真实图像相似的图像，鉴别器模型(作为艺术评论家)区分真实图像和生成的图像。</p><blockquote class="lb lc ld"><p id="5049" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">注意，鉴别器网络也被称为<strong class="kh ir">对抗网络</strong>，因此得名生成对抗网络。</p></blockquote><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/6f346c3220ad49dd0a522990178f52bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5-pgiDKD9d2iEDMF.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/generative/dcgan</a></p></figure><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nw"><img src="../Images/ae10b2acc52e011ead2316b2b56e95f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_iGDkXoaz3xrMg8W.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/generative/dcgan</a></p></figure><p id="586e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GAN网络的理想条件是鉴别器网络无法区分真实图像和发生器网络产生的图像。</p><p id="e8ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在非常基础的水平上，发生器和鉴别器网络的工作可以概括为</p><p id="a904" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成器模型不会直接看到真实图像，并且只对观察到的图像(或DCGAN情况下的矢量)起作用。在生成图像之后，该图像被发送到鉴别器网络，该网络将其分类为真或假。鉴别器网络也能看到真实的图像，因此能从中获取一些信息。对于生成图像的输入，我们从鉴别器网络获得的输出包含一些关于真实图像的信息。然后，我们根据该输出计算发电机网络的损耗。通过这种方式，生成器网络获得关于真实图像的信息，然后在下一次尝试中，它试图生成与真实图像更相似的图像。通过不断的练习，生成器模型变得能够生成与鉴别器不能分类的真实图像非常相似的图像。</p><h2 id="fa82" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">2.GANs的当前使用案例:</h2><p id="a1b8" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">对于技术社区来说，gan并不是什么新鲜事物，几十年来，我们的技术社区已经发展到如此程度，以至于研究的速度比以前更快了。因此，我们已经在许多领域实施了GANs，并且效果很好。让我们揭开其中的一些领域:</p><h2 id="b77e" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">a)深度假:</h2><p id="6a0f" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">DeepFake是GANs的一个惊人的实现，其中图像或视频中的一个人被一个长相相似的人取代。不仅是图像在这里被替换，而且说话者的声音有时也会改变。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ea5588cdcb3d5c249f1d913a56f86fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/0*6bPRUZSnLIirbOsB.gif"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://en.wikipedia.org/wiki/Deepfake" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Deepfake</a></p></figure><p id="6d6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这种技术引入了各种负面用法，如<a class="ae ml" href="https://en.wikipedia.org/wiki/Celebrity_sex_tape" rel="noopener ugc nofollow" target="_blank">名人色情视频</a>、<a class="ae ml" href="https://en.wikipedia.org/wiki/Revenge_porn" rel="noopener ugc nofollow" target="_blank">复仇色情</a>、<a class="ae ml" href="https://en.wikipedia.org/wiki/Fake_news" rel="noopener ugc nofollow" target="_blank">假新闻</a>、<a class="ae ml" href="https://en.wikipedia.org/wiki/Hoax" rel="noopener ugc nofollow" target="_blank">恶作剧</a>和<a class="ae ml" href="https://en.wikipedia.org/wiki/Accounting_scandals" rel="noopener ugc nofollow" target="_blank">金融诈骗</a>。虽然DeepFake可以用于许多错误的任务，但它也有各种积极的实现，如电影配音，教育学生等。</p><h2 id="4af8" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">b)生成肖像:</h2><p id="5bf4" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">GANs的一个非常有益的实现是从零开始生成美丽而有创意的肖像。我们可以以成对(其中存在b/w观测图像和真实图像的直接映射)和不成对(其中不存在b/w观测图像和真实图像的直接映射)的形式向这些模型提供训练样本。</p><h2 id="df3f" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">c) ExGANs:</h2><p id="5f0a" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">ExGANs或范例生成对抗网络是脸书正在实施的一种方法。ExGANs是一种有条件的GANs，它利用样本信息来产生高质量、个性化的内部绘制结果。这种方法特别使用ExGANs将闭着眼睛的人的图像转换成睁着眼睛的相同图像。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8693412c6a63814845cce399eb9a8f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*2aLKXIa8PCb7BmxDZtiItw.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://research.fb.com/wp-content/uploads/2018/06/Eye-In-Painting-with-Exemplar-Generative-Adversarial-Networks.pdf" rel="noopener ugc nofollow" target="_blank">https://research . FB . com/WP-content/uploads/2018/06/Eye-In-Painting-with-Exemplar-Generative-Adversarial-networks . pdf</a></p></figure><blockquote class="lb lc ld"><p id="01da" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">还有其他领域也大量使用了GANs。请参阅参考资料部分了解更多信息。</p></blockquote><h1 id="5566" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">3.gan的类型:</h1><p id="5f36" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">虽然GAN有多种变体，但今天我们将讨论三种常见类型的GAN，即DCGAN、Pix2Pix GAN和CyclicGAN。这些GANs的实现由tensor flow<strong class="kh ir">发布为官方教程，所以不要忘记查看。好了，让我们开始吧:</strong></p><h2 id="0854" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">a) DCGAN:</h2><p id="0558" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated"><strong class="kh ir"> DCGANs </strong>或<strong class="kh ir">深度卷积生成对抗网络</strong>是学习从<strong class="kh ir">噪声向量</strong>到图像的映射的GANs的一些基本形式。现在你可能会想，我们如何把一个矢量转换成图像？这个问题的答案很简单。它使用<strong class="kh ir">转置卷积层</strong>对输入执行<em class="le">反卷积</em>。这与<strong class="kh ir">卷积层</strong>所做的正好相反。卷积层从输入图像中提取特征，使得通道的数量增加，但是在转置卷积层中，新的特征被添加到输入向量中，从而将它们转换成图像。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nz"><img src="../Images/b2e8b46eeab268957ca76a6bef0acec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*h3XULKlwyjMbjnAy6eaiig.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:作者</p></figure><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oa"><img src="../Images/e0a5205e250b469a669c88b3d35bb207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QBFGLJ9nHgUcOD24oG_CQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:作者</p></figure><p id="a324" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成器网络建立在一系列[转置卷积+ BatchNorm + LeakyReLU层]之上。鉴别器网络是一个正常的分类器，它使用卷积层来分类输入图像是真实的还是生成的。</p><p id="41dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">鉴别器损耗</strong>是真实图像输出和生成图像输出的综合损耗。使用的损失函数是<strong class="kh ir"> BinaryCrossEntropy </strong>(但选择不受限制)。<strong class="kh ir">发生器损耗</strong>是根据我们从鉴频器网络获得的输出计算的，用于将生成的图像作为输入。</p><pre class="nh ni nj nk gt ob oc od oe aw of bi"><span id="ac4f" class="mr lj iq oc b gy og oh l oi oj">def discriminator_loss(real_op, gen_op):<br/>    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)<br/>    real_loss = loss(tf.ones_like(real_op), real_op)<br/>    gen_loss = loss(tf.zeros_like(gen_op), gen_op)<br/>    return real_loss + gen_loss</span><span id="2e23" class="mr lj iq oc b gy ok oh l oi oj">def generator_loss(gen_op):<br/>   loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)<br/>   return loss(tf.ones_like(gen_op), gen_op)</span></pre><h2 id="2a23" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">b)pix 2 pix gan:</h2><p id="573e" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">这些类别的任务属于<strong class="kh ir">条件任务</strong>的范畴。条件甘是那些基于输入图像中的条件生成图像的甘类。例如，考虑下面的图像-</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/8b2195d575fdf7d4121c23c931a831f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*I2KPuwQjK-Guf1V_xFMydg.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.07004.pdf</a></p></figure><p id="61b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的图像中，我们必须使用左边的图像生成右边的图像。在这里，我们可以看到输入和输出之间存在直接关系，并且该模型非常容易学会基于这样的对来生成图像。这种数据也被称为<strong class="kh ir">成对数据</strong>。</p><p id="bf87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，一个特定的Pix2Pix GAN也有一个生成器和一个鉴别器模型。但是在这里，它们和在DCGANs中使用的不一样。发生器网络利用<strong class="kh ir">U-Net</strong>T26】架构，鉴别器网络利用<strong class="kh ir"> PatchGAN </strong>架构。</p><p id="ea26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">U-Net架构基本上是一个普通的<strong class="kh ir">编码器-解码器网络</strong>,增强了层间的<strong class="kh ir">跳跃连接</strong>。添加跳过连接的论点可能是模型将同等地学习每一层的编码和解码特征。Pix2Pix网络中使用的U-Net架构(特别是在关于<em class="le">城市景观</em>数据集的tensorflow教程中)可以被可视化为-</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi om"><img src="../Images/a7120accf8e3a025c735b2b26899ce92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Al3PEhG-GqN4oAw6zFNW2A.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:作者</p></figure><p id="a6b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在来描述鉴别器网络！鉴别器网络是一个PatchGAN网络。但是首先什么是PatchGAN网络呢？PatchGAN网络属于机器学习分类器类，它学习对输入数据中的高频结构进行分类。他们在<em class="le">面片</em>的尺度上惩罚结构。这些模型使用从图像中提取的小块用于分类任务。鉴别器使用一个N×N的小块来鉴别一幅图像是真是假。通常图像的一个<em class="le"> 70x70 </em>部分被用于分类，因为它产生更好的结果。这一点从<a class="ae ml" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">原论文</a>中的解释可见一斑。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi on"><img src="../Images/310ace6a8deda4413540b1d1f904f774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNUGAKjLuU8mLfpbi5jIDw.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.07004.pdf</a></p></figure><p id="dbeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴别器损耗通过计算真实图像和生成图像的输出的交叉熵之和来获得。生成器除了计算生成图像的损失外，还计算目标图像和生成图像之间距离的<strong class="kh ir"> L1范数</strong>。</p><pre class="nh ni nj nk gt ob oc od oe aw of bi"><span id="13e5" class="mr lj iq oc b gy og oh l oi oj">def discriminator_loss(disc_real_op, disc_gen_op):<br/>    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)<br/>    real_loss = loss(tf.ones_like(disc_real_op), disc_real_op)<br/>    gen_loss = loss(tf.zeros_like(disc_gen_op), disc_gen_op)<br/>    return real_loss + gen_loss</span><span id="ba16" class="mr lj iq oc b gy ok oh l oi oj">def generator_loss(disc_gen_op, generated_op, target, lambda=10):<br/>    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)<br/>    gan_loss = loss(tf.ones_like(disc_gen_op), disc_gen_op)<br/>    l1_loss = tf.reduce_mean(tf.abs(target - generated_op))<br/>    gen_loss = gan_loss + (lambda*l1_loss)<br/>    return gen_loss</span></pre><h2 id="a0a9" class="mr lj iq bd lk ms mt dn lo mu mv dp ls ko mw mx lu ks my mz lw kw na nb ly nc bi translated">c)循环甘斯:</h2><p id="2910" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">环状氮化镓是应用最广泛的一类氮化镓。在深入循环GANs之前，让我们先谈谈<strong class="kh ir">未配对数据</strong>。考虑下面的图像-</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f73577e2f0bf10d4f9f41729fcb951a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*02K0cjhOrMcQqEkxZY1uxw.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:<a class="ae ml" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.10593.pdf</a></p></figure><p id="5b08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">左侧的一对图像在它们之间具有一定程度的对应性，正如我们已经讨论过的，它们被称为<strong class="kh ir">配对数据</strong>。右侧的图像在它们之间没有任何对应关系，被称为<strong class="kh ir">不成对数据</strong>。在许多情况下，我们只能得到不成对的数据，因为创建成对数据的成本更高，而且不容易获得。</p><p id="6e3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你可能会问-</p><blockquote class="op"><p id="2896" class="oq or iq bd os ot ou ov ow ox oy la dk translated">只针对不成对数据的训练，你在实现一个全新的架构？我们不能用以前的方法学习不成对的数据吗？</p></blockquote><p id="be48" class="pw-post-body-paragraph kf kg iq kh b ki oz jr kk kl pa ju kn ko pb kq kr ks pc ku kv kw pd ky kz la ij bi translated">答案是肯定的，您可以使用以前的方法，但这在性能方面不会有太大的好处。因此，我们需要一种新的方法来处理这种数据。不仅是未配对的数据，我们还可以使用<strong class="kh ir"> CyclicGANs </strong>从配对的数据中学习，并产生非常有希望的结果。</p><p id="6473" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们来考虑一下使用周期因子的一些原因</p><blockquote class="lb lc ld"><p id="659a" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">我们考虑两组图像，即X和Y，作为我们要在其间转换图像的两个域。</p></blockquote><ol class=""><li id="cdcc" class="ma mb iq kh b ki kj kl km ko pe ks pf kw pg la mh mi mj mk bi translated">假设我们学会了从集合x中的图像“a”生成集合Y中的图像“b”。现在我们对图像“b”运行生成器模型。你能保证我们能找回a的形象吗？不，如果使用上述GANs的变体，我们不能保证这一点。</li><li id="d4e9" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la mh mi mj mk bi translated">我们能自信地说集合X中的所有图像都将被映射到集合Y(双射)中的所有图像吗？</li></ol><p id="7b59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个问题都已经被CyclicGANs解决了。现在让我们了解一下CyclicGANs的架构。</p><p id="d734" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个特定的循环GAN由两个发生器网络和两个鉴别器网络组成。</p><p id="251c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两个发电机网络即<strong class="kh ir"> G </strong>和<strong class="kh ir"> F </strong>互为逆。<strong class="kh ir"> G </strong>从集合<strong class="kh ir"> X </strong>到<strong class="kh ir"> Y </strong>，<strong class="kh ir"> G:X →Y </strong>，以及<strong class="kh ir"> F </strong>从集合<strong class="kh ir"> Y </strong>到<strong class="kh ir"> X </strong>，<strong class="kh ir"> F:Y →X </strong>中的图像学习映射。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1c8da89496577957bbb9b7609a837c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*l4Fz_DeTd1feKqwax6cR-g.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:作者</p></figure><p id="9646" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了将两个发生器网络的输出分类为真或假，我们有两个鉴别器网络，即<strong class="kh ir"> Dx </strong>和<strong class="kh ir"> Dy </strong>。<strong class="kh ir"> Dx </strong>基本将<strong class="kh ir"> F </strong>生成的图像分为真假，<strong class="kh ir"> Dy </strong>将<strong class="kh ir"> G </strong>生成的图像分为真假。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d4493d555e5e9a2e0f6f64e03216ec46.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*ekl55jnPmylnQNTxM2t9ug.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:作者</p></figure><p id="3232" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴别器损耗(对抗性损耗)和发电机损耗实现为-</p><pre class="nh ni nj nk gt ob oc od oe aw of bi"><span id="ac44" class="mr lj iq oc b gy og oh l oi oj">def discriminator_loss(real_op, gen_op):<br/>    loss = tf.keras.layers.BinaryCrossentropy(from_logits=True)<br/>    real_loss = loss(tf.ones_like(real_op), real_op)<br/>    gen_loss = loss(tf.zeros_like(gen_op), gen_op)<br/>    return real_loss + gen_loss</span><span id="2182" class="mr lj iq oc b gy ok oh l oi oj">def generator_loss(gen_op):<br/>    loss = tf.keras.layers.BinaryCrossentropy(from_logits=True)<br/>    return loss(tf.ones_like(gen_op), gen_op)</span></pre><blockquote class="lb lc ld"><p id="a7a1" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">请注意，我们通常将鉴频器损耗乘以1/2，这是因为与发生器网络相比，它的学习速度较慢。</p></blockquote><p id="8e98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了这些损失之外，还有一个<strong class="kh ir">循环一致性损失</strong>，它完成了循环一致性的目标函数。循环一致性丢失解决了我们前面遇到的反向映射问题。这种损失确保了从集合<strong class="kh ir"> X </strong>映射到集合<strong class="kh ir"> Y </strong>的图像具有到其自身的反向映射。让我们听听这篇论文中关于为什么要引入循环一致性损失的一些话</p><blockquote class="lb lc ld"><p id="eb61" class="kf kg le kh b ki kj jr kk kl km ju kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">有了足够大的容量，网络可以将同一组输入图像映射到目标域中图像的任意随机排列，其中任何学习到的映射都可以导致与目标分布匹配的输出分布。因此，单独的对抗性损失不能保证学习的函数能够将单个输入<em class="iq">【Xi】映射到期望的输出<em class="iq">易</em>。</em></p></blockquote><p id="4bd3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">敌对网络不能保证期望的输出，这导致我们引入循环一致性损失。考虑下面的图片-</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3bb6cde1e219a9856ec9e7e064cde351.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*MqbPu2Y9yBuZ9J0YcjjK6A.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图片来源:【https://arxiv.org/pdf/1703.10593.pdf T4】</p></figure><p id="16a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑图像的左侧，我们可以看到它使用<strong class="kh ir"> G </strong>从图像<strong class="kh ir"> x </strong>生成图像<strong class="kh ir"> y_hat </strong>，我们再次传递这个生成的图像以使用<strong class="kh ir"> F </strong>获得图像<strong class="kh ir"> x_hat </strong>。恰恰相反的是在图像的右侧。<br/>现在一致性损失基本上衡量的是<strong class="kh ir"> x </strong>和<strong class="kh ir"> x_hat </strong>(左侧)之间的相似性，我们也称之为<strong class="kh ir">前向循环一致性损失</strong>，根据<strong class="kh ir"> y </strong>和<strong class="kh ir"> y_hat </strong>(右侧)之间的相似性计算的损失也称为<strong class="kh ir">后向循环一致性损失</strong>。这个损失基本上是真实({x，y})和生成({x_hat，y_hat})图像之间的距离的<strong class="kh ir"> L1(或曼哈顿)范数</strong>。</p><p id="fa2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">循环一致性丢失可以实现为</p><pre class="nh ni nj nk gt ob oc od oe aw of bi"><span id="e7e1" class="mr lj iq oc b gy og oh l oi oj">def cyclic_consistency_loss(generator, cycled_image, real_image, lambda=10):<br/>    loss = tf.reduce_mean(tf.abs(cycled_image - real_image))<br/>    return loss*lambda</span></pre><p id="cc1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，[对抗性损失+λ*(前向循环一致性损失+后向循环一致性损失)+发电机损失]一起构成了循环性问题的完整目标函数。这里的<strong class="kh ir">λ</strong>是一个要调整的超参数。</p><blockquote class="op"><p id="20c8" class="oq or iq bd os ot ou ov ow ox oy la dk translated">注意，在上面讨论的任何方法中，用于模型建立的其他超参数的选择完全是场景特定的。但在我看来，原始文件中指定的值通常会有更好的结果。</p></blockquote><h1 id="dd10" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw pk jx lu jz pl ka lw kc pm kd ly lz bi translated">4.结论:</h1><p id="c15f" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">虽然GANs在架构和工作方面都非常棒，但是还有更多值得探索的地方。我们不需要认为甘斯会接受工作或其他什么。甘人在他们的地方很酷，但仍然依赖人类完成各种任务。</p><h1 id="8710" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">5.参考资料:</h1><ul class=""><li id="40a5" class="ma mb iq kh b ki mc kl md ko me ks mf kw mg la pn mi mj mk bi translated"><a class="ae ml" href="https://arxiv.org/abs/1705.04058" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.04058</a></li><li id="f24c" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated"><a class="ae ml" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.10593.pdf</a></li><li id="dc6a" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated"><a class="ae ml" href="https://drive.google.com/file/d/1KayaSPOZrkOwruYvfygopODy3zdrd74x/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/1 kayaspozrkowruyvfygopody 3 ZD rd 74 x/view？usp =共享</a></li><li id="ac68" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated">https://www.tensorflow.org/tutorials/generative/dcgan<a class="ae ml" href="https://www.tensorflow.org/tutorials/generative/dcgan" rel="noopener ugc nofollow" target="_blank"/></li><li id="cd24" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated">【https://www.tensorflow.org/tutorials/generative/pix2pix T4】</li><li id="a594" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated"><a class="ae ml" href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/generative/cyclegan</a></li><li id="f617" class="ma mb iq kh b ki mm kl mn ko mo ks mp kw mq la pn mi mj mk bi translated"><a class="ae ml" href="https://research.fb.com/wp-content/uploads/2018/06/Eye-In-Painting-with-Exemplar-Generative-Adversarial-Networks.pdf" rel="noopener ugc nofollow" target="_blank">https://research . FB . com/WP-content/uploads/2018/06/Eye-In-Painting-with-Exemplar-Generative-Adversarial-networks . pdf</a></li></ul><p id="8b24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望我很成功地向你表达了我的观点。我已经尽力解释了原文中的观点。然而，如果你仍然对任何部分有疑问，或者觉得我没有正确地描述一件事，请随意在评论中写下。<br/>~快乐编码~</p></div></div>    
</body>
</html>