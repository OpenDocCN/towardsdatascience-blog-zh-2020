<html>
<head>
<title>Machine Learning Algorithms from Start to Finish in Python: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头到尾的机器学习算法:线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-linear-regression-aa8c1d6b1169?source=collection_archive---------21-----------------------#2020-10-16">https://towardsdatascience.com/machine-learning-algorithms-from-start-to-finish-in-python-linear-regression-aa8c1d6b1169?source=collection_archive---------21-----------------------#2020-10-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d496" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习、理解和实现所有数据科学和机器学习中最重要和最基本的算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0fd494396d5c072a1448e59a4f8be4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rL4gkYXKMD08CnzwBNtAmg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">朱利安·埃伯特在<a class="ae ky" href="https://unsplash.com/s/photos/diagonal?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="af89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归可能是最常见的算法之一，是机器学习从业者必须知道的。这通常是初学者第一次接触真正的机器学习算法，了解它在更深层次上的运行方式对于更好地理解它至关重要。</p><p id="a89a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，简单来说，我们把真题分解一下；什么<em class="lv">真的</em>是<strong class="lb iu">线性</strong> <strong class="lb iu">回归</strong>？</p><h1 id="6905" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">线性回归定义</h1><p id="856e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">线性回归是一种监督学习算法，旨在采用线性方法对因变量和自变量之间的关系进行建模。换句话说，它旨在拟合一条线性<em class="lv">趋势线</em>，该趋势线能够最好地捕捉数据的<em class="lv">关系</em>，并且，根据这条线，它可以预测目标值可能是多少。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/3bae4d17db5db2dfb44d962b27da88e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dnFljsilN6-wpo0orHg3Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="07d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了，我知道这个定义，但是它到底是如何工作的呢？很棒的问题！为了回答这个问题，让我们一步一步地看看线性回归是如何运作的:</p><ol class=""><li id="bb86" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">趋势线符合数据(如上图所示)。</li><li id="2c9e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">计算点之间的距离(图上的红点是点，绿线是距离)，然后平方，然后求和(值被平方以确保负值不会产生不正确的值并妨碍计算)。这是算法的错误，或者更好地称为<em class="lv">残差</em></li><li id="6ce0" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">存储迭代的残差</li><li id="37c5" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">基于<em class="lv">优化算法，</em><em class="lv"/>线<em class="lv"> </em>被轻微“移位”，以便该线可以更好地拟合数据。</li><li id="f10b" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">重复步骤2-4，直到达到期望的结果，或者残余误差已经减小到零。</li></ol><p id="610b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种拟合直线的方法被称为<em class="lv">最小二乘法。</em></p><h1 id="0eb7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">线性回归背后的数学</h1><blockquote class="ni nj nk"><p id="f720" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:请随意跳过这一部分，因为不幸的是，它确实含有一些看起来很奇怪的乳胶。但是，如果你想真正明白是怎么回事，我建议冒险来一块饼干，继续看下去！</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/9fc9f039ea315f241c62e0a3d7575242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeFM73ybaKRt8ttHU2tW8w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安托万·道特里在<a class="ae ky" href="https://unsplash.com/s/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5aaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归算法如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/93ecd45391bd230c0073a0c7e5ad596f.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*vcbxhGYo0W15A9EEENtgvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="578b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以简称为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3f5abcd695d873ba19ac396bc9f72e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*vZ8knICumMjU-2RNsv64nw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="6fac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的算法将基本上完成以下工作:</p><ol class=""><li id="402e" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">取一个Y向量(你的数据的标签，(房价，股票价格，等等..))</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/cc67abd0e0384fb1910f9d2f7cdb69a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*WuG96nwwy1ZB-VfTtuTwSA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="63ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是你的目标向量，稍后<strong class="lb iu">会用它来评估</strong>你的数据(稍后会详细介绍)。</p><p id="c697" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.取一个矩阵X(数据的特征):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3f887810fde2aa8f8bcdf2d19d1236fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*XtTbO6vLiKGrTlAGCOwriw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="1a65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是你的数据的特征，即年龄，性别，性别，身高等。这是算法将实际用来进行预测的数据。请注意特征x0是如何出现的。这被称为你的<em class="lv">截距项</em>，并且总是等于1。</p><p id="005d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.取<em class="lv">权重</em>的向量，并转置它们:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/aff63f3193c206b6eca59db4841fb826.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/format:webp/1*ioOr0B1y7bK3zAoxXmGJBQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ce488c4c3734af4c27aea4c2498882b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*INU2h03GPKVBnusNvg4N8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="d71c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是算法的神奇之处。所有的特征向量都会乘以这些权重。这被称为<strong class="lb iu">点积。</strong>本质上，您将试图为给定的数据集找到这些值的最佳组合。这就是所谓的<strong class="lb iu">优化。</strong></p><p id="d64c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.得到一个输出向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/79c3bb0434cfa50d1aa8da6f2df72525.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*Sm18cSvSJ2iHn9tbQpiDaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="e630" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是从数据中输出的预测向量。然后，您可以通过使用<strong class="lb iu">成本函数来评估模型的性能。</strong></p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="69f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这基本上是数学上表示的整个算法。现在，您应该对线性回归的工作原理有了很好的理解。但问题是，什么是一个<strong class="lb iu"> <em class="lv">优化</em> </strong> <em class="lv"> </em> <strong class="lb iu"> <em class="lv">算法</em> </strong> <em class="lv">？而我们如何挑选</em> <strong class="lb iu"> <em class="lv">最优权重</em> </strong> <em class="lv">？而我们如何评价</em><strong class="lb iu"><em class="lv"/></strong><em class="lv">的表现呢？</em></p><h1 id="ea6f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">成本函数</h1><p id="c8be" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">成本函数本质上是一个衡量损失的公式，或模型的“成本”。如果你曾经参加过任何Kaggle比赛，你可能会遇到一些。一些常见的包括:</p><ul class=""><li id="274d" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ob na nb nc bi translated">均方误差</li><li id="7bf0" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ob na nb nc bi translated">均方根误差</li><li id="1447" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ob na nb nc bi translated">绝对平均误差</li></ul><p id="bf7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些函数对于模型训练和开发是必不可少的，因为它们回答了“<em class="lv">我的模型预测新实例的能力如何？</em>”。请记住这一点，因为这与我们的下一个主题有关。</p><h1 id="c543" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">优化算法</h1><p id="00c5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">优化通常被定义为改进某样东西的过程，以使其发挥最大潜力。这也适用于机器学习。在ML的世界中，优化本质上是试图为某个数据集找到最佳的参数组合。这本质上是机器学习的“学习”部分。</p><p id="3bbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然存在许多优化算法，但我将讨论其中最常见的两种:梯度下降和正规方程。</p><h1 id="671d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">梯度下降</h1><p id="abee" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">梯度下降是一种优化算法，旨在找到一个函数的最小值。它通过在斜率的负方向迭代地采取步骤来实现这个目标。在我们的例子中，梯度下降通过移动函数切线的斜率来不断更新权重。好极了，听起来很棒。请说英语。:)</p><h2 id="02e9" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated">梯度下降的一个具体例子</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/72435ab32d26958cb9428453927dbd19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpCekxwVWmmmrRl3M-B8vw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卢卡斯·克拉拉在<a class="ae ky" href="https://unsplash.com/s/photos/mountain?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a5d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地说明梯度下降，让我们通过一个简单的例子。想象一个人在山顶，他/她想到达山下。他们可以做的是环顾四周，看看他们应该朝哪个<em class="lv">方向</em>迈步，以便更快地下来。然后，他们可能会朝那个方向迈出第<em class="lv">步</em>，现在他们离目标更近了。然而，他们下来时必须小心，因为他们可能会在某个点卡住，所以我们必须确保相应地选择步长。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="1207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，梯度下降的目标是最小化一个函数。在我们的案例中，是为了最小化我们模型的成本。这是通过找到函数的切线并向那个方向移动来实现的。算法的“<em class="lv">步骤</em>的大小由所谓的<em class="lv">学习率来定义。这基本上控制了我们向下移动的距离。有了这个参数，我们必须小心两种情况:</em></p><ol class=""><li id="6eac" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">学习率太大，算法可能不收敛(达到最小值)并在最小值附近跳动，但永远不会收敛</li><li id="7101" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">学习率太小，算法将花费太长时间达到最小值，还可能“卡”在次优点。</li></ol><p id="1eba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还有一个参数来控制算法在数据集上迭代的次数。</p><p id="754d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从视觉上看，该算法会做这样的事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/81c5b4105263e035edff9ccc55c6ee7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*8igWexUP3iudvcpsoEcJxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="bb62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为这种算法对机器学习来说非常重要，所以让我们来回顾一下它的作用:</p><ol class=""><li id="e794" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">随机初始化权重。这叫做(你猜对了)<em class="lv">随机初始化</em></li><li id="8ff6" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">然后，该模型使用这些随机权重进行预测。</li><li id="7db2" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">模型的预测通过成本函数进行评估。</li><li id="f32e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">然后，模型运行梯度下降，通过找到函数的切线，然后在切线的斜率中采取一个步骤</li><li id="0594" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">该过程重复N次迭代，或者如果满足标准。</li></ol><h1 id="e9f9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">梯度下降的优点和缺点</h1><h2 id="e7ff" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated"><strong class="ak">优势:</strong></h2><ol class=""><li id="42cf" class="mu mv it lb b lc mo lf mp li oq lm or lq os lu mz na nb nc bi translated">很可能将成本函数降低到全局最小值(非常接近或= 0)</li><li id="651c" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">最有效的优化算法之一</li></ol><h2 id="1855" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated">缺点:</h2><ol class=""><li id="f723" class="mu mv it lb b lc mo lf mp li oq lm or lq os lu mz na nb nc bi translated">在大型数据集上可能会很慢，因为它使用整个数据集来计算函数切线的梯度</li><li id="4de9" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">容易陷入次优点(或局部最小值)</li><li id="c946" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">用户必须手动选择学习速率和迭代次数，这可能很耗时</li></ol><p id="14c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然已经介绍了梯度下降，那就来介绍法线方程吧。</p><h1 id="7b2a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">正态方程</h1><p id="035c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如果我们回到我们的例子，而不是采取步骤反复下山，我们将能够立即到达底部。法线方程就是这种情况。它利用线性代数来产生权重，可以在很短的时间内产生与梯度下降一样好的结果。</p><h1 id="0deb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">法线方程的优点和缺点</h1><h2 id="8ddc" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated">优势</h2><ol class=""><li id="9c72" class="mu mv it lb b lc mo lf mp li oq lm or lq os lu mz na nb nc bi translated">不需要选择学习率或迭代次数</li><li id="34ef" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">极快</li></ol><h2 id="79b9" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated">不足之处</h2><ol class=""><li id="5cfe" class="mu mv it lb b lc mo lf mp li oq lm or lq os lu mz na nb nc bi translated">无法很好地扩展到大型数据集</li><li id="5cc8" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">倾向于产生良好的权重，但不是最佳的权重</li></ol></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="621f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">咻，好多东西要消化！我建议你在继续前进之前休息一下，喝杯咖啡</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/d5eeb6c52376a8d7a817fe1387604436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdEC6hnSOZ88R1cjgiOEog.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@wildlittlethingsphoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">海伦娜·洛佩斯</a>在<a class="ae ky" href="https://unsplash.com/s/photos/break?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="9774" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">特征缩放</h1><p id="b3e3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这是许多机器学习算法的重要预处理步骤，尤其是那些使用距离度量和计算的算法(如线性回归和梯度下降)。它基本上缩放了我们的特征，使它们在相似的范围内。把它想象成一栋房子，一栋房子的比例模型。两者的<strong class="lb iu">形状</strong>相同(都是房子)，但是<strong class="lb iu">大小</strong>不同(5m！= 500m)。我们这样做的原因如下:</p><ol class=""><li id="5161" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">它加速了算法</li><li id="8361" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">有些算法对规模很敏感。换句话说，如果特征具有不同的比例，则具有较高量值的特征有可能被赋予较高的权重。这将影响机器学习算法的性能，显然，我们不希望我们的算法偏向一个特征。</li></ol><p id="88e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了证明这一点，让我们假设我们有三个特性，分别命名为A、B和C:</p><ul class=""><li id="20ae" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ob na nb nc bi translated">缩放前AB的距离= &gt;</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/afae18d6e127d0a8394c33136461a72a.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*0-nFSskvxPHfNSbl"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0db6266c5d2013c3e6fd0295a3daf2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*bXXcDD1SSBaQmxZu.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="c892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放前BC的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/6994e87d9329fc3247895fc10f906ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*c3udqtdy0xAfVQ9D"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5773e4efb7c72cfeb224db9cd7c07e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/0*g8BbJ-QqdUUCdhfT.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="9e34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放后AB的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3b22ba92b91b54afddf176cd1afa70e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*SOJGfVWF0i5YfD5A"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/2b67fb394933a735cf73581c10a10707.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/0*FeQFSv5PoOqNHTjA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="3d03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放后BC的距离= &gt;</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/39b1480383690cc1d643c3dea452c892.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*UAovrTFcufV8oHwC"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e06e20e8c485c6842c93c5b5aa7e36d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/0*tTKcTRGnSPFIkRq0.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">分析公司Vidhya </a>拍摄</p></figure><p id="71d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以清楚地看到，这些特性比缩放之前更具可比性，也更公正。如果你想要一个关于功能扩展的很棒的教程，请查看这篇由Analytics Vidhya撰写的<a class="ae ky" href="http://photo%20by%20analytics%20vidhya/" rel="noopener ugc nofollow" target="_blank">博客文章</a>。</p><p id="70c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">咻！这需要吸收大量的信息。所以，我建议你休息一下，喝杯咖啡，享受生活，当你觉得，准备好了，你就可以从头开始编写这个算法了。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="2d75" class="lw lx it bd ly lz oz mb mc md pa mf mg jz pb ka mi kc pc kd mk kf pd kg mm mn bi translated">从头开始编码线性回归</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/5c63acf00277c662fb96b2ff7fafea0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X0-98EiQNkwBJj2vnTTqQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯·里德在<a class="ae ky" href="https://unsplash.com/s/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="563b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在是你一直在等待的时刻；实施！事不宜迟，我们开始吧！</p><blockquote class="ni nj nk"><p id="bb6d" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>:所有代码都可以从<a class="ae ky" href="https://github.com/Vagif12/ML-Algorithms-From-Scratch/blob/main/Linear%20Regression%20from%20Scratch.py" rel="noopener ugc nofollow" target="_blank">这个</a> Github repo下载。但是，我建议您在这样做之前先跟随教程，因为这样您会对您实际编码的内容有更好的理解！</p></blockquote><p id="2cab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们做一些基本的导入:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="10b6" class="oc lx it pg b gy pk pl l pm pn">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_boston</span></pre><p id="eeab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，那是所有的进口货！我们使用numpy进行数学实现，使用matplotlib绘制图形，使用来自scikit-learn的波士顿数据集。</p><p id="4eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们加载数据并定义我们的特征和标签:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="41d5" class="oc lx it pg b gy pk pl l pm pn"># Load and split data<br/>data = load_boston()<br/>X,y = data['data'],data['target']</span></pre><p id="5f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们创建一个自定义的train_test_split函数，将我们的数据分成一个训练集和测试集:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="48ef" class="oc lx it pg b gy pk pl l pm pn"># Custom train test split<br/>def train_test_divide(X,y,test_size=0.3,random_state=42):<br/>    np.random.seed(random_state)<br/>    train_size = 1 - test_size<br/>    arr_rand = np.random.rand(X.shape[0])<br/>    split = arr_rand &lt; np.percentile(arr_rand,(100*train_size))<br/>    <br/>    X_train = X[split]<br/>    y_train = y[split]<br/>    X_test =  X[~split]<br/>    y_test = y[~split]<br/>    <br/>    return X_train, X_test, y_train, y_test</span><span id="fe42" class="oc lx it pg b gy po pl l pm pn">X_train,X_test,y_train,y_test = train_test_divide(X,y,test_size=0.3,random_state=42)</span></pre><p id="e04c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，我们只是</p><ol class=""><li id="17d3" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">进入测试尺寸。</li><li id="efce" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">设置随机种子以确保我们的结果是可重复的。</li><li id="cc15" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">基于测试集大小获得训练集大小</li><li id="387d" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">从我们的特征中随机选取样本</li><li id="a6ad" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">将随机选择的实例分成训练集和测试集</li></ol><h2 id="4e01" class="oc lx it bd ly od oe dn mc of og dp mg li oh oi mi lm oj ok mk lq ol om mm on bi translated">我们的成本函数</h2><p id="83d6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将实现MSE或均方误差，这是一种用于回归任务的常见成本函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/a38984158b96b993148fbc9826ab0155.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*8vcm2SFj4rCFdTZvcoDqMg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="aa79" class="oc lx it pg b gy pk pl l pm pn">def mse(preds,y):<br/>        m = len(y)<br/>        return 1/(m) * np.sum(np.square((y - preds)))</span></pre><ul class=""><li id="4cb4" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ob na nb nc bi translated">我指的是训练样本的数量</li><li id="a3bc" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ob na nb nc bi translated"><em class="lv">易</em>引用我们标签向量中的单个实例</li><li id="3257" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ob na nb nc bi translated">preds指的是我们的预测</li></ul><p id="a668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了制作干净、可重复和高效的代码，以及遵守软件开发惯例，我们将制作创建一个线性回归类:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="2333" class="oc lx it pg b gy pk pl l pm pn">class LinReg:<br/>    def __init__(self,X,y):<br/>        self.X = X<br/>        self.y = y<br/>        self.m = len(y)<br/>        self.bgd = False</span></pre><ul class=""><li id="d2c5" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ob na nb nc bi translated">bgd布尔值是一个参数，它定义了我们是否应该使用批量梯度下降。</li></ul><p id="0a50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将创建一个添加截取项的方法:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="1c4c" class="oc lx it pg b gy pk pl l pm pn">def add_intercept_term(self,X):<br/>        X = np.insert(X,1,np.ones(X.shape[0:1]),axis=1).copy()<br/>        return X</span></pre><ul class=""><li id="9ab7" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu ob na nb nc bi translated">这基本上是在我们的特性的开头插入一个列。</li><li id="b990" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu ob na nb nc bi translated">如果我们不加上这一点，那么我们将迫使超平面通过原点，导致它倾斜相当大，因此不能正确拟合数据。</li></ul><p id="495a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将扩展我们的功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/bc7a582062c972b4e6cf60fca02b27b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*n8Bso9Er84uUZO8D1T4kyA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="e21f" class="oc lx it pg b gy pk pl l pm pn">def feature_scale(self,X):<br/>        X = (X - X.mean()) / (X.std())<br/>        return X</span></pre><p id="ee55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将随机初始化权重:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="5489" class="oc lx it pg b gy pk pl l pm pn">def initialise_thetas(self):<br/>        np.random.seed(42)<br/>        self.thetas = np.random.rand(self.X.shape[1])</span></pre><p id="80d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将使用以下公式从头开始编写法线方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/85b542781c46848803a2b773311a7d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*joir9WdYWB0ZoWtkfkChmg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="1e65" class="oc lx it pg b gy pk pl l pm pn">def normal_equation(self):<br/>        A = np.linalg.inv(np.dot(self.X.T,self.X))<br/>        B = np.dot(self.X.T,self.y)<br/>        thetas = np.dot(A,B)<br/>        return thetas</span></pre><p id="ec2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，我们将算法分为3个部分:</p><ol class=""><li id="2b50" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">我们得到X转置和X的点积的倒数</li><li id="fa90" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们得到重量和标签的点积</li><li id="e896" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们得到两个计算值的点积</li></ol><p id="6e64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，这就是正规方程！不算太差！现在，我们将使用以下公式实现批量梯度下降:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/060a79a4893370f3823f73717ecf77f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*huva75AyPlmgEeKJrDcfDA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="f2eb" class="oc lx it pg b gy pk pl l pm pn">def batch_gradient_descent(self,alpha,n_iterations):<br/>        self.cost_history = [0] * (n_iterations)<br/>        self.n_iterations = n_iterations<br/>        <br/>        for i in range(n_iterations):<br/>            h = np.dot(self.X,self.thetas.T)<br/>            gradient = alpha * (1/self.m) * ((h - self.y)).dot(self.X)<br/>            <br/>            self.thetas = self.thetas - gradient<br/>            self.cost_history[i] = mse(np.dot(self.X,self.thetas.T),self.y)<br/>            <br/>        return self.thetas</span></pre><p id="19ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们执行以下操作:</p><ol class=""><li id="3dfd" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">我们接受alpha，或者学习率，和迭代次数</li><li id="f160" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们创建一个列表来存储我们的成本函数历史，以便稍后绘制成线图</li><li id="b50e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们遍历数据集n次迭代，</li><li id="7144" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们获得预测，并计算梯度(函数切线的斜率)。这被称为h(x)</li><li id="91f0" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们通过从实际值中减去我们的预测值并乘以每个特征来更新权重，以沿梯度向下移动</li><li id="2aa6" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">我们使用自定义的MSE函数记录这些值</li><li id="7648" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">重复，完成后，返回我们的结果</li></ol><p id="e9ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个拟合函数来拟合我们的数据:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="0d73" class="oc lx it pg b gy pk pl l pm pn">def fit(self,bgd=False,alpha=0.158,n_iterations=4000):<br/>        self.X = self.add_intercept_term(self.X)<br/>        self.X = self.feature_scale(self.X)<br/>        if bgd == False:<br/>            <br/>            self.thetas = self.normal_equation()<br/>        else:<br/>            self.bgd = True<br/>            self.initialise_thetas()<br/>            self.thetas = self.batch_gradient_descent(alpha,n_iterations)</span></pre><p id="aec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们只是检查用户是否想要梯度下降，并相应地执行我们的步骤。</p><p id="bbe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们构建一个函数来绘制成本函数:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="24b0" class="oc lx it pg b gy pk pl l pm pn">def plot_cost_function(self):<br/>        <br/>        if self.bgd == True:<br/>            plt.plot(range((self.n_iterations)),self.cost_history)<br/>            plt.xlabel('No. of iterations')<br/>            plt.ylabel('Cost Function')<br/>            plt.title('Gradient Descent Cost Function Line Plot')<br/>            plt.show()<br/>        else:<br/>            print('Batch Gradient Descent was not used!')</span></pre><p id="0b51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后是预测未标记实例的方法:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="7ffa" class="oc lx it pg b gy pk pl l pm pn">def predict(self,X_test):<br/>        self.X_test = X_test.copy()<br/>        self.X_test = self.add_intercept_term(self.X_test)<br/>        self.X_test = self.feature_scale(self.X_test)<br/>        predictions = np.dot(self.X_test,self.thetas.T)<br/>        return predictions</span></pre><p id="98e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们看看哪个优化产生了更好的结果。首先，让我们试试梯度下降:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="facb" class="oc lx it pg b gy pk pl l pm pn">lin_reg_bgd = LinReg(X_train,y_train)<br/>lin_reg_bgd.fit(bgd=True)</span><span id="75f8" class="oc lx it pg b gy po pl l pm pn">mse(y_test,lin_reg_bgd.predict(X_test))</span><span id="44a9" class="oc lx it pg b gy po pl l pm pn">OUT:<br/>28.824024414708344</span></pre><p id="0cd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们绘制我们的函数，看看成本函数如何降低，如果梯度下降收敛:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/8a070f71f67c6476b3e9ef5ee310856f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*1UU9jzhi3nQbYHpxfLxqxg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="0bc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以看到，在大约1000次迭代时，它开始收敛。</p><p id="0d7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是正常方程:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="547f" class="oc lx it pg b gy pk pl l pm pn">lin_reg_normal = LinReg(X_train,y_train)<br/>lin_reg_normal.fit()</span><span id="c0ad" class="oc lx it pg b gy po pl l pm pn">mse(y_test,lin_reg_normal.predict(X_test))</span><span id="09e6" class="oc lx it pg b gy po pl l pm pn">OUT:<br/>22.151417764247284</span></pre><p id="5438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们可以看到，正常方程略优于梯度下降。这可能是因为数据集很小，我们没有为学习率选择最佳参数。</p><h1 id="12a7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">活动</h1><ol class=""><li id="8fdc" class="mu mv it lb b lc mo lf mp li oq lm or lq os lu mz na nb nc bi translated">尽量大幅度提高学习率。会发生什么？</li><li id="1227" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">不应用要素缩放。有区别吗？</li><li id="e07e" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">尝试研究，看看你是否能实现一个更好的优化算法。在测试集上评估您的模型</li></ol></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="03d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">写这篇文章真的很有趣，虽然有点长，但我希望你今天学到了一些东西。敬请期待，继续阅读！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/2fd74c62612b8c3b45cacbbbf350bd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BFSgPd16SgS8DRULo9SiMw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔纳森·肯珀在<a class="ae ky" href="https://unsplash.com/s/photos/bye?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>