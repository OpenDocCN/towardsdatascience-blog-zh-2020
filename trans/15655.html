<html>
<head>
<title>How to Visually Explain any CNN based Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何直观地解释任何基于CNN的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-visually-explain-any-cnn-based-models-80e0975ce57?source=collection_archive---------12-----------------------#2020-10-28">https://towardsdatascience.com/how-to-visually-explain-any-cnn-based-models-80e0975ce57?source=collection_archive---------12-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a834" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解并实现引导Grad CAM，为任何基于CNN的模型直观地解释类别区分可视化</h2></div><blockquote class="ki kj kk"><p id="4fde" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="ko iu"> CNN深度学习模型——它为什么解释它所解释的东西？</strong></p></blockquote><p id="092d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">深度学习模型现在能够给出非常高的准确度。在图像分类、对象检测、语义分割、图像字幕或视觉问答中大规模采用计算机视觉算法的最关键部分是理解CNN模型为什么解释它们所解释的内容。</p><blockquote class="ll"><p id="1242" class="lm ln it bd lo lp lq lr ls lt lu lh dk translated">CNN模型的可解释性或可解释性是建立信任和采用信任的关键</p></blockquote><p id="5489" class="pw-post-body-paragraph kl km it ko b kp lv ju kr ks lw jx ku li lx kx ky lj ly lb lc lk lz lf lg lh im bi translated">只有当我们理解了为什么模型不能识别一个类或一个对象时，我们才能集中精力解决模型的失败。更好的可解释或可解释的深度学习模型将帮助人类建立信任，并导致更高的采用率。</p><blockquote class="ll"><p id="ce20" class="lm ln it bd lo lp lq lr ls lt lu lh dk translated">一个好的可解释或可理解的模型应该突出图像中的细粒度细节，以直观地解释为什么模型预测了一个类。</p></blockquote><p id="d5ec" class="pw-post-body-paragraph kl km it ko b kp lv ju kr ks lw jx ku li lx kx ky lj ly lb lc lk lz lf lg lh im bi translated">有几种方法可以解释CNN模型，比如</p><ul class=""><li id="3c66" class="ma mb it ko b kp kq ks kt li mc lj md lk me lh mf mg mh mi bi translated"><strong class="ko iu">导向反向传播</strong> <strong class="ko iu">可视化图像中的精细细节</strong>。它的前提是:<strong class="ko iu">神经元就像特定图像特征的检测器</strong>，所以<strong class="ko iu">当反向传播时，梯度、负梯度被设置为零</strong>以突出图像中重要的像素。</li><li id="90ca" class="ma mb it ko b kp mj ks mk li ml lj mm lk mn lh mf mg mh mi bi translated"><strong class="ko iu">类别激活图(CAM)是类别鉴别的</strong>，它定位图像的类别或类别。CAM要求要素地图直接位于预测图层之前。<strong class="ko iu"> CAM因此适用于在预测层</strong>之前对卷积映射执行全局平均池的CNN架构，因此不适用于其他计算机视觉算法。</li><li id="cca7" class="ma mb it ko b kp mj ks mk li ml lj mm lk mn lh mf mg mh mi bi translated"><strong class="ko iu"> Grad CAM可视化</strong>具有<strong class="ko iu">类区分性</strong>并定位相关图像区域<strong class="ko iu">，但不会像导向反向传播一样突出精细像素的重要性</strong>；然而，与CAM不同的是，<strong class="ko iu"> Grad CAM适用于任何CNN架构。</strong></li></ul><p id="fcb2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">引导式Grad CAM结合了Grad CAM和引导式反向传播的优点，Grad CAM具有类别区分能力，可定位相关图像区域，而引导式反向传播可将负梯度设置为零的图像的梯度可视化，从而在通过ReLU层反向传播时突出显示图像中的导入像素。</strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/2b38116bc27fde2221c302ea24ee3bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlQ2z1z9g1vwg5CfNdrk_w.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">由作者生成的代码:比较不同的技术来解释CNN</p></figure><blockquote class="ll"><p id="f3ea" class="lm ln it bd lo lp ne nf ng nh ni lh dk translated">导向梯度加权类激活图(导向梯度凸轮)具有类区分性，并突出显示图像的细粒度重要区域，用于任何CNN架构的高分辨率预测。</p></blockquote><h1 id="6b3a" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">制导Grad CAM高级工作</h1><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ob"><img src="../Images/2eaf151795672d099b5a4fcf3e898b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0cBlupNrp5GBHTchQ1NoFQ.png"/></div></div></figure><p id="f47f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">图像通过CNN网络前向传播，生成特征图，然后应用特定于任务的网络来获得类别的分数。</strong></p><p id="ae92" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">不同类型的任务可以是图像分类、图像字幕、物体检测、视觉问答</strong>等。</p><p id="01fc" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">所需类别的梯度设置为1，</strong>所有其他类别的梯度设置为零。这样做是为了获得类别区分定位图。</p><p id="d6e2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">关于激活的梯度被反向传播，直到最后的卷积层；这抓住了重要的特征。<strong class="ko iu">前向激活图和反向传播梯度的加权组合后跟随一个ReLU，以获得梯度凸轮定位</strong>(蓝色热图)，该定位解释了模型为做出特定决策而查看的特征。</p><p id="f28b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">应用引导反向传播来计算梯度，然后将负梯度和反向传播归零。</strong></p><p id="8aea" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">最后一步，<strong class="ko iu">将Grad CAM热图与导向反向传播进行逐点相乘，以获得导向Grad-CAM可视化</strong>。<strong class="ko iu">这些可视化现在具有类别区分性，并以高分辨率定位相关图像区域。</strong></p><h1 id="6b42" class="nj nk it bd nl nm nn no np nq nr ns nt jz oc ka nv kc od kd nx kf oe kg nz oa bi translated">梯度凸轮、导向反向传播和导向梯度凸轮在TensorFlow中的实现</h1><p id="3f3c" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated">在下面的代码中，我们将通过VGG19模型的Conv层向前传播图像。</p><p id="13ad" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">非负梯度被反向传播到感兴趣的校正卷积特征图。然后，将这些梯度组合起来计算粗略的Grad-CAM定位，表示图像中的特征，模型在图像中寻找这些特征以做出特定的决定。</p><p id="ea0f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">最后，我们应用梯度CAM热图与引导反向传播的逐点乘法来获得引导梯度CAM可视化，其既是高分辨率的又是概念特定的或类别判别式的。</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="7bac" class="op nk it ol b gy oq or l os ot"># Import the required libraraies<br/><strong class="ol iu">import cv2<br/>import tensorflow as tf<br/>import tensorflow.keras.backend as K<br/>import numpy as np<br/>from skimage.transform import resize<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</strong></span><span id="ace3" class="op nk it ol b gy ou or l os ot">#Setting the image apth and the last conv layer for VGG19<br/><strong class="ol iu">IMAGE_PATH = 'cat_and_dog1.jpg'<br/>LAYER_NAME='block5_conv4'</strong></span><span id="58e3" class="op nk it ol b gy ou or l os ot">#Load the image<br/><strong class="ol iu">img = tf.keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(224, 224))</strong></span><span id="88aa" class="op nk it ol b gy ou or l os ot"># Displaying the original image<br/><strong class="ol iu">plt.axis("off")<br/>plt.imshow(img)<br/>plt.show()<br/></strong></span><span id="d3f8" class="op nk it ol b gy ou or l os ot"># Preprocess the image using vgg19 preprocess function<br/><strong class="ol iu">img =  tf.keras.preprocessing.image.img_to_array(img)<br/>x = np.expand_dims(img, axis=0)<br/>preprocessed_input = tf.keras.applications.vgg19.preprocess_input(x)</strong></span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e03aa2b8f27c6406c684ca07edf02696.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*kczl5DJPC_N65PaYUxrrzg.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">原象</p></figure><p id="06b6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们在Imagenet数据集上使用VGG19迁移学习模型。</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="1f04" class="op nk it ol b gy oq or l os ot">#Create the transfer learned model<br/><strong class="ol iu">model = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=True)</strong></span></pre><p id="69d3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们从VGG19传输学习模型直到最后一个卷积层创建一个模型。Conv层保留了当我们在深度学习CNN模型中应用全连接层时丢失的空间信息。</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="94fb" class="op nk it ol b gy oq or l os ot">'''create a model till  last convolutional layers to have the best compromise between high-level semantics and detailed spatial<br/>information'''<br/><strong class="ol iu">gb_model = tf.keras.models.Model(<br/>    inputs = [model.inputs],    <br/>    outputs = [model.get_layer(LAYER_NAME).output]<br/>)</strong><br/><strong class="ol iu">layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]</strong></span></pre><p id="a43d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">通过应用<a class="ae ow" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> @tf </strong> </a> <strong class="ko iu">覆盖ReLU的渐变。custom_gradient </strong>允许对梯度进行细粒度控制，以便反向传播非负梯度，从而获得更有效或数值稳定的梯度。</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="fe74" class="op nk it ol b gy oq or l os ot"><a class="ae ow" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank"><strong class="ol iu">@tf</strong></a><strong class="ol iu">.custom_gradient<br/>def guidedRelu(x):<br/>  def grad(dy):<br/>    return tf.cast(dy&gt;0,"float32") * tf.cast(x&gt;0, "float32") * dy<br/>  return tf.nn.relu(x), grad</strong></span></pre><p id="b087" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">将引导式ReLU应用于所有重新激活的Conv图层</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="7037" class="op nk it ol b gy oq or l os ot"><strong class="ol iu">for layer in layer_dict:<br/>  if layer.activation == tf.keras.activations.relu:<br/>    layer.activation = guidedRelu</strong></span></pre><p id="c1be" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">使用梯度带记录前向传递的预处理输入图像，这将有助于计算后向传递的梯度。使用GradientTape对象捕捉最后一个Conv图层上的渐变。</p><p id="11e2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这里，我们找到了目标类分数相对于最后一个卷积层的特征图的梯度</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="30b9" class="op nk it ol b gy oq or l os ot"><strong class="ol iu">with tf.GradientTape() as tape:<br/> inputs = tf.cast(preprocessed_input, tf.float32)<br/> tape.watch(inputs)<br/> outputs = gb_model(inputs)[0]</strong></span><span id="4f64" class="op nk it ol b gy ou or l os ot"><strong class="ol iu">grads = tape.gradient(outputs,inputs)[0]</strong></span></pre><h2 id="40ee" class="op nk it bd nl ox oy dn np oz pa dp nt li pb pc nv lj pd pe nx lk pf pg nz ph bi translated">Grad分类激活图(Grad CAM)</h2><p id="0825" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated">对梯度进行空间平均，其中每个条目是特定要素地图通道上梯度的平均强度。根据梯度重要性建立一个有权重的过滤器图。结果是最终的类别区分显著图。</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="8f71" class="op nk it ol b gy oq or l os ot"><strong class="ol iu">weights = tf.reduce_mean(grads, axis=(0, 1))<br/>grad_cam = np.ones(outputs.shape[0: 2], dtype = np.float32)<br/>for i, w in enumerate(weights):<br/>    grad_cam += w * outputs[:, :, i]</strong></span></pre><p id="ab19" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">用最大强度值<br/>除热图的每个强度值，以标准化热图，使所有值落在0和1之间，并在图像上绘制渐变类别激活图</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="af18" class="op nk it ol b gy oq or l os ot"><strong class="ol iu">grad_cam_img = cv2.resize(grad_cam.numpy(), (224, 224))<br/>grad_cam_img = np.maximum(grad_cam_img, 0)<br/>heatmap = (grad_cam_img - grad_cam_img.min()) / (grad_cam_img.max() - grad_cam_img.min())</strong></span><span id="5ee6" class="op nk it ol b gy ou or l os ot"><strong class="ol iu">grad_cam_img = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)</strong></span><span id="3c75" class="op nk it ol b gy ou or l os ot"><strong class="ol iu">output_image = cv2.addWeighted(cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2BGR), 0.5, grad_cam_img, 1, 0)</strong></span><span id="5e2b" class="op nk it ol b gy ou or l os ot"><strong class="ol iu"><br/>plt.imshow(output_image)<br/>plt.axis("off");<br/>plt.show()</strong></span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/87c9ab28e5906a39f3f348f9e8ceb204.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*MrSYpBNzzpNR4I_x2Ff7Qg.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">Grad CAM</p></figure><h2 id="e637" class="op nk it bd nl ox oy dn np oz pa dp nt li pb pc nv lj pd pe nx lk pf pg nz ph bi translated">导向反向传播</h2><p id="e6f8" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated">使用<strong class="ko iu">引导反向传播来捕捉神经元检测到的像素，而不是抑制神经元的像素。导向反向传播计算目标输出相对于输入</strong>的梯度，其中<strong class="ko iu">负梯度在通过ReLU层</strong>反向传播时被抑制</p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="2c40" class="op nk it ol b gy oq or l os ot">#Visualizing the guided back prop<br/><strong class="ol iu">guided_back_prop =grads</strong></span><span id="fa78" class="op nk it ol b gy ou or l os ot"><strong class="ol iu">gb_viz = np.dstack((<br/>            guided_back_prop[:, :, 0],<br/>            guided_back_prop[:, :, 1],<br/>            guided_back_prop[:, :, 2],<br/>        ))       <br/>gb_viz -= np.min(gb_viz)<br/>gb_viz /= gb_viz.max()<br/>    <br/>imgplot = plt.imshow(gb_viz)<br/>plt.axis("off")<br/>plt.show()</strong></span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8c46ea74baa634d66d79c6405dd370f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*60X8n-KZlZe2upB389c3nw.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">导向反向传播</p></figure><h2 id="ee69" class="op nk it bd nl ox oy dn np oz pa dp nt li pb pc nv lj pd pe nx lk pf pg nz ph bi translated">导向梯度类激活图(导向梯度凸轮)</h2><p id="3e69" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated"><strong class="ko iu">导向梯度CAM通过逐元素乘法结合了梯度CAM和导向反向传播的最佳方面。制导Grad CAM的可视化既具有高分辨率，又具有类别区分能力</strong></p><pre class="mp mq mr ms gt ok ol om on aw oo bi"><span id="88b6" class="op nk it ol b gy oq or l os ot"><strong class="ol iu">guided_cam = np.maximum(grad_cam, 0)<br/>guided_cam = guided_cam / np.max(guided_cam) # scale 0 to 1.0<br/>guided_cam = resize(guided_cam, (224,224), preserve_range=True)</strong></span><span id="f466" class="op nk it ol b gy ou or l os ot">#pointwise multiplcation of guided backprop and grad CAM <strong class="ol iu"><br/>gd_gb = np.dstack((<br/>        guided_back_prop[:, :, 0] * guided_cam,<br/>        guided_back_prop[:, :, 1] * guided_cam,<br/>        guided_back_prop[:, :, 2] * guided_cam,<br/>    ))</strong></span><span id="e3fb" class="op nk it ol b gy ou or l os ot"><strong class="ol iu">imgplot = plt.imshow(gd_gb)<br/>plt.axis("off")<br/>plt.show()</strong></span></pre><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/ff76f7406cf3cf9a0e10e0b891c23e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*FNPZ_CpGpcM3LQ1dTEMFVQ.png"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">制导Grad CAM</p></figure><p id="735f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">可视化突出显示了耳朵、鼻子、狗的身体和它的爪子的像素，模型看起来要对狗和猫的特征进行预测，但没有突出显示。</p><h2 id="fbbb" class="op nk it bd nl ox oy dn np oz pa dp nt li pb pc nv lj pd pe nx lk pf pg nz ph bi translated">结论:</h2><p id="9536" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated">制导Grad CAM具有类别识别能力，在突出显示精细像素细节的同时定位相关图像区域。使用引导Grad CAM的可视化有助于解释CNN模型在进行预测时看什么，从而解释为什么模型解释它所解释的内容，从而明确建立对深度学习模型的信任。</p><h1 id="26e4" class="nj nk it bd nl nm nn no np nq nr ns nt jz oc ka nv kc od kd nx kf oe kg nz oa bi translated">参考资料:</h1><p id="d8fd" class="pw-post-body-paragraph kl km it ko b kp of ju kr ks og jx ku li oh kx ky lj oi lb lc lk oj lf lg lh im bi translated"><a class="ae ow" href="https://arxiv.org/pdf/1610.02391.pdf" rel="noopener ugc nofollow" target="_blank"> Grad-CAM:通过基于梯度的定位来自深度网络的视觉解释</a></p><p id="1ff6" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><a class="ae ow" href="https://www.sicara.ai/blog/2019-08-28-interpretability-deep-learning-tensorflow" rel="noopener ugc nofollow" target="_blank">https://www . si cara . ai/blog/2019-08-28-可解释性-深度学习-tensorflow </a></p></div></div>    
</body>
</html>