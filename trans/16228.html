<html>
<head>
<title>Face Mask Detection using YOLOv5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用YOLOv5进行人脸面具检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/face-mask-detection-using-yolov5-3734ca0d60d8?source=collection_archive---------4-----------------------#2020-11-09">https://towardsdatascience.com/face-mask-detection-using-yolov5-3734ca0d60d8?source=collection_archive---------4-----------------------#2020-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f3d9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">实践教程</h2><div class=""/><div class=""><h2 id="1622" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">新冠肺炎:指南建立一个面具检测器使用YOLOv5和运行推理视频流</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3ecba2f84150da0808330843746ed797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*V_D4JtNnlzmaCVk0ueA6sQ.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">YOLOv5推论(视频由作者提供)</p></figure><p id="a794" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本帖中，我们将逐步指导如何训练一个 <a class="ae md" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">约洛夫5 </strong> </a> <strong class="lj jd">模型来检测视频流中的人是否戴着面具。</strong></p><p id="0554" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将从物体检测模型背后的一些基本概念开始，并激励使用<strong class="lj jd"> YOLOv5 </strong>来解决这个问题。</p><p id="df52" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从那里，我们将回顾我们将用于训练模型的数据集，并查看如何对其进行调整，使其符合<a class="ae md" href="https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885" rel="noopener ugc nofollow" target="_blank"> darknet格式</a>。</p><p id="4feb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我将向您展示如何使用下载的数据集训练YOLOv5模型，并对图像或视频文件进行推理。</p><p id="2241" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">在这里</strong>  <strong class="lj jd">找到本节使用的代码</strong> <a class="ae md" href="https://github.com/AlexanderNixon/Machine-learning-reads/blob/master/Face-Mask-Detection-YOLOv5.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">的笔记本。</strong></a></p><p id="a61e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个项目是和弗兰·佩雷斯一起完成的，他也是这个社区的贡献者。我建议你读一些他的精彩文章！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="1fb3" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">背景</h2><p id="ab1a" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">“YOLO”，指的是“<em class="ni">你只看一次</em>”，是Joseph Redmon用2016年的出版物“<a class="ae md" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的、实时的物体检测</a>”推出的一系列<strong class="lj jd">物体检测</strong>模型。</p><p id="95d9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从那时起，几个更新的版本已经发布，其中，前三个是由约瑟夫·雷德蒙发布的。6月29日，Glenn Jocher发布了最新版本<strong class="lj jd"> YOLOv5 </strong>，<a class="ae md" href="https://blog.roboflow.com/yolov5-is-here/" rel="noopener ugc nofollow" target="_blank">声称相对于其前作有重大改进</a>。</p><p id="a185" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最有趣的改进，是它的“<em class="ni">惊人地快速推断”。</em>正如Roboflow <em class="ni">、</em>在特斯拉P100中运行的<a class="ae md" href="https://blog.roboflow.com/yolov5-is-here/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中所发布的，YOLOv5实现了每幅图像高达0.007秒的推理时间，<strong class="lj jd">意味着140 FPS </strong>！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/594a2ec41fb1dbae6c5ed2053a4eaa9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0p9WkZ48Yf9QYpvg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">YOLO车型对比(<a class="ae md" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">图片来源</a>)</p></figure><h2 id="3c91" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">为什么是YOLO？</h2><p id="b041" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">使用一个<a class="ae md" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">对象检测</strong> </a>模型，比如YOLOv5，很可能是解决这个问题的最简单和最合理的方法。这是因为我们将计算机视觉流水线限制为一个单一步骤<strong class="lj jd">、</strong>，因为物体检测器被训练来检测:</p><ul class=""><li id="bcb7" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated"><strong class="lj jd">包围盒</strong>和一个</li><li id="b80f" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">对应标签</strong></li></ul><p id="f2c5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这正是我们试图解决这个问题的目的。在我们的例子中，边界框将是检测到的人脸，相应的标签将指示这个人是否戴着面具。</p><p id="e17e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">或者，如果我们想建立自己的深度学习模型，它会更复杂，因为它必须是2 <strong class="lj jd"> -fold </strong>:我们需要一个模型来检测图像中的人脸，另一个模型来检测找到的边界框中是否存在人脸面具。</p><p id="a0f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了复杂性之外，这样做的缺点是推断时间会慢得多，尤其是在有许多人脸的图像中。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="28f2" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">数据呢？</h2><p id="0c88" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们现在知道可以用哪个模型来解决这个问题。下一个自然的，也可能是最重要的方面是… <em class="ni">数据呢？？</em></p><p id="c967" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">幸运的是，在<strong class="lj jd"> Kaggle </strong>中有一个公开可用的数据集，名为<a class="ae md" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank">人脸面具检测</a>，这将使我们的生活变得更加轻松。</p><p id="bc78" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据集包含853幅图像及其对应的注释文件，指示一个人是<em class="ni">正确佩戴</em>、<em class="ni">错误佩戴</em>还是<em class="ni">没有佩戴</em>。</p><p id="9200" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下是数据集中的一个示例:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/7ed989f6560c474ed440675a806d9572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMxrK1X5ZzBrcFSnR6Ig5A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自<a class="ae md" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank">面罩数据集</a>的样本图像(图片由作者提供)</p></figure><p id="2949" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这种情况下，我们将简化上面的内容来检测一个人是否戴着面具(我们将在Roboflow部分看到如何戴面具)。</p><h1 id="f681" class="nz mm it bd mn oa ob oc mq od oe of mt ki og kj mw kl oh km mz ko oi kp nc oj bi translated">关于自定义数据的培训</h1><p id="d378" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们现在知道了开始工作所需的一切，所以是时候动手了！</p><h2 id="c97d" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">项目布局</h2><p id="692e" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们需要做的第一件事是从<a class="ae md" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> ultralytics/yolov5 </a>中克隆存储库，并安装所有需要的依赖项:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="66ba" class="ml mm it ol b gy op oq l or os">!git clone <a class="ae md" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">https://github.com/ultralytics/yolov5</a> # clone repo<br/>!pip install -U -r yolov5/requirements.txt # install dependencies</span></pre><p id="0027" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们需要从存储库中获取的主要文件的结构如下:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="3169" class="ml mm it ol b gy op oq l or os">yolov5           # project root<br/>├── models       # yolov5 models<br/>├── train.py     # training script<br/>└── detect.py    # inference script</span></pre><p id="473b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这📁<strong class="lj jd"> models </strong>文件夹包含几个<code class="fe ot ou ov ol b">.yml</code>文件，包含不同的建议型号。在其中我们可以找到4种不同的型号，按从小到大的顺序排列(根据参数的数量):<em class="ni"> yolov5-s </em>、<em class="ni"> yolov5-m </em>、<em class="ni"> yolov5-l </em>和<em class="ni"> yolov5-x、</em>详细对比<em class="ni">、</em>、<a class="ae md" href="https://github.com/ultralytics/yolov5#user-content-pretrained-checkpoints" rel="noopener ugc nofollow" target="_blank">见此处</a> <em class="ni">。</em></p><p id="8d07" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe ot ou ov ol b">train.py</code>和<code class="fe ot ou ov ol b">detect.py</code>将是我们将分别调用来训练模型和预测新图像/视频的脚本。</p><h2 id="d07b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">Roboflow</h2><p id="b08a" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">为了训练模型，一个必要的步骤是改变<code class="fe ot ou ov ol b">.xml</code>注释文件的格式，使其符合<a class="ae md" href="https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885" rel="noopener ugc nofollow" target="_blank">暗网格式</a>。在链接的github线程中，我们会看到每个图像都必须有一个与之相关联的<code class="fe ot ou ov ol b">.txt</code>文件，其行的格式为:</p><p id="97ea" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe ot ou ov ol b">&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</code></p><p id="7616" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">每行代表图像中每个对象的注释，其中<code class="fe ot ou ov ol b">&lt;x&gt; &lt;y&gt;</code>是边界框中心的坐标，<code class="fe ot ou ov ol b">&lt;width&gt; &lt;height&gt;</code>是各自的宽度和高度。</p><p id="30d1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，一个<code class="fe ot ou ov ol b">img1.jpg</code>必须有一个关联的<code class="fe ot ou ov ol b">img1.txt</code>，包含:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="75a6" class="ml mm it ol b gy op oq l or os">1 0.427234 0.123172 0.191749 0.171239<br/>0 0.183523 0.431238 0.241231 0.174121<br/>1 0.542341 0.321253 0.191289 0.219217</span></pre><p id="155d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好消息是，多亏了Roboflow，这一步变得非常简单。<a class="ae md" href="https://roboflow.com/" rel="noopener ugc nofollow" target="_blank"> Roboflow </a>能够轻松<em class="ni">在注释格式</em>之间转换，以及扩充我们的图像数据和<em class="ni">将其分成训练和验证</em>集，这将非常方便！</p><p id="339e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这可以通过5个简单的步骤来完成:</p><ul class=""><li id="a2f1" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated">上传图像和注释</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/78d9b0d1052ad27ed96a974078233663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5FpNXH7SWrQKdFKn5BIldg.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Roboflow，上传注释(图片由作者提供)</p></figure><ul class=""><li id="9c40" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated">选择您想要的训练、验证和测试比例(训练和验证就足够了)</li><li id="3e51" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated">添加一个从现有滤镜中选择的增强步骤，例如<em class="ni">模糊</em>、<em class="ni">亮度</em>、<em class="ni">旋转</em>等。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/75eca4f78481d7674e6d5e9b973b3f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hR3NG0TCPcataLw6K0Vx3A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">增强(图片由作者提供)</p></figure><ul class=""><li id="4db7" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated">最后，生成新的图像并导出为<strong class="lj jd"> YOLO Darknet </strong>格式</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/3e2d45651b53d7f5820ba996935200bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlVz36aj6eMWmcebmqYxng.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">选择输出格式(按作者排列的图像)</p></figure><p id="95e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们现在应该为每个分割、<strong class="lj jd">训练</strong>和<strong class="lj jd">验证</strong>(以及<strong class="lj jd">测试</strong>如果包括在内)都有一个单独的文件夹，其中每个文件夹都应该包含<code class="fe ot ou ov ol b">.jpg</code>增强图像、相应的<code class="fe ot ou ov ol b">.txt</code>注释文件和一个<code class="fe ot ou ov ol b">._darknet.labels</code>文件，这些文件的标签按其相应的顺序排列:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="f97a" class="ml mm it ol b gy op oq l or os">mask_weared_incorrect # label 0<br/>with_mask             # label 1<br/>without_mask          # label 2</span></pre><h2 id="3c93" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">改变标签</h2><p id="5f19" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这将取决于你希望你的模型如何表现，但是如上所述，我决定通过限制标签为<strong class="lj jd">遮罩</strong>或<strong class="lj jd">无遮罩</strong>来简化问题。<strong class="lj jd"> </strong>此外，标签<code class="fe ot ou ov ol b">mask_weared_incorrect</code>在数据集中出现的次数很少，因此模型很可能在分类上表现不佳。</p><p id="3508" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个简单的脚本可以做到:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="e730" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，注释文件中的数字标签将映射到:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="b4de" class="ml mm it ol b gy op oq l or os">without_mask          # label 0<br/>with_mask             # label 1</span></pre><h2 id="c845" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">数据平衡[可选]</h2><p id="2fd7" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">数据集有点不平衡，有更多的<code class="fe ot ou ov ol b">with_mask</code>标签，因此我们可以做的是增加没有戴面具的人的图像。这也将有助于显著提高模型的性能，因为我们将使用的数据集非常小。</p><p id="c751" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">在这里</strong>  <strong class="lj jd">找到本节使用的代码</strong> <a class="ae md" href="https://github.com/AlexanderNixon/Machine-learning-reads/blob/master/Face-Mask-Detection-YOLOv5.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">的笔记本。</strong></a></p><p id="e69a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以做到这一点的一个方法是从<a class="ae md" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO数据集</a>下载图像，然后我们自己注释它们。COCO数据集有一个官方API，<a class="ae md" href="https://pypi.org/project/pycocotools/" rel="noopener ugc nofollow" target="_blank"> pycocotools </a>，我们可以用它来下载带有<code class="fe ot ou ov ol b">person</code>标签的图像:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="29e8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以上内容将为我们提供一个字典列表，其中包含图片的详细信息和url，我们可以使用以下内容下载到本地:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="0ed7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下一步将是获得这些图像的面部边界框。为此，一种简单的方法是使用预先训练的模型来检测人脸，并以适当的格式标记它们。</p><p id="20fb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为此，您需要先安装<code class="fe ot ou ov ol b">pytorch</code>，然后再安装<code class="fe ot ou ov ol b">facenet-pytorch</code>:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="9143" class="ml mm it ol b gy op oq l or os">conda install pytorch torchvision torchaudio cpuonly -c pytorch<br/>pip install facenet-pytorch</span></pre><p id="7960" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我还使用<code class="fe ot ou ov ol b">cv2</code>来检查注释是否正确。要获得暗网格式的注释，我们需要转换由<code class="fe ot ou ov ol b">facenet-pytorch</code>进行的检测。我们可以通过下面的函数做到这一点:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="09f9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，您可以使用以下脚本获取下载图像的注释:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="e1dc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其还打印检测到的注释，以验证它们是正确的。以下是一些例子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/8b4f20944868726bd38f998243198b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*U1bJTWZXyX3oavdtkBDFkA.png"/></div></figure><h2 id="bc5b" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">最后一档</h2><p id="1e2b" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在训练模型之前，我们需要创建一个<code class="fe ot ou ov ol b">data.yml</code>，指定训练和验证图像的位置和<strong class="lj jd">标签</strong>的数量以及训练数据的标签名称。该文件的结构应该是:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="9498" class="ml mm it ol b gy op oq l or os">train: train/images<br/>val: valid/images</span><span id="35bc" class="ml mm it ol b gy pc oq l or os">nc: 2<br/>names: ['bad', 'good']</span></pre><h2 id="e4a3" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">培养</h2><p id="3eb9" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">为了训练模型，我们必须运行<code class="fe ot ou ov ol b">train.py</code>，它采用以下参数:</p><ul class=""><li id="8401" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated"><strong class="lj jd"> img: </strong>输入图像尺寸</li><li id="8ae4" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">批量:</strong>批量大小</li><li id="2356" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">时期:</strong>时期的数量</li><li id="3932" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">数据:</strong>路径到<code class="fe ot ou ov ol b">data.yml</code>文件</li><li id="ddce" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd"> cfg: </strong>模型在预先存在的中进行选择📁<strong class="lj jd">型号</strong></li><li id="602b" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">权重:</strong>初始权重路径，默认为<code class="fe ot ou ov ol b">yolov5s.pt</code></li><li id="0e09" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">名称:</strong>重命名输出文件夹</li><li id="cd0c" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">设备</strong>:是否在<em class="ni"> cpu </em>或<em class="ni"> gpu上训练。</em></li></ul><p id="f5b8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我的例子中，我用GPU训练了模型，但否则你必须添加参数<code class="fe ot ou ov ol b">--device cpu</code>才能在CPU上本地运行。其余的，你只需要注意指定的路线:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="4b2a" class="ml mm it ol b gy op oq l or os">python ~/github/yolov5/train.py --img 416 --batch 16 --epochs 150 --data data.yaml --cfg yolov5s.yaml --weights ‘’ --name robo4_epoch150_s --adam</span></pre><p id="31d1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦训练完成，您可以通过检查生成的日志以及保存为<code class="fe ot ou ov ol b">.png</code>文件的损失图来了解您的模型的表现。</p><p id="c689" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">训练损失和性能指标被记录到Tensorboard和一个<code class="fe ot ou ov ol b">runs/exp0/results.txt</code>日志文件中，在训练完成后被标为<code class="fe ot ou ov ol b">results.png</code>。</p><p id="db21" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们看到<code class="fe ot ou ov ol b">yolov5s.pt</code>被训练到150个历元(蓝色)，和300个历元(橙色):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/21117e0d729b6df764cabe9f6ca4c5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBrht9K3qJ8odS_qrVV9Pw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">培训损失(图片由作者提供)</p></figure><p id="5e6d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦第一个时期完成，我们将有一个显示地面实况和测试图像预测结果的图像镶嵌图，看起来像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/4cfa34561d8c91c45811f7ea188a4c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__DWh7G646G80CwraY7Ymg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">示例生成的马赛克(图片由作者提供)</p></figure><p id="b056" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在ultralytics的colab笔记本上有一个<a class="ae md" href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb#scrollTo=N3qM6T0W53gh" rel="noopener ugc nofollow" target="_blank">详细教程，在那里你会发现上面有更详细的解释，你可以自己运行！</a></p><h2 id="2b0c" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">推理</h2><p id="9ba3" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">现在我们已经在一个人脸面具图像的数据集上训练了这个模型，它已经准备好在单个图像和<strong class="lj jd">视频流</strong>上运行推理了！</p><p id="f2a0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">要运行推理，我们必须调用<code class="fe ot ou ov ol b">detect.py</code>，并调整以下参数:</p><ul class=""><li id="232b" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated"><strong class="lj jd">权重:</strong>训练好的模型的权重</li><li id="f0bc" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">来源:</strong>输入运行推理的文件/文件夹，<code class="fe ot ou ov ol b">0</code>用于网络摄像头</li><li id="013e" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">输出:</strong>保存结果的目录</li><li id="376a" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd">iou-thres</strong>:NMS的IOU阈值，默认为<code class="fe ot ou ov ol b">0.45</code></li><li id="aa3d" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd"> conf-thres </strong>:对象可信度阈值</li></ul><p id="8aaa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后两个您可能需要根据生成的结果稍微调整一下:</p><ul class=""><li id="4e06" class="nk nl it lj b lk ll ln lo lq nm lu nn ly no mc np nq nr ns bi translated"><strong class="lj jd"> iou-thres </strong>定义为给定对象获得的<em class="ni"> </em> <strong class="lj jd">交集/并集<em class="ni"> </em> </strong>度量的阈值。默认为<code class="fe ot ou ov ol b">0.45</code>，一般来说一个<strong class="lj jd">借据</strong>高于<code class="fe ot ou ov ol b">0.5</code>被认为是一个好的预测。参见<a class="ae md" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">Adrian rose Brock<a class="ae md" href="https://www.pyimagesearch.com/author/adrian/" rel="noopener ugc nofollow" target="_blank"/><strong class="lj jd"/>的这篇伟大的文章</a>，了解关于这一指标的更多细节。</li><li id="267f" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated"><strong class="lj jd"> conf-thres </strong>是确定模型对给定边界框包含对象的置信度的度量。它等于<strong class="lj jd"> IoU </strong>乘以一个代表物体概率的因子。这一指标主要阻止模型预测背景。阈值越高，<em class="ni">误报越少。</em>然而，如果我们将其设置得太高，模型将错过许多正确的检测，因为它的可信度较低。</li></ul><p id="e156" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">经过几次尝试和出错后，以下设置似乎能对此问题产生最佳结果:</p><pre class="ks kt ku kv gt ok ol om on aw oo bi"><span id="ae78" class="ml mm it ol b gy op oq l or os">python ~/github/yolov5/detect.py --weights raw/roboflow4/weights/last_robo4_epoch300_s.pt --source ./office_covid.mp4 --output raw/roboflow4/inference/output --iou-thres 0.3 --conf-thres 0.6</span></pre><p id="206d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了测试这个模型，我用手机拍摄了一段视频。这些是结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3ecba2f84150da0808330843746ed797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*V_D4JtNnlzmaCVk0ueA6sQ.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">对视频流的推理测试(图片由作者提供)</p></figure><p id="f82a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这里，您可以看到该模型能够准确地检测面罩。</p><h2 id="e386" class="ml mm it bd mn mo mp dn mq mr ms dp mt lq mu mv mw lu mx my mz ly na nb nc iz bi translated">改进建议</h2><ul class=""><li id="f115" class="nk nl it lj b lk nd ln ne lq pf lu pg ly ph mc np nq nr ns bi translated">更多<strong class="lj jd">数据训练多样性</strong>。该模型在某些条件下很难检测到面具，例如，它往往会混淆长胡子和面具。这可以通过在训练图像中增加更多的多样性以及可能运行额外的数据增强技术来减轻。</li><li id="ce6d" class="nk nl it lj b lk nt ln nu lq nv lu nw ly nx mc np nq nr ns bi translated">比较不同的YOLOv5型号，根据指标并考虑培训时间，检查哪种型号性能最佳。更大的型号<a class="ae md" href="https://github.com/ultralytics/yolov5/releases" rel="noopener ugc nofollow" target="_blank"> YOLOv5x </a>将需要更长的训练时间。</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="c91c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">非常感谢你花时间阅读这篇文章，希望你喜欢:)</p></div></div>    
</body>
</html>