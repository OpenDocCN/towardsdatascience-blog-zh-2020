<html>
<head>
<title>1st Presidential Debate: By the Numbers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第一次总统辩论:按数字</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/1st-presidential-debate-by-the-numbers-dee50b35f4ac?source=collection_archive---------22-----------------------#2020-10-02">https://towardsdatascience.com/1st-presidential-debate-by-the-numbers-dee50b35f4ac?source=collection_archive---------22-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/664072da2b41ca529fb12ec5770134c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuUqU-mX_xQiNHXGxUDcTg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="89ad" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">第一场辩论<strong class="kh iu"> </strong>很乱。<strong class="kh iu"> </strong>但就像今天的大多数新闻一样，随着下一个故事的出现，它无疑会褪色(例如Trump对COVID的检测呈阳性)。所以，在辩论退居幕后之前，让我们用一些数据科学和工具来尽快分析和可视化辩论吧！</p><p id="843a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">与其他许多以数据科学为导向的文章不同，我将更多地关注快速而肮脏的数据处理、分析和可视化方法，因为在完全透明的情况下，尽管上面和下面的视觉效果可能看起来很有趣，并能吸引眼球，但它们并不令人难以置信地深刻，也没有讲述太多超越表面水平的故事。然而，文本数据本身是相当丰富的，我鼓励您自己进一步探索这些数据！<a class="ae ld" href="https://www.kaggle.com/theogoe/first-pres-debate-2020" rel="noopener ugc nofollow" target="_blank">(我已经把数据上传到Kaggle这里了)。</a></p><h1 id="3449" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">当你时间很紧的时候该做什么</h1><p id="7729" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">在我从事目前的工作之前，我在《国家杂志》工作，这是大西洋媒体(印刷和在线媒体公司)的政治/政策部门，我们的团队每天都会即时创建可视化效果。为了创造下面的视觉效果，这里有一些我推荐的技巧。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/cebcd3b890c508bf192ef72549ebef8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ceX1XfsmUYLRYQqL8f7Lxg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><blockquote class="mm mn mo"><p id="8ee8" class="kf kg mp kh b ki kj kk kl km kn ko kp mq kr ks kt mr kv kw kx ms kz la lb lc im bi translated">第一个问题:数据里有故事吗？</p><p id="38d3" class="kf kg mp kh b ki kj kk kl km kn ko kp mq kr ks kt mr kv kw kx ms kz la lb lc im bi translated">第二个问题:它容易争论吗？</p></blockquote><p id="df17" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">当我观看辩论时，我首先想想象的是贯穿始终的打断次数。但是这容易争论吗？遗憾的是，没有。通过快速的谷歌搜索，我找到了一份来自Rev.com的辩论记录，似乎他们在停顿时将每位发言者的发言分开了。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mt"><img src="../Images/94f660672491b093bb12b47791e6956c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mUE5NI4K-0VTeJkAMU-8HQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">文本抄本将单个发言分成多个片段。</p></figure><p id="f47b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">换句话说，除了手动通读和连接这些重叠的区域，没有快速和可靠的方法来识别说话者是否打断了另一个人，被打断了，或者转录只是在搞笑。</p><p id="5112" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">事实上，如果我们不考虑前一位发言者是谁，只分析这些事件的发生频率，特朗普总统有150次，拜登副总统有136次——这一比例与现实相去甚远。下面是检查这一点的代码:</p><pre class="mi mj mk ml gt mu mv mw mx aw my bi"><span id="1ef7" class="mz lf it mv b gy na nb l nc nd">import pandas as pd</span><span id="7c08" class="mz lf it mv b gy ne nb l nc nd">df = pd.read_csv('debate_csv.csv')  # read in the csv</span><span id="00db" class="mz lf it mv b gy ne nb l nc nd"># split into a list of words then count length of list<br/>df['num_words'] = df['text'].str.split().str.len()  </span><span id="06b7" class="mz lf it mv b gy ne nb l nc nd"># subset for only 8 words or less<br/>df = df[df['num_words'] &lt;= 8]</span><span id="fbf6" class="mz lf it mv b gy ne nb l nc nd"># check count by speaker<br/>d_count = df.groupby('speaker').count()<br/>print(d_count)</span></pre><h1 id="a45c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用Excel</h1><p id="4409" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">可以，用Excel。对我来说，这是从Rev网站复制并粘贴原始转录的最快方式，并使用文本到列和F5 -&gt;选择空白-&gt;删除选定的行来创建数据集。</p><h1 id="86c5" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创建时间图</h1><p id="9ee5" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">编码时可视化需要时间。因为我不容易分析我感兴趣的第一个故事角度，所以我决定想象每个演讲者发言的时间。</p><p id="9f49" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了创建时间序列，我使用了<a class="ae ld" href="https://flourish.studio/2020/07/16/heatmap-template/" rel="noopener ugc nofollow" target="_blank">fluorescent Studio</a>。虽然我不是他们的免费层的最大粉丝，这要求你的所有数据集都是公开的…对于像这样的快速项目来说，当“新闻价值”溜走时，时间是至关重要的，这是一个很好的工具。(其他快速但有吸引力的viz工具还包括<a class="ae ld" href="https://www.datawrapper.de/" rel="noopener ugc nofollow" target="_blank"> DataWrapper </a>(与fluid相同的公共数据要求)和<a class="ae ld" href="https://rawgraphs.io/" rel="noopener ugc nofollow" target="_blank">Raw Graphs</a>——这是完全开源的，让你保持隐私，双赢)。</p><p id="dca9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">快速浏览了一下fluorescent的样本数据，我意识到我自己的数据的结构应该是一行= 1秒，其中X轴是经过的秒数(连续变量)，Y轴是说话者(分类变量)。在处理之前，辩论数据集对每个发言人有一行，一列是发言人开始讲话的分:秒。</p><pre class="mi mj mk ml gt mu mv mw mx aw my bi"><span id="33b5" class="mz lf it mv b gy na nb l nc nd">import pandas as pd</span><span id="aafa" class="mz lf it mv b gy ne nb l nc nd">df = pd.read_csv('debate_csv.csv')  # read in the csv</span><span id="1923" class="mz lf it mv b gy ne nb l nc nd"># function to convert the time to seconds<br/>def time_to_sec(text):<br/>    minsec = text.split(':')<br/>    minutes = minsec[0]<br/>    seconds = minsec[1]<br/>    tseconds = (int(minutes) * 60) + int(seconds)<br/>    return tseconds</span><span id="878d" class="mz lf it mv b gy ne nb l nc nd"># convert timestamp (string) to seconds (int)<br/>df['seconds'] = df['Time'].apply(time_to_sec)<br/><br/># create multiple rows based on the number of seconds spoken</span><span id="9323" class="mz lf it mv b gy ne nb l nc nd"># replace 0 seconds spoken with 1<br/>df['seconds_spoken'] = df['seconds_spoken'].replace(0, 1)<br/># fill empty values<br/>df['seconds_spoken'] = df['seconds_spoken'].fillna(1)<br/># now we can run repeat to create one row per second<br/>df = df.loc[df.index.repeat(df.seconds_spoken)]</span><span id="22df" class="mz lf it mv b gy ne nb l nc nd"># by resetting the index and making it a column<br/># we can have a column that increases +=1 seconds<br/>df = df.reset_index(drop=True)<br/>df['running_seconds'] = df.index</span><span id="0733" class="mz lf it mv b gy ne nb l nc nd"># export this file<br/>df.to_csv('export.csv')</span></pre><p id="8d8d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在导入这个文件并设置轴之后，我们得到了最终的图表(我截屏了这个可视化，并使用<a class="ae ld" href="https://www.photopea.com/" rel="noopener ugc nofollow" target="_blank"> Photopea </a>(免费Photoshop)添加了最后的润色，就像图例一样)。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/2d2880e629bee12e07be67f1a37875b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkQBGhGgQl3-LbUXe8hfSQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h1 id="0404" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创造单词云</h1><p id="b4f8" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">很明显，我会说我不是文字云的粉丝。他们不能有效地交流信息，而且用不讨人喜欢的调色板看起来很廉价。我的团队知道永远不要给我带来单词云。</p><p id="4b8e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">话虽如此，但如果它们是为了吸引读者而恰当地创作出来的，它们并不是世界末日。</p><p id="2cb9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我使用了来自<a class="ng nh ep" href="https://medium.com/u/cc7314ace45c?source=post_page-----dee50b35f4ac--------------------------------" rel="noopener" target="_blank"> Shashank Kapadia </a>的关于主题建模的<a class="ae ld" rel="noopener" target="_blank" href="/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0">数据科学文章</a>的代码，并对其进行了最小的编辑，以返回一个数据帧，其中包含最频繁使用的单词及其对应的每个说话者的频率。顺便说一句，我推荐他的文章，它全面而中肯，有助于我们新的数据科学实习生掌握LDA。</p><p id="25dd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在运行他的代码之前，我很快清理了文本，以确保wordcloud没有被冠词和介词占据。</p><pre class="mi mj mk ml gt mu mv mw mx aw my bi"><span id="2714" class="mz lf it mv b gy na nb l nc nd"># quick text cleaning<br/>def remove_accented_chars(text):<br/>    text = unidecode.unidecode(text)<br/>    return text</span><span id="27f2" class="mz lf it mv b gy ne nb l nc nd">def expand_contractions(text):<br/>    text = list(cont.expand_texts([text], precise=True))[0]<br/>    return text</span><span id="bef9" class="mz lf it mv b gy ne nb l nc nd">custom_remove_string = ['a','and','its','it','did','going','want','know','look','said','got','just','think','crosstalk','say','tell','00','way','like','lot','does','let','happened','came','doing','000','47','seen','shall','are']</span><span id="54a7" class="mz lf it mv b gy ne nb l nc nd">def remove_custom_words(text):<br/>    text = text.split()<br/>    text = [w for w in text if w not in custom_remove_string]<br/>    text = ' '.join(text)<br/>    return text</span><span id="2fac" class="mz lf it mv b gy ne nb l nc nd"># run remove accented characters<br/>df['text'] = df['text'].apply(remove_accented_chars)<br/># lowercase the text and remove punctuation<br/>df['text'] = df['text'].str.lower().apply(lambda x: re.sub(r'[^\w\s]','',x))</span><span id="27ce" class="mz lf it mv b gy ne nb l nc nd"># run expand contractions and remove custom words<br/>df['text'] = df['text'].apply(expand_contractions)<br/>df['text'] = df['text'].apply(remove_custom_words)</span></pre><p id="4f8f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">虽然你可以使用Python Wordcloud库，但这个项目的目标是尽可能快地建立这些库，所以我在辩论中使用了<a class="ae ld" href="https://wordart.com/" rel="noopener ugc nofollow" target="_blank">WordArt.com</a>和特朗普总统和拜登副总统的相应字符串。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/b1d0d26adf97bf19a6e9ce220d5b66c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMJqowaODr-34VTo_ui_Bg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h1 id="9bfd" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">词性分析</h1><p id="0b26" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">可视化的最后一部分是词性分析。</p><p id="d4d0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是其中最快的一个，互联网上有很多关于如何做到这一点的资料。下面是我创建可视化数据集所需的所有代码。</p><pre class="mi mj mk ml gt mu mv mw mx aw my bi"><span id="46a5" class="mz lf it mv b gy na nb l nc nd">from nltk.tag import pos_tag<br/>from nltk.tokenize import word_tokenize<br/>from collections import Counter</span><span id="528c" class="mz lf it mv b gy ne nb l nc nd"># subset the data and create a string of the words used by Trump<br/>trump = df[df['speaker'] == 'President Donald J. Trump '].text.tolist()<br/>trump = " ".join(trump)</span><span id="2ea3" class="mz lf it mv b gy ne nb l nc nd"># use nltk's libraries to determine pos<br/>trump_text = pos_tag(word_tokenize(bid))</span><span id="00f1" class="mz lf it mv b gy ne nb l nc nd">count= Counter([j for i,j in pos_tag(word_tokenize(bid))])</span><span id="0fdd" class="mz lf it mv b gy ne nb l nc nd"># determine relative usage of each part of speech<br/>total = sum(count.values())<br/>tcount = dict((word, float(co)/total) for word,co in count.items())</span><span id="249c" class="mz lf it mv b gy ne nb l nc nd"># convert output to dataframe<br/>tcount = pd.DataFrame(tcount.items())</span></pre><p id="12e0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，我将这个脚本的输出拖到Tableau，并创建了一个快速图表(Tableau对学生来说是<a class="ae ld" href="https://www.tableau.com/academic/students" rel="noopener ugc nofollow" target="_blank">免费的一年</a>，作为一个提醒)。我在<a class="ae ld" href="https://help.tableau.com/current/pro/desktop/en-us/formatting_create_custom_colors.htm" rel="noopener ugc nofollow" target="_blank"> preferences.tps文件中设置了一个自定义调色板，而不是标准调色板。</a>然后我用<a class="ae ld" href="https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/" rel="noopener ugc nofollow" target="_blank">加入了词性输出文件，这个文件定义了每个词性缩写</a>(复制/粘贴到excel →文本到列)。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/ac9726ebabfb99365eb85373abb17218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDxws9Dq1vPHYu1ZzQjslA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h1 id="8911" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">外卖食品</h1><p id="d3d2" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">如果你正在进行像这样的快速、探索性的数据可视化，要记住的最重要的事情之一是在触摸键盘之前设置你将追求的故事/叙述/假设问题。与此同时，设想最终产品可能是什么样子——不是根据您将要编写的代码，而是根据查看者将看到的内容。</p><p id="e90f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这将最大限度地减少你最终要做的工作，因为每一步都变成了一个可以勾掉的复选框，而不是一个永无止境的数据探索阶段，这导致了100个视觉效果，其中没有一个足够有凝聚力来讲述一个令人信服的故事。</p><p id="7760" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你是这个领域的初学者，当你想到分析数据的101种方法时，这种方法可以帮助减轻不知所措的感觉。如果你熟悉我用来制作这些视觉效果的标准NLP库和工具，这种“故事优先”的方法仍然有助于减少项目所需的时间。</p><p id="98a2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作为参考，这个项目花了大约2个半小时完成——从谷歌搜索“2020年辩论记录”到一起拍照(写这篇文章需要更长时间😂).</p><p id="f541" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从所有这些分析中，一个实际的收获是特朗普总统使用的所有格代词的数量(例如，“我”“我的”“我的”“我们的”等)。当我看到那件事时，我笑了。</p><blockquote class="mm mn mo"><p id="93ee" class="kf kg mp kh b ki kj kk kl km kn ko kp mq kr ks kt mr kv kw kx ms kz la lb lc im bi translated"><strong class="kh iu">关于我:</strong>Basil Labs的创始人，Basil Labs是一家大数据消费者智能初创公司，帮助组织量化消费者的去向和价值。</p><p id="99ff" class="kf kg mp kh b ki kj kk kl km kn ko kp mq kr ks kt mr kv kw kx ms kz la lb lc im bi translated">热爱音乐，开放数据政策和数据科学。更多文章关注我<a class="ae ld" href="https://medium.com/@theo_goe" rel="noopener">中</a>。<br/>如果你对数据伦理、开放数据和/或音乐充满热情，请随时在<a class="ae ld" href="https://twitter.com/theo_goe" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上添加我。</p></blockquote></div></div>    
</body>
</html>