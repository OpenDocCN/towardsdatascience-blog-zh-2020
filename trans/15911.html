<html>
<head>
<title>How to Use Pyspark For Your Machine Learning Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将Pyspark用于您的机器学习项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-pyspark-for-your-machine-learning-project-19aa138e96ec?source=collection_archive---------24-----------------------#2020-11-02">https://towardsdatascience.com/how-to-use-pyspark-for-your-machine-learning-project-19aa138e96ec?source=collection_archive---------24-----------------------#2020-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="99d3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Pyspark进行数据清理、EDA、特征工程和机器学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3ccaf726082e80aba28a49349130f325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z6jQKXAoFD2zFiCI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://unsplash.com/photos/Q1p7bh3SHj8<a class="ae ky" href="https://unsplash.com/photos/Q1p7bh3SHj8" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="8d79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pyspark是一个支持Apache Spark的Python API，Apache Spark是一个用于处理大数据分析的分布式框架。当您处理大型数据集时，这是一个令人惊叹的框架，它正在成为任何数据科学家的必备技能。</p><p id="7447" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我将介绍如何使用Pyspark来做你习惯在Kaggle笔记本上看到的事情(清洁、EDA、特征工程和构建模型)。</p><p id="7e59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我为一家电信公司使用了一个包含客户信息的数据库。目标是预测未来三个月哪些客户会离开(流失)。包含数据的CSV文件包含超过800，000行和8个特征，以及一个二进制变动变量。</p><blockquote class="lv lw lx"><p id="7233" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><strong class="lb iu">这里的目标不是找到最佳解决方案。而是向您展示如何使用Pyspark。</strong>在这个过程中，我将尝试展示许多可用于您机器学习项目所有阶段的功能！</p></blockquote></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="beb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先创建一个SparkSession，这是任何Spark功能的入口点。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="1ce3" class="mo mp it mk b gy mq mr l ms mt">import pyspark<br/>from pyspark.sql import SparkSession</span><span id="869a" class="mo mp it mk b gy mu mr l ms mt">spark = SparkSession.builder.master("local[4]")\<br/>       .appName("test").getOrCreate()</span></pre><h2 id="dd9e" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">获取数据</h2><p id="2d86" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">以下是如何使用Pyspark读取CSV文件。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="c74c" class="mo mp it mk b gy mq mr l ms mt">df=spark.read.csv('train.csv',header=True,sep= ",",inferSchema=True)</span></pre><p id="ab43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是数据帧的样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/9319b636613956441fd2fdc37c11ce15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxI4brIKqEK_UZ8JdzGCWQ.png"/></div></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="4d39" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">清理数据</h2><p id="505c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Pyspark.sql模块允许您在Pyspark中做几乎任何可以用sql做的事情。</p><p id="67e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，让我们先清理一下数据。首先，如上图所示，我们有一些空值。我将删除所有包含空值的行。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="2fae" class="mo mp it mk b gy mq mr l ms mt">df = df.na.drop()</span></pre><p id="0731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，<code class="fe ns nt nu mk b">when/otherwise </code>函数允许您过滤一列，并根据每行中找到的内容分配一个新值。举个例子，我用0和1代替男性和女性作为性别变量。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="9eb3" class="mo mp it mk b gy mq mr l ms mt">from pyspark.sql.functions import when<br/>df = df.withColumn("gender",when(df["gender"]=='M',0).otherwise(1))</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="dd4a" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">探索性数据分析</h2><p id="f88c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">一旦数据全部清理完毕，许多类似SQL的函数可以帮助分析数据。</p><p id="882e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，<code class="fe ns nt nu mk b">groupBy</code>函数允许您对值进行分组，并为每个类别返回count、sum或其他值。让我们看看有多少数据点属于流失变量的每个类。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="7ef9" class="mo mp it mk b gy mq mr l ms mt">df.groupBy('churnIn3Month').count().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/37c6cee61b732c3cc9bb011d6a181a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*fbtImZJzwGpU_chGjpkjMQ.png"/></div></figure><p id="81e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们这里有不平衡的班级。虽然我不会在本教程中做任何事情，但在接下来的一个教程中，我将向您展示如何使用Pyspark处理不平衡的类，如欠采样、过采样和SMOTE。</p><p id="505b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一件有趣的事情是观察两组(离开的客户和没有离开的客户)之间某些特征的不同。我发现了一个有趣的结果。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="2fbb" class="mo mp it mk b gy mq mr l ms mt"><strong class="mk iu">from</strong> pyspark.sql.functions <strong class="mk iu">import</strong> avg<br/>df.select("phoneBalance","churnIn3Month").\<br/>          groupBy("ChurnIn3Month").agg(avg("phoneBalance"))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/80d682dedc414cd1118786c39d656039.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*4MmAlqz71t6fo8vulRdcyg.png"/></div></figure><p id="b017" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，离开的客户平均手机余额少得多，这意味着他们的手机更接近完全支付(这当然使他们更容易离开电话公司)。</p><p id="8f11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们来看一个相关矩阵。我使用Pyspark中的<code class="fe ns nt nu mk b">correlation</code>函数创建了它。我也做了一点手脚，在这里用了熊猫，只是为了更容易地创造一些视觉上的东西。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="60c5" class="mo mp it mk b gy mq mr l ms mt">from pyspark.ml.stat import Correlation</span><span id="c387" class="mo mp it mk b gy mu mr l ms mt">x=df.columns[2:11]<br/>corr_plot = pd.DataFrame()</span><span id="2475" class="mo mp it mk b gy mu mr l ms mt">for i in x:<br/>    corr=[]<br/>    for j in x:<br/>        corr.append(round(df.stat.corr(i,j),2))<br/>    corr_plot = pd.concat([corr_plot,pd.Series(corr)],axis=1)</span><span id="ceef" class="mo mp it mk b gy mu mr l ms mt">corr_plot.columns=x<br/>corr_plot.insert(0,'',x)<br/>corr_plot.set_index('')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/866d36f8554527eb612bfb8136087a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqzpdwNN1bpMDFmIvGnyiA.png"/></div></div></figure><p id="3036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，<code class="fe ns nt nu mk b">phoneBalance</code>与客户流失变量的相关性最强。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="4b16" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">特征工程</h2><p id="bffd" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">考虑到上面的结果，我决定创建一个新变量，它将是<code class="fe ns nt nu mk b">phoneBalance</code>变量的平方。下面是如何用Pyspark做到这一点。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="a47f" class="mo mp it mk b gy mq mr l ms mt">from pyspark.sql.functions import col, pow<br/>df = df.withColumn('phoneBalance2',pow(col('phoneBalance'),2))</span></pre><p id="4dad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ns nt nu mk b">withColumn</code>函数允许您向pyspark数据框架添加列。超级有用！</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="d566" class="ny mp it bd mv nz oa ob my oc od oe nb jz of ka ne kc og kd nh kf oh kg nk oi bi translated">机器学习</h1><p id="67d5" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Pyspark中的机器学习库肯定还没有达到Scikit Learn的标准。也就是说，你仍然可以用它做很多事情。</p><p id="5741" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，你要做的第一件事是创建一个包含你所有特征的矢量。我们将使用的所有方法都需要它。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="af14" class="mo mp it mk b gy mq mr l ms mt">from pyspark.ml.feature import VectorAssembler</span><span id="62cf" class="mo mp it mk b gy mu mr l ms mt">ignore=['churnIn3Month', 'ID','_c0']<br/>vectorAssembler = VectorAssembler(inputCols=[x for x in df.columns  <br/>                  if x not in ignore], outputCol = 'features')</span><span id="2b2d" class="mo mp it mk b gy mu mr l ms mt">new_df = vectorAssembler.transform(df)<br/>new_df = new_df.select(['features', 'churnIn3Month'])</span></pre><p id="0010" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，让我们将数据分成训练集和验证集。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="f70a" class="mo mp it mk b gy mq mr l ms mt">train, test = new_df.randomSplit([0.75, 0.25], seed = 12345)</span></pre><p id="041a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在让我们建立一些模型。我将只展示几个模型，只是为了让您了解如何使用Pyspark。</p><h2 id="da8b" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">1.逻辑回归</h2><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="2aa6" class="mo mp it mk b gy mq mr l ms mt">from pyspark.ml.classification import LogisticRegression</span><span id="f6ae" class="mo mp it mk b gy mu mr l ms mt">lr = LogisticRegression(featuresCol = 'features', <br/>                         labelCol='churnIn3Month')<br/>lr_model = lr.fit(train)</span></pre><p id="7448" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看看模型的ROC曲线。提醒一下，AUC(曲线下面积)越接近1，模型就越能区分不同的类别。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b1af" class="mo mp it mk b gy mq mr l ms mt">import matplotlib.pyplot as plt</span><span id="e016" class="mo mp it mk b gy mu mr l ms mt">plt.plot(lr_model.summary.roc.select('FPR').collect(),<br/>         lr_model.summary.roc.select('TPR').collect())<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.show()</span><span id="c748" class="mo mp it mk b gy mu mr l ms mt">lr_model.summary.areaUnderROC</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/14eebed4ff9eeca425a2dc1d64bbe836.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*ywcVmDVfRFpU7NYhZzqYZg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/47096d14ac5005a7c550ba93575544df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*xhh8G-jXX7qDdWd_QysEVw.png"/></div></figure><h2 id="4e89" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">2.随机森林分类器</h2><p id="51e2" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">让我们再做一个模型，展示一旦数据以Pyspark的正确格式，即向量，拟合模型是多么容易。下面是如何创建一个随机森林模型。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="bbe2" class="mo mp it mk b gy mq mr l ms mt">from pyspark.ml.classification import RandomForestClassifier</span><span id="914a" class="mo mp it mk b gy mu mr l ms mt">rf = RandomForestClassifier(featuresCol = 'features', labelCol = <br/>                            'churnIn3Month')<br/>rf_model = rf.fit(train)</span></pre><p id="c3e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是如何获得模型的AUC:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="d950" class="mo mp it mk b gy mq mr l ms mt">from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="5895" class="mo mp it mk b gy mu mr l ms mt">predictions = rf_model.transform(test)<br/>auc = BinaryClassificationEvaluator().setLabelCol('churnIn3Month')<br/>print('AUC of the model:' + str(auc.evaluate(predictions)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/892e6cbfedec7925b27dd11de5ea847d.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*0KxnxN7bCHYXlBvRNuv1wg.png"/></div></figure><p id="b2f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个模型非常相似，但结果表明，在我们的情况下，逻辑回归模型略好。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="ed79" class="mo mp it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated"><strong class="ak">结论</strong></h2><p id="6e7a" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Scikit Learn非常棒，只要您不处理太多数据，它的表现就会令人钦佩。可悲的是，你的项目越大，你就越有可能需要Spark。幸运的是，正如您在这里看到的，开始使用Pyspark的学习曲线并不太陡，尤其是如果您熟悉Python和SQL的话。在我看来，Pyspark的主要弱点是数据可视化，但希望随着时间的推移，这种情况会有所改变！</p><p id="6e42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是了。对于想用Pyspark进行机器学习的人来说，这篇文章应该是一个很好的起点。我希望你喜欢它，并感谢阅读！</p></div></div>    
</body>
</html>