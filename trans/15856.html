<html>
<head>
<title>The Intuition Behind Transformers — Attention is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚背后的直觉——注意力是你所需要的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-intuition-behind-transformers-attention-is-all-you-need-393b5cfb4ada?source=collection_archive---------14-----------------------#2020-11-01">https://towardsdatascience.com/the-intuition-behind-transformers-attention-is-all-you-need-393b5cfb4ada?source=collection_archive---------14-----------------------#2020-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/03a863017d6388f5ed7d1156d333fc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M-eueiSQSSKJ178v"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@morsha?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Mor妮莎</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><p id="13ea" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统的递归神经网络及其变体已经广泛用于自然语言处理问题。近年来，《变形金刚》的表现超过了大多数RNN车型。在看变形金刚之前，让我们重温一下递归神经网络，它们是如何工作的，以及它们落后于何处。</p><p id="936d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">递归神经网络(RNN)处理语言翻译和时序数据等序列数据。有不同类型的递归神经网络。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi le"><img src="../Images/0cf0f28d56113ce24161aa0099e3268f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HUgyDtuOpfu9AYBP.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><ul class=""><li id="9e5d" class="lj lk jj ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">向量到序列模型——接受向量并返回任意长度的序列。</li><li id="f1c6" class="lj lk jj ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">序列到向量模型—这些模型将序列作为输入，并将向量作为输出返回。例如，这些模型通常用于情感分析问题。</li><li id="5816" class="lj lk jj ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">序列到序列模型，正如你现在已经猜到的，它们将一个序列作为输入，输出另一个序列。它们在语言翻译应用程序中很常见。</li></ul><h2 id="61bc" class="lx ly jj bd lz ma mb dn mc md me dp mf kr mg mh mi kv mj mk ml kz mm mn mo mp bi translated">自然语言处理和RNNs</h2><p id="b1d0" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">说到自然语言处理rnn，它们在一个<strong class="ki jk"> <em class="mv">编码器-解码器</em> </strong>架构中工作。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/8f6a656cde59658e7553588aeaa756e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YObUaSx87hPqtkPPGj02mg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="e791" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编码器将总结输入句子的所有信息，解码器将使用编码器的输出来创建正确的输出。编码器的最终状态传达开始解码的信息。解码器使用先前的状态和输出来计算新的隐藏状态和字。多个rnn用于编码器和解码器层。</p><p id="0ff9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，递归神经网络有其局限性。</p><ul class=""><li id="89e6" class="lj lk jj ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">首先，它们很慢，事实上，训练非常慢，我们经常不得不使用类似于<strong class="ki jk"> <em class="mv">的技术来截断训练，及时截断反向传播</em> </strong>。</li><li id="c118" class="lj lk jj ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">其次，也是更常见的，rnn会遇到<strong class="ki jk"> <em class="mv">消失和</em> </strong>爆炸梯度的问题。当应用于NLP问题时，从句子开始的信息会丢失。</li></ul><h2 id="8e75" class="lx ly jj bd lz ma mb dn mc md me dp mf kr mg mh mi kv mj mk ml kz mm mn mo mp bi translated">长短期记忆(LSTM)</h2><p id="16f8" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">引入长短期记忆(LSTM)网络来解决递归神经网络的这些问题。</p><p id="cb87" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它们的工作原理是有一个被称为记忆单元的隐藏状态，允许信息从前一个单元流向当前单元，同时跳过当前单元的大部分处理。这使得以前哑的神经元现在有了记忆，可以用来在需要时保留信息。这反过来允许模型在更长的序列中保留信息。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/527e5f13453a391443b28ac78b08f993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_yOeKOMOavUFnXJb.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="9032" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，虽然常规的递归神经网络的训练速度很慢，但是LSTMs的训练速度甚至更慢。其次，由于序列的每个字都被单独传递到网络，并且处理仍然在网络内顺序发生，所以这种架构没有利用当今GPU的并行处理。</p><h2 id="f1d2" class="lx ly jj bd lz ma mb dn mc md me dp mf kr mg mh mi kv mj mk ml kz mm mn mo mp bi translated">注意机制与RNNs</h2><p id="2d95" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">一个<strong class="ki jk"> <em class="mv">注意机制</em> </strong>被添加到它们中，以解决传统rnn和LSTMs的一些限制。注意机制通过使用全局向量来工作，上下文向量包含编码器所有隐藏状态的加权和。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/52b0eb3a4d2166f1b09efd4ed8801ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/0*0ICURg92l76NRWlj.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">巴赫达瑙等人</a>。</p></figure><p id="711a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上下文向量表示解码器的当前状态如何与全局输入序列相关。虽然注意机制解决了rnn的一些固有缺陷，但我们仍然是单独输入单词并顺序处理它们，这意味着这些架构仍然不能让我们利用当今硬件提供的并行处理。</p><h2 id="8a6f" class="lx ly jj bd lz ma mb dn mc md me dp mf kr mg mh mi kv mj mk ml kz mm mn mo mp bi translated">注意力是你所需要的——变形金刚</h2><p id="9970" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我们可以完全取消注册护士吗？输入变压器。2017年，transformer架构在标题为<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">的文章中进行了介绍</a>。事实证明，注意力是你解决最复杂的自然语言处理任务所需要的全部。让我们来看看。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/c38afd925b142c60807e044cf72df5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ir-j0HTT-IoWcZSyaE2F-A.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="d99f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变压器架构使用一个编码器和一个解码器，但只使用注意力，没有RNNs。</p><p id="f2ad" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与之前架构的主要区别在于，编码器的<strong class="ki jk"> <em class="mv">输入是整个句子</em> </strong>，而不是像RNNs那样一次输入一个单词。类似地，解码器的<strong class="ki jk"> <em class="mv">输入也是整个句子(右移)</em> </strong>。我们同时传递句子中的所有单词，并同时确定单词嵌入。</p><p id="658c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们打破这种架构，深入研究各个组件，从<strong class="ki jk"> <em class="mv">编码器模块</em> </strong>开始。</p><p id="23ce" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从序列中所有<strong class="ki jk">单词嵌入</strong>的网络输入开始。单词嵌入是单词的向量表示，使得具有相似意思的单词更接近；具体来说，相关单词在嵌入空间内彼此更接近。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/408f725b42473c7224f420a91790cdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*u_ZQa0xAthEkXfTDPdZEYA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="b8e5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在语言中，一个词在句子中的位置可以改变它的意思。例如，在句子“<em class="mv">之间，猫是一种动物。你像动物一样吃东西。</em>”字<em class="mv">动物的位置</em>改变了它的意思。我们在做CNN和RNNs的时候，保留了这个词的位置。然而，在transformer模型中，我们需要一个显式的<strong class="ki jk"> <em class="mv">位置编码</em> </strong>层来在嵌入完成后保留单词在序列中的位置。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/83861c4c79f84847a8b0cc989ab9993b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDS4ZynXo0iDebkYwYDrrw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="3900" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该论文提到了不同频率的正弦和余弦函数的使用。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/2f3dfb8d43390d2bb68bd285e5883478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ucZzISO4xXFwM_GyymygQw.png"/></div></div></figure><p id="0006" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了单词嵌入和位置编码，我们就可以将它传递给<strong class="ki jk"> <em class="mv">多头注意力模块。</em>T19】</strong></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/bb6ad45c9da5db4c716379399b934222.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*_NU9XwToqEfcAZCyGEyQiQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:瓦斯瓦尼等人。</p></figure><p id="d9dc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多头关注块重点关注<strong class="ki jk"> <em class="mv">自我关注</em></strong>；即序列中的每个单词如何与同一序列中的其他单词相关联。自我关注由关注块中产生的关注向量来表示。这个想法是为了捕捉句子中单词之间的上下文关系。</p><p id="836b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是如何工作的？我们通过计算缩放后的点积来找到两个向量之间的关系。</p><p id="23ff" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数学上，点积给出了两个向量之间的相似性。总之，如果点积为1(或负相关时为-1)，则两个向量密切相关，如果点积为0，则两个向量不相关。</p><p id="24e3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的transformer模型使用一个<strong class="ki jk"> <em class="mv">比例点积</em> </strong>函数来计算关注度。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/13ad99fd6a44982a493959870e69ea3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*aW6pO8SmXAmKg8bx.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="22b7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">转换器使用的注意函数接受三个输入:Q(查询)、K(键)、V(值)，下面的等式用于计算权重。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/2569370d74e10e3416f70a86af349051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*av8LyKkFN4Sgj5auYJwFqg.png"/></div></div></figure><p id="d02c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意力块被称为多头注意力块，因为我们对每个单词使用多个注意力向量，然后进行加权平均。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/27a00ba4a88c74571cb484fd7cd39d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*qvaRWPdHPao_6ABM7DzMWA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="8731" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个注意力向量都是相互独立的，这允许我们使用并行化。还记得GPU吗？</p><p id="32d8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们有一个<strong class="ki jk"> <em class="mv">前馈网络</em> </strong> (FFN)。这是一个应用于每个注意力向量的常规前馈网络。应用FFN，使得输出可以被下一个编码器块或解码器块使用。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b21ed815b3af6f25c0e6d2aa8aa0d689.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*6cgcjaLV8vehGtrqo_p-Lw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="3edd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个FFN由两个密集的线性层组成，其间有ReLU激活。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d6c55e331e7504c43112848698d4c5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*Gjy_IcMdxrTaosfx19ZYsQ.png"/></div></figure><p id="5452" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于FFN层的几个关键点</p><ul class=""><li id="1f33" class="lj lk jj ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">FFN分别应用于每个位置，并且完全相同。</li><li id="acc5" class="lj lk jj ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">每个子层的FFN不同。</li></ul><p id="3b43" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们有一个<strong class="ki jk"> <em class="mv">添加&amp;归一化层</em> </strong>应用在每个关注块和每个FFN块之后。FFN层对输出进行归一化，并在通过剩余连接进行反向传播的过程中帮助学习。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/36326768b68d96ff8afc021b4de3f282.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*yUjG5Jro6_iz1YPbQhB5vQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="6f95" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="mv">解码器模块</em> </strong>的工作方式类似。我们传入在嵌入中编码的单词的目标序列以及位置编码。</p><p id="9324" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器的自关注模块为目标序列生成关注向量，以找出目标序列中的每个单词与序列中的其他单词的相关程度。解码器的第一个关注块被称为<strong class="ki jk"> <em class="mv">掩蔽关注块</em> </strong>，这是因为我们对该块应用了掩蔽层。这确保了在为目标序列生成注意力向量时，我们可以使用输入序列中的所有单词，但只能使用目标序列的前一个单词。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/afb0fae9099558e5eb08dbee2820ad57.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*XOjsPoCnjgHtnxM8H-VBow.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:瓦斯瓦尼等人。</p></figure><p id="77f0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器具有<strong class="ki jk"> <em class="mv">额外的关注块</em> </strong>，其从输入序列和目标序列中获取嵌入，以确定输入序列中的每个单词如何与目标序列中的每个单词相关。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ac7e43a7e953f7e76e8d2fe8d868c3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*CgZDvytjdzKw0laEGSpRgA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="1643" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二关注层的输出被发送到FFN层，该层类似于具有类似功能的编码器块的FFN层。</p><p id="29a4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，在最后，我们有一个线性层，这只是另一个FFN和一个softmax函数来获得所有接下来的单词的概率分布，因此，下一个预测的单词具有最高的概率得分。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b98d70fbdefa97198717fda00b706856.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*60kES9lm-psTrIuu8yW3jA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人</a>。</p></figure><p id="5f9b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个过程被执行多次，直到为序列生成句子结束标记。</p><p id="0347" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章给了你NLP转换架构背后的基本直觉。也可以在这里阅读原论文<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="0806" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想亲手实现变形金刚，TensorFlow有一个很棒的分步<a class="ae jg" href="https://www.tensorflow.org/tutorials/text/transformer" rel="noopener ugc nofollow" target="_blank">教程</a>。</p></div></div>    
</body>
</html>