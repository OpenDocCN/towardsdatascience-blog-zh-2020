<html>
<head>
<title>LightGBM for Quantile Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分位数回归的LightGBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lightgbm-for-quantile-regression-4288d0bb23fd?source=collection_archive---------11-----------------------#2020-10-03">https://towardsdatascience.com/lightgbm-for-quantile-regression-4288d0bb23fd?source=collection_archive---------11-----------------------#2020-10-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="22fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解分位数回归</h2></div><p id="cca4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于回归预测任务，并非所有时候我们都只追求绝对准确的预测，事实上，我们的预测总是不准确的，因此，我们不寻求绝对精度，有时需要预测区间，在这种情况下，我们需要分位数回归，即我们预测目标的区间估计。</p><h1 id="4aef" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">损失函数</h1><p id="b66a" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">幸运的是，强大的<code class="fe mb mc md me b">lightGBM</code>使分位数预测成为可能，分位数回归与一般回归的主要区别在于损失函数，这被称为弹球损失或分位数损失。弹球损失<a class="ae mf" href="https://www.lokad.com/pinball-loss-function-definition#:~:text=The%20pinball%20loss%20function%20is,forecast%20is%20a%20subtle%20problem." rel="noopener ugc nofollow" target="_blank">在这里</a>有一个很好的解释，它有公式:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/9ba6388b1c51fdd518d940792da61f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*cN7H6267vrK20XP23Jtziw.png"/></div></figure><p id="b95f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mb mc md me b">y</code>是实际值，<code class="fe mb mc md me b">z</code>是预测值，𝛕是目标分位数。所以第一眼看到损失函数，我们可以看到，除了当分位数等于0.5，损失函数是不对称的。让我们来看看:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/599ed705254e0a12b8aa42688c55cd41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uScZoWqDRwCFgk6TkTwWvg.png"/></div></div></figure><p id="7631" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实现可以在我的<a class="ae mf" href="https://github.com/MJeremy2017/Machine-Learning-Models/blob/master/quantile-regression/lgb-quantile-regression.ipynb" rel="noopener ugc nofollow" target="_blank"> Git Repo </a>上找到。在图中，绘制了三个不同的分位数，以分位数0.8为例，当误差为正时(<code class="fe mb mc md me b">z &gt; y</code> —预测值高于实际值)，损失小于误差为负时的损失。在另一个世界中，更高的误差受到更少的惩罚，这是有意义的，因为<strong class="kk iu">对于高分位数预测，损失函数鼓励更高的预测值，反之对于低分位数预测</strong>。</p><h1 id="7809" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">生成样本数据集</h1><p id="79d1" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">现在让我们为lightGBM预测生成一些数据。</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mv"><img src="../Images/1360c70543903ba0cf7e15a32cc1573f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hCJPbk2CDPQh8gBqVoDsVQ.png"/></div></div></figure><p id="9083" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们使用一个sin(x)函数和一些额外的噪声作为训练集。</p><h1 id="4ba4" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">LightGBM预测</h1><p id="a08c" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">发起<code class="fe mb mc md me b">LGMRegressor</code>:</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="3242" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意和一般回归不同的是，<code class="fe mb mc md me b">objective</code>和<code class="fe mb mc md me b">metric</code>都是<code class="fe mb mc md me b">quantile</code>，<code class="fe mb mc md me b">alpha</code>是我们需要预测的分位数(详情可以查看我的<a class="ae mf" href="https://github.com/MJeremy2017/Machine-Learning-Models/blob/master/quantile-regression/lgb-quantile-regression.ipynb" rel="noopener ugc nofollow" target="_blank"> Repo </a>)。</p><h2 id="137a" class="mw lf it bd lg mx my dn lk mz na dp lo kr nb nc lq kv nd ne ls kz nf ng lu nh bi translated">预测可视化</h2><p id="c73f" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">现在让我们看看分位数预测结果:</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mv"><img src="../Images/36c23cf06f51b8ced66b128a452652f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lD5X0ecI8yYHZJoMVL_GyA.png"/></div></div></figure><p id="45fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，大多数噪声点位于预测范围内，其中绿线是0.9分位数的上限，蓝色是0.1分位数。</p><p id="62fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章最初是受<a class="ae mf" href="http://ethen8181.github.io/machine-learning/ab_tests/quantile_regression/quantile_regression.html" rel="noopener ugc nofollow" target="_blank"> this </a>的启发，这是一个很好的起点分位数回归启动器。</p></div></div>    
</body>
</html>