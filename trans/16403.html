<html>
<head>
<title>Fine-tuning a BERT model with transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用变压器微调BERT模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b?source=collection_archive---------8-----------------------#2020-11-12">https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b?source=collection_archive---------8-----------------------#2020-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eac6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">设置自定义数据集，用Transformers Trainer微调BERT，并通过ONNX导出模型</h2></div><p id="cd0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章描述了一个开始微调变压器模型的简单方法。它将涵盖基础知识，并向您介绍来自<code class="fe lb lc ld le b">transformers</code>库的惊人的<code class="fe lb lc ld le b">Trainer</code>类。你可以从<a class="ae lf" href="https://colab.research.google.com/github/thigm85/blog/blob/master/_notebooks/2020-11-12-fine-tune-bert-basic-transformers-trainer.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>运行代码，但是不要忘记启用GPU支持。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/095c0c0558640c517da5175e001a37b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CTI2sk66EK5Q4JSXiqnOTQ.jpeg"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@samule?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae lf" href="https://unsplash.com/s/photos/transformers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b05c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用从<a class="ae lf" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank">新冠肺炎公开研究数据集挑战赛</a>构建的数据集。这项工作是一个更大的项目的一小部分，该项目是建立<a class="ae lf" href="https://cord19.vespa.ai/" rel="noopener ugc nofollow" target="_blank"> cord19搜索应用</a>。</p><h2 id="1c33" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">安装所需的库</h2><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="6f3b" class="lw lx iq le b gy mt mu l mv mw">!pip install pandas transformers</span></pre><h2 id="a054" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">加载数据集</h2><p id="824f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">为了微调cord19应用程序的BERT模型，我们需要生成一组查询文档特征和标签，以指示哪些文档与特定查询相关。在本练习中，我们将使用<code class="fe lb lc ld le b">query</code>字符串表示查询，使用<code class="fe lb lc ld le b">title</code>字符串表示文档。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="6683" class="lw lx iq le b gy mt mu l mv mw">training_data = read_csv("https://thigm85.github.io/data/cord19/cord19-query-title-label.csv")<br/>training_data.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/f60fc903e5f16d549486026ba1663c22.png" data-original-src="https://miro.medium.com/v2/format:webp/1*qhS2TgX5CPca6WQb-JXQ5w.png"/></div></figure><p id="3df6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有50个唯一的查询。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="ccd2" class="lw lx iq le b gy mt mu l mv mw">len(training_data["query"].unique())</span><span id="fd91" class="lw lx iq le b gy nd mu l mv mw">50</span></pre><p id="3eda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每个查询，我们都有一个文档列表，分为相关(<code class="fe lb lc ld le b">label=1</code>)和不相关(<code class="fe lb lc ld le b">label=0</code>)。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="be7b" class="lw lx iq le b gy mt mu l mv mw">training_data[["title", "label"]].groupby("label").count()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/0cd780c37ee91c06f727e17443d4e10c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fU0V3xZBVpyt-5Q10Iu7jw.png"/></div></figure><h2 id="33f0" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">数据分割</h2><p id="86c2" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">为了便于说明，我们将使用一个简单的数据划分为训练集和验证集。即使我们在考虑独特的查询和文档对时有超过5万个数据点，我相信这个特定的案例将受益于交叉验证，因为它只有50个包含相关性判断的查询。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="9bd8" class="lw lx iq le b gy mt mu l mv mw">from sklearn.model_selection import train_test_split<br/>train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(<br/>    training_data["query"].tolist(), <br/>    training_data["title"].tolist(), <br/>    training_data["label"].tolist(), <br/>    test_size=.2<br/>)</span></pre><h2 id="5e8a" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">创建BERT编码</h2><p id="9863" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">创建训练和验证编码。为此，我们需要选择<a class="ae lf" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">使用哪个BERT模型</a>。我们将使用<a class="ae lf" href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation" rel="noopener ugc nofollow" target="_blank">填充和截断</a>，因为训练例程期望一批中的所有张量具有相同的维数。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="cf02" class="lw lx iq le b gy mt mu l mv mw">from transformers import BertTokenizerFast<br/><br/>model_name = "google/bert_uncased_L-4_H-512_A-8"<br/>tokenizer = BertTokenizerFast.from_pretrained(model_name)<br/><br/>train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding='max_length', max_length=128)<br/>val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding='max_length', max_length=128)</span></pre><h2 id="0e16" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">创建自定义数据集</h2><p id="2b3a" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">现在我们有了编码和标签，我们可以创建一个<code class="fe lb lc ld le b">Dataset</code>对象，如变形金刚网页中关于<a class="ae lf" href="https://huggingface.co/transformers/custom_datasets.html" rel="noopener ugc nofollow" target="_blank">自定义数据集</a>的描述。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="0aa4" class="lw lx iq le b gy mt mu l mv mw">import torch<br/><br/>class Cord19Dataset(torch.utils.data.Dataset):<br/>    def __init__(self, encodings, labels):<br/>        self.encodings = encodings<br/>        self.labels = labels<br/><br/>    def __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}<br/>        item['labels'] = torch.tensor(self.labels[idx])<br/>        return item<br/><br/>    def __len__(self):<br/>        return len(self.labels)<br/><br/>train_dataset = Cord19Dataset(train_encodings, train_labels)<br/>val_dataset = Cord19Dataset(val_encodings, val_labels)</span></pre><h2 id="bd27" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">微调BERT模型</h2><p id="2e8d" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">我们将使用<code class="fe lb lc ld le b">BertForSequenceClassification</code>，因为我们试图将查询和文档对分为两个不同的类别(不相关、相关)。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="5c95" class="lw lx iq le b gy mt mu l mv mw">from transformers import BertForSequenceClassification<br/><br/>model = BertForSequenceClassification.from_pretrained(model_name)</span></pre><p id="fe6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以将所有基本模型参数的<code class="fe lb lc ld le b">requires_grad</code>设置为<code class="fe lb lc ld le b">False</code>，以便仅微调特定于任务的参数。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="71b6" class="lw lx iq le b gy mt mu l mv mw">for param in model.base_model.parameters():<br/>    param.requires_grad = False</span></pre><p id="3619" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们可以用<code class="fe lb lc ld le b">Trainer</code>微调模型。下面是一个带有一组现成参数的基本例程。选择下面的参数时应该小心，但这超出了本文的范围。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="00ae" class="lw lx iq le b gy mt mu l mv mw">from transformers import Trainer, TrainingArguments<br/><br/>training_args = TrainingArguments(<br/>    output_dir='./results',          # output directory<br/>    evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.<br/>    num_train_epochs=3,              # total number of training epochs<br/>    per_device_train_batch_size=16,  # batch size per device during training<br/>    per_device_eval_batch_size=64,   # batch size for evaluation<br/>    warmup_steps=500,                # number of warmup steps for learning rate scheduler<br/>    weight_decay=0.01,               # strength of weight decay<br/>    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    <br/>)<br/><br/><br/>trainer = Trainer(<br/>    model=model,                         # the instantiated 🤗 Transformers model to be trained<br/>    args=training_args,                  # training arguments, defined above<br/>    train_dataset=train_dataset,         # training dataset<br/>    eval_dataset=val_dataset             # evaluation dataset<br/>)<br/><br/>trainer.train()</span></pre><h2 id="4eff" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">将模型导出到ONNX</h2><p id="939f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">一旦训练完成，我们可以使用<a class="ae lf" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>格式导出模型，以部署到其他地方。下面我假设你有一个GPU，比如你可以从Google Colab获得。</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="e8b6" class="lw lx iq le b gy mt mu l mv mw">from torch.onnx import export<br/><br/>device = torch.device("cuda") <br/><br/>model_onnx_path = "model.onnx"<br/>dummy_input = (<br/>    train_dataset[0]["input_ids"].unsqueeze(0).to(device), <br/>    train_dataset[0]["token_type_ids"].unsqueeze(0).to(device), <br/>    train_dataset[0]["attention_mask"].unsqueeze(0).to(device)<br/>)<br/>input_names = ["input_ids", "token_type_ids", "attention_mask"]<br/>output_names = ["logits"]<br/>export(<br/>    model, dummy_input, model_onnx_path, input_names = input_names, <br/>    output_names = output_names, verbose=False, opset_version=11<br/>)</span></pre><h2 id="d3f8" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">结束语</h2><p id="e501" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">如前所述，这篇文章涵盖了基本的培训设置。这是一个需要改进的良好起点。最好从简单的开始，然后补充，而不是相反，尤其是在学习新东西的时候。我将超参数调优、交叉验证和更详细的模型验证等重要主题留到后续文章中。但是有一个基本的训练设置是一个很好的第一步。</p></div></div>    
</body>
</html>