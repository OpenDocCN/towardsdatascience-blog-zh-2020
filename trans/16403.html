<html>
<head>
<title>Fine-tuning a BERT model with transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨å˜å‹å™¨å¾®è°ƒBERTæ¨¡å‹</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b?source=collection_archive---------8-----------------------#2020-11-12">https://towardsdatascience.com/fine-tuning-a-bert-model-with-transformers-c8e49c4e008b?source=collection_archive---------8-----------------------#2020-11-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eac6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">è®¾ç½®è‡ªå®šä¹‰æ•°æ®é›†ï¼Œç”¨Transformers Trainerå¾®è°ƒBERTï¼Œå¹¶é€šè¿‡ONNXå¯¼å‡ºæ¨¡å‹</h2></div><p id="cd0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">è¿™ç¯‡æ–‡ç« æè¿°äº†ä¸€ä¸ªå¼€å§‹å¾®è°ƒå˜å‹å™¨æ¨¡å‹çš„ç®€å•æ–¹æ³•ã€‚å®ƒå°†æ¶µç›–åŸºç¡€çŸ¥è¯†ï¼Œå¹¶å‘æ‚¨ä»‹ç»æ¥è‡ª<code class="fe lb lc ld le b">transformers</code>åº“çš„æƒŠäººçš„<code class="fe lb lc ld le b">Trainer</code>ç±»ã€‚ä½ å¯ä»¥ä»<a class="ae lf" href="https://colab.research.google.com/github/thigm85/blog/blob/master/_notebooks/2020-11-12-fine-tune-bert-basic-transformers-trainer.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>è¿è¡Œä»£ç ï¼Œä½†æ˜¯ä¸è¦å¿˜è®°å¯ç”¨GPUæ”¯æŒã€‚</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/095c0c0558640c517da5175e001a37b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CTI2sk66EK5Q4JSXiqnOTQ.jpeg"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated"><a class="ae lf" href="https://unsplash.com/@samule?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Samuleå­™</a>åœ¨<a class="ae lf" href="https://unsplash.com/s/photos/transformers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>ä¸Šçš„ç…§ç‰‡</p></figure><p id="b05c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬ä½¿ç”¨ä»<a class="ae lf" href="https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge" rel="noopener ugc nofollow" target="_blank">æ–°å† è‚ºç‚å…¬å¼€ç ”ç©¶æ•°æ®é›†æŒ‘æˆ˜èµ›</a>æ„å»ºçš„æ•°æ®é›†ã€‚è¿™é¡¹å·¥ä½œæ˜¯ä¸€ä¸ªæ›´å¤§çš„é¡¹ç›®çš„ä¸€å°éƒ¨åˆ†ï¼Œè¯¥é¡¹ç›®æ˜¯å»ºç«‹<a class="ae lf" href="https://cord19.vespa.ai/" rel="noopener ugc nofollow" target="_blank"> cord19æœç´¢åº”ç”¨</a>ã€‚</p><h2 id="1c33" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">å®‰è£…æ‰€éœ€çš„åº“</h2><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="6f3b" class="lw lx iq le b gy mt mu l mv mw">!pip install pandas transformers</span></pre><h2 id="a054" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">åŠ è½½æ•°æ®é›†</h2><p id="824f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">ä¸ºäº†å¾®è°ƒcord19åº”ç”¨ç¨‹åºçš„BERTæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ç»„æŸ¥è¯¢æ–‡æ¡£ç‰¹å¾å’Œæ ‡ç­¾ï¼Œä»¥æŒ‡ç¤ºå“ªäº›æ–‡æ¡£ä¸ç‰¹å®šæŸ¥è¯¢ç›¸å…³ã€‚åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe lb lc ld le b">query</code>å­—ç¬¦ä¸²è¡¨ç¤ºæŸ¥è¯¢ï¼Œä½¿ç”¨<code class="fe lb lc ld le b">title</code>å­—ç¬¦ä¸²è¡¨ç¤ºæ–‡æ¡£ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="6683" class="lw lx iq le b gy mt mu l mv mw">training_data = read_csv("https://thigm85.github.io/data/cord19/cord19-query-title-label.csv")<br/>training_data.head()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/f60fc903e5f16d549486026ba1663c22.png" data-original-src="https://miro.medium.com/v2/format:webp/1*qhS2TgX5CPca6WQb-JXQ5w.png"/></div></figure><p id="3df6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æœ‰50ä¸ªå”¯ä¸€çš„æŸ¥è¯¢ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="ccd2" class="lw lx iq le b gy mt mu l mv mw">len(training_data["query"].unique())</span><span id="fd91" class="lw lx iq le b gy nd mu l mv mw">50</span></pre><p id="3eda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œåˆ†ä¸ºç›¸å…³(<code class="fe lb lc ld le b">label=1</code>)å’Œä¸ç›¸å…³(<code class="fe lb lc ld le b">label=0</code>)ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="be7b" class="lw lx iq le b gy mt mu l mv mw">training_data[["title", "label"]].groupby("label").count()</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/0cd780c37ee91c06f727e17443d4e10c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fU0V3xZBVpyt-5Q10Iu7jw.png"/></div></figure><h2 id="33f0" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">æ•°æ®åˆ†å‰²</h2><p id="86c2" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">ä¸ºäº†ä¾¿äºè¯´æ˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚å³ä½¿æˆ‘ä»¬åœ¨è€ƒè™‘ç‹¬ç‰¹çš„æŸ¥è¯¢å’Œæ–‡æ¡£å¯¹æ—¶æœ‰è¶…è¿‡5ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œæˆ‘ç›¸ä¿¡è¿™ä¸ªç‰¹å®šçš„æ¡ˆä¾‹å°†å—ç›Šäºäº¤å‰éªŒè¯ï¼Œå› ä¸ºå®ƒåªæœ‰50ä¸ªåŒ…å«ç›¸å…³æ€§åˆ¤æ–­çš„æŸ¥è¯¢ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="9bd8" class="lw lx iq le b gy mt mu l mv mw">from sklearn.model_selection import train_test_split<br/>train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(<br/>    training_data["query"].tolist(), <br/>    training_data["title"].tolist(), <br/>    training_data["label"].tolist(), <br/>    test_size=.2<br/>)</span></pre><h2 id="5e8a" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">åˆ›å»ºBERTç¼–ç </h2><p id="9863" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ç¼–ç ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©<a class="ae lf" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">ä½¿ç”¨å“ªä¸ªBERTæ¨¡å‹</a>ã€‚æˆ‘ä»¬å°†ä½¿ç”¨<a class="ae lf" href="https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation" rel="noopener ugc nofollow" target="_blank">å¡«å……å’Œæˆªæ–­</a>ï¼Œå› ä¸ºè®­ç»ƒä¾‹ç¨‹æœŸæœ›ä¸€æ‰¹ä¸­çš„æ‰€æœ‰å¼ é‡å…·æœ‰ç›¸åŒçš„ç»´æ•°ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="cf02" class="lw lx iq le b gy mt mu l mv mw">from transformers import BertTokenizerFast<br/><br/>model_name = "google/bert_uncased_L-4_H-512_A-8"<br/>tokenizer = BertTokenizerFast.from_pretrained(model_name)<br/><br/>train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding='max_length', max_length=128)<br/>val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding='max_length', max_length=128)</span></pre><h2 id="0e16" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†</h2><p id="2b3a" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">ç°åœ¨æˆ‘ä»¬æœ‰äº†ç¼–ç å’Œæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª<code class="fe lb lc ld le b">Dataset</code>å¯¹è±¡ï¼Œå¦‚å˜å½¢é‡‘åˆšç½‘é¡µä¸­å…³äº<a class="ae lf" href="https://huggingface.co/transformers/custom_datasets.html" rel="noopener ugc nofollow" target="_blank">è‡ªå®šä¹‰æ•°æ®é›†</a>çš„æè¿°ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="0aa4" class="lw lx iq le b gy mt mu l mv mw">import torch<br/><br/>class Cord19Dataset(torch.utils.data.Dataset):<br/>    def __init__(self, encodings, labels):<br/>        self.encodings = encodings<br/>        self.labels = labels<br/><br/>    def __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}<br/>        item['labels'] = torch.tensor(self.labels[idx])<br/>        return item<br/><br/>    def __len__(self):<br/>        return len(self.labels)<br/><br/>train_dataset = Cord19Dataset(train_encodings, train_labels)<br/>val_dataset = Cord19Dataset(val_encodings, val_labels)</span></pre><h2 id="bd27" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">å¾®è°ƒBERTæ¨¡å‹</h2><p id="2e8d" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">æˆ‘ä»¬å°†ä½¿ç”¨<code class="fe lb lc ld le b">BertForSequenceClassification</code>ï¼Œå› ä¸ºæˆ‘ä»¬è¯•å›¾å°†æŸ¥è¯¢å’Œæ–‡æ¡£å¯¹åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„ç±»åˆ«(ä¸ç›¸å…³ã€ç›¸å…³)ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="5c95" class="lw lx iq le b gy mt mu l mv mw">from transformers import BertForSequenceClassification<br/><br/>model = BertForSequenceClassification.from_pretrained(model_name)</span></pre><p id="fe6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰åŸºæœ¬æ¨¡å‹å‚æ•°çš„<code class="fe lb lc ld le b">requires_grad</code>è®¾ç½®ä¸º<code class="fe lb lc ld le b">False</code>ï¼Œä»¥ä¾¿ä»…å¾®è°ƒç‰¹å®šäºä»»åŠ¡çš„å‚æ•°ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="71b6" class="lw lx iq le b gy mt mu l mv mw">for param in model.base_model.parameters():<br/>    param.requires_grad = False</span></pre><p id="3619" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ç„¶åæˆ‘ä»¬å¯ä»¥ç”¨<code class="fe lb lc ld le b">Trainer</code>å¾®è°ƒæ¨¡å‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸€ç»„ç°æˆå‚æ•°çš„åŸºæœ¬ä¾‹ç¨‹ã€‚é€‰æ‹©ä¸‹é¢çš„å‚æ•°æ—¶åº”è¯¥å°å¿ƒï¼Œä½†è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="00ae" class="lw lx iq le b gy mt mu l mv mw">from transformers import Trainer, TrainingArguments<br/><br/>training_args = TrainingArguments(<br/>    output_dir='./results',          # output directory<br/>    evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.<br/>    num_train_epochs=3,              # total number of training epochs<br/>    per_device_train_batch_size=16,  # batch size per device during training<br/>    per_device_eval_batch_size=64,   # batch size for evaluation<br/>    warmup_steps=500,                # number of warmup steps for learning rate scheduler<br/>    weight_decay=0.01,               # strength of weight decay<br/>    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    <br/>)<br/><br/><br/>trainer = Trainer(<br/>    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained<br/>    args=training_args,                  # training arguments, defined above<br/>    train_dataset=train_dataset,         # training dataset<br/>    eval_dataset=val_dataset             # evaluation dataset<br/>)<br/><br/>trainer.train()</span></pre><h2 id="4eff" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">å°†æ¨¡å‹å¯¼å‡ºåˆ°ONNX</h2><p id="939f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<a class="ae lf" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>æ ¼å¼å¯¼å‡ºæ¨¡å‹ï¼Œä»¥éƒ¨ç½²åˆ°å…¶ä»–åœ°æ–¹ã€‚ä¸‹é¢æˆ‘å‡è®¾ä½ æœ‰ä¸€ä¸ªGPUï¼Œæ¯”å¦‚ä½ å¯ä»¥ä»Google Colabè·å¾—ã€‚</p><pre class="lh li lj lk gt mp le mq mr aw ms bi"><span id="e8b6" class="lw lx iq le b gy mt mu l mv mw">from torch.onnx import export<br/><br/>device = torch.device("cuda") <br/><br/>model_onnx_path = "model.onnx"<br/>dummy_input = (<br/>    train_dataset[0]["input_ids"].unsqueeze(0).to(device), <br/>    train_dataset[0]["token_type_ids"].unsqueeze(0).to(device), <br/>    train_dataset[0]["attention_mask"].unsqueeze(0).to(device)<br/>)<br/>input_names = ["input_ids", "token_type_ids", "attention_mask"]<br/>output_names = ["logits"]<br/>export(<br/>    model, dummy_input, model_onnx_path, input_names = input_names, <br/>    output_names = output_names, verbose=False, opset_version=11<br/>)</span></pre><h2 id="d3f8" class="lw lx iq bd ly lz ma dn mb mc md dp me ko mf mg mh ks mi mj mk kw ml mm mn mo bi translated">ç»“æŸè¯­</h2><p id="e501" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">å¦‚å‰æ‰€è¿°ï¼Œè¿™ç¯‡æ–‡ç« æ¶µç›–äº†åŸºæœ¬çš„åŸ¹è®­è®¾ç½®ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ”¹è¿›çš„è‰¯å¥½èµ·ç‚¹ã€‚æœ€å¥½ä»ç®€å•çš„å¼€å§‹ï¼Œç„¶åè¡¥å……ï¼Œè€Œä¸æ˜¯ç›¸åï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¹ æ–°ä¸œè¥¿çš„æ—¶å€™ã€‚æˆ‘å°†è¶…å‚æ•°è°ƒä¼˜ã€äº¤å‰éªŒè¯å’Œæ›´è¯¦ç»†çš„æ¨¡å‹éªŒè¯ç­‰é‡è¦ä¸»é¢˜ç•™åˆ°åç»­æ–‡ç« ä¸­ã€‚ä½†æ˜¯æœ‰ä¸€ä¸ªåŸºæœ¬çš„è®­ç»ƒè®¾ç½®æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥ã€‚</p></div></div>    
</body>
</html>