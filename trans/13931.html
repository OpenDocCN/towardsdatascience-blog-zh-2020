<html>
<head>
<title>Machine Learning With R: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带R的机器学习:线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-with-r-linear-regression-558fa2edaaf0?source=collection_archive---------10-----------------------#2020-09-25">https://towardsdatascience.com/machine-learning-with-r-linear-regression-558fa2edaaf0?source=collection_archive---------10-----------------------#2020-09-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="99de" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习基础和r .和一堆其他的东西。</h2></div><p id="3195" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我决定用<em class="le"> R </em>开始关于机器学习的整个系列。不，这并不意味着我要退出<em class="le"> Python </em>(但愿不会如此)，但我最近一直在探索<em class="le"> R </em>，它并没有我最初想象的那么糟糕。所以，让我们从基础开始——线性回归。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/24bebe2d21290a13d8f2049932a8ccf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q9tYr6MATpfiLQNE"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae lv" href="https://unsplash.com/@nathananderson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内森·安德森</a>在<a class="ae lv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e28e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想知道我对编程语言的最初印象，这里有几周前的一篇文章:</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/ive-tried-r-for-the-first-time-how-bad-was-it-ba344f22e90b"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">我第一次尝试R——有多糟糕？</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">空前的Pythonista尝试R——包括与Python的比较。</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn lp lz"/></div></div></a></div><p id="846c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这应该是一个有趣的系列。我打算涵盖所有主要的机器学习算法，将“奇怪”的部分与其<em class="le"> Python </em>替代方案进行比较，并在这个过程中学习很多东西。</p><p id="652a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的结构如下:</p><ul class=""><li id="2f14" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated">线性回归简介</li><li id="af50" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">数据集介绍和加载</li><li id="9e38" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">基础EDA</li><li id="58de" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">模型训练和评估</li><li id="e1c2" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated">结论</li></ul><p id="0ba8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你更喜欢视频，我可以帮你搞定:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="233d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有，你可以在这里获得源代码<a class="ae lv" href="https://github.com/betterdatascience/MachineLearningWithR" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="8409" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有很多东西要讲，所以让我们马上开始吧！</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="726c" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">线性回归简介</h1><p id="c056" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">我会抓住机会说，这可能不是你第一次接触线性回归。这是不应该的，因为这篇文章不会在理论上做太多的深入。不过，我们将从算法的高级概述开始。</p><p id="3402" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性回归是最初在<strong class="kk iu">统计</strong>领域开发的简单算法。它被作为理解输入和输出变量之间关系的模型来研究。顾名思义，它是一个线性模型，所以它假设输入变量和单个(连续)输出变量之间是线性关系。</p><p id="cd9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该输出变量被计算为输入变量的线性组合。</p><p id="e8d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">存在两种主要类型的线性回归:</p><ul class=""><li id="e585" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="le">简单线性回归</em> —当我们只有一个输入变量时</li><li id="9288" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="le">多元线性回归</em> —当有多个输入变量时</li></ul><p id="0fd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练线性回归模型实质上是为每个输入变量添加一个系数，该系数决定了输入变量的重要性。然后将输入变量的值乘以相应的系数，并将偏差(截距)项添加到总和中。这基本上是我们的预测值。</p><p id="f6a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">厉害！线性回归有一些假设，我们作为数据科学家必须意识到它们:</p><ul class=""><li id="3a5a" class="mo mp it kk b kl km ko kp kr mq kv mr kz ms ld mt mu mv mw bi translated"><em class="le">线性假设</em> —模型假设变量之间的关系是线性的</li><li id="a27a" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="le">无噪声</em> —模型假设输入和输出变量没有噪声——因此如果可能的话移除异常值</li><li id="ed5d" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="le">无共线性</em>-当输入变量高度相关时，模型会过度拟合</li><li id="be9b" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="le">正态分布</em>-如果输入和输出变量呈正态分布，模型将做出更可靠的预测。如果不是这样，试着对你的变量进行一些变换，使它们看起来更正常</li><li id="5df8" class="mo mp it kk b kl mx ko my kr mz kv na kz nb ld mt mu mv mw bi translated"><em class="le">重定标输入</em> —使用定标器或规格化器进行更可靠的预测</li></ul><p id="f116" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是高级概述。现在让我们继续好东西。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="24f9" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">数据集介绍和加载</h1><p id="72f4" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">我们将使用<a class="ae lv" href="https://www.kaggle.com/aungpyaeap/fish-market" rel="noopener ugc nofollow" target="_blank">鱼市场</a>数据集来构建我们的模型。是的，我也是第一次用，所以你不是一个人。下载数据集并将其存储在友好的位置。</p><p id="5e76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以从进口开始:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="701a" class="on nm it oj b gy oo op l oq or">library(dplyr)<br/>library(ggplot2)<br/>library(caTools)<br/>library(corrgram)</span></pre><p id="beb8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之——<code class="fe os ot ou oj b">dplyr</code>用于数据操作，<code class="fe os ot ou oj b">ggplot2</code>用于可视化，<code class="fe os ot ou oj b">caTools</code>用于训练/测试分割，<code class="fe os ot ou oj b">corrgram</code>用于制作简洁的相关矩阵图。</p><p id="aa09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以读入数据集，并检查前几行看起来如何:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="74b3" class="on nm it oj b gy oo op l oq or">df &lt;- read.csv('data/Fish.csv')<br/>head(df)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/5470168df7c55e10fc6a5e22bbbc7d99.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UPYKKi5Jv1mJkxbhGvJSPQ.png"/></div></figure><p id="d172" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">厉害！一切似乎都很好，所以我们可以继续进行基本的探索性数据分析。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="ada1" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">基础EDA</h1><p id="b1d2" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">我们需要做的第一件事是检查丢失的值。这个过程和使用<em class="le"> Python </em>一样简单:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="1d7c" class="on nm it oj b gy oo op l oq or">any(is.na(df))</span></pre><p id="e298" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行这段代码会在控制台中产生一个大写的<em class="le"> FALSE </em>，表示没有丢失值——因此我们可以继续进行分析。</p><p id="a348" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来让我们做几个可视化。没什么复杂的，只是感受一下我们的数据集的行为。第一个可视化是鱼的重量与高度的散点图，用鱼的种类来着色。下面的代码片段实现了这个目的:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="7e2d" class="on nm it oj b gy oo op l oq or">ggplot(data=df, aes(x=Weight, y=Height)) +<br/>  geom_point(aes(color=Species, size=10, alpha=0.7))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/0ea79d1c8eb12b81a3ad336abfba8a62.png" data-original-src="https://miro.medium.com/v2/format:webp/1*iDoBqL-inpJIx87cpGQ1Og.png"/></div></figure><p id="d852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅仅从颜色上，我们可以看到鱼的种类被很好的分开了(在大多数情况下)。接下来，让我们检查属性之间的<strong class="kk iu">相关性</strong>。在这里，corrgram库就派上了用场。代码如下:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="9080" class="on nm it oj b gy oo op l oq or">corrgram(df, lower.panel=panel.shade, upper.panel=panel.cor)</span></pre><p id="213a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">情节是这样的:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/37e13399947cb6063d24d154829a8f05.png" data-original-src="https://miro.medium.com/v2/format:webp/1*u3esAlkqVd72PYrgNOuEPw.png"/></div></figure><p id="eba0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">几乎所有属性之间的相关性都高得惊人。但是我们不会做任何事情，因为本文的目的是讨论线性回归，而不是探索性的数据分析。</p><p id="f420" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请记住——这最有可能导致模型<strong class="kk iu">过度拟合</strong>，但稍后会有更多相关内容。</p><p id="66fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">EDA部分到此结束，我们可以继续实际建模。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4bf6" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">模型训练和评估</h1><p id="6de1" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">这将是迄今为止最长的部分，所以给自己一杯咖啡。我们将从培训/测试拆分开始。我们希望将数据集分成两部分，一部分(较大)用于训练模型，另一部分(较小)用于模型评估。</p><p id="a8f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们做任何事情之前，让我们设置一个随机种子。训练/测试分割是一个随机过程，seed确保随机化在您和我的计算机上的效果相同:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="c642" class="on nm it oj b gy oo op l oq or">set.seed(42)</span></pre><p id="afe6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！让我们现在进行分割。70%的数据用于训练，剩下的30%用于测试。代码如下:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="602b" class="on nm it oj b gy oo op l oq or">sampleSplit &lt;- sample.split(Y=df$Weight, SplitRatio=0.7)<br/>trainSet &lt;- subset(x=df, sampleSplit==TRUE)<br/>testSet &lt;- subset(x=df, sampleSplit==FALSE)</span></pre><p id="cc1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">执行代码后，您应该会在右上面板中看到另外两个变量:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/b3716f59a89d1045c55d120336b8dd75.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QvvilL6VsUCQvX3diENBWA.png"/></div></figure><p id="f650" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们总共有159行，其中111行分配给模型训练，其余48行用于测试模型。</p><p id="3ae4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以进行模型训练了。</p><p id="ed90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le"> R </em>对线性回归模型使用以下语法:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="b6a1" class="on nm it oj b gy oo op l oq or">model &lt;- lm(target ~ var_1 + var_2 + ... + var_n, data=train_set)</span></pre><p id="1ec2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">没关系，但是想象一下我们有100个预测值，那么把每一个都写到等式里将是一场噩梦。相反，我们可以使用以下语法:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="ee32" class="on nm it oj b gy oo op l oq or">model &lt;- lm(target ~. , data=train_set)</span></pre><p id="8714" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请记住，这只有在您决定使用所有预测器进行模型训练时才有效。相应地，我们可以这样训练模型:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="451c" class="on nm it oj b gy oo op l oq or">model &lt;- lm(formula=Weight ~ ., data=trainSet)</span></pre><p id="cc4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，我们试图将权重属性预测为所有其他属性的线性组合。r也自动处理分类属性。接招吧，蟒蛇！</p><p id="c73e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们可以看看我们模型的总结:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="19fd" class="on nm it oj b gy oo op l oq or">summary(model)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/be691b2be1aa974a077a99385fec6336.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ebCGzbYkIWhk-8gfk0fqqA.png"/></div></figure><p id="23c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里最有趣的是P值，显示在<code class="fe os ot ou oj b">Pr(&gt;|t|)</code>列中。这些值显示变量对预测不重要的概率。通常使用5%的显著性阈值，因此如果P值为0.05或更低，我们可以说它对分析不显著的可能性很低。抱歉否定，假设就是这么形成的。</p><p id="7d58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">厉害！接下来，我们可以制作一个残差图，或者更精确的残差直方图。这里我们期望看到近似正态分布的东西。让我们看看直方图是什么样子的:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="9e31" class="on nm it oj b gy oo op l oq or">modelResiduals &lt;- as.data.frame(residuals(model)) </span><span id="b530" class="on nm it oj b gy ow op l oq or">ggplot(modelResiduals, aes(residuals(model))) +<br/>  geom_histogram(fill='deepskyblue', color='black')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/d121eb4001169ac1beaf768d7997770d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*yI6bJGBXkGNes4pD7oL0Dg.png"/></div></figure><p id="a0a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，由于最右边的值有一点偏斜，但目测之后，我们可以得出结论，残差近似正态分布。</p><p id="b95c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们终于可以预测了！这样做很容易:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="1866" class="on nm it oj b gy oo op l oq or">preds &lt;- predict(model, testSet)</span></pre><p id="3f79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以评估。首先，我们将创建一个实际值和预测值的数据框架:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="11e1" class="on nm it oj b gy oo op l oq or">modelEval &lt;- cbind(testSet$Weight, preds)<br/>colnames(modelEval) &lt;- c('Actual', 'Predicted')<br/>modelEval &lt;- as.data.frame(modelEval)</span></pre><p id="e74f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是前几行的样子:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/4ccee117599ef0bb967782edd31df8fa.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Sf9WIG4pyGJp4BTqhF06uA.png"/></div></figure><p id="dae5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这不是最好的模型——至少在没有任何调整的情况下不是，但我们仍然得到了不错的结果。多体面？这就是像MSE和RMSE这样的指标会告诉我们的。以下是计算结果:</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="1d08" class="on nm it oj b gy oo op l oq or">mse &lt;- mean((modelEval$Actual - modelEval$Predicted)²)<br/>rmse &lt;- sqrt(mse)</span></pre><p id="dd28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到的RMSE值是95.9，MSE是它的平方。这意味着我们平均错了95.9个重量单位。我会让你来决定这是好是坏。</p><p id="e09e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我看来，由于输入变量之间的相关系数较大，该模型过度拟合了训练数据。此外，列车组的判定值系数(R2)高得惊人(0.93+)。</p><p id="76bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就足够了。让我们在下一部分总结一下。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="1ed7" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">在你走之前</h1><p id="4d9c" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">我希望这篇文章很容易理解。我们讨论了最简单的机器学习算法，并略微涉及了探索性数据分析。我知道对于一篇文章来说有很多东西需要消化，但是这个主题并不难。</p><p id="05fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们接下来将讨论逻辑回归，大约在3-4天内，如果你对此感兴趣，请继续关注。</p><p id="e347" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。</p><h2 id="75c4" class="on nm it bd nn ox oy dn nr oz pa dp nv kr pb pc nx kv pd pe nz kz pf pg ob ph bi translated"><a class="ae lv" href="https://mailchi.mp/46a3d2989d9b/bdssubscribe" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">加入我的私人邮件列表，获取更多有用的见解。</strong>T3】</a></h2></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="6302" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">喜欢这篇文章吗？成为</em> <a class="ae lv" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="le">中等会员</em> </a> <em class="le">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="lw lx gp gr ly lz"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">medium.com</p></div></div><div class="mi l"><div class="pi l mk ml mm mi mn lp lz"/></div></div></a></div></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="6780" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">原载于2020年9月25日https://betterdatascience.com</em><a class="ae lv" href="https://betterdatascience.com/machine-learning-with-r-linear-regression/" rel="noopener ugc nofollow" target="_blank"><em class="le"/></a><em class="le">。</em></p></div></div>    
</body>
</html>