<html>
<head>
<title>Create your own smart baby monitor with a RaspberryPi and Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RaspberryPi和Tensorflow创建您自己的智能婴儿监视器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-your-own-smart-baby-monitor-with-a-raspberrypi-and-tensorflow-5b25713410ca?source=collection_archive---------2-----------------------#2020-10-30">https://towardsdatascience.com/create-your-own-smart-baby-monitor-with-a-raspberrypi-and-tensorflow-5b25713410ca?source=collection_archive---------2-----------------------#2020-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/9dd1b13d43ae85ba3fa7d1b633f6f48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XsDtwGqCjJdxItHcX4mHw.png"/></div></div></figure><p id="b408" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个故事的更新版本可以在 <a class="ae la" href="https://blog.platypush.tech/article/Create-your-smart-baby-monitor-with-Platypush-and-Tensorflow" rel="noopener ugc nofollow" target="_blank"> <em class="kz"> Platypush博客</em> </a> <em class="kz">上找到。</em></p><p id="5322" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你们中的一些人可能已经注意到，距离我的上一篇文章已经有一段时间了。这是因为在此期间，我已经成为一名父亲，我不得不暂时从我的项目中抽出时间来处理一些(还)不能自动化的父母任务。</p><p id="042a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者，他们能吗？虽然我们可能还需要几年时间才能拥有完全负责给儿子换尿布的机器人(假设有足够多的疯狂父母同意在自己的孩子身上测试这种设备)，但有一些风险较小的父母职责为自动化提供了一些空间。</p><p id="2c03" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作为一名父亲，我首先意识到的一件事是，婴儿真的会哭，即使我在家，我也不一定能听到儿子的哭声。商用婴儿监听器通常会填补这一空白，它们充当对讲机，让您即使在另一个房间也能听到宝宝的声音。但我很快意识到，商用婴儿监视器比我想要的理想设备还要笨。它们检测不到宝宝的哭声——它们只是像对讲机一样将声音从声源传到扬声器。当他们移动到不同的房间时，由父母来移动扬声器，因为他们不能在任何其他现有的音频基础设施上播放声音。它们通常配备低功率扬声器，并且通常不能连接到外部扬声器——这意味着如果我在另一个房间播放音乐，我可能会错过我宝宝的哭声，即使显示器与我在同一个房间。而且大多数都是低功率无线电波，这意味着如果婴儿在他/她的房间里，你必须走一小段路到地下室，它们通常不会工作。</p><p id="531f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以我带来了一份智能婴儿监视器的说明书。</p><ul class=""><li id="155a" class="lb lc it kd b ke kf ki kj km ld kq le ku lf ky lg lh li lj bi translated">它应该可以在任何简单便宜的东西上运行，比如带有廉价USB麦克风的RaspberryPi。</li><li id="ce65" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">它应该能够检测到我宝宝的哭声，并在他开始/停止哭泣时通知我(最好是在我的手机上)，或者跟踪我仪表盘上的数据点，或者在我儿子哭泣时执行任何我想执行的任务。它不应该只是充当一个无声的对讲机，将声音从一个声源传递到一种兼容设备。</li><li id="8a59" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">它应该能够在任何设备上流式传输音频——我自己的扬声器，我的智能手机，我的电脑等。</li><li id="a7a8" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">无论声源和扬声器之间的距离有多远，它都可以工作，不需要在房子里到处移动扬声器。</li><li id="4659" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">它还应该配备一个摄像头，这样我就可以实时检查我的宝宝在做什么，或者当他开始哭的时候，我可以拍一张照片或一个简短的视频，以检查一切都好。</li></ul><p id="1c32" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们看看如何使用我们最喜欢的开源工具来完成这项工作。</p><h1 id="4133" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">录制一些音频样本</h1><p id="35dc" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">首先，获得一个RaspberryPi，并在SD卡上闪存任何兼容的Linux OS最好使用任何RaspberryPi 3或更高版本来运行Tensorflow模型。还需要一个兼容的USB麦克风——任何东西都可以，真的。</p><p id="714a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后安装我们需要的依赖项:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e3c2" class="nb lq it mx b gy nc nd l ne nf">[sudo] apt-get install ffmpeg lame libatlas-base-dev alsa-utils<br/>[sudo] pip3 install tensorflow</span></pre><p id="5e21" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作为第一步，我们必须在婴儿哭和不哭的地方记录足够的音频样本，我们稍后将使用这些样本来训练音频检测模型。<em class="kz">注意:在这个例子中，我将展示如何使用声音检测来识别婴儿的哭声，但同样的程序可以用于检测任何类型的声音——只要它们足够长(例如，警报或邻居的钻孔声)，并且在背景噪音中足够大。</em></p><p id="e07c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，看看可识别的音频输入设备:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="eb39" class="nb lq it mx b gy nc nd l ne nf">arecord -l</span></pre><p id="5f29" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我的RaspberryPi上，我得到以下输出(注意，我有两个USB麦克风):</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ddd8" class="nb lq it mx b gy nc nd l ne nf">**** List of CAPTURE Hardware Devices ****<br/>card 1: Device [USB PnP Sound Device], device 0: USB Audio [USB Audio]<br/>  Subdevices: 0/1<br/>  Subdevice #0: subdevice #0<br/>card 2: Device_1 [USB PnP Sound Device], device 0: USB Audio [USB Audio]<br/>  Subdevices: 0/1<br/>  Subdevice #0: subdevice #0</span></pre><p id="bc4f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我想用第二个麦克风来录音——那就是<code class="fe ng nh ni mx b">card 2, device 0</code>。识别它的ALSA方式是<code class="fe ng nh ni mx b">hw:2,0</code>(直接访问硬件设备)或<code class="fe ng nh ni mx b">plughw:2,0</code>(如果需要，推断采样率和格式转换插件)。请确保SD卡上有足够的空间，或者插入外部USB驱动器，然后开始录制一些音频:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7c8a" class="nb lq it mx b gy nc nd l ne nf">arecord -D plughw:2,0 -c 1 -f cd | lame - audio.mp3</span></pre><p id="5a7a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当你的宝宝在同一个房间时，录下几分钟或几个小时的音频——最好是长时间的沉默、婴儿啼哭和其他不相关的声音——完成后按Ctrl-C键。尽可能多次重复该过程，以获取一天中不同时刻或不同日期的音频样本。</p><h1 id="ac0e" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">标记音频样本</h1><p id="cc35" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">一旦你有了足够的音频样本，就该把它们复制到你的电脑上来训练模型了——要么使用<code class="fe ng nh ni mx b">scp</code>来复制文件，要么直接从SD卡/USB驱动器中复制。</p><p id="c5ac" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们把它们都放在同一个目录下，例如<code class="fe ng nh ni mx b">~/datasets/sound-detect/audio</code>。此外，让我们为每个示例创建一个新文件夹。每个文件夹将包含一个音频文件(名为<code class="fe ng nh ni mx b">audio.mp3</code>)和一个标签文件(名为<code class="fe ng nh ni mx b">labels.json</code>)，我们将使用它们来标记音频文件中的负/正音频片段。因此，原始数据集的结构类似于:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8a46" class="nb lq it mx b gy nc nd l ne nf">~/datasets/sound-detect/audio<br/>  -&gt; sample_1<br/>    -&gt; audio.mp3<br/>    -&gt; labels.json</span><span id="d429" class="nb lq it mx b gy nj nd l ne nf">  -&gt; sample_2<br/>    -&gt; audio.mp3<br/>    -&gt; labels.json</span><span id="f6e6" class="nb lq it mx b gy nj nd l ne nf">  ...</span></pre><p id="223c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在无聊的部分来了:给录制的音频文件贴标签——如果它们包含你自己的宝宝几个小时的哭声，这可能是特别自虐的。在您最喜欢的音频播放器或Audacity中打开每个数据集音频文件，并在每个样本目录中创建新的<code class="fe ng nh ni mx b">labels.json</code>文件。确定哭泣开始和结束的确切时间，并在<code class="fe ng nh ni mx b">labels.json</code>中以<code class="fe ng nh ni mx b">time_string -&gt; label</code>的形式作为键值结构报告它们。示例:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9a49" class="nb lq it mx b gy nc nd l ne nf">{<br/>  "00:00": "negative",<br/>  "02:13": "positive",<br/>  "04:57": "negative",<br/>  "15:41": "positive",<br/>  "18:24": "negative"<br/>}</span></pre><p id="bd66" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在上面的例子中，00:00和02:12之间的所有音频段将被标记为负，02:13和04:56之间的所有音频段将被标记为正，依此类推。</p><h1 id="b819" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">生成数据集</h1><p id="c95a" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">一旦您标记了所有音频样本，让我们继续生成将被馈送到Tensorflow模型的数据集。我创建了一个通用库和一套声音监听工具，名为<em class="kz"> micmon </em>。让我们从安装开始:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0508" class="nb lq it mx b gy nc nd l ne nf">git clone git@github.com:/BlackLight/micmon.git<br/>cd micmon<br/>[sudo] pip3 install -r requirements.txt<br/>[sudo] python3 setup.py build install</span></pre><p id="88a3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该模型旨在处理频率样本，而不是原始音频。原因是，如果我们想要检测特定的声音，该声音将具有特定的“频谱”特征，即基频(或基频通常可能落入的狭窄范围)和以特定比率绑定到基频的特定谐波集。此外，这些频率之间的比率既不受振幅的影响(无论输入音量如何，频率比率都是恒定的)，也不受相位的影响(无论何时开始录制，连续声音都将具有相同的频谱特征)。与我们简单地将原始音频样本馈送到模型的情况相比，这种幅度和时间不变属性使得这种方法更有可能训练稳健的声音检测模型。此外，该模型可以更简单(我们可以在不影响性能的情况下轻松地将频率分组到频段中，因此我们可以有效地执行降维)，更轻便(无论样本持续时间如何，该模型都将有50到100个频带作为输入值，而一秒钟的原始音频通常包含44100个数据点，输入的长度会随着样本持续时间的增加而增加)，并且更不容易过拟合。</p><p id="5b74" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe ng nh ni mx b">micmon</code>提供计算音频样本某些片段的<a class="ae la" href="https://en.wikipedia.org/wiki/Fast_Fourier_transform" rel="noopener ugc nofollow" target="_blank"> FFT </a>(快速傅立叶变换)的逻辑，使用低通和高通滤波器将产生的频谱分组，并将结果保存到一组numpy压缩(<code class="fe ng nh ni mx b">.npz</code>)文件中。您可以在命令行上通过<code class="fe ng nh ni mx b">micmon-datagen</code>命令来完成:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="c5ef" class="nb lq it mx b gy nc nd l ne nf">micmon-datagen \<br/>    --low 250 --high 2500 --bins 100 \<br/>    --sample-duration 2 --channels 1 \<br/>    ~/datasets/sound-detect/audio  ~/datasets/sound-detect/data</span></pre><p id="2bff" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在上面的例子中，我们从存储在<code class="fe ng nh ni mx b">~/dataset/sound-detect/audio</code>下的原始音频样本生成数据集，并将结果频谱数据存储到<code class="fe ng nh ni mx b">~/datasets/sound-detect/data</code>。<code class="fe ng nh ni mx b">--low</code>和<code class="fe ng nh ni mx b">--high</code>分别标识在结果频谱中要考虑的最低和最高频率。默认值分别为20 Hz(人耳可听到的最低频率)和20 kHz(健康和年轻人耳可听到的最高频率)。但是，您通常可能想要限制此范围，以尽可能多地采集您想要检测的声音，并尽可能多地限制任何其他类型的音频背景和不相关的泛音。我发现在我的情况下，250-2500赫兹的范围足以检测婴儿的哭声。婴儿的哭声通常很高(考虑到歌剧女高音可以达到的最高音约为1000 Hz)，通常你可能希望至少将最高频率加倍，以确保获得足够的高次谐波(谐波是实际上赋予声音音色或颜色的较高频率)，但不会太高，以免被其他背景声音的谐波污染频谱。我还截掉了250赫兹以下的任何东西——婴儿的哭声在这些低频上可能不会发生太多，包含它们也可能会扭曲检测。一个好的方法是在Audacity或任何均衡器/频谱分析仪中打开一些阳性音频样本，检查哪些频率在阳性样本中占主导地位，并将数据集集中在这些频率周围。<code class="fe ng nh ni mx b">--bins</code>指定频率空间的组数(默认值:100)。较高数量的仓意味着较高的频率分辨率/粒度，但如果太高，可能会使模型易于过拟合。</p><p id="e1ad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该脚本将原始音频分割成更小的片段，并计算每个片段的频谱“签名”。<code class="fe ng nh ni mx b">--sample-duration</code>指定这些片段的长度(默认值:2秒)。对于持续时间较长的声音，较高的值可能效果更好，但它会减少检测时间，并且可能会在声音较短时失败。较低的值可能更适合较短的声音，但如果声音较长，则采集的片段可能没有足够的信息来可靠地识别声音。</p><p id="e234" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe ng nh ni mx b">micmon-datagen</code>脚本的另一种方法是通过提供的<code class="fe ng nh ni mx b">micmon</code> API创建自己的脚本来生成数据集。示例:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a092" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">无论您使用的是<code class="fe ng nh ni mx b">micmon-datagen</code>还是<code class="fe ng nh ni mx b">micmon</code> Python API，在这个过程的最后，您应该会在<code class="fe ng nh ni mx b">~/datasets/sound-detect/data</code>下找到一堆<code class="fe ng nh ni mx b">.npz</code>文件，每个文件对应原始数据集中的一个带标签的音频文件。我们可以使用这个数据集来训练我们的神经网络进行声音检测。</p><h1 id="ad9b" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">训练模型</h1><p id="a05d" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated"><code class="fe ng nh ni mx b">micmon</code>使用Tensorflow+Keras定义和训练模型。使用提供的Python API可以很容易地做到这一点。示例:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="471e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">运行这个脚本之后(并且对模型的准确性感到满意之后)，您会发现您的新模型保存在<code class="fe ng nh ni mx b">~/models/sound-detect</code>下。在我的情况下，从我的婴儿房间收集大约5个小时的声音，并定义一个良好的频率范围来训练一个准确率为98%的模型就足够了。如果你在你的电脑上训练了这个模型，只要把它复制到RaspberryPi就可以开始下一步了。</p><h1 id="0241" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">使用模型进行预测</h1><p id="6cb8" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">是时候制作一个脚本了，它使用之前根据麦克风的实时音频数据训练的模型，并在我们的宝宝哭泣时通知我们:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="97b4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在RaspberryPi上运行脚本，让它运行一会儿——如果在过去2秒内没有检测到哭声，它将打印<code class="fe ng nh ni mx b">negative</code>,否则打印<code class="fe ng nh ni mx b">positive</code>。</p><p id="00c3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，如果我们的孩子在哭，那么简单地在标准输出中打印一条消息的脚本没有多大用处——我们希望得到通知！让我们用<a class="ae la" href="https://github.com/BlackLight/platypush" rel="noopener ugc nofollow" target="_blank">板推</a>盖住这个部分。在本例中，我们将使用<code class="fe ng nh ni mx b"><a class="ae la" href="https://platypush.readthedocs.io/en/latest/platypush/plugins/pushbullet.html" rel="noopener ugc nofollow" target="_blank">pushbullet</a></code>集成在检测到哭泣时向我们的手机发送消息。让我们安装Redis(Platypush用来接收消息)和Platypush，并集成HTTP和Pushbullet:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e853" class="nb lq it mx b gy nc nd l ne nf">[sudo] apt-get install redis-server<br/>[sudo] systemctl start redis-server.service<br/>[sudo] systemctl enable redis-server.service<br/>[sudo] pip3 install 'platypush[http,pushbullet]'</span></pre><p id="af22" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在你的智能手机上安装Pushbullet应用程序，前往pushbullet.com获取API令牌。然后创建一个<code class="fe ng nh ni mx b">~/.config/platypush/config.yaml</code>文件，启用HTTP和Pushbullet集成:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="f806" class="nb lq it mx b gy nc nd l ne nf">backend.http:<br/>  enabled: True</span><span id="0193" class="nb lq it mx b gy nj nd l ne nf">pushbullet:<br/>  token: YOUR_TOKEN</span></pre><p id="7e0d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，让我们修改前面的脚本，使它触发一个可以被Platypush钩子捕获的<code class="fe ng nh ni mx b"><a class="ae la" href="https://platypush.readthedocs.io/en/latest/platypush/events/custom.html" rel="noopener ugc nofollow" target="_blank">CustomEvent</a></code>,而不是将消息输出到标准输出:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4316" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将上面的脚本保存为例如<code class="fe ng nh ni mx b">~/bin/micmon_detect.py</code>。只有在<code class="fe ng nh ni mx b">window_length</code>秒的滑动窗口内检测到至少<code class="fe ng nh ni mx b">positive_samples</code>个样本时，脚本才会触发一个事件(这是为了减少由预测错误或临时故障引起的噪声)，并且只有当当前预测从负变为正或相反时，脚本才会触发一个事件。然后该事件通过<code class="fe ng nh ni mx b">RedisBus</code>被分派给Platypush。该脚本还应该具有足够的通用性，可以处理任何声音模型(不一定是哭泣的婴儿)、任何肯定/否定标签、任何频率范围和任何类型的输出事件。</p><p id="0a2c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在让我们创建一个Platypush钩子来对事件做出反应，并向我们的设备发送通知。首先，准备Platypush脚本目录(如果还没有创建的话):</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4ce6" class="nb lq it mx b gy nc nd l ne nf">mkdir -p ~/.config/platypush/scripts<br/>cd ~/.config/platypush/scripts</span><span id="5898" class="nb lq it mx b gy nj nd l ne nf"># Define the directory as a module<br/>touch __init__.py</span><span id="a103" class="nb lq it mx b gy nj nd l ne nf"># Create a script for the baby-cry events<br/>vi babymonitor.py</span></pre><p id="2bd8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe ng nh ni mx b">babymonitor.py</code>的内容:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="8a75" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在为Platypush创建一个服务文件(如果它还不存在的话),并启动/启用该服务，这样它将在终止或重启时自动重启:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="1348" class="nb lq it mx b gy nc nd l ne nf">mkdir -p ~/.config/systemd/user<br/>wget -O ~/.config/systemd/user/platypush.service \<br/>    <a class="ae la" href="https://raw.githubusercontent.com/BlackLight/platypush/master/examples/systemd/platypush.service" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/BlackLight/platypush/master/examples/systemd/platypush.service</a></span><span id="0cd3" class="nb lq it mx b gy nj nd l ne nf">systemctl --user start platypush.service<br/>systemctl --user enable platypush.service</span></pre><p id="00c1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">并为婴儿监视器创建一个服务文件—例如<code class="fe ng nh ni mx b">~/.config/systemd/user/babymonitor.service</code>:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e150" class="nb lq it mx b gy nc nd l ne nf">[Unit]<br/>Description=Monitor to detect my baby's cries<br/>After=network.target sound.target</span><span id="80fd" class="nb lq it mx b gy nj nd l ne nf">[Service]<br/>ExecStart=/home/pi/bin/micmon_detect.py -i plughw:2,0 -e baby-cry -w 10 -n 2 ~/models/sound-detect<br/>Restart=always<br/>RestartSec=10</span><span id="4881" class="nb lq it mx b gy nj nd l ne nf">[Install]<br/>WantedBy=default.target</span></pre><p id="8e88" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该服务将启动ALSA设备<code class="fe ng nh ni mx b">plughw:2,0</code>上的麦克风监视器，如果在过去10秒内检测到至少2个2秒钟的正样本且先前状态为<code class="fe ng nh ni mx b">negative</code>，则该服务将触发一个带有<code class="fe ng nh ni mx b">state=positive</code>的<code class="fe ng nh ni mx b">baby-cry</code>事件，如果在过去10秒内检测到不到2个正样本且先前状态为<code class="fe ng nh ni mx b">positive</code>，则该服务将触发<code class="fe ng nh ni mx b">state=negative</code>事件。然后，我们可以启动/启用服务:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7f0c" class="nb lq it mx b gy nc nd l ne nf">systemctl --user start babymonitor.service<br/>systemctl --user enable babymonitor.service</span></pre><p id="4955" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">确认一旦婴儿开始哭闹，你的手机就会收到通知。如果你不这样做，你可以检查你应用到你的音频样本的标签，你的神经网络的结构和参数，或者样本长度/窗口/频带参数。</p><p id="4b52" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，考虑这是自动化的一个相对基本的例子——可以随意添加更多的自动化任务。例如，您可以使用<code class="fe ng nh ni mx b"><a class="ae la" href="https://platypush.readthedocs.io/en/latest/platypush/plugins/tts.html" rel="noopener ugc nofollow" target="_blank">tts</a></code>插件向另一个Platypush设备(例如，在您的卧室或客厅)发送请求，大声说出婴儿在哭。您还可以扩展<code class="fe ng nh ni mx b">micmon_detect.py</code>脚本，以便捕获的音频样本也可以通过HTTP传输——例如使用Flask wrapper和用于音频转换的<code class="fe ng nh ni mx b">ffmpeg</code>。另一个有趣的用例是当婴儿开始/停止哭泣时将数据点发送到您的本地数据库(您可以参考我以前的文章<a class="ae la" rel="noopener" target="_blank" href="/how-to-build-your-home-infrastructure-for-data-collection-and-visualization-and-be-the-real-owner-af9b33723b0c">如何使用Platypush+PostgreSQL+Mosquitto+Grafana来创建您灵活和自我管理的仪表板</a>):这是一组有用的数据，用于跟踪您的婴儿何时睡觉、醒来或需要喂食。同样，监视我的孩子是开发<code class="fe ng nh ni mx b">micmon</code>的主要动机，但完全相同的程序可以用来训练和使用模型来检测任何类型的声音。最后，你可以考虑使用一个好的电源组或一组锂电池，让你的声音监测器可以移动。</p><h1 id="0acc" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">婴儿照相机</h1><p id="434e" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">一旦你有了一个好的音频源和一种方法来检测一个积极的音频序列何时开始/停止，你可能想要添加一个视频源来关注你的宝宝。在我的第一次设置中，我在用于音频检测的RaspberryPi 3上安装了一个PiCamera，我发现这个配置很不实用。一个RaspberryPi 3放在它的盒子里，附带一组电池和一个不知何故粘在顶部的相机可能会很笨重，如果你正在寻找一个可以轻松安装在支架或柔性臂上的轻型相机，你可以四处移动，随时随地关注你的宝宝。我最终选择了一个更小的RaspberryPi Zero，配有PiCamera兼容的外壳和一个小的电源组。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/41d1fba8506cd10e353d68fd3f5cfd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Crn6Fpfn7hd_rmM8vMJVMQ.jpeg"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">我的婴儿监视器相机模块的第一个原型</p></figure><p id="d503" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">像在另一个设备上一样，插入一个兼容RaspberryPi操作系统的SD卡。然后将兼容RaspberryPi的相机插入其插槽，确保相机模块在<code class="fe ng nh ni mx b">raspi-config</code>中启用，并安装Platypush和PiCamera集成:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="36f9" class="nb lq it mx b gy nc nd l ne nf">[sudo] pip3 install 'platypush[http,camera,picamera]'</span></pre><p id="cece" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后在<code class="fe ng nh ni mx b">~/.config/platypush/config.yaml</code>中添加摄像头配置:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="87c3" class="nb lq it mx b gy nc nd l ne nf">camera.pi:<br/>    listen_port: 5001</span></pre><p id="eefc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您已经可以在Platypush restart上检查此配置，并通过HTTP:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="e5dd" class="nb lq it mx b gy nc nd l ne nf">wget <a class="ae la" href="http://turing:8008/camera/pi/photo.jpg" rel="noopener ugc nofollow" target="_blank">http://raspberry-pi:8008/camera/pi/photo.jpg</a></span></pre><p id="b77c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者在浏览器中打开视频源:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4feb" class="nb lq it mx b gy nc nd l ne nf">http://raspberry-pi:8008/camera/pi/video.mjpg</span></pre><p id="d5fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者您可以创建一个钩子，当应用程序启动时，它通过TCP/H264开始流式传输摄像机输入:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="fbc0" class="nb lq it mx b gy nc nd l ne nf">mkdir -p ~/.config/platypush/scripts<br/>cd ~/.config/platypush/scripts<br/>touch __init__.py<br/>vi camera.py</span></pre><p id="6f39" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe ng nh ni mx b">camera.py</code>的内容:</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="ba97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您将能够在VLC等地播放提要:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3cf3" class="nb lq it mx b gy nc nd l ne nf">vlc tcp/h264://raspberry-pi:5001</span></pre><p id="7c05" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者在你的手机上通过VLC应用程序或像<a class="ae la" href="https://play.google.com/store/apps/details?id=ca.frozen.rpicameraviewer&amp;hl=en_US&amp;gl=US" rel="noopener ugc nofollow" target="_blank"> RPi相机浏览器</a>这样的应用程序。</p><h1 id="b62f" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">音频监视器</h1><p id="7f06" class="pw-post-body-paragraph kb kc it kd b ke mn kg kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky im bi translated">最后一步是建立一个直接的麦克风流，从你的宝宝的RaspberryPi到你想要使用的客户端。当婴儿哭泣时，Tensorflow模型可以很好地推动你，但我们都知道，机器学习模型并不是因为达到100%的准确率而臭名昭著。有时你可能只是坐在另一个房间里，想听到宝宝房间里发生的事情。</p><p id="bf1e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我为此制作了一个名为<a class="ae la" href="https://github.com/BlackLight/micstream/" rel="noopener ugc nofollow" target="_blank"> <em class="kz"> micstream </em> </a>的工具/库——它实际上可以用于任何想要通过HTTP/mp3从麦克风设置音频馈送的情况。注意:如果您使用麦克风向Tensorflow模型提供音频，那么您将需要另一个麦克风进行流式传输。</p><p id="8807" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">只需克隆存储库并安装软件(唯一的依赖是系统上安装的<code class="fe ng nh ni mx b">ffmpeg</code>可执行文件):</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="816c" class="nb lq it mx b gy nc nd l ne nf">git clone <a class="ae la" href="https://github.com/BlackLight/micstream/" rel="noopener ugc nofollow" target="_blank">https://github.com/BlackLight/micstream.git</a><br/>cd micstream<br/>[sudo] python3 setup.py install</span></pre><p id="c329" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可以通过<code class="fe ng nh ni mx b">micstream --help</code>获得可用选项的完整列表。例如，如果您想在第三个音频输入设备(使用<code class="fe ng nh ni mx b">arecord -l</code>获取完整列表)上设置流，在<code class="fe ng nh ni mx b">/baby.mp3</code>端点上，监听端口8088，比特率为96 kbps，那么命令将是:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="446f" class="nb lq it mx b gy nc nd l ne nf">micstream -i plughw:3,0 -e '/baby.mp3' -b 96 -p 8088</span></pre><p id="05b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你现在可以简单地从任何浏览器或音频播放器中打开<code class="fe ng nh ni mx b"><a class="ae la" href="http://your-rpi:8088/baby.mp3" rel="noopener ugc nofollow" target="_blank">http://your-rpi:8088/baby.mp3</a></code>,你将拥有来自婴儿监视器的实时音频。</p></div></div>    
</body>
</html>