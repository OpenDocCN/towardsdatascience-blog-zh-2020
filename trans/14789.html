<html>
<head>
<title>Implementing Recurrent Neural Network using Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Numpy实现递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-recurrent-neural-network-using-numpy-c359a0a68a67?source=collection_archive---------21-----------------------#2020-10-12">https://towardsdatascience.com/implementing-recurrent-neural-network-using-numpy-c359a0a68a67?source=collection_archive---------21-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3da5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="73ba" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">关于如何使用Numpy实现递归神经网络的综合教程</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/665917158c6f8840ac21032880c410a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8ySVBX1IlfVnbs-dezFGg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@chengfengrecord?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">程峰</a>在<a class="ae le" href="https://unsplash.com/s/photos/spiral?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="c0a4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="28f0" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">递归神经网络(RNN)是最早能够在自然语言处理领域提供突破的神经网络之一。这个网络的美妙之处在于它能够存储以前序列的记忆，因此它们也被广泛用于时间序列任务。像Tensorflow和PyTorch这样的高级框架抽象了这些神经网络背后的数学，使得任何人工智能爱好者都难以用正确的参数和层知识来编码深度学习架构。为了解决这些类型的低效率，这些网络背后的数学知识是必要的。通过帮助人工智能爱好者理解研究论文中的不同符号并在实践中实现它们，从头开始编写这些算法具有额外的优势。</p><p id="27da" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如果你对RNN的概念不熟悉，请参考<a class="ae le" href="https://youtu.be/SEnXr6v2ifU" rel="noopener ugc nofollow" target="_blank"> MIT 6。S191 </a>课程，这是最好的讲座之一，让你直观地了解RNN是如何运作的。这些知识将帮助您理解本教程中解释的不同符号和概念实现。</p><h1 id="48f7" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">最终目标</h1><p id="6da9" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这个博客的最终目标是让AI爱好者对他们从深度学习领域的研究论文中获得的理论知识进行编码。</p><h1 id="5e0a" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参数初始化</h1><p id="bd74" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">与传统的神经网络不同，RNN拥有3个权参数，即输入权、内部状态权(用于存储记忆)和输出权。我们首先用随机值初始化这些参数。我们将word_embedding维数和output维数分别初始化为100和80。输出维度是词汇表中出现的唯一单词的总数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">权重初始化器</p></figure><p id="d5a3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">变量<strong class="lz ja"> <em class="na"> prev_memory </em> </strong>指的是internal_state(这些是前面序列的内存)。用于更新权重的梯度等其他参数也已初始化。输入_权重梯度、内部_状态_权重梯度和输出_权重梯度分别被命名为<em class="na"> </em> <strong class="lz ja"> <em class="na">杜</em> </strong> <em class="na">、</em> <strong class="lz ja"> <em class="na"> dW </em> </strong> <em class="na">和</em> <strong class="lz ja"> <em class="na"> dV </em> </strong>。变量<strong class="lz ja"><em class="na">bptt _ truncate</em></strong>指的是网络在反向传播时必须回顾的时间戳的数量，这样做是为了克服<a class="ae le" href="https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>问题。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="ec98" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">正向传播直觉:</h1><p id="7d45" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><strong class="lz ja">输入和输出向量:</strong></p><p id="584e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">考虑到我们有一句话<strong class="lz ja"> <em class="na">“我喜欢玩。”。词汇表中的</em> </strong>假设<strong class="lz ja"> <em class="na"> I </em> </strong> <em class="na"> </em>映射到索引2，<strong class="lz ja"> <em class="na">像</em> </strong>映射到索引<strong class="lz ja"> 45 </strong>，<strong class="lz ja"> <em class="na">到</em> </strong>在索引<strong class="lz ja"> 10 </strong>和<strong class="lz ja"> <em class="na">在索引<strong class="lz ja"> 64 </strong>和标点符号</em></strong>在索引<strong class="lz ja"> 1 </strong>处。为了获得从输入到输出的真实场景，让我们为每个单词随机初始化word_embedding。</p><p id="5862" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> <em class="na">注意</em> : </strong>你也可以尝试对每个单词使用一个hot编码向量，并将其作为输入传递。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="cd45" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在我们已经完成了输入，我们需要考虑每个单词输入的输出。RNN单元应该输出当前输入的下一个最可能的单词。为了训练RNN，我们提供了第<strong class="lz ja"> <em class="na"> t+1个</em> </strong>字作为第<strong class="lz ja"> <em class="na">个</em> </strong>输入值的输出，例如:RNN单元格应该输出字<strong class="lz ja"> <em class="na">，就像给定的输入字<strong class="lz ja"><em class="na"/></strong>一样</em></strong></p><p id="90d3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"/></p><p id="d51b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">既然输入是以嵌入向量的形式，那么计算损失所需的输出格式应该是<strong class="lz ja">一次性编码的</strong>向量。对于输入字符串中除第一个单词之外的每个单词都要这样做，因为我们只考虑这个神经网络要学习的一个例句，而初始输入是句子的第一个单词。</p><p id="f5e8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> <em class="na">我们为什么要对输出的字进行一次性编码？</em> </strong></p><p id="6a5d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">原因是，原始输出只是每个单词的分数，它们对我们来说并不重要。相反，我们需要每个单词相对于前一个单词的<strong class="lz ja"> <em class="na">概率</em> </strong>。</p><p id="57f4" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们如何从原始输出值中找到概率？T13】</p><p id="5b94" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了解决这个问题，在得分向量上使用了一个<strong class="lz ja"> <em class="na"> softmax </em> </strong>激活函数，使得所有这些概率加起来等于1。<strong class="lz ja"> Img 1 </strong>显示单个时间戳的输入输出管道。顶行是ground _truth输出，第二行表示预测输出。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5d95a6e6163252f4927ea811a0985d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*JXpSMnwL7Hx1cGny310DYg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">img 1:RNN的输入和输出管道，图片由作者提供</p></figure><p id="2d6c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> <em class="na">注意:</em> </strong> <em class="na">在实现过程中我们需要注意output_mapper的键值。我们需要用其时间戳值重置键值，以便算法知道在特定时间戳需要使用哪个基本事实字来计算损失。</em></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><pre class="kp kq kr ks gt nc nd ne nf aw ng bi"><span id="167a" class="nh lg iq nd b gy ni nj l nk nl">Before reset:<br/>45: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</span><span id="eff5" class="nh lg iq nd b gy nm nj l nk nl">After reset:<br/>{0: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,<br/>        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</span></pre><h2 id="ae1a" class="nh lg iq bd lh nn no dn ll np nq dp lp mg nr ns lr mk nt nu lt mo nv nw lv iw bi translated">RNN盒子计算:</h2><p id="cf94" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">现在我们有了权重，也知道了如何传递输入，也知道了预期的输出，我们将从正向传播计算开始。训练神经网络需要以下计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/09d4cb32cf178913f793a9e35ff8c2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*rWSS4F8Fn9Rh4dXeXsemqQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 2:作者图片</p></figure><p id="c629" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这里的<strong class="lz ja"> <em class="na"> U </em> </strong>表示输入_权重，<strong class="lz ja"> <em class="na"> W </em> </strong>表示内部_状态_权重，<strong class="lz ja"> <em class="na"> V </em> </strong> <em class="na"> </em>表示输出权重。输入权重乘以输入(<strong class="lz ja"> <em class="na"> x </em> </strong>)，内部_状态_权重乘以之前的激活，在我们的符号中是<strong class="lz ja"> <em class="na"> prev_memory </em> </strong>。层间使用的激活函数是<strong class="lz ja"> Tanh。</strong>它提供非线性，最终有助于学习。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="17f3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> <em class="na">注意:</em> </strong> <em class="na">在本教程中，不使用RNN计算的偏差术语，因为它会导致更复杂的理解。</em></p><p id="d18d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">由于上面的代码只计算一个特定时间戳的输出，我们现在必须对整个单词序列的前向传播进行编码。</p><p id="5787" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在下面的代码中，<strong class="lz ja"> <em class="na">输出字符串</em> </strong>包含每个时间戳的输出向量列表。<strong class="lz ja"> <em class="na">内存</em> </strong> <em class="na"> </em>是一个字典<em class="na"> </em>，其中<em class="na"> </em>包含每个时间戳的参数，这些参数在反向传播期间是必不可少的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="073e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">损失计算:</strong></p><p id="c9e6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们还将损失或误差定义为<strong class="lz ja">交叉熵损失</strong>，由下式给出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/fcc766cdf61e4f40f90b249f64e993eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/0*dX_PBVBbyuiw62O-.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 3:作者图片</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="6f42" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最重要的是，我们需要在上面的代码中查看第<strong class="lz ja"> <em class="na">行和第</em> </strong>行。众所周知，地面真实输出(<strong class="lz ja"> <em class="na"> y </em> </strong>)的形式为[0，0，…。,1,..0]和predicted _ output(<strong class="lz ja"><em class="na"/></strong>)的形式是[0.34，0.03，…，0.45]，我们需要损失是一个单一的值来从中推断总损失。为此，我们使用<strong class="lz ja"> <em class="na">求和函数</em> </strong>来获得该特定时间戳的y和y^hat向量中每个值的差值/误差之和。total_loss是包括所有时间戳在内的整个模型的损失。</p><h1 id="e216" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">反向传播</h1><p id="5d17" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">如果你听说过反向传播，那么你一定听说过链式法则，它是计算<a class="ae le" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">梯度</a>的重要方面。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c50504b7496f0ef2b305db0d25197108.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*NGHGQ5k5-XcR3rMXy4lQWA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 4:作者图片</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/70d681d5b6d5cc2acbe62112fd6772cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*q14WJP7w6bkmbAxhBP7nsg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 5:作者图片</p></figure><p id="ed05" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">根据上面的<strong class="lz ja"> Img 4 </strong>，成本<strong class="lz ja"> C </strong>表示误差，这是y^hat达到y所需的变化。由于成本是激活<strong class="lz ja"> <em class="na"> a、</em> </strong>的函数输出，激活反映的变化由<strong class="lz ja"> <em class="na"> dCost/da表示。</em> </strong>实际上是指从激活节点<strong class="lz ja"> <em class="na">的角度看的变化(误差)值。</em> </strong>类似地，激活相对于<strong class="lz ja"><em class="na"/></strong>的变化由<strong class="lz ja"> <em class="na"> da/dz </em> </strong>和<strong class="lz ja"> <em class="na"> z </em> </strong>相对于<strong class="lz ja"><em class="na"/></strong>w<strong class="lz ja"><em class="na">dw/dz给出。我们关心的是重量的变化(误差)有多大。由于重量和成本之间没有直接关系，所以从成本到重量的中间变化值会相乘(如上述等式所示)。</em></strong></p><h2 id="3ba3" class="nh lg iq bd lh nn no dn ll np nq dp lp mg nr ns lr mk nt nu lt mo nv nw lv iw bi translated"><strong class="ak">RNN反向传播:</strong></h2><p id="0ac7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">因为RNN有三个重量，我们需要三个梯度。input _ weights(<strong class="lz ja"><em class="na">dLoss/dU</em></strong>)、internal _ state _ weights(<strong class="lz ja"><em class="na">dLoss/dW</em></strong>)和output _ weights(<strong class="lz ja"><em class="na">dLoss/dV</em></strong>)的渐变。</p><p id="afc0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这三个梯度的链可以表示如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/cd315701f23947ab09b9d549cd9acb40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*jlhJPB4OWj90BEPlPHpcmg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk">:</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9f9b9e4e6f421a11db19ac68e84fb044.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*HqCOmH-muJMXBt1pXwOIiA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">img 6:RNN使用的权重梯度方程，图片由作者提供</p></figure><p id="e6a0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> <em class="na">注意</em> </strong> <em class="na">:这里的T代表转置。</em></p><p id="5c3e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"><em class="na">dLoss/dy _ unactivated</em></strong>编码如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="8058" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了了解更多关于损失衍生品的信息，请参考这篇<a class="ae le" href="https://deepnotes.io/softmax-crossentropy#derivative-of-cross-entropy-loss-with-softmax" rel="noopener ugc nofollow" target="_blank">博客</a>。我们将计算两个梯度函数，一个是<strong class="lz ja">乘法_后退</strong>，另一个是<strong class="lz ja">加法_后退</strong>。在<strong class="lz ja">乘法_反向</strong>的情况下，我们返回2个参数，一个是关于权重的梯度(<strong class="lz ja"> <em class="na"> dLoss/dV </em> </strong>),另一个是链梯度，其将是链的一部分以计算另一个权重梯度。在<strong class="lz ja">加法向后</strong>的情况下，在计算导数时，我们发现加法函数(<strong class="lz ja"><em class="na">ht _ unactivated</em></strong>)中单个分量的导数为1。例如:<strong class="lz ja"><em class="na">DH _ unactivated/dU _ frd</em></strong>= 1 as(<strong class="lz ja"><em class="na">h _ unactivated</em></strong>=<strong class="lz ja"><em class="na">U _ frd+W _ frd _</em></strong>)以及<strong class="lz ja"><em class="na">dU _ frd/dU _ frd</em></strong>= 1。但是1的个数是相对于U_frd的维数而言的。要了解更多关于渐变的信息，你可以参考这个<a class="ae le" href="https://cedar.buffalo.edu/~srihari/CSE676/10.2.2%20RNN-Gradients.pdf" rel="noopener ugc nofollow" target="_blank">源</a>。就是这样，计算梯度只需要这两个函数。<strong class="lz ja"> <em class="na">乘法_后退</em> </strong>函数用于包含矢量点积的方程，而<strong class="lz ja"> <em class="na">加法_后退</em> </strong>函数用于包含两个矢量相加的方程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/041c10860c3bfe7ac1f74ae3146b9160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*I53kqgecmb6FZGNU-1-ADQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 7:下面编码的梯度函数背后的数学直觉，图片由作者提供</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5c0a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在，您已经分析并理解了RNN的反向传播，是时候为单个时间戳实现它了，这将在以后用于计算所有时间戳的梯度。如下面的代码所示，<strong class="lz ja"> <em class="na"> forward_params_t </em> </strong>是一个包含网络在特定时间步长的前向参数的列表。变量<strong class="lz ja"> <em class="na"> ds </em> </strong>是一个至关重要的部分，因为这行代码考虑了先前时间戳的隐藏状态，这将有助于提取反向传播时所需的足够有用的信息。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="cc14" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">对于RNN，由于消失梯度问题，我们将使用截断反向传播，而不是使用普通反向传播。在这种技术中，当前单元将向后看k个时间戳，而不是只向后看一个时间戳，其中k表示要向后看的先前单元的数量，以便检索更多的知识。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="2f9f" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">更新权重:</h1><p id="aa62" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">一旦我们使用反向传播计算了梯度，我们必须更新权重，这是使用批量梯度下降方法完成的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="c4e9" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">训练序列:</h1><p id="b1c0" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">一旦我们把所有的功能都准备好了，我们就可以进入高潮了，也就是训练神经网络。考虑用于训练的学习率是静态的，您甚至可以使用基于使用<a class="ae le" href="https://cs231n.github.io/neural-networks-3/#anneal" rel="noopener ugc nofollow" target="_blank">步长衰减</a>的动态方法来改变学习率。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6a5e35982dd64b7cf8d33d93068492ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*Wd7SXXPC5ED-BpjCAZF83A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Img 7:损失-产出，按作者分类的图像</p></figure><h1 id="5ab5" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结论:</h1><p id="9381" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">现在你已经实现了一个递归神经网络，是时候向前迈出一步，使用像LSTM和GRU这样的高级架构，以更有效的方式利用隐藏状态来保留更长序列的含义。还有很长的路要走。随着NLP领域的大量进步，出现了高度复杂的算法，如Elmo和Bert。理解它们，并尝试自己实现。它遵循同样的记忆概念，但引入了加权单词的元素。由于这些模型高度复杂，使用Numpy是不够的，而是灌输PyTorch或TensorFlow的技能来实现它们，并构建可以服务于社区的惊人的AI系统。</p><p id="3c5d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">创建本教程的灵感来自这个github <a class="ae le" href="https://github.com/pangolulu/rnn-from-scratch" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="eeca" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">你可以在这里访问本教程<a class="ae le" href="https://github.com/rishit13/Recurrent-Neural-Network-from-scratch-using-numpy" rel="noopener ugc nofollow" target="_blank">的笔记本。</a></p><p id="b59f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">希望你们都喜欢这个教程！</p><h1 id="f189" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">参考文献:</strong></h1><p id="da6d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1]萨古尔·斯里哈里，RNN-Gradients，<a class="ae le" href="https://cedar.buffalo.edu/~srihari/CSE676/10.2.2%20RNN-Gradients.pdf" rel="noopener ugc nofollow" target="_blank">https://cedar . buffalo . edu/~斯里哈里/CSE 676/10 . 2 . 2% 20 rnn-Gradients . pdf</a></p><p id="2e6c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">[2]https://github.com/pangolulu/rnn-from-scratch龚玉【RNN】从无到有</p></div></div>    
</body>
</html>