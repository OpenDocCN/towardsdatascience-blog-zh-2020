<html>
<head>
<title>Can a neural network train other networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个神经网络可以训练其他网络吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-a-neural-network-train-other-networks-cf371be516c6?source=collection_archive---------18-----------------------#2020-10-05">https://towardsdatascience.com/can-a-neural-network-train-other-networks-cf371be516c6?source=collection_archive---------18-----------------------#2020-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6cb3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">知识提炼导论</h2></div><p id="9c7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你曾经使用过神经网络来解决一个复杂的问题，你就会知道它们的规模可能是巨大的，包含数百万个参数。例如，著名的伯特模型大约有1.1亿英镑。</p><p id="96cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了说明这一点，这是NLP中最常见架构的参数数量，正如最近由<a class="ae lb" href="https://www.twitter.com/nathanbenaich" rel="noopener ugc nofollow" target="_blank">内森·贝纳奇</a>和<a class="ae lb" href="https://www.twitter.com/soundboy" rel="noopener ugc nofollow" target="_blank">伊恩·霍加斯</a>撰写的《2020年人工智能报告的<a class="ae lb" href="https://www.stateof.ai/" rel="noopener ugc nofollow" target="_blank">状态》中所总结的。</a></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/b8fbf816bcf483c056550db2125e4237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkG-n0N1pG5nNdUgh4tkOg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">给定架构中的参数数量。资料来源:【2020年人工智能状况报告作者<a class="ae lb" href="https://www.twitter.com/nathanbenaich" rel="noopener ugc nofollow" target="_blank">内森·贝纳奇</a>和<a class="ae lb" href="https://www.twitter.com/soundboy" rel="noopener ugc nofollow" target="_blank">伊恩·霍加斯</a></p></figure><p id="83ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Kaggle竞赛中，获胜者模型通常是由几个预测器组成的集合。虽然它们在精确度上可以大幅度击败简单模型，但它们巨大的计算成本使它们在实践中完全无法使用。</p><p id="80c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有什么方法可以在不扩展硬件的情况下，利用这些强大而庞大的模型来训练最先进的模型？</p><p id="7c8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前，在保持预测性能的同时压缩神经网络有三种主要方法:</p><ul class=""><li id="212f" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b"> <em class="mb">权重修剪</em> </a>，</li><li id="7de6" class="ls lt iq kh b ki mc kl md ko me ks mf kw mg la lx ly lz ma bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7"> <em class="mb">量化</em> </a>，</li><li id="de58" class="ls lt iq kh b ki mc kl md ko me ks mf kw mg la lx ly lz ma bi translated">和<em class="mb">知识的升华</em>。</li></ul><p id="ec1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我的目标是向你介绍<em class="mb">知识提炼</em>的基本原理，这是一个令人难以置信的令人兴奋的想法，建立在训练一个更小的网络来接近大网络的基础上。</p><h1 id="5425" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">什么是知识蒸馏？</h1><p id="4638" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">让我们设想一个非常复杂的任务，比如对几千个类进行图像分类。通常情况下，你不能简单地使用ResNet50并期望它达到99%的准确率。所以，你建立了一个模型集合，平衡了每个模型的缺陷。现在你有了一个庞大的模型，尽管它表现出色，但没有办法将其部署到生产中并在合理的时间内得到预测。</p><p id="38b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，该模型对看不见的数据进行了很好的概括，因此可以放心地相信它的预测。(我知道，情况可能不是这样，但现在让我们开始思想实验吧。)</p><p id="7671" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们使用来自庞大而笨重的模型的预测来训练一个更小的，所谓的<em class="mb">学生</em>模型来逼近大模型，会怎么样？</p><p id="6957" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这实质上是知识的提炼，在Geoffrey Hinton、Oriol Vinyals和Jeff Dean的论文<a class="ae lb" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">中介绍了这一点。</a></p><p id="6439" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大致来说，这个过程如下。</p><ol class=""><li id="777c" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la ne ly lz ma bi translated">训练一个执行和概括都非常好的大型模型。这就是所谓的<em class="mb">教师模型</em>。</li><li id="0081" class="ls lt iq kh b ki mc kl md ko me ks mf kw mg la ne ly lz ma bi translated">利用你所有的数据，计算教师模型的预测。包含这些预测的总数据集被称为<em class="mb">知识、</em>，预测本身通常被称为<em class="mb">软目标</em>。这是<em class="mb">知识提炼</em>步骤。</li><li id="44af" class="ls lt iq kh b ki mc kl md ko me ks mf kw mg la ne ly lz ma bi translated">利用之前获得的知识训练较小的网络，称为<em class="mb">学生模型</em>。</li></ol><p id="e3fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了形象化这个过程，您可以考虑以下内容。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nf"><img src="../Images/c4a938846b575c0e1bcfd43a09c31356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNYuAQ54Ij58xB6FA9FoMg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">知识升华(图片由作者提供)</p></figure><p id="40b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们稍微关注一下细节。知识是如何获得的？</p><p id="3a51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在分类器模型中，类别概率由一个<em class="mb"> softmax </em>层给出，将<a class="ae lb" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"> <em class="mb">逻辑</em> </a>转换成概率:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1286872a7a77298935165a763da27619.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*v3Dl0kSY1fKLyqUTmfemVA@2x.png"/></div></figure><p id="2686" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8129d120b4b1cc1f015bcf6dfe2ec817.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*tLuFWo4r2tmimuRvKhE33w@2x.png"/></div></figure><p id="7df2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是由最后一层产生的逻辑。代替这些，使用稍微修改的版本:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee4e468035e45faa4ef246db8f0961d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*tJWbMT9nIRBe3aHkWZ52Ug@2x.png"/></div></figure><p id="37e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="mb"> T </em>是一个称为<em class="mb">温度</em>的超参数。这些值被称为<em class="mb">软目标</em>。</p><p id="8a97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<em class="mb"> T </em>较大，则类概率“较软”，即它们之间的距离会更近。在极端情况下，当<em class="mb"> T </em>接近无穷大时，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c052b4da49051afd93b383e81e9c3a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*M0Ni0WrJogsfTd-gwziIXQ@2x.png"/></div></figure><p id="76ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<em class="mb"> T = 1 </em>，我们得到softmax函数。出于我们的目的，温度被设置为高于1，因此命名为<em class="mb">蒸馏</em>。</p><p id="68aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Hinton、Vinyals和Dean表明，一个经过提炼的模型可以和由10个大型模型组成的集合表现得一样好。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0356e224946f971fcf5fde15e96f0613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*cX4oDjNZokKxu50uGOD9UA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">知识提取由Geoffrey Hinton、Oriol Vinyals和Jeff Dean撰写的论文<a class="ae lb" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a>中的一个语音识别问题</p></figure><h2 id="7345" class="nl mi iq bd mj nm nn dn mn no np dp mr ko nq nr mt ks ns nt mv kw nu nv mx nw bi translated">为什么不一开始就培养一个小网络呢？</h2><p id="eea7" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">你可能会问，为什么不从一开始就训练一个更小的网络呢？不是更容易吗？当然，但是它不一定会起作用。</p><p id="af7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">经验证据表明，更多的参数导致更好的泛化和更快的收敛。例如，Sanjeev Arora、Nadav Cohen和Elad Hazan在他们的论文《深度网络的优化:过度参数化的隐式加速》中对此进行了研究。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nx"><img src="../Images/ae09f2593716471956ed81c594e12eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zXEdLZew6InQ4ElEYjYf3g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">左图:单层网络与4层和8层线性网络。右图:使用张量流教程的MNIST分类的过度参数化与基线模型。来源:<a class="ae lb" href="https://arxiv.org/abs/1802.06509" rel="noopener ugc nofollow" target="_blank">深度网络优化:过度参数化的隐式加速</a>作者:Sanjeev Arora、Nadav Cohen和Elad Hazan</p></figure><p id="973b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于复杂的问题，简单的模型很难在给定的训练数据上进行很好的归纳。然而，我们拥有的不仅仅是训练数据:教师模型对所有可用数据的预测。</p><p id="2260" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这对我们有两方面的好处。</p><p id="e9b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，教师模型的知识可以教会学生模型如何通过训练数据集之外的可用预测进行归纳。回想一下，我们使用教师模型对所有可用数据的预测<strong class="kh ir">来训练学生模型，而不是原始的训练数据集。</strong></p><p id="50e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二，软目标提供了比类别标签更多的有用信息:<strong class="kh ir">表示两个类别是否相似</strong> <em class="mb">。</em>例如，如果任务是对狗的品种进行分类，那么像<em class="mb">“柴犬和秋田犬非常相似”</em>这样的信息对于模型的泛化非常有价值。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ny"><img src="../Images/0c2b01c63e8e24a401ff1e301fac8c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fFus6Tr08SNFYBEZ0AaCXA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">左:秋田犬。右图:柴犬。来源:维基百科</p></figure><h1 id="d662" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">迁移学习的区别</h1><p id="1c8c" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">正如<a class="ae lb" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank"> Hinton et al. </a>所指出的，通过转移知识来压缩模型的最早尝试之一是重用经过训练的集合的某些层，正如<a class="ae lb" href="https://dl.acm.org/doi/10.1145/1150402.1150464" rel="noopener ugc nofollow" target="_blank"> Cristian Bucilu、Rich Caruana和Alexandru Niculescu-Mizil在他们2006年题为“模型压缩</a>的论文中所做的那样。</p><p id="0ca4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用Hinton等人的话说，</p><blockquote class="nz oa ob"><p id="dcab" class="kf kg mb kh b ki kj jr kk kl km ju kn oc kp kq kr od kt ku kv oe kx ky kz la ij bi translated">“…我们倾向于用学习到的参数值来识别训练模型中的知识，这使得我们很难看到如何改变模型的形式，但保持相同的知识。对知识的一个更抽象的观点是，它是从输入向量到输出向量的一种习得性映射，这种观点将知识从任何特定的实例化中解放出来。”— <a class="ae lb" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a></p></blockquote><p id="91ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，与迁移学习相反，知识提炼不直接使用学习到的权重。</p><h1 id="ff17" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">使用决策树</h1><p id="b257" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">如果您想进一步压缩模型，可以尝试使用更简单的模型，如决策树。虽然它们的表达能力不如神经网络，但它们的预测可以通过单独查看节点来解释。</p><p id="9637" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是由Nicholas Frosst和Geoffrey Hinton完成的，他们在论文<a class="ae lb" href="https://arxiv.org/abs/1711.09784" rel="noopener ugc nofollow" target="_blank">中将神经网络提取为软决策树</a>中对此进行了研究。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi of"><img src="../Images/79e098f4accdec4e6c9f166ee4806814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*0BQ-ENE-KG8Cite8GPA5Bw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">来源:<a class="ae lb" href="https://arxiv.org/abs/1711.09784" rel="noopener ugc nofollow" target="_blank">将神经网络提取为软决策树</a></p></figure><p id="d85e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们表明提取确实有一点帮助，尽管甚至更简单的神经网络也胜过它们。在MNIST数据集上，提取的决策树模型达到了96.76%的测试准确率，这比基线94.34%的模型有所提高。然而，一个简单的两层深度卷积网络仍然达到了99.21%的准确率。因此，在性能和可解释性之间有一个权衡。</p><h1 id="f5e5" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">蒸馏伯特</h1><p id="0cb5" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">到目前为止，我们只看到了理论结果，而没有看到实际例子。为了改变这一点，让我们考虑一下近年来最流行和最有用的模型之一:BERT。</p><p id="034e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最初发表在Google的Jacob Devlin等人的论文<a class="ae lb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>中，它很快被广泛用于各种NLP任务，如文档检索或情感分析。这是一个真正的突破，推动了几个领域的技术发展。</p><p id="caac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，有一个问题。BERT包含约1.1亿个参数，需要大量时间来训练。作者报告说，训练需要4天，在4个吊舱中使用16个TPU芯片。按照<a class="ae lb" href="https://cloud.google.com/tpu/pricing#pod-pricing" rel="noopener ugc nofollow" target="_blank">目前可用的TPU pod每小时价格</a>计算，培训成本将在10000美元左右<a class="ae lb" href="https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/" rel="noopener ugc nofollow" target="_blank">，还不包括碳排放等环境成本</a>。</p><p id="7e00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过<a class="ae lb" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>成功尝试了减少BERT的大小和计算成本。他们使用知识提炼来训练DistilBERT，它的大小是原始模型的60%，速度快60%，并保持了97%的语言理解能力。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi og"><img src="../Images/c5ccd4a7ff8e08d111021645b4aba1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fh7YK-utNohgszZDhjBtvA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">蒸馏器的性能。来源:<a class="ae lb" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">蒸馏伯特，伯特的蒸馏版本:更小、更快、更便宜、更轻</a>作者:维克多·桑、弗拉达利·佩登、朱利安·肖蒙德、托马斯·沃尔夫</p></figure><p id="61f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">较小的架构需要更少的时间和计算资源:在8个16GB V100 GPUs上运行90小时。</p><p id="5678" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对更多细节感兴趣，你可以阅读原文<a class="ae lb" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> DistilBERT，一个BERT的蒸馏版本:更小，更快，更便宜，更轻</a>，或者总结文章是其中一位作者写的。这是一个奇妙的阅读，所以我强烈建议你这样做！</p><div class="oh oi gp gr oj ok"><a href="https://medium.com/huggingface/distilbert-8cf3380435b5" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">🏎更小，更快，更便宜，更轻:介绍伯特，伯特的精华版本</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">你可以在这里找到代码来重现呆伯特的训练以及呆伯特的预训练权重。</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">medium.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy lm ok"/></div></div></a></div><h1 id="3cfb" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结论</h1><p id="0803" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">知识提炼是压缩神经网络并使其适用于功能较弱的硬件的三种主要方法之一。</p><p id="5fd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与权重剪枝和量化这两种强大的压缩方法不同，知识提取并不直接减少网络。相反，它使用原来的模型来训练一个更小的模型，叫做<em class="mb">学生模型</em>。由于教师模型甚至可以对未标记的数据进行预测，因此学生模型可以学习如何像教师一样进行归纳。</p><p id="0b08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们已经看到了两个关键的结果:最初的论文，介绍了这个想法，以及后续的，表明简单的模型，如决策树，可以用作学生模型。</p><p id="f968" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您对该领域的更广泛概述感兴趣，我推荐下面这篇文章，它就发表在这里的《走向数据科学》:</p><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/knowledge-distillation-a-survey-through-time-187de05a278a"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">知识蒸馏——穿越时间的调查</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">通过这个博客，你将回顾知识蒸馏(KD)和六篇后续论文。</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="oz l ov ow ox ot oy lm ok"/></div></div></a></div></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><p id="4543" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://www.tivadardanka.com/blog" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="mb">如果你喜欢把机器学习概念拆开，理解是什么让它们运转，我们有很多共同点。看看我的博客，我经常在那里发表这样的技术文章！</em> </strong> </a></p><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">如何用量化加速和压缩神经网络</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">从浮点数到整数</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="ph l ov ow ox ot oy lm ok"/></div></div></a></div><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/can-you-remove-99-of-a-neural-network-without-losing-accuracy-915b1fab873b"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">你能移除99%的神经网络而不损失准确性吗？</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">权重剪枝简介</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="pi l ov ow ox ot oy lm ok"/></div></div></a></div></div></div>    
</body>
</html>