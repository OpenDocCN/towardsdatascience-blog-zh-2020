<html>
<head>
<title>Reinforcement Learning in Autonomous Race Car</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自主赛车中的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-in-autonomous-race-car-c25822def9f8?source=collection_archive---------25-----------------------#2020-10-17">https://towardsdatascience.com/reinforcement-learning-in-autonomous-race-car-c25822def9f8?source=collection_archive---------25-----------------------#2020-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6b3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“邀请所有有志RL从业者”系列第2集</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ac0fcdfae1c1a4731aacb7ab1ade4555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K9wFg2v7mDnESI_T"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">菲利普·维特在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3fc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本系列的<a class="ae kv" rel="noopener" target="_blank" href="/invitation-to-all-aspiring-reinforcement-learning-practitioner-5f87384cee67">第一部分中，我们已经了解了强化学习(RL)中的一些重要术语和概念。我们还学习了RL在高层是如何工作的。</a></p><p id="46fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们深入研究RL背后的理论之前，我邀请你和我一起了解基于其超级酷的应用程序AWS DeepRacer的RL。</p><h1 id="0489" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是AWS DeepRacer？</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="372a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AWS DeepRacer是一款1/18比例的自主赛车，旨在通过在物理赛道上比赛来测试RL模型。AWS DeepRacer中有三种比赛类型:</p><ol class=""><li id="df69" class="mm mn iq ky b kz la lc ld lf mo lj mp ln mq lr mr ms mt mu bi translated">计时赛——代理在没有固定障碍物或移动竞争者的标记清晰的跑道上与时间赛跑。</li><li id="019d" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">物体回避——车辆在双车道轨道上行驶，轨道上放置了固定数量的固定障碍物。</li><li id="267b" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">短兵相接的比赛——车辆在双车道赛道上与其他行驶中的车辆进行比赛。</li></ol></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="0184" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated">AWS DeepRacer中的强化学习</h1><p id="304f" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">我假设你熟悉RL在高层是如何工作的。如果您不知道，请查看本系列的第一篇文章以了解更多信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/510fa79007e02a2f2ddfcb57a32e495e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJKMZ4sNyzFucu7lx5s4Dw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">RL如何在AWS DeepRacer中工作的基本想法。[图片由作者提供]</p></figure><p id="ebc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在AWS DeepRacer中，我们希望我们的车辆(<em class="ns"> agent </em>)在赛道(<em class="ns">环境</em>)中独自比赛，同时获得最大的累计总奖励，以便我们可以实现我们的目标——赢得与时间的比赛，或避开所有障碍，或赢得与另一辆车的比赛。</p><blockquote class="nt nu nv"><p id="8a16" class="kw kx ns ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">因此，它被归类为一个阶段性任务，其最终状态可能是终点线或脱离轨道。</p></blockquote><p id="c1d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">国家呢？这里，状态是由车辆上的前置摄像头捕获的图像。车辆将以大约15 fps的速度拍摄图像(160x120)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/c51a3bc80219e33fc73a915d7add19f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6dIhtCpY1wFvjhuA"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰克·布吕克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="78db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在现实驾驶体验中，我们可以“平稳”地控制速度和转向角度，而在这里，我们只能“艰难”地控制它们。换句话说，动作空间是一组离散的动作。</p><blockquote class="nt nu nv"><p id="0087" class="kw kx ns ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">需要注意的是:动作空间越细，模型收敛的时间就越长。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/ffe3752f04ea243be1a2ec6f6b9b6266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPTG8bGj8bhS8xD693pIXQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AWS DeepRacer行动空间示例。[图片来自AWS DeepRacer控制台]</p></figure><p id="9ed8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">至于奖励，它是由一个奖励函数返回的，这个函数只是一个Python函数，它将基于捕获图像的一些参数(<em class="ns">状态</em>)作为输入。AWS DeepRacer中有几个重要参数:</p><ol class=""><li id="424e" class="mm mn iq ky b kz la lc ld lf mo lj mp ln mq lr mr ms mt mu bi translated">轨道上的位置</li><li id="3c04" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">标题</li><li id="e0a6" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">航点</li><li id="1cff" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">磁迹宽度</li><li id="c7be" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">离中心线的距离</li><li id="6dfc" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">所有车轮都在轨道上</li><li id="3aa1" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">速度</li><li id="c1db" class="mm mn iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">转向角度。</li></ol><p id="9ff7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以利用这些参数作为我们自己定义的奖励函数的输入。请参见<a class="ae kv" href="https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-reward-function-input.html" rel="noopener ugc nofollow" target="_blank">文档</a>以了解所有可用参数的更多信息。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="e146" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated">培训过程怎么样？</h1><p id="9c17" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">培训的目标是找出在所有可能的未来驾驶决策中，哪种状态下的哪种行动将导致最大的预期累积奖励。</p><blockquote class="nt nu nv"><p id="68cc" class="kw kx ns ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">换句话说，网络必须计算出车辆在每种状态下应该获得多少<strong class="ky ir">速度</strong>和<strong class="ky ir">转向角</strong>，这样我们才能获得<strong class="ky ir">最大预期累积回报</strong>。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/c8f9978752e3d02b67bbbb548c6fd308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dWUjg2ULKBo2gihM"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰克·吉文斯在Unsplash上的照片</p></figure><h2 id="00e1" class="oc lt iq bd lu od oe dn ly of og dp mc lf oh oi me lj oj ok mg ln ol om mi on bi translated">但是……网络是什么？</h2><p id="7168" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">在MDP(马尔可夫决策过程)中，政策是在开始时给我们的，而在RL中不是。这里，我们利用神经网络(NN)来帮助我们创建一个策略网络。神经网络的输出是每个可能动作的概率。</p><p id="8d36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在AWS DeepRacer中，NN模型基本上是CNN +前馈NN，它接收来自特定剧集的经验(超参数)作为输入。一旦模型被训练，它将结果发送回代理以收集更多的经验。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/811ae0f064e5760e34a28c448d49a4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NL3PpCych31NB3M0GksOsw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络架构。[图片由作者提供]</p></figure><p id="2d94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练过程中，网络将发现哪种行为更有可能给予更多奖励，这取决于熵超参数。熵(随机程度)告诉代理探索更多的动作，或者只利用已经被理解的动作。</p><blockquote class="nt nu nv"><p id="1775" class="kw kx ns ky b kz la jr lb lc ld ju le nw lg lh li nx lk ll lm ny lo lp lq lr ij bi translated">熵越大，代理采取的行动就越随机。</p></blockquote><p id="c06e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据探索-开发策略，车辆仍可能有小概率采取随机行动来探索环境。RL <a class="ae kv" href="https://medium.com/@awjuliani/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=Entropy%20in%20Reinforcement%20Learning&amp;text=With%20these%20kinds%20of%20policies,entropy%20of%20that%20probability%20distribution.&amp;text=The%20greater%20the%20entropy%2C%20the,discrete%20probability%20distribution%20(p)." rel="noopener">这里</a>有一篇关于熵的好文章。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="60f6" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated">关于AWS DeepRacer的更多信息</h1><p id="0837" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">AWS DeepRacer令人兴奋的部分之一是车库。我们可以定义我们的车辆配置:要使用的传感器、动作空间(速度和转向旋转可变性)、车辆的颜色和车辆的名称！</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="b298" class="ls lt iq bd lu lv nh lx ly lz ni mb mc jw nj jx me jz nk ka mg kc nl kd mi mj bi translated"><strong class="ak">最后的话</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/5b3474786be66d9e31c2402972346d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w7ZAjALOcRb2Opaf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@thesollers?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安东·大流士</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="529b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">恭喜你坚持到这一步！！</p><p id="4585" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看完这篇文章，你应该知道RL是如何应用在自主赛车上的。请记住，我们的RL之旅仍处于早期阶段！我还有很多材料要和大家分享。所以，如果你喜欢这些内容，并想在接下来的两个月里继续和我一起学习，请关注我的媒体账号，以获得关于我未来帖子的通知！</p><p id="5292" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在下一篇文章中了解更多关于RL算法分类的鸟瞰图。<a class="ae kv" href="https://louisowen6.medium.com/birds-eye-view-of-reinforcement-learning-algorithms-landscape-2aba7840211c" rel="noopener">来看看</a>！</p><h1 id="bdf5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">关于作者</h1><p id="74ee" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">Louis Owen是一名数据科学爱好者，他总是渴望获得新知识。他在印度尼西亚最好的大学之一<a class="ae kv" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"><em class="ns">Institut Teknologi Bandung</em></a>攻读数学专业，并获得了最后一年的全额奖学金。最近，2020年7月，他刚刚以优异的成绩毕业。</p><p id="934e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Louis曾在多个行业领域担任分析/机器学习实习生，包括OTA(<a class="ae kv" href="https://www.linkedin.com/company/traveloka-com/" rel="noopener ugc nofollow" target="_blank"><em class="ns"/></a>)、电子商务(<a class="ae kv" href="https://www.linkedin.com/company/pt--tokopedia/" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> Tokopedia </em> </a>)、FinTech ( <a class="ae kv" href="https://www.linkedin.com/company/doitglotech/" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> Do-it </em> </a>)、智慧城市App ( <a class="ae kv" href="https://www.linkedin.com/company/qluesmartcity/" rel="noopener ugc nofollow" target="_blank"> <em class="ns"> Qlue智慧城市</em> </a>)，目前在<a class="ae kv" href="https://www.linkedin.com/company/the-world-bank/" rel="noopener ugc nofollow" target="_blank"> <em class="ns">世界银行</em> </a>担任数据科学顾问。</p><p id="7766" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">去路易斯的网站了解更多关于他的信息吧！最后，如果您有任何疑问或需要讨论的话题，请通过LinkedIn联系Louis。</p></div></div>    
</body>
</html>