<html>
<head>
<title>Backpropagation made easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播变得容易</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-made-easy-e90a4d5ede55?source=collection_archive---------15-----------------------#2020-10-04">https://towardsdatascience.com/backpropagation-made-easy-e90a4d5ede55?source=collection_archive---------15-----------------------#2020-10-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1aef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">反向传播在机器学习中是如此基础，却又如此令人生畏。但实际上，这比看起来容易。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/066ab3f04d0dfe955292cc49782a7eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tx2VxKsbWbwP20To"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@jamie452?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰米街</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="8868" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">学习机器学习(ML)并不需要数学天才。基本上，你所需要的只是大学一年级水平的微积分、线性代数和概率论，你已经准备好了。但是在ML看似良性的第一印象背后，有大量与ML相关的数学理论。对于许多人来说，学习ML的第一个真正障碍是反向传播(BP)。这是我们用来推导神经网络(NN)中参数梯度的方法。训练模型是梯度下降算法中的一个必要步骤。</p><p id="0da3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BP是任何NN训练中非常基础的一步。它涉及链规则和矩阵乘法。但很多ML课程或教程中BP的引入方式并不尽如人意。当我第一次在Coursera的机器学习课上学习BP时，我对它的计算过程非常困惑，以至于暂停了几个月。同时，我搜索了更多关于BP的解释。我设法通过了这门课程。我完成了编码作业。但是英国石油公司在我的脑海中仍然是一个非常混乱和令人困惑的模糊形象。</p><p id="8b11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你完全不了解BP，简单地把它当成一个黑箱，其实也没什么坏处，因为Tensorflow或者Pytorch可以自动为你执行BP。但是最近我在复习关于ML的笔记，我开始正确理解BP。我的方法是建立一个简单的神经网络，明确地写下每个参数和变量矩阵/向量，并通过链式法则逐步写下每个参数矩阵/向量的梯度计算。最终，BP比我最初想象的要容易得多。</p><h1 id="1bf3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">使用两层NN和单个输入样本作为例子</h1><p id="3c3e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我的内涵是基于Coursera的深度学习规范课程1。我将用一个两层神经网络作为例子。两层刚好“足够深”，因为计算层1中的梯度的技术可以重复用于任何附加层。让我们看到关于每层神经元数量的参数的维数也是有帮助的，例如，注意输入是一个2维向量，第1层中有三个神经元，因此w[1]是一个3*2矩阵。第2层或输出层有两个神经元。如果神经网络用于二值分类，输出层是一个神经元；如果神经网络用于多重分类，输出层的神经元数量与类别相同。为了概括我们的分析，我们只考虑一个双神经元输出层作为例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/a522ce7477b1f7ff931612b63d149b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxgbLxOyfQ071iUjDxlQOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单的两层神经网络(图片由作者提供)</p></figure><p id="a5d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在输入层，即第0层，我们有一个2*1的输入向量x，现在我们只关注单个样本输入。如果我们想把输入看作一批m个样本，那么输入的维数实际上是2*m .但是我们这里只考虑一个输入实例。</p><p id="462c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第一层，我们有三个神经元，矩阵w[1]是一个3*2的矩阵。在这个NN中，每层也有一个偏置向量b[1]和b[2]。b[1]是一个3*1向量，b[2]是一个2*1向量。在正向传递中，我们有以下关系(以矩阵形式和矢量化形式书写):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/407d15ea2d11ea3986458fa1dd7cb719.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/0*lOpSrhK2uBnqs2F0"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一层运算的矩阵乘法</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3fe505f4101eee66b39e1514588d3b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*Koa-CtJkORtDqJbG"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一层操作的矢量化表达</p></figure><p id="1f8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，第2层的矢量化运算可以写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/65f995bc1e22c3fd73e2d8dfa94406ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/0*GGmBUNTfe4co1iuA"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第二层操作的矢量化表达。请注意，a^[1]作为这一层的输入。</p></figure><blockquote class="mt mu mv"><p id="fa53" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">暂且忽略激活功能和损耗功能的细节。</strong></p></blockquote><p id="fd79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每一层，我们将使用非线性激活函数g(z)，例如sigmoid函数或ReLU。为了简单起见，我们省略了g(z)的细节，只是继续使用g(z)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/59e7e05e80086d3f605bdf868dfa8394.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*asM442o3s1fCJVcl"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">a[1]和z[1]之间的关系</p></figure><p id="fb85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，一旦我们决定使用哪个g(z ), g(z)对z的导数是已知的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b59d0eade1a0f84a59939422711ab5a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/0*vbr6Ob7YXmQmtOio"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">g'(z)是我们已知的。</p></figure><p id="9d7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在神经网络的输出端，我们有一个损失函数J()，我们将努力使其最小化。通常是交叉熵。为了简单起见，我们暂时不讨论J()的细节。同样，它是a[2]和y的函数，所以我们可以直接计算它对a[2]的导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/87c5a2a911446b3b5f907d86f182fafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/0*5awWQ4yT7iCg5Uaw"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">损失函数</p></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><blockquote class="mt mu mv"><p id="f217" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">从最后一层开始，得到部分方向的dz[2]。</strong></p></blockquote><p id="5d8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要执行BP，我们需要从最后一层向后到第一层。在最后一层，我们可以直接从损失函数J得到da[2]的偏导数，因为J是a[2]的函数。假定我们知道导数g'(z)，通过下面的函数从da[2]获得偏导数dz[2]也是直接的。请注意，最后一个等式中的运算符(圆圈中的点)表示元素级乘法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/983bd65ad0bbaef2f10effa24b4c5d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*cuVafDBFNY_6BtC0"/></div></figure><blockquote class="mt mu mv"><p id="5ce9" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">一旦我们获得了dz[2]，我们将试图找到dw[2]和db[2]。</strong></p></blockquote><p id="19f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以w[2]中的w7为例，这个参数乘以a1，然后加到z4。根据链式法则，dw7等于:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/22327965a033d006b8c3894e2b244bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/0*iON36yXfqjeh4xOK"/></div></figure><p id="085e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，w[2]和dz[2]之间的关系如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e0bdef4dc912d049ca80a2353006430a.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/0*BWapHfEZhKgRw6Cv"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/fcdb887df6f231491c0213e2014172b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*GAJ-njfpxe1Th-_D"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/47bdbe0539ae333ee51f4ed5585063b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/0*4bv_IDJ-4awoqsaQ"/></div></figure><p id="8ccd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以看到dw[2]无非是dz[2]的向量相乘和a[1]的转置。db[2]就是dz[2]本身。</p><blockquote class="mt mu mv"><p id="e966" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">然后，我们可以进入第1层，计算da[1]和dz[1]。</strong></p></blockquote><p id="36f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以a[1]中的a1为例。请注意，a1乘以w7和w8，并分别与z4和z5相加。根据链式法则，da1也将由这两部分组成。da[1]和dz[1]的细节如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b42235239c5a6146273b8d0d49be3055.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*8nQ0RtDCTKn5fOtj"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d7e7d567c0458072d4527f03e521b2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/0*1UgdQk-6DbhWEjZ1"/></div></figure><p id="7710" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，da[1]是用第2层的参数w[2]和dz[2]计算的。</p><blockquote class="mt mu mv"><p id="9fe2" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><strong class="ky ir">使用dz[1]，我们可以获得dw[1]和db[1]，就像在第2层中一样。</strong></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/864c9cc554479e23e787e9c0cee23054.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/0*QGVZmEHhxv7TZ0Tj"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d2da81333a5299ce9ebdc6501cb8717b.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/0*Gv9KIAhWTZ_HT5jF"/></div></figure><p id="41ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到dw[1]的计算是dz[1]向量和x的转置相乘，你也可以把x想象成“0”层的输出向量a[0]。因此得到dw[l]=dz[l]*a[l-1]的一致表达式。t(。t表示转置，l是随机层数)。</p><p id="0f90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过以上步骤，我们获得了dw[2]、db[2]、dw[1]和db[1]。对于超过2层的NN，可以对每一层重复上述步骤:对于任何一层，我们只需要获得dz[l](或者直接从J获得，或者从上一层获得)，用它来计算dw[l]和db[l]；得到da[l-1]和dz[l-1]，进行到l-1层。</p><p id="94ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于具有L层的NN，BP归结为以下四个主要步骤:</p><ol class=""><li id="3917" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated"><strong class="ky ir">从最后一层</strong>，从损失函数J计算da[L]；然后从da[L]得到dz[L]；设l= L，</li><li id="ad8a" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">用dz[l] </strong>计算当前层参数的梯度:计算dw[l]=dz[l]a[l-1]。t；db[l]= dz[l]；</li><li id="5a80" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated"><strong class="ky ir">用dz[l] </strong>计算上一层输出的梯度:用da[l-1] = w[l]计算da[l-1]。T * dz[l]，从da[l-1]得到dz[l-1]。</li><li id="a9a5" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">设置l=l-1，从第二步<strong class="ky ir">开始重复，直到到达第一层</strong>。</li></ol><p id="63fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，在每一层，我们都想找出dz[l]，一切都从那里开始。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="d13b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">然后我们来看J()和g()的细节。</strong>我们假设g(z)是sigmoid函数，J()是交叉熵。它们的格式和渐变如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/60881f8907c1062e0bdcb089a0790756.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*DZ6gyuAiXrnO3ur1"/></div></figure><p id="7ecd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们NN的情况下，J是a[2]和y的函数，我们可以从下面的函数得到dz[2]。一旦我们有了dz[2]的显式表达式，我们就可以用上面的步骤执行BP。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fc4752c32f2ea9030acb205b2a5ffcee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/0*K0xHQSn8STpJ2gij"/></div></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="ba4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们来看看多个输入实例的情况。如果输入包含m个实例，那么输出和中间矩阵A[1]，A[2]，Z[1]和Z[2]都有m列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/72c99da6cfa689cdbc29ea76f63573da.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/0*7gEZJZJNt4QxNQeO"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">x，Y，A[1]，A[2]，Z[1]，Z[2]有m列</p></figure><p id="12f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果损失函数J是预期的交叉熵，则dw和db的计算需要包括1/m项，但其他一切基本上仍然相同:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f64f4ae00c5d7ac8bbb28e1d35591c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*NMYOBydFMqstm7CE"/></div></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="1651" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样。我们使用简单明了的两层神经网络作为例子，并对单个输入和多个输入执行BP。希望这个简单的教程能帮助你更好地了解BP。</p><p id="7842" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考:</p><p id="8d2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] Coursera深度学习规范课程1:神经网络和深度学习</p><p id="3801" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://www.youtube.com/watch?v=ibJpTrp5mcE&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ibJpTrp5mcE&amp;feature = youtu . be</a></p><p id="f89c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]<a class="ae kv" href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/BP.pdf" rel="noopener ugc nofollow" target="_blank">http://speech . ee . NTU . edu . tw/~ tlkagk/courses/ML _ 2016/Lecture/BP . pdf</a></p></div></div>    
</body>
</html>