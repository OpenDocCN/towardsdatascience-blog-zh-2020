<html>
<head>
<title>Revision for Deep Image Inpainting and Review: Patch-Based Image Inpainting with Generative Adversarial Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深层图像修复的修订与回顾:基于生成式对抗网络的图像修补</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/revision-for-deep-image-inpainting-and-review-patch-based-image-inpainting-with-generative-4197d29c5468?source=collection_archive---------18-----------------------#2020-10-08">https://towardsdatascience.com/revision-for-deep-image-inpainting-and-review-patch-based-image-inpainting-with-generative-4197d29c5468?source=collection_archive---------18-----------------------#2020-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1e20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">欢迎回来的家伙:)今天，我想给一个修订的深层图像修复，我们已经谈到目前为止。还有，想再复习一篇图像修复的论文，巩固深度图像修复的知识。让我们一起学习和享受吧！</p><h1 id="5320" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">回忆</h1><p id="3a2a" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在这里，让我们首先简单回顾一下我们从以前的帖子中学到了什么。</p><p id="bf98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">上下文编码器(CE)</strong>【1】是文献中第一个<strong class="jp ir">基于GAN的修复算法</strong>。它强调理解整个图像的上下文对于修复任务的<strong class="jp ir">重要性，并且<strong class="jp ir">(通道方式)全连接层</strong>用于实现这样的功能。详情可以点击<a class="ae lo" href="https://medium.com/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244" rel="noopener">此处</a>链接往期帖子。</strong></p><p id="1a71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">多尺度神经面片合成(MNPS)</strong>【2】可以看作是CE的改进版。它由两个网络组成，即内容网络和纹理网络。内容网络是CE，纹理网络是用于对象分类任务的预训练的VGG-19。使用纹理网络的想法来自于最近神经风格转移的成功。简单地说，用于高级视觉任务(例如物体分类)的预训练网络中的神经响应包含关于图像风格的信息。通过鼓励缺失区域内外类似的神经反应，我们可以进一步增强生成像素的纹理细节，从而使完成的图像看起来更真实。强烈建议感兴趣的读者在此浏览帖子<a class="ae lo" href="https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">了解详情。</a></p><p id="a62d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">全局和局部一致图像补全(GL CIC)</strong>【3】是深度图像修复任务中的一个里程碑。作者采用全卷积网络(FCN)和扩展卷积(DilatedConv)作为他们提出的模型的框架。FCN允许各种输入大小和DilatedConv取代(通道方式)完全连接层，用于了解整个图像的背景。此外，两个鉴别器用于在两个尺度上区分完整图像和真实图像。全局鉴别器查看整个图像，而局部鉴别器关注局部填充的图像块。我强烈推荐读者在这里看一看帖子<a class="ae lo" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df"/>，尤其是CNN中的扩张卷积。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="422c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天，我们将回顾这篇论文，基于生成性对抗网络的图像修补[4]。这可以看作是GLCIC的一种变体，因此我们可以对这种典型的网络结构做一些修改。</p><h1 id="22ec" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">动机</h1><p id="52ff" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">本文作者希望利用剩余连接和PatchGAN鉴别器的优势来进一步改善他们的修复效果。</p><p id="d46a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于图像识别的深度残差学习(<a class="ae lo" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">ResNet</a>)【5】在深度学习方面取得了显著的成功。通过使用残差块(残差连接)，我们能够训练非常深的网络，许多论文已经表明残差学习对于获得更好的结果是有用的。</p><p id="f238" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PatchGAN  [6]在图像到图像的翻译方面也取得了巨大的成功。<strong class="jp ir">与典型GAN中的鉴别器相比，PatchGAN鉴别器(参见下面的图1)输出一个矩阵(2d阵列),而不仅仅是单个值。</strong>简单来说，典型GAN鉴频器的输出是从0到1的单一值。这意味着鉴别器查看整个图像并决定这个图像是真的还是假的。如果图像是真实的，它应该给1。如果图像是假的(即生成的图像)，它应该给0。这个公式关注于整个图像，因此图像的局部纹理细节可能被忽略。另一方面，PatchGAN鉴别器的输出是一个矩阵，该矩阵中的每个元素的范围从0到1。注意，每个元素代表输入图像中的一个局部区域，如图1所示。因此，这一次，鉴别器查看多个局部图像块，并且必须判断每个块是否是真实的。通过这样做，可以增强生成的图像的局部纹理细节。这就是PatchGAN被广泛用于图像生成任务的原因。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/2d964c1ce513bde684eb4f7571f3ab34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*f-cmriSHtg8PKOXR-uWaEg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图一。PatchGAN鉴别器。输出是矩阵，矩阵中的每个元素代表输入图像中的局部区域。如果局部区域是真实的，我们应该得到1，否则0。摘自[4]</p></figure><h1 id="3537" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="2a6b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">图像修复可以看作是一种图像生成任务。我们希望填充图像中的缺失区域(即生成缺失的像素)，以使图像完整且看起来逼真。</p><p id="e115" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了生成逼真的图像，GAN通常用于不同的图像生成任务，包括图像修复。典型的GAN鉴别器查看整个图像，仅通过单个值[0，1]来判断输入是否真实。这种GAN鉴别器在本文中称为全局GAN (G-GAN)。</p><p id="b12d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，PatchGAN查看输入中的多个局部区域，并独立决定每个局部区域的真实度，如前一节所述。研究人员已经表明，使用PatchGAN可以通过关注更多的局部纹理细节来进一步提高生成图像的视觉质量。</p><h1 id="1e99" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">解决方案(简而言之)</h1><ul class=""><li id="a86f" class="mi mj iq jp b jq lj ju lk jy mk kc ml kg mm kk mn mo mp mq bi translated">在生成器中使用了具有扩展卷积的残差块(<strong class="jp ir">扩展残差块</strong>)。(作者期望通过使用残差学习可以增强修复结果)</li><li id="58f2" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">patch GAN和G-GAN鉴别器的混合(PGGAN) </strong>被提出以鼓励输出的完整图像应该是全局和局部逼真的。(与GLCIC中的意图相同，使用两个鉴别器，一个全局的，一个局部的)</li></ul><h1 id="e433" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">贡献</h1><ul class=""><li id="65cc" class="mi mj iq jp b jq lj ju lk jy mk kc ml kg mm kk mn mo mp mq bi translated"><strong class="jp ir">patch GAN和G-GAN鉴别器的组合(PGGAN) </strong>其中早期卷积层被共享。他们的实验结果表明，它能进一步增强生成像素的局部纹理细节。</li><li id="2848" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">扩张和插值卷积</strong>用于发生器网络。通过使用<strong class="jp ir">膨胀的残余块</strong>，修复结果得到了改善。</li></ul><h1 id="a6d3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">方法</h1><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/35f571ba92726682b269124f2e387569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHDQtkrsqHsGfJ0-_uZvCQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图二。提出的生成ResNet结构和PGGAN鉴别器。摘自[4]</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nb"><img src="../Images/7329ee65c36c3bcda84461c6616e08a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xFke9lsPYeN1q7F_.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图3。全球土地信息中心的拟议结构。摘自[3]</p></figure><p id="df46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图2和图3分别显示了本文和GLCIC提出的网络结构。很明显，它们是相似的。两个主要区别在于:I)发生器中使用了膨胀的残差块；<strong class="jp ir"> ii)修改GLCIC中的全局和局部鉴别器。</strong></p><p id="94f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在GLCIC中，全局鉴别器将整个图像作为输入，而局部鉴别器将填充区域周围的子图像作为输入。将两个鉴别器的输出连接起来，然后返回一个值，以显示输入是真的还是假的(<strong class="jp ir">一次不利损失</strong>)。从这个角度来看，局部鉴别器将集中在局部填充的图像补片上，因此可以增强填充补片的局部纹理细节。一个主要缺点是<strong class="jp ir">局部鉴别器的输入依赖于缺失区域</strong>并且作者在训练期间假设单一矩形缺失区域。</p><p id="aa85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于PGGAN鉴别器，我们有几个<strong class="jp ir">早期共享卷积层</strong>如图2所示。然后，我们有<strong class="jp ir">两个分支</strong>，一个给出单个值作为输出(G-GAN)，一个给出矩阵作为输出(PatchGAN)。请注意，1×256是16×16矩阵的变形版本。如上所述，这也是一种让鉴别器在区分完整图像和真实图像时同时关注全局(整个图像)和局部(局部图像小块)信息的方式。请注意，我们将有两个不利的损失，因为在这种情况下我们有两个分支。</p><h1 id="a3f4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">扩张的残余阻滞</h1><p id="285e" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在我之前的文章中，我已经介绍了CNN中的扩张卷积。简单回忆一下，<strong class="jp ir">通过跳过连续的空间位置</strong>，扩大的卷积增加了感受野而没有增加额外的参数。对于忘记这个概念的读者，请随意先重温一下<a class="ae lo" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">我之前的帖子</a>。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/59df6714a978b39d29ac0c504c77110a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*2ISL4sHL1glPMtAnM28lZQ.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图4。剩余块类型。自上而下:标准残余阻滞，先有扩张回旋的扩张残余阻滞，后有扩张回旋的扩张残余阻滞。摘自[4]</p></figure><p id="997a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图4示出了不同类型的残差块。为了便于我们的进一步讨论，我将简要介绍图4顶部所示的一个基本残差块。</p><p id="fd4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单地说，残差块可以公式化为Y = X + F(X)，其中Y是输出，X是输入，F是几层的序列。在图4的基本剩余块中，F是conv-诺姆-雷鲁-Conv。这意味着我们将X馈送到一个卷积层，然后是归一化层、ReLU激活层，最后是另一个卷积层，以获得F(X)。一个要点是输入X直接加到输出Y上，这就是我们称之为跳过连接的原因。由于沿此路径没有任何可训练参数，我们可以确保在反向传播期间必须有足够的梯度传递到早期层。因此，我们可以训练非常深的网络，而不会遇到梯度消失的问题。</p><h1 id="0690" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">为什么是残块？</h1><p id="fa5c" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">你可能想知道使用剩余块的好处。你们有些人可能已经知道答案了。下面我来说说我的看法。</p><p id="64fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们来比较一下Y = X + F(X)和Y = F(X)。对于Y = X + F(X)，我们实际学到的是<strong class="jp ir"> F(X) = Y - X，Y和X的差，这就是所谓的剩余学习，X可以作为剩余学习的参考。</strong>另一方面，对于Y = F(X)，我们直接学习将输入X映射到输出Y，无需参考。所以，人们认为剩余学习相对容易。更重要的是，很多论文都表明，残差学习可以带来更好的效果！</p><p id="a4f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于扩展卷积有助于增加感受野，而感受野对于修复任务非常重要，因此作者用扩展卷积层替换了两个标准卷积层中的一个，如图4所示。有两种类型的扩张残余阻滞，<strong class="jp ir"> i)首先放置扩张的回旋</strong>和<strong class="jp ir"> ii)其次放置扩张的回旋</strong>。在本文中，基于所采用的膨胀的残余块的数量，膨胀率从1开始增加两倍。例如，如果有4个膨胀的残余块，膨胀率将是1、2、4、8。</p><h1 id="5218" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">插值卷积</h1><p id="3435" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">为了解决由标准反卷积(即转置卷积)引起的伪影，作者在这项工作中采用了插值卷积。对于插值卷积，首先使用双线性和双三次插值等典型插值方法将输入<strong class="jp ir">调整到所需大小。<strong class="jp ir">然后，应用标准卷积</strong>。下图5显示了转置卷积和插值卷积之间的差异。</strong></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b71244c4236342bab1fb1b961fd5f113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*L_jPi5JAr3Bh9NqOsqlj1g.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图5。使用转置卷积(上)和插值卷积(下)获得的结果的视觉比较。摘自[4]</p></figure><p id="4af4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我看来，这两种卷积都有相似的性能。有时候转置卷积更好，有时候插值卷积更好。</p><h1 id="7dde" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">鉴别器网络</h1><p id="50e9" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们已经讨论了本文中使用的PGGAN鉴别器。这里，回想一下，鉴别器有两个分支，一个分支给出单个值，就像global-GAN (G-GAN)一样，另一个分支给出256个值，其中每个值表示输入中局部区域的真实度。</p><blockquote class="ne nf ng"><p id="ac34" class="jn jo nh jp b jq jr js jt ju jv jw jx ni jz ka kb nj kd ke kf nk kh ki kj kk ij bi translated">关注输入中多个局部区域的真实性有助于改善完整图像的局部纹理细节。</p></blockquote><h1 id="2cbe" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">目标函数</h1><p id="896d" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">实际上，本文中使用的损失函数(即目标函数)或多或少与我们以前覆盖的论文相同。</p><p id="dff9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">重建损失</strong>:该损失是为了保证逐像素重建精度。对于这种损失，我们通常采用L1或L2(欧几里得)距离。本文使用<strong class="jp ir"> L1损失</strong>作为它们的重建损失，</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/addbdf28ad419686b262efa571d5aa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*_XjimrypMU_CrwPu6IvYsg.png"/></div></figure><p id="51dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nh"> N </em>是训练批次中图像的数量。<em class="nh"> W </em>、<em class="nh"> H </em>和<em class="nh"> C </em>是训练图像的宽度、高度和通道。<em class="nh"> x </em>和<em class="nh"> y </em>是模型给出的地面真实和完整图像。</p><p id="4d10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对抗性损失:我想你们大多数人现在都很熟悉这种典型的对抗性损失。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/131764230027bb28284609dce3c95638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*X6VMD2YnkQQbtL2VDtKklw.png"/></div></figure><p id="c26e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nh"> x </em>是地面真值，所以我们要<em class="nh"> D </em> ( <em class="nh"> x </em>)返回1，否则0。注意<em class="nh"> D </em>只是鉴别器的函数形式。</p><p id="2a1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">关节损失</strong>:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a2d2110d2aa77c30fb432a1b2dab6135.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*JJmwfiTNIz6WO1S5FWOnaQ.png"/></div></figure><p id="3b31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式3是它们的联合损失函数。λ1，2，3用于平衡每个损失的重要性。<em class="nh"> g_adv </em>表示全局分支给出的输出，而<em class="nh"> p_adv </em>表示PatchGAN分支给出的输出。注意，在它们的实验中，λ1、2、3分别被设置为0.995、0.0025和0.0025。</p><h1 id="060f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">实验结果</h1><p id="8ec3" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在他们的实验中使用了三个数据集。<strong class="jp ir"> i) </strong> <strong class="jp ir">巴黎街景</strong>【7】包含14900张训练图像和100张测试图像。<strong class="jp ir"> ii) </strong> <strong class="jp ir">谷歌街景</strong>有62058张高分辨率图片，分为10个部分。第一和第十部分用于测试，第九部分用于验证，其余部分用于训练。总共有46，200个训练图像。<strong class="jp ir"> iii) </strong> <strong class="jp ir">地点</strong>由超过800万张训练图像组成。该数据集仅用于测试，以显示概化能力。</p><p id="6e4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了比较典型残余块和扩张残余块的性能，作者训练了两个模型，即<strong class="jp ir"> PGGAN-Res </strong>和<strong class="jp ir"> PGGAN-DRes </strong>。对于<strong class="jp ir"> PGGAN-Res，使用了基本残差块和3个子采样块</strong>。这意味着输入被下采样2/3倍。对于<strong class="jp ir"> PGGAN-DRes，使用了扩张的残差块和2个子采样块</strong>。这意味着输入被下采样2倍。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi no"><img src="../Images/47f8288ca37a9155bb0fd97b9c9ec101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVVnSmoV6EPAn9zhtdSPcg.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图6。训练具有不同鉴别器结构的相同生成器网络的结果。摘自[4]</p></figure><p id="7ec5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图6示出了用不同鉴别器结构训练相同生成器网络的修复结果。从图6的最后一列可以看出，如果仅使用G-GAN鉴别器，则观察到窗口的局部纹理细节较差。与G-GAN相比，PatchGAN提供了更好的窗口局部纹理细节，但窗口的角看起来与全局结构不一致。总的来说，PGGAN可以提供最佳视觉质量的结果。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b195ff2d60deff131323c4324a82d3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*Crk1sn7guwuLHVLbH10sGg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">表1。巴黎街景256x256图像的定量比较。摘自[4]</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi np"><img src="../Images/373f0c378406c7ec9ac9885455d65022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*UTska4ndADqpewUaYC6MIw.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">表二。巴黎街景512×512图像的定量比较。摘自[4]</p></figure><p id="e0c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">表1和表2显示了在256×256和512×512两种分辨率的巴黎街景数据集上不同方法的定量比较。注意CE是上下文编码器[1]，NPS是多尺度神经补片合成(MNPS) [2]，GLGAN是全局和局部一致的图像完成(GLCIC) [3]。我们已经在以前的文章中讨论了所有这些方法。</p><p id="1389" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从表1和表2中可以明显看出，PGGAN在所有这些指标上都有所改善。但是，请记住，视觉质量比这些客观的评估指标更重要。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/67de1aa1203904343c0bfda9efbc6631.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*J7P1R8irimI5or-pR2RsDg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图7。使用不同方法对完整图像的感知比较。摘自[4]</p></figure><p id="954f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者对这些方法进行了感知评估，如图7所示。要求12个投票者对原始图像的自然度和各种方法的修复结果进行评分。每位投票者被随机分配了来自巴黎街景数据集中的500张照片。请注意，CE是在128×128图像上训练的，因此它在256×256测试图像上的性能很差。其他方法在这种感知评估中具有相似的性能。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nr"><img src="../Images/3f80e6f958dc415d737ea5dc08f31730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrDa0BIRpM4NAhnTg2ix8w.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图8。256x256巴黎街景数据集上的定性比较。摘自[4]</p></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ns"><img src="../Images/a09da53828d589cebd9d40a95ad761fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPDxPNEo71gjutSDbvk3Jw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图9。512x512巴黎街景数据集上的定性比较。摘自[4]</p></figure><p id="0706" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图8和图9分别显示了尺寸为256×256和512×512的图像的修复结果。我建议读者放大以更好地查看结果。在我看来，PGGAN-DRes和PGGAN-Res通常会给出具有更好的局部纹理细节的结果，例如，参见图8中的第4行和图9中的第3行。</p><h1 id="fae3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结论</h1><p id="1097" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">首先，残差学习的概念以扩张残差块的形式嵌入到生成器网络中。从他们的实验结果来看，残差学习有助于提高修复性能。</p><p id="aa03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其次，PatchGAN鉴别器的概念与传统的GAN鉴别器(G-GAN)相结合，以鼓励更好的局部纹理细节和全局结构一致性。</p><h1 id="f2de" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">外卖食品</h1><p id="d233" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">和以前一样，我想在这一部分列出一些有用的观点。如果你关注过我以前的帖子，你会发现这个帖子相对简单。</p><p id="7176" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其实文中的大部分东西都和GLCIC [3]差不多。在网络结构中嵌入了两个新概念，即残差块和PatchGAN鉴别器，以进一步增强修复效果。</p><p id="c850" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望你能实现这种典型的图像修复网络架构。在后来的修复论文中提出的网络或多或少是相同的。</p><p id="6963" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你还应该注意到，重建损失和对抗损失是图像修复任务的两个基本损失。在后来的修复论文中提出的方法必须包括L1损失和对抗性损失。</p><h1 id="0a27" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">下一步是什么？</h1><p id="1dd7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这是我的第四篇关于深度图像修复的文章。到目前为止，我们实际上已经涵盖了几乎所有深度图像修复的基础知识，包括图像修复的目标，修复的典型网络架构，损失函数，一般图像修复的困难，以及获得更好修复结果的技术。</p><p id="f0e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从下一篇文章开始，我们将会深入到更多的修复论文中，这些论文为图像修复设计了更具体的技术。假设你们已经知道了基础知识，我可以花更多的时间来解释这些修复技术。尽情享受吧！:)</p><h1 id="196d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">参考</h1><ol class=""><li id="d12c" class="mi mj iq jp b jq lj ju lk jy mk kc ml kg mm kk nt mo mp mq bi translated">Deepak Pathak，Philipp krhenbüHL，Jeff Donahue，Trevor Darrell和Alexei A. Efros，"<a class="ae lo" href="https://arxiv.org/pdf/1604.07379.pdf" rel="noopener ugc nofollow" target="_blank">上下文编码器:通过修补进行特征学习</a>，"<em class="nh"> Proc .计算机视觉与模式识别</em> ( <em class="nh"> CVPR </em>)，2016年6月27-30日。</li><li id="5fda" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">杨超，吕鑫，林哲，Eli Shechtman，Oliver Wang，和郝力，<a class="ae lo" href="https://arxiv.org/pdf/1611.09969.pdf" rel="noopener ugc nofollow" target="_blank">使用多尺度神经补片合成的高分辨率图像修复</a>，<em class="nh"> Proc .计算机视觉与模式识别</em> ( <em class="nh"> CVPR </em>)，2017年7月21-26日。</li><li id="fb14" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">饭冢聪、埃德加·西蒙-塞拉和石川宽，“<a class="ae lo" href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf" rel="noopener ugc nofollow" target="_blank">全球和局部一致的图像完成</a>、<em class="nh"> ACM Trans。论图形</em>，第36卷第4期第107条，出版日期:2017年7月。</li><li id="a2f8" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">https://arxiv.org/pdf/1803.07422.pdf，<a class="ae lo" href="https://arxiv.org/pdf/1803.07422.pdf" rel="noopener ugc nofollow" target="_blank">乌古尔·德米尔和戈兹德·乌纳尔</a>，<a class="ae lo" href="https://arxiv.org/pdf/1803.07422.pdf" rel="noopener ugc nofollow" target="_blank">基于补丁的图像修复。</a></li><li id="b7f8" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">、何、、、任、、、<a class="ae lo" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">【用于图像识别的深度残差学习】、<em class="nh"> Proc .计算机视觉与模式识别</em> ( <em class="nh"> CVPR </em>)，2016年6月27-30日。</a></li><li id="7cab" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">Phillip Isola，，Tinghui Zhou，和Alexei A. Efros，《<a class="ae lo" href="https://arxiv.org/pdf/1611.07004.pdf%EF%BC%89" rel="noopener ugc nofollow" target="_blank">有条件对抗网络的图像到图像翻译》。计算机视觉与模式识别 ( <em class="nh"> CVPR </em>)，2017年7月21-26日。</a></li><li id="59f4" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nt mo mp mq bi translated">C.Doersch，S. Singh，A. Gupta，J. Sivic和A. A. Efros。“是什么让巴黎看起来像巴黎？，"<em class="nh"> ACM Trans。关于图形</em>，第31卷第4期，第101条，出版日期:2012年7月。</li></ol><p id="b962" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢您阅读我的帖子！如果您有任何问题，请随时询问或在此留下评论。下次见！:)</p></div></div>    
</body>
</html>