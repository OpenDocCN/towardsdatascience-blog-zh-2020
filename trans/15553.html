<html>
<head>
<title>Reverse Engineering Graph Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逆向工程图卷积网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reverse-engineering-graph-convolutional-networks-dd78d4682ea1?source=collection_archive---------43-----------------------#2020-10-26">https://towardsdatascience.com/reverse-engineering-graph-convolutional-networks-dd78d4682ea1?source=collection_archive---------43-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/85c1858e6c22a5df5c4eca400e9b1678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tmfGcgbyw2l-TvLl"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">让我们对GCN进行逆向工程。在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae jd" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></p></figure><div class=""/><p id="580b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇博客文章将总结“<a class="ae jd" href="https://arxiv.org/pdf/1902.07153.pdf" rel="noopener ugc nofollow" target="_blank">简化图卷积网络</a>【1】”这篇文章试图对图卷积网络进行逆向工程。因此，让我们向后发展图卷积网络。</p><p id="4493" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图是结构的普遍模型。它们无处不在，从社交网络到化学分子。各种事物都可以用图表来表示。然而，将机器学习应用于这些结构并不是我们直接想到的。机器学习中的一切都来自于一个小的简单的想法或模型，它根据需要随着时间变得复杂。举个例子，最初，我们有发展成多层感知的感知机，类似地，我们有发展成非线性CNN的图像滤波器，等等。然而，图卷积网络，被称为GCN，是我们直接从现有的想法中衍生出来的，并有一个更复杂的开始。因此，为了揭穿GCNs，本文试图对GCN进行逆向工程，并提出一种简化的线性模型，称为<strong class="kf jh">简单图卷积(SGC)。</strong> SGC在应用时可提供与GCNs相当的性能，甚至比快速GCN还要快。</p><h1 id="0e17" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">基本符号</h1><p id="55f7" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">图卷积网络的输入是:<br/> 1。节点标签<br/> 2。邻接矩阵</p><p id="6ef0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">邻接矩阵:</strong>邻接矩阵<strong class="kf jh"> A </strong>为<strong class="kf jh"> n x n，</strong>矩阵其中n为节点个数，若节点I与节点j相连则a(i，j) = 1否则a(i，j) = 0。如果边是加权的，那么a(i，j) =边权重。</p><p id="6397" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">对角矩阵:</strong>对角矩阵<strong class="kf jh"> D </strong>是n×n矩阵，d(i，i) =邻接矩阵的第<strong class="kf jh"> i </strong>行之和。</p><p id="2b19" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">输入特征:</strong> X是大小为<strong class="kf jh"> n x c </strong>的输入特征矩阵，c为类别数。</p><p id="4664" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在逆向工程之前，让我们看看GCNs实际上是如何工作的。</p><h1 id="9446" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">图卷积网络在行动</strong></h1><p id="5f5b" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">当我们通过CNN层输入图像时，会发生什么？它使用一个像素及其相邻像素来给出特定索引的输出。同样的行为在GCNs中也很普遍。对于每一层，我们传播一个新的特征:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/48a30b00d09c5bb7792535566663bf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*E3GHB_PZ9Z7jeXJLb_HhRg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">正向传播步长</p></figure><p id="ce51" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，该步骤可以矩阵形式表示为:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/4553adb297f4e792ac1b03e7fa1054c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*SfRbaAuzk2Jd5EN764cF3g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">第k个输出通过将S乘以前一个输出获得</p></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d4ad3869cbc6f25120e79d8ee120b34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*tqTtIF8nMIhacTWCWt8HeA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">归一化邻接矩阵</p></figure><p id="488a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，每下一步都可以表示为S与H(i-1)的稀疏矩阵乘法</p><p id="066f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是平滑局部隐藏的表示。怎么会？每一步都从邻居传播特征。因此，在第一步之后，节点<strong class="kf jh"> x </strong>具有其自身以及其邻居的信息。在第二步之后，它再次从它的邻居那里获取信息，但是他们已经有了它的2步远邻居的信息，所以这些添加到节点<strong class="kf jh"> x </strong>上。同样，信息是在每一跳收集的。现在，将结果乘以权重矩阵<strong class="kf jh"> W </strong>，并且应用逐点非线性函数(ReLU)来获得下一个特征矩阵。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/45a0dd22d4c86fe9a837aaa80512e0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*9PAClBwpoZkfqRnF_NkiTg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">非线性激活函数</p></figure><p id="9da0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由此获得的结果可以被输入到softmax层以获得分类概率。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/1352ad51ae7177c69bcd8295a4574378.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*JQXxbJVHDv_pphkwSOh3pg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用GCNs分类(等式1)</p></figure><p id="ca4a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图卷积网络可以用下图来概括:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/6fdb99dc0e0730b48d2bc55696c7a201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6pN7EFe4YPbAkbFHUC1g2w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">GCN在行动| <a class="ae jd" href="https://arxiv.org/abs/1902.07153" rel="noopener ugc nofollow" target="_blank">来源</a> [1]</p></figure><h1 id="b82e" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">简单图形卷积:GCN简化</strong></h1><p id="913a" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">让我们回忆一下MLP和CNN。在CNN的节目中，每一层都有一个局部的感知，形成了我们的特征地图，随着我们越走越深，每个值都比以前有更多的接收。类似地，GCNs从每一跳的邻居获得信息，因此每一层之后，每个节点都有更多关于整个网络的信息。这是从邻居提取特征的部分。</p><p id="b7a1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，每一层都有一个非线性函数，但它不会增加网络的接收能力。因此，除去非线性部分，我们得到一个简单的模型:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9666d880cb816f5e888254cf39ca5242.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*ae7fJDwEuCp-IEhtUsdnMw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">线性GCN被称为SGC，在去除了GCN的非线性功能之后</p></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/40577da13149c52488552f925c08605c.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*yOReU7HYzblTkMU1Xx2PJQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">简化的SGC方程(相当于线性化的方程1)</p></figure><p id="b1c5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是<strong class="kf jh">简单图形卷积的方程。</strong>我们可以清楚地看到，这个模型被简化为一个简单的预处理步骤，采用直接多类回归。多酷啊！需要注意的一点是，当S和X都是输入时，上面的等式有一个可学习的参数<strong class="kf jh">θ</strong>。所以<strong class="kf jh"> S </strong> ᵏ <strong class="kf jh"> X </strong>可以重写给x ':</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d80d15fd87f789f6c9246d8999df90f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*biaHHelqriAZZKwgPCi79w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd mr"> <em class="ms"> X </em> </strong>(输出)相当于作者的特征工程化输入|图像</p></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cff76b95a5907dedb961e0cc4f27c70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*a5HGt6hr1IJ4e_rjCQuAJQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">得到的分类器方程</p></figure><p id="52e2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们可以在下图中看到，我们如何对GCNs进行逆向工程，并实现删除非线性层，这有助于我们理解跳跃只不过是机器学习中的一个特征工程步骤。这部分的实现可以在<a class="ae jd" href="https://github.com/pulkit1joshi/SGC" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。此外，SGC可以总结为下图所示。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/1628358ee6b384dea47db1943ce76f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8F6E-3nyZRaahLD_nuCghQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">简单图卷积| <a class="ae jd" href="https://arxiv.org/abs/1902.07153" rel="noopener ugc nofollow" target="_blank">来源</a> [1]</p></figure><h1 id="dc63" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">图形卷积网络的数学基础</h1><p id="78e7" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">这一部分将解释GCNs的数学流程，给出<a class="ae jd" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">图卷积网络的半监督分类</a>【3】</p><ul class=""><li id="d34d" class="mu mv jg kf b kg kh kk kl ko mw ks mx kw my la mz na nb nc bi translated"><strong class="kf jh">图拉普拉斯:</strong></li></ul><p id="9d8a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵是表示图形的好方法。我们将图拉普拉斯算子及其归一化版本定义为:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c7a5ce6827ec52d9d7bb77475c9267c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/format:webp/1*hEL2Qgsk4E34WvWbTf0NLw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图拉普拉斯(D =对角矩阵，A =邻接矩阵)</p></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ff5f454b5b7827340e87a8f23c98d132.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*lbtZc9paj_smkVliMmROSw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标准化形式</p></figure><ul class=""><li id="4002" class="mu mv jg kf b kg kh kk kl ko mw ks mx kw my la mz na nb nc bi translated"><strong class="kf jh">特征分解</strong></li></ul><p id="a91c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个半定矩阵都可以分解成以下形式:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f9a394f756eca13cda17b248cab6bca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*w8p9hBsEmrzixORq6GHung.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">特征分解</p></figure><p id="2aee" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kf jh"> U </strong>包括标准正交特征向量，λ是以特征值作为其对角元素的对角矩阵。<br/>现在，你有正交向量，我们能定义关于它们的图形傅立叶变换吗？绝对是！阅读关于<a class="ae jd" href="https://en.wikipedia.org/wiki/DFT_matrix" rel="noopener ugc nofollow" target="_blank"> DFT </a>的内容。我们把x的傅立叶变换写成:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/14acaa270d21cbca2a81a5d5f4b6c8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/format:webp/1*3yENJ3fZJb-nOCEEnl7yQg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">傅里叶变换</p></figure><p id="bd3f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和逆变换为:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/45d4e6932e1f2f6f55b7d00e302cc2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*ahAKFF7HybU4-OmxJqRrFg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">逆变换</p></figure><ul class=""><li id="e1c7" class="mu mv jg kf b kg kh kk kl ko mw ks mx kw my la mz na nb nc bi translated"><strong class="kf jh">根据傅立叶变换的图形卷积</strong></li></ul><p id="237f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们从信号和系统知识中知道，我们可以把任意两个信号的卷积写成它们在傅里叶变换中的乘积！使用这个我们得到:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b11f131b3defaafa24a37a0d43005848.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*T07kh7nU8JZsBy4Hml7KSg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">滤波器与输入的卷积</p></figure><p id="3e68" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里<strong class="kf jh"> G </strong>是以对角线作为滤波器系数的对角矩阵(你要学习的参数)<br/>最后，我们使用一个使用k阶多项式拉普拉斯的近似来给出:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/be641f0f1c610f6c9ddf4564a792bd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*TkB97UaDxZtZ-63jW6jbfA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd mr"> G </strong>是大括号内的近似值！！</p></figure><ul class=""><li id="dcaa" class="mu mv jg kf b kg kh kk kl ko mw ks mx kw my la mz na nb nc bi translated"><strong class="kf jh">最终表情</strong></li></ul><p id="d737" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述近似通过使用(导致一阶切比雪夫滤波器)进一步近似:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cb6bde2c51e40e657db8be64fe9ae474.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*dlkAeIkx4da_No80aQ12XQ.png"/></div></figure><p id="ad73" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给予</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/ff68d9e148b22ba7137460e77cd79f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*5q1aCP87QkPsAahzKRrufQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基本GCN卷积运算</p></figure><p id="1e9c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，矩阵I + D^(-1/2)AD^(-1/2可写成:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d4ad3869cbc6f25120e79d8ee120b34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*tqTtIF8nMIhacTWCWt8HeA.png"/></div></figure><p id="5cde" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中A' = A + I，D' = D + I。</p><h1 id="5e62" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">简单图形卷积和低通滤波</h1><p id="1f64" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们导出的传播矩阵S是一阶切比雪夫滤波器。根据归一化拉普拉斯算子，它可以写成:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4f765028e6c8a8a5b535d851540f12f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*PJZCd8zIFDJ6US4Et5x_tA.png"/></div></figure><p id="f29f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，第k阶滤波器系数变为:</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/92e6fff0e502bec5bf1b8e058eb49f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*cIlWGQEfYaYfP5DpywjAfg.png"/></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/117de13e71692f23f5529df627c292d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMas3PCONPftmOW4hXTf2g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Cora数据集上不同传播矩阵的特征(红色)和过滤器(蓝色)光谱系数|来自<a class="ae jd" href="https://arxiv.org/abs/1902.07153" rel="noopener ugc nofollow" target="_blank"> SGC论文</a> [1]的结果</p></figure><p id="b624" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图所示，K值越大，滤波器系数爆炸并过度放大信号。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d4ad3869cbc6f25120e79d8ee120b34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*tqTtIF8nMIhacTWCWt8HeA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">归一化的S给出了归一化的拉普拉斯算子，其进一步影响特征值</p></figure><p id="1eea" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是在GCN帮助中使用<em class="no">重正化技巧</em>的地方。通过重正化，滤波器系数变成重正化传播矩阵的多项式。</p><p id="75f3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，添加自环γ &gt; 0后，归一化图拉普拉斯的最大特征值变小</p><h1 id="13c7" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="48fe" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">gcn是神经网络复兴的结果，神经网络直接进化为复杂的网络来研究。在这里，GCN被简化成它可能的前身SGC。首先，我们研究了GCNs的工作原理，然后将其逆向工程为一个<em class="no">简化的回归模型，并通过特征工程</em>增加了复杂性<em class="no">。</em>进一步使用GCNs的数学表达式，网络被表示为切比雪夫滤波器！从而简化了网络！</p><h1 id="38bf" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">接下来呢？</h1><p id="b36d" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">关于SGC和GCNs实现和比较更多细节，可以在参考文献[2]中找到作者实现。我在CORA数据集上实现的SGC可以在<a class="ae jd" href="https://github.com/pulkit1joshi/SGC" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="34b4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">参考文献:</strong></p><ol class=""><li id="71eb" class="mu mv jg kf b kg kh kk kl ko mw ks mx kw my la nw na nb nc bi translated"><a class="ae jd" href="https://www.youtube.com/watch?v=-EO5diMJskk" rel="noopener ugc nofollow" target="_blank">图卷积网络|图论2020 </a></li><li id="f9cc" class="mu mv jg kf b kg nx kk ny ko nz ks oa kw ob la nw na nb nc bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1902.07153.pdf" rel="noopener ugc nofollow" target="_blank">吴，费利克斯，等，“简化图卷积网络”<em class="no"> arXiv预印本arXiv:1902.07153 </em> (2019)。</a></li><li id="c9f5" class="mu mv jg kf b kg nx kk ny ko nz ks oa kw ob la nw na nb nc bi translated">实现可以在<a class="ae jd" href="https://github.com/pulkit1joshi/SGC" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到</li><li id="a917" class="mu mv jg kf b kg nx kk ny ko nz ks oa kw ob la nw na nb nc bi translated">作者编写的简单图形卷积的代码可在<a class="ae jd" href="https://github.com/Tiiiger/SGC" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得</li><li id="417e" class="mu mv jg kf b kg nx kk ny ko nz ks oa kw ob la nw na nb nc bi translated">基普夫、托马斯·n和马克斯·韦林。"图卷积网络的半监督分类."<em class="no"> arXiv预印本arXiv:1609.02907 </em> (2016)。</li></ol></div></div>    
</body>
</html>