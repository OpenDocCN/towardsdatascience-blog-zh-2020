<html>
<head>
<title>Algorithms From Scratch: PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的算法:PCA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-from-scratch-pca-cde10b835ebc?source=collection_archive---------28-----------------------#2020-10-28">https://towardsdatascience.com/algorithms-from-scratch-pca-cde10b835ebc?source=collection_archive---------28-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2eae" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank">从零开始的算法</a></h2><div class=""/><div class=""><h2 id="671d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始详述和构建PCA算法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/a192f99571924c482dbf0f724fe23fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/0*ihfTlqX61PTz8hh0"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd ld">图一</strong> : PCA(来源:<a class="ae le" href="https://twitter.com/DeepLearningAI_/status/1254832685954011139" rel="noopener ugc nofollow" target="_blank">深度学习。AI Twitter </a> ) —原创者；劳纳克·乔希</p></figure><h2 id="d8dc" class="lf lg it bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iz bi translated">介绍</h2><p id="c776" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi lo mj mk ml ls mm mn mo lw mp mq mr ms im bi translated">主成分分析(PCA)是卡尔·皮尔逊在1901年发明的一种技术，通常用于降低数据的维度，以进行探索性数据分析，也用于构建预测模型时的特征选择——下面将详细介绍特征选择和数据可视化。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/getting-started-with-feature-selection-3ecfb4957fd4"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd jd gy z fp nb fr fs nc fu fw jc bi translated">特征选择入门</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">选择有用功能的初学者指南</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk kx mw"/></div></div></a></div><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/effective-data-visualization-ef30ae560961"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd jd gy z fp nb fr fs nc fu fw jc bi translated">有效的数据可视化</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">构建有效的数据可视化的技巧可以简化为3个简单的步骤</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nl l nh ni nj nf nk kx mw"/></div></div></a></div><p id="ac4e" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">通常，当我们谈到PCA时，我们指的是计算主成分，然后使用这些主成分对数据进行基变换的过程。用人类的话来说，我们会说PCA允许我们降低数据的维数。</p><p id="e82e" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">更多从零开始的算法，可以访问“<a class="ae le" href="https://towardsdatascience.com/tagged/algorithms-from-scratch" rel="noopener" target="_blank"> <em class="nr">从零开始的算法系列</em> </a>”。</p><div class="mt mu gp gr mv mw"><a href="https://towardsdatascience.com" rel="noopener follow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd jd gy z fp nb fr fs nc fu fw jc bi translated">走向数据科学</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">共享概念、想法和代码的媒体出版物。</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ns l nh ni nj nf nk kx mw"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="0e3f" class="lf lg it bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iz bi translated">创建模型</h2><p id="bc9e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi lo mj mk ml ls mm mn mo lw mp mq mr ms im bi translated">正如我们之前提到的，PCA通常用于降维。这是通过将每个数据点投影到前几个分量来实现的，这样我们最终得到的是低维数据，但保留了尽可能多的数据方差。</p><p id="de58" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">关于这一点，需要知道两件事:</p><blockquote class="oa ob oc"><p id="e01a" class="ma mb nr mc b md nm kd mf mg nn kg mi od no mk ml oe np mn mo of nq mq mr ms im bi translated"><strong class="mc jd">注</strong>:以下列表摘自维基百科(<strong class="mc jd">来源</strong> : <a class="ae le" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></blockquote><ol class=""><li id="f2d2" class="og oh it mc b md nm mg nn lo oi ls oj lw ok ms ol om on oo bi translated">第一主分量可以等效地定义为使投影数据的方差最大化的方向</li><li id="0ab2" class="og oh it mc b md op mg oq lo or ls os lw ot ms ol om on oo bi translated"><code class="fe ou ov ow ox b">i^th</code>主分量可以取为与第一个<code class="fe ou ov ow ox b">i - 1</code>主分量正交的方向，其最大化投影数据的方差。</li></ol><p id="b378" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">但是主要成分是什么呢？很棒的问题！主成分是数据协方差矩阵的特征向量。因此，我们不必深入研究数学元素，观看接下来的两个视频可以更好地理解特征向量、特征值和协方差矩阵。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="da0a" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">特征向量</strong> →(线性变换的)是一个非零向量，当对其应用线性变换时，它会改变一个标量因子。</p><p id="e375" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">特征值</strong> →特征向量缩放的因子</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="0f92" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">协方差矩阵</strong> →协方差矩阵是一个方阵，给出给定随机向量的每对元素之间的协方差。任何协方差矩阵都是对称正半定的，其主对角线包含方差(即每个元素与其自身的协方差)(<strong class="mc jd">来源</strong> : <a class="ae le" href="https://en.wikipedia.org/wiki/Covariance_matrix" rel="noopener ugc nofollow" target="_blank">维基百科</a>)。</p><p id="edf3" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">为了执行PCA，我们应该从数据的协方差矩阵中获得特征向量和特征值。为了做到这一点，我们的意思是标准化我们的数据，然后获得协方差矩阵，最后执行奇异值分解(SVD)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/df5b91b2cf507cd6d13403fb1746554a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*uMfjsLYxc5ym2n06SCc3eg.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd ld">图2 </strong>:获取一组不相关的特征(图片由作者提供)</p></figure><p id="b694" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">接下来，我们必须使用从上一步任务中检索到的特征向量和特征值，将我们的数据投影到一组新的特征。为此，我们取我们的特征和我们的特征向量的前<strong class="mc jd"> n </strong>列的点积。如果我们想把数据的维数减少到2维，我们将使用特征向量的前两列。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/3c7c008efd4f6bd0d2f13dc7dd58bebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*8mCEzw3qYxJiY2y2Tn51wg.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="bd ld">图3 </strong>:将特征数据投影到一组新的特征。<strong class="bd ld"> U </strong>表示特征向量，S表示特征值(图片由作者提供)</p></figure><p id="50da" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">我们现在准备实现PCA，我们将使用它来可视化虹膜数据集。</p><p id="5377" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">分块算法</strong></p><ol class=""><li id="c81a" class="og oh it mc b md nm mg nn lo oi ls oj lw ok ms ol om on oo bi translated">获取一组不相关的特征</li><li id="17f9" class="og oh it mc b md op mg oq lo or ls os lw ot ms ol om on oo bi translated">将数据投影到新要素</li></ol><p id="e2bc" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">实施</strong></p><p id="aadc" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">让我们从使用<code class="fe ou ov ow ox b">sklearn</code>框架对iris数据集执行PCA开始…</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pc oz l"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="ak">图4 </strong> : Sklearn实现PCA</p></figure><p id="ca21" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">对于我们自己的实施，我们从获取不相关的特征开始，为此我们遵循图2中的步骤。</p><p id="718e" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">均值归一化数据→获取协方差矩阵→执行奇异值分解</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pc oz l"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="ak">图5 </strong>:步骤1——获取不相关的特征。我的实现</p></figure><p id="3980" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">下一步是将数据投影到一组新的特征上——参见图3。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pc oz l"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><strong class="ak">图6 </strong>:将数据投影到一组新的特征上</p></figure><p id="218a" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">现在你知道了！</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="9d80" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated"><strong class="mc jd">总结</strong></p><p id="c941" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">PCA是降低数据维度的一种很好的方法，这些数据可能出于不同的原因而需要，即在构建预测模型时，2维数据用于数据可视化，或<strong class="mc jd"> n </strong>维数据用于特征选择。在这篇文章中，我们还学习了特征向量和特征值，以及它们在将高维数据投射到<strong class="mc jd"> n </strong>维中的作用。</p><p id="6a1e" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">这可能是我写过的最难的帖子，我非常希望听到反馈，所以请留下回复或在LinkedIn上与我联系，让我们继续对话…</p><div class="mt mu gp gr mv mw"><a href="https://www.linkedin.com/in/kurtispykes/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd jd gy z fp nb fr fs nc fu fw jc bi translated">Kurtis Pykes -数据科学家-自由职业者，自由职业者| LinkedIn</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">在世界上最大的职业社区LinkedIn上查看Kurtis Pykes的个人资料。Kurtis有3个工作列在他们的…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">www.linkedin.com</p></div></div><div class="nf l"><div class="pd l nh ni nj nf nk kx mw"/></div></div></a></div><p id="56e8" class="pw-post-body-paragraph ma mb it mc b md nm kd mf mg nn kg mi lo no mk ml ls np mn mo lw nq mq mr ms im bi translated">如果你对写博客感兴趣(尤其是在媒体上),并且想要一些关于开始、扩大你的频道或任何你需要帮助的方面的建议……你会很高兴地知道，我已经创建了一个专门帮助你写博客的<a class="ae le" href="https://www.youtube.com/channel/UCu6zdBQhvEY5_j-ifHWljYw?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>——订阅并耐心等待本周日的第一个视频！</p><div class="mt mu gp gr mv mw"><a href="https://www.youtube.com/channel/UCu6zdBQhvEY5_j-ifHWljYw?view_as=subscriber" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd jd gy z fp nb fr fs nc fu fw jc bi translated">库尔蒂斯·派克斯</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">欣赏您喜爱的视频和音乐，上传原创内容，并在上与朋友、家人和全世界分享这些内容…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">www.youtube.com</p></div></div><div class="nf l"><div class="pe l nh ni nj nf nk kx mw"/></div></div></a></div></div></div>    
</body>
</html>