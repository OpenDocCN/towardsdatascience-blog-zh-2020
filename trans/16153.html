<html>
<head>
<title>Medical X-ray ⚕️ Image Classification using Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积神经网络的医学x射线⚕️图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a?source=collection_archive---------1-----------------------#2020-11-07">https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a?source=collection_archive---------1-----------------------#2020-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1f9a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从无到有构建x线肺炎检测的CNN模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a6daf5281e26fbbb5e7baf283f769549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ig0f5fAZqytQjdFV7drlAQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Projectional_rendering_of_CT_scan_of_thorax_(thumbnail).gif" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><h2 id="55d8" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">在本文中，我们将创建一个CNN模型，它可以将X射线图像分类为肺炎病例或正常病例。</h2><p id="79d9" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir"> web应用</strong>已经部署到streamlit share:<a class="ae kv" href="https://share.streamlit.io/smarthardik10/xray-classifier/main/webapp.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/smarthardk 10/Xray-classifier/main/web app . py</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/6c41245723741d559c71f8be32823227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gK4cHm-shwTMWOgPGblPog@2x.jpeg"/></div></div></figure></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h2 id="ba13" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">目录</h2><ol class=""><li id="aa9e" class="mt mu iq lu b lv lw ly lz lf mv lj mw ln mx mk my mz na nb bi translated">数据集</li><li id="b057" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">初始化</li><li id="912e" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">准备数据</li></ol><ul class=""><li id="7849" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk nm mz na nb bi translated">3.1数据扩充</li><li id="db0a" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">3.2加载图像</li></ul><p id="3ac5" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">4.卷积神经网络</p><ul class=""><li id="ec32" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk nm mz na nb bi translated">4.1必要的进口</li><li id="356e" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">4.2 CNN架构</li><li id="102f" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">4.3拟合模型</li></ul><p id="6786" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">5.评价</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h1 id="0ff5" class="nq kx iq bd ky nr ns nt lb nu nv nw le jw nx jx li jz ny ka lm kc nz kd lq oa bi translated">1数据集</h1><p id="4062" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们将用于图像分类的数据集是胸部X射线图像，它包括两个类别，肺炎和正常。这个<a class="ae kv" href="https://www.kaggle.com/pcbreviglieri/pneumonia-xray-images" rel="noopener ugc nofollow" target="_blank">数据集</a>由Paulo Breviglieri发布，是Paul Mooney最受欢迎的<a class="ae kv" href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia" rel="noopener ugc nofollow" target="_blank">数据集</a>的修订版。数据集的这个更新版本在验证集和测试集中具有更平衡的图像分布。数据集被组织成3个文件夹(train、test、val ),并包含每个图像类别不透明度(即&amp;正常肺炎)。</p><p id="e529" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><em class="ob">总观察值(图像):5856<br/>训练观察值:4192(正常1082例，肺部阴影3110例)<br/>验证观察值:1040(正常267例，肺部阴影773例)<br/>测试观察值:624(正常234例，肺部阴影390例)</em></p><p id="cd2a" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">首先，我们将使用Kaggle API直接从Kaggle中提取数据集。为此，我们需要创建一个API令牌，它位于Kaggle API选项卡下的Account部分。点击“创建一个新的API令牌”,一个json文件将被下载。<br/>运行下面几行代码来安装所需的库并上传json文件。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="abaa" class="kw kx iq od b gy oh oi l oj ok">! pip install -q kaggle<br/>from google.colab import files<br/>files.upload()<br/>! mkdir ~/.kaggle<br/>! cp kaggle.json ~/.kaggle/<br/>! chmod 600 ~/.kaggle/kaggle.json</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b9fc73e59b1d8fc6c0842e3714af7c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*fIF8cA0x_FdPjnItvI21Kw.png"/></div></figure><p id="3d99" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">当提示“选择文件”时，上传下载的json文件。运行下一行代码将下载数据集。要获得数据集API命令来下载数据集，单击Kaggle数据集页面的数据部分中的3个点，然后单击“复制API命令”按钮并用<code class="fe om on oo od b">!</code>将其粘贴</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="3d1d" class="kw kx iq od b gy oh oi l oj ok">! kaggle datasets download -d pcbreviglieri/pneumonia-xray-images</span></pre><p id="289a" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">因为我使用Google Colab来运行这个项目，所以数据集zip文件被下载到Sample Data文件夹中。现在，通过运行下面几行代码，我们使用zipfile库将文件夹和文件解压缩到所需的目标文件夹。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="9ad5" class="kw kx iq od b gy oh oi l oj ok">import zipfile</span><span id="866e" class="kw kx iq od b gy op oi l oj ok">zf = "/content/pneumonia-xray-images.zip"<br/>target_dir = "/content/dataset/cnn/pneumonia_revamped"</span><span id="61f2" class="kw kx iq od b gy op oi l oj ok">zfile = zipfile.ZipFile(zf)<br/>zfile.extractall(target_dir)</span></pre><p id="86a2" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">现在我们的数据集已经准备好了，让我们开始吧！</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/yXzMsbJfjrhLy/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><h1 id="b216" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">2初始化</h1><p id="8716" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">让我们看看我们的数据集目录树。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="a287" class="kw kx iq od b gy oh oi l oj ok">content<br/>└───dataset<br/>    └───cnn<br/>        └───pneumonia_revamped<br/>            ├───test<br/>            │   ├───Normal<br/>            │   │   ├───image1.jpg<br/>            │   │   └───image2.jpg<br/>            │   └───Opacity<br/>            │       ├───image1.jpg<br/>            │       └───image2.jpg<br/>            ├───train<br/>            │   ├───Normal<br/>            │   │   ├───image1.jpg<br/>            │   │   └───image2.jpg<br/>            │   └───Opacity<br/>            │       ├───image1.jpg<br/>            │       └───image2.jpg<br/>            └───val<br/>                ├───Normal<br/>                │   ├───image1.jpg<br/>                │   └───image2.jpg<br/>                └───Opacity<br/>                    ├───image1.jpg<br/>                    └───image2.jpg</span></pre><p id="1f18" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">在这部分代码中，我们将定义目录路径，导入一些需要的库，并定义一些我们将在项目的后面部分经常使用的公共常量参数。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="a61b" class="kw kx iq od b gy oh oi l oj ok">#Some Basic Imports</span><span id="5604" class="kw kx iq od b gy op oi l oj ok">import matplotlib.pyplot as plt #For Visualization<br/>import numpy as np              #For handling arrays<br/>import pandas as pd             # For handling data</span><span id="65b4" class="kw kx iq od b gy op oi l oj ok">#Define Directories for train, test &amp; Validation Set<br/>train_path = '/content/dataset/cnn/pneumonia_revamped/train'<br/>test_path = '/content/dataset/cnn/pneumonia_revamped/test'<br/>valid_path = '/content/dataset/cnn/pneumonia_revamped/val'</span><span id="b664" class="kw kx iq od b gy op oi l oj ok">#Define some often used standard parameters<br/>#The batch refers to the number of training examples utilized in one #iteration<br/>batch_size = 16 </span><span id="c528" class="kw kx iq od b gy op oi l oj ok">#The dimension of the images we are going to define is 500x500 img_height = 500<br/>img_width = 500</span><span id="db8d" class="kw kx iq od b gy op oi l oj ok">The dimension size of 500 or more than 500 with batch size greater than 16 may result in a crash as the RAM gets completely used in such cases. A lower dimension size with greater batch size is one of the options to try.</span></pre><h1 id="788d" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">3准备数据</h1><h2 id="6d7a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">3.1数据扩充</h2><p id="fb80" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们将通过执行一些图像增强技术来人为地增加图像训练数据集的大小。</p><blockquote class="ox oy oz"><p id="1be5" class="ls lt ob lu b lv nh jr lx ly ni ju ma pa nn mc md pb no mf mg pc np mi mj mk ij bi translated">图像增强通过创建现有训练集图像的修改版本来扩展数据集的大小，这有助于增加数据集变化并最终提高模型预测新图像的能力。</p></blockquote><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="5de5" class="kw kx iq od b gy oh oi l oj ok">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><span id="0e0c" class="kw kx iq od b gy op oi l oj ok"># Create Image Data Generator for Train Set<br/>image_gen = ImageDataGenerator(<br/>                                  rescale = 1./255,<br/>                                  shear_range = 0.2,<br/>                                  zoom_range = 0.2,<br/>                                  horizontal_flip = True,          <br/>                               )</span><span id="a695" class="kw kx iq od b gy op oi l oj ok"># Create Image Data Generator for Test/Validation Set<br/>test_data_gen = ImageDataGenerator(rescale = 1./255)</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pd or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/t1HJXy5Q5NKA8/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="fdae" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">使用<code class="fe om on oo od b">tensorflow.keras.preprocessing.image</code>库，对于训练集，我们创建了一个图像数据生成器，它将定义的参数随机应用于训练集，对于测试&amp;验证集，我们将重新调整它们，以避免事先操纵测试数据。</p><p id="4a14" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">定义一些图像数据发生器参数:- </strong></p><ol class=""><li id="f906" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk my mz na nb bi translated"><code class="fe om on oo od b">rescale</code>—每个数字图像由一个值在0到255之间的像素创建。黑色为0，白色为255。因此，重新调整原始图像像素值的比例数组，使其介于[0，1]之间，这使得图像对整体损失的贡献更加均等。否则，更高像素范围的图像导致更大的损失，并且应该使用更低的学习率，更低像素范围的图像将需要更高的学习率。</li><li id="2b89" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">shear_range</code> —图像的形状是剪切的变换。它固定一个轴，并以某个角度拉伸图像，该角度称为剪切角。</li><li id="1a0d" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">zoom_range</code> —图像以小于1.0的倍率放大。图片缩小了1.0倍以上。</li><li id="6ec0" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">horizontal_flip</code>—一些图像被随机水平翻转</li><li id="9e37" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">vertical_flip</code> —一些图像随机垂直翻转</li><li id="0cad" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">roataion_range</code> —随机地，图像在0°到180°范围内旋转一定角度。</li><li id="0c04" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">width_shift_range</code> —水平移动图像。</li><li id="ba20" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">height_shift_range</code> —垂直移动图像。</li><li id="2c72" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">brightness_range</code> —亮度0.0对应绝对无亮度，1.0对应最大亮度</li><li id="4847" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">fill_mode</code> —将图像中缺少的值填充到最接近的值、包裹值或反射值。</li></ol><p id="8c6f" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">除了重新缩放之外，这些变换技术被随机应用于图像。所有图像已被重新缩放。</p><h2 id="6dc9" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">3.2加载图像</h2><p id="9926" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">图像数据生成器有一个名为flow from directory的类，用于从包含图像的文件夹中读取图像。返回目录操作符类型<code class="fe om on oo od b">tensorflow.python.keras.preprocessing.image.DirectoryIterator</code>。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="76e6" class="kw kx iq od b gy oh oi l oj ok">train = image_gen.flow_from_directory(<br/>      train_path,<br/>      target_size=(img_height, img_width),<br/>      color_mode='grayscale',<br/>      class_mode='binary',<br/>      batch_size=batch_size<br/>      )</span><span id="9bde" class="kw kx iq od b gy op oi l oj ok">test = test_data_gen.flow_from_directory(<br/>      test_path,<br/>      target_size=(img_height, img_width),<br/>      color_mode='grayscale',<br/>      shuffle=False, <br/>#setting shuffle as False just so we can later compare it with predicted values without having indexing problem <br/>      class_mode='binary',<br/>      batch_size=batch_size<br/>      )</span><span id="f243" class="kw kx iq od b gy op oi l oj ok">valid = test_data_gen.flow_from_directory(<br/>      valid_path,<br/>      target_size=(img_height, img_width),<br/>      color_mode='grayscale',<br/>      class_mode='binary', <br/>      batch_size=batch_size<br/>      )</span></pre><p id="d2dc" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><code class="fe om on oo od b">Found 4192 images belonging to 2 classes. Found 624 images belonging to 2 classes. Found 1040 images belonging to 2 classes.</code></p><p id="675d" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">它接受的一些参数定义如下:- </strong></p><ol class=""><li id="a921" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk my mz na nb bi translated"><code class="fe om on oo od b">directory</code> —使用的第一个参数是我们之前定义的train，test &amp;验证文件夹的路径。</li><li id="e2ad" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">target_size</code> —目标尺寸是您的输入图像的尺寸，每个图像将被调整到这个尺寸。我们之前已经将目标尺寸定义为500 x 500。</li><li id="c678" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">color_mode</code>—如果图像是黑白或灰度设置为“灰度”，或者如果图像有三个颜色通道设置为“rgb”我们将使用灰度，因为这是x光图像。</li><li id="e071" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">batch_size</code> —生成器批量生成的图像数量。我们之前将批量定义为16。我们选择16，因为图像的大小太大，无法处理RAM。</li><li id="6d88" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">class_mode</code> —如果您只有两个类别要预测，则设置“二进制”，如果您没有设置为“分类”，如果您开发了一个自动编码器系统，则输入和输出很可能是同一个图像，在这种情况下设置为“输入”。这里我们将它设置为二进制，因为我们只有2个类要预测。</li></ol><p id="a729" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">让我们看看我们从数据扩充</strong>中获得的一些列车组图像</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pd or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/YWy93Zf9eW8RMlK0gK/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="2e2a" class="kw kx iq od b gy oh oi l oj ok">plt.figure(figsize=(12, 12))<br/>for i in range(0, 10):<br/>    plt.subplot(2, 5, i+1)<br/>    for X_batch, Y_batch in train:<br/>        image = X_batch[0]        <br/>        dic = {0:’NORMAL’, 1:’PNEUMONIA’}<br/>        plt.title(dic.get(Y_batch[0]))<br/>        plt.axis(’off’)<br/>        plt.imshow(np.squeeze(image),cmap=’gray’,interpolation=’nearest’)<br/>        break<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/4dee568fd86ca62460fce2b2ed2e77a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4oilq29e6YOlp8yTF_njBg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">查看列车组的一些图像</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/1kkxWqT5nvLXupUTwK/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="c5ad" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">嗯，我不能仅仅通过看这些图片来判断哪一个是肺炎病例，哪一个是正常病例。为此，我需要一个放射学学士学位，需要2到4年时间，仅第一年就要花费466万卢比。 <strong class="lu ir"> <em class="ob">好吧，不要担心，作为一个数据科学从业者，你可以教计算机分辨它们之间的区别。</em> </strong> <em class="ob">我们有希望在这方面达到很高的精确度，否则就是放射科医师的学位了。</em></p><h1 id="c658" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">4卷积神经网络</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/c9d5662496f50a9365f8fda3af21c86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iqn0cxf6pwDTG1O2.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN架构的例子由<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Typical_cnn.png" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><blockquote class="ph"><p id="a01b" class="pi pj iq bd pk pl pm pn po pp pq mk dk translated"><strong class="ak">用一句话告诉我什么是CNN</strong>—它是一种人工神经网络，能够在图像中定位或检测模式。</p></blockquote><figure class="pr ps pt pu pv kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/9e8631c40478ebf876d9c47f80115562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Tr1Qhsf1mE4lmr7v.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最大共享示例由<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:RoI_pooling_animated.gif" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><p id="00d1" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">解释CNN架构内部的情况— </strong> CNN CNN架构基于多层卷积。卷积层接收输入并转换图像中的数据，然后将其作为输入传递给下一层。这种变换称为卷积运算。我们需要为每个卷积层定义过滤器的数量。这些滤镜检测边缘、形状、曲线、对象、纹理甚至颜色等图案。它检测到的更复杂的图案或物体的层次更深。本质上，滤镜是图像内核，我们可以定义为3×3或4×4，这是一个应用于图像整体的小矩阵。我们将池层与卷积层一起使用，目标是对输入表示(图像)进行下采样，通过保留子区域绑定中的最大值(激活的特征)来降低其维数。在输入矩阵中移动的像素数量称为步幅。当步幅为1时，我们一次将过滤器移动1个像素。当步幅为2时，我们将过滤器一次移动2个像素，依此类推。较大的过滤器尺寸和跨度可用于将大图像的尺寸减小到中等尺寸。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pw or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://media.giphy.com/media/i4NjAwytgIRDW/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a>的卷积运算示例</p></figure><p id="40b2" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">好吧，如果你讨厌数学，所有这些复杂的数学运算都是在幕后进行的，我们需要做的就是定义超参数和层。如果你热爱数学并想了解这些<em class="ob">数学运算</em>是如何工作的，你可以参考参考资料部分的链接。</p><p id="d019" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">YT上有一个很棒的视频，他们试图创建人类神经网络。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="px or l"/></div></figure><p id="8668" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">锁定并加载我们开始创建的CNN架构。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="py or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/3o7buflZ5B9MTxcU7u/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><h2 id="d706" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak"> 4.1必要进口</strong></h2><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="b2c3" class="kw kx iq od b gy oh oi l oj ok">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D<br/>from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau</span></pre><h2 id="30f4" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">4.2 CNN架构</h2><p id="9cb8" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">开始构建CNN模型之前需要注意的事项:-</p><ol class=""><li id="3445" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk my mz na nb bi translated">总是从一个较低的过滤值开始，如32，然后逐层增加。</li><li id="7dd6" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">用Conv2D层和MaxPooling层构建模型。</li><li id="e5fb" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">内核大小最好是奇数，如3×3。</li><li id="c627" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">Tanh，relu等。可用于激活功能，但relu是最优选的激活功能。</li><li id="48f6" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated"><code class="fe om on oo od b">input_shape</code>取最后一个尺寸的图像宽度&amp;高度作为颜色通道。</li><li id="48b5" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">在CNN层和添加ANN层之后平坦化输入。</li><li id="3cb1" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">如果问题超过2类，则使用激活函数作为最后一层的softmax，将单位定义为类的总数，并使用sigmoid进行二进制分类，并将单位设置为1。</li></ol><p id="acb5" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">注意:-您可以随时试验这些超参数，因为没有我们可以确定的固定值。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="6fe1" class="kw kx iq od b gy oh oi l oj ok">cnn = Sequential()</span><span id="dcff" class="kw kx iq od b gy op oi l oj ok">cnn.add(Conv2D(32, (3, 3), activation="relu", input_shape=(img_width, img_height, 1)))<br/>cnn.add(MaxPooling2D(pool_size = (2, 2)))</span><span id="b3e8" class="kw kx iq od b gy op oi l oj ok">cnn.add(Conv2D(32, (3, 3), activation="relu", input_shape=(img_width, img_height, 1)))<br/>cnn.add(MaxPooling2D(pool_size = (2, 2)))</span><span id="85b4" class="kw kx iq od b gy op oi l oj ok">cnn.add(Conv2D(32, (3, 3), activation="relu", input_shape=(img_width, img_height, 1)))<br/>cnn.add(MaxPooling2D(pool_size = (2, 2)))</span><span id="c318" class="kw kx iq od b gy op oi l oj ok">cnn.add(Conv2D(64, (3, 3), activation="relu", input_shape=(img_width, img_height, 1)))<br/>cnn.add(MaxPooling2D(pool_size = (2, 2)))</span><span id="d612" class="kw kx iq od b gy op oi l oj ok">cnn.add(Conv2D(64, (3, 3), activation="relu", input_shape=(img_width, img_height, 1)))<br/>cnn.add(MaxPooling2D(pool_size = (2, 2)))</span><span id="ebcb" class="kw kx iq od b gy op oi l oj ok">cnn.add(Flatten())</span><span id="6a00" class="kw kx iq od b gy op oi l oj ok">cnn.add(Dense(activation = 'relu', units = 128))<br/>cnn.add(Dense(activation = 'relu', units = 64))<br/>cnn.add(Dense(activation = 'sigmoid', units = 1))</span><span id="c505" class="kw kx iq od b gy op oi l oj ok">cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])</span></pre><p id="5917" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">现在我们已经开发了CNN模型，让我们深入看看这里发生了什么。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="3099" class="kw kx iq od b gy oh oi l oj ok">cnn.summary()<br/></span><span id="e973" class="kw kx iq od b gy op oi l oj ok">Model: "sequential_1" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= conv2d_3 (Conv2D)            (None, 498, 498, 32)      320        _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 249, 249, 32)      0          _________________________________________________________________ conv2d_4 (Conv2D)            (None, 247, 247, 32)      9248       _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 123, 123, 32)      0          _________________________________________________________________ conv2d_5 (Conv2D)            (None, 121, 121, 32)      9248       _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 60, 60, 32)        0          _________________________________________________________________ conv2d_6 (Conv2D)            (None, 58, 58, 64)        18496      _________________________________________________________________ max_pooling2d_6 (MaxPooling2 (None, 29, 29, 64)        0          _________________________________________________________________ conv2d_7 (Conv2D)            (None, 27, 27, 64)        36928      _________________________________________________________________ max_pooling2d_7 (MaxPooling2 (None, 13, 13, 64)        0          _________________________________________________________________ flatten_1 (Flatten)          (None, 10816)             0          _________________________________________________________________ dense_2 (Dense)              (None, 128)               1384576    _________________________________________________________________ dense_3 (Dense)              (None, 64)                8256       _________________________________________________________________ dense_4 (Dense)              (None, 1)                 65         ================================================================= Total params: 1,467,137 Trainable params: 1,467,137 Non-trainable params: 0 _________________________________________________________________</span></pre><h2 id="5b3a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">口译模式总结</strong></h2><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="be2e" class="kw kx iq od b gy oh oi l oj ok"># Hyperparameters of Conv2D<br/>Conv2D(<br/>    filters,<br/>    kernel_size,<br/>    strides=(1, 1),<br/>    padding="valid",<br/>    activation=None,<br/>    <!-- -->input_shape=(height,width,color channel)<br/>    )</span><span id="1a56" class="kw kx iq od b gy op oi l oj ok"># Hyperparameters of MaxPooling2D <br/>MaxPooling2D(<br/>    pool_size=(2, 2), strides=None, padding="valid"<br/>    )</span></pre><p id="3e2e" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">图像的输入形状是我们之前定义的高度&amp;宽度<code class="fe om on oo od b">(500,500,1)</code>。并且<code class="fe om on oo od b">1</code>代表颜色通道，因为图像是灰度的，所以它的颜色通道是1，对于rgb图像是3。</p><p id="3131" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><code class="fe om on oo od b">(none,500,500,1)</code>在这里，Keras增加了一个额外的维度<code class="fe om on oo od b">none</code>，因为批量大小可以变化。</p><p id="461b" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">在第一个<code class="fe om on oo od b">Conv2d</code>层卷积操作中，对<code class="fe om on oo od b">(500,500)</code>的图像使用<code class="fe om on oo od b">(3,3)</code>内核大小，步长和膨胀默认设置为1，填充设置为“有效”，它输出<code class="fe om on oo od b">(500-3+1 , 500-3+1 ) = (498,498)</code>的输出大小，我们定义的过滤器数量为32，输出形状现在为<code class="fe om on oo od b">(None,498,498,32)</code></p><p id="09a1" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">现在，在第一个Max Pooling层中，我们已经将内核大小定义为<code class="fe om on oo od b">(2,2)</code>，默认情况下,<code class="fe om on oo od b">(2,2)</code>会将其应用到图像大小的输入中，我们得到的是<code class="fe om on oo od b">((498–2//2)+1,(498–2//2)+1))= (249,249)</code></p><p id="424d" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">展平层采用所有通道上的所有像素，并创建一个1D矢量，而不考虑批次大小。因此，<code class="fe om on oo od b">(13, 13, 64)</code>的输入被拉平为<code class="fe om on oo od b">(13*13*64) = 10816</code>的值。</p><p id="b3cc" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">参数值由第一层的<code class="fe om on oo od b">(3*3*1*32)+(32) = 320</code>给出的<code class="fe om on oo od b">(kernel_height * kernel_width * input_channels * output_channels) + (output_channels)</code>计算。</p><p id="cb8c" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">整流线性激活函数或短期ReLU是分段线性函数，如果为正，则直接输出输入，否则输出零。校正的线性激活函数克服了消失梯度的问题，允许模型更快地学习和更好地执行。</p><p id="7a83" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">填充— <code class="fe om on oo od b">"SAME"</code>:输出尺寸与输入尺寸<strong class="lu ir">相同</strong>。这要求滤波器窗口滑动到输入映射之外，因此需要填充。<code class="fe om on oo od b">"VALID"</code>:滤波窗口停留在输入图内的<strong class="lu ir">有效</strong>位置，输出尺寸缩小<code class="fe om on oo od b">filter_size - 1</code>。不会出现填充。</p><p id="ebc2" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">激活函数-简单地说，激活是一种添加到人工神经网络中的函数，用于帮助网络学习数据中的复杂模式。当与我们大脑中基于神经元的模型进行比较时，激活功能在一天结束时决定对下一个神经元做什么。由于分类在两个类之间，我们将对最后一层使用sigmoid激活函数，该函数返回值在0到1的范围内。对于2个以上的类，我们可以使用softmax激活功能。</p><h2 id="b71a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">定义模型编译</h2><ul class=""><li id="5a82" class="mt mu iq lu b lv lw ly lz lf mv lj mw ln mx mk nm mz na nb bi translated">学习率——训练时，随机梯度下降的目标是最小化训练集的实际值和预测值之间的损失。减少损失的途径需要几个步骤。Adam是一种自适应学习率方法，这意味着它计算不同参数的个人学习率。</li><li id="6611" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">损失函数-由于这是一个二元分类，我们将在训练期间使用二元交叉熵来评估损失。如果有4个以上的类，我们会选择分类交叉熵。</li><li id="1a81" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">度量—准确性—计算实际标签与预测值相等的频率。它将测量训练和验证的损失和准确性。</li></ul><p id="77f9" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">可视化CNN模型</strong></p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="7a9b" class="kw kx iq od b gy oh oi l oj ok">from tensorflow.keras.utils import plot_model</span><span id="c8c5" class="kw kx iq od b gy op oi l oj ok">plot_model(cnn,show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/31b22a3bf78c41f4526dde7d58d4af4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*hvade--d7LqAPdXFEDYJ1w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">绘制CNN架构</p></figure><h2 id="4436" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">4.3拟合模型</h2><p id="e694" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir">定义回调列表</strong></p><p id="8845" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">根据一些度量标准(<code class="fe om on oo od b">monitor</code>)和条件(<code class="fe om on oo od b">mode, patience</code>)调用<code class="fe om on oo od b">EarlyStopping</code>来停止历元。这有助于避免过度拟合模型。在这里，我们告诉停止基于<code class="fe om on oo od b">val_loss</code>指标，我们需要它是最小的。<code class="fe om on oo od b">patience</code>表示在达到最小val_loss之后，在接下来的迭代中，如果val_loss在3次迭代中的任何一次中增加，则训练将在该时期停止。</p><p id="860b" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">当指标停止改善时，降低学习率。一旦学习停滞，模型通常会受益于将学习速度降低2-10倍。这种回调监控一个数量，如果在“耐心”次数内没有看到改进，则学习率降低。<a class="ae kv" href="https://keras.io/api/callbacks/reduce_lr_on_plateau/" rel="noopener ugc nofollow" target="_blank">来源</a></p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="a847" class="kw kx iq od b gy oh oi l oj ok">early = EarlyStopping(monitor=”val_loss”, mode=”min”, patience=3)</span><span id="f2da" class="kw kx iq od b gy op oi l oj ok">learning_rate_reduction = ReduceLROnPlateau(monitor=’val_loss’, patience = 2, verbose=1,factor=0.3, min_lr=0.000001)</span><span id="7a68" class="kw kx iq od b gy op oi l oj ok">callbacks_list = [ early, learning_rate_reduction]</span></pre><p id="f0ca" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">分配类别权重</strong></p><p id="11c3" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">为每个类分配类权重是一种很好的做法。它强调少数类的权重，以便模型能够平等地从所有类中学习。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="eccf" class="kw kx iq od b gy oh oi l oj ok">from sklearn.utils.class_weight import compute_class_weight<br/>weights = compute_class_weight('balanced', np.unique(train.classes), train.classes)<br/>cw = dict(zip( np.unique(train.classes), weights))<br/>print(cw)</span></pre><p id="e34a" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><code class="fe om on oo od b">{0: 1.9371534195933457, 1: 0.6739549839228296}</code></p><h2 id="b219" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">现在一切都准备好了，进入最后一步<strong class="ak">训练💪</strong></h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qa or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/3o7qE4gcYTW1zZPkre/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="4ae2" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">我们传递给model.fit的参数是训练集、epochs as 25、用于计算val_loss和val_accuracy的验证集、类权重和回调列表。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="1d0e" class="kw kx iq od b gy oh oi l oj ok">cnn.fit(train,epochs=25, validation_data=valid, class_weight=cw, callbacks=callbacks_list)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/33627b2a551b269c4da3bc9134160f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WBc9LcRBdFdEFjkpPwiJxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练模型</p></figure><p id="88c8" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">看起来早期停止在val_loss =14.9%和val_accuracy = 94.6%的第10个历元停止。</p><h1 id="1f72" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">5评估</h1><p id="37ef" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">让我们直观地看到所有指标在整个时期生命周期中的进展</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="8a26" class="kw kx iq od b gy oh oi l oj ok">pd.DataFrame(cnn.history.history).plot()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/aaf51c5ee34ecbba2f3c50cb7c15f8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*1ywJmkFVY5MJGSzwhHPT-A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">绘制度量进度</p></figure><p id="9836" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">我们在测试数据集上获得的准确率是91%</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="460d" class="kw kx iq od b gy oh oi l oj ok">test_accu = cnn.evaluate(test)</span><span id="1ef1" class="kw kx iq od b gy op oi l oj ok">print('The testing accuracy is :',test_accu[1]*100, '%')</span></pre><p id="570c" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><code class="fe om on oo od b">39/39 [==============================] — 50s 1s/step — loss: 0.3132 — accuracy: 0.9119 The testing accuracy is : 91.18589758872986 %</code></p><p id="a43a" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">让我们预测测试数据集，并详细查看一些性能测量指标来评估我们的模型。</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="c006" class="kw kx iq od b gy oh oi l oj ok">preds = cnn.predict(test,verbose=1)</span></pre><p id="20e9" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><code class="fe om on oo od b">39/39 [==============================] — 46s 1s/step</code></p><p id="737a" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">因为最后一层的激活函数是sigmoid，所以该模型给出0到1范围内的预测，而不是0或1的精确分类。因此，我们将0.5到1范围内的所有值分类为0，将小于0.5的值分类为1。注(0表示正常情况，1表示肺炎情况)</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="ca43" class="kw kx iq od b gy oh oi l oj ok">predictions = preds.copy()<br/>predictions[predictions &lt;= 0.5] = 0<br/>predictions[predictions &gt; 0.5] = 1</span></pre><p id="a5a8" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">混淆矩阵</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/d79f96a732591b13dc26b08a25f41a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*J95GZ3QIp7nGh3Te.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">混乱矩阵的例子由<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:ConfusionMatrixRedBlue.png" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><p id="01a0" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">让我们来解释混淆矩阵的输出。左上(TP)表示被正确预测为正常病例的图像数量，右下(TN)表示被正确预测为肺炎病例的图像数量。作为肺炎病例，右上表示不正确预测但实际上是正常病例的图像的数量，左下表示不正确预测但实际上是肺炎病例的正常病例图像的数量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qe or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/1oJLpejP9jEvWQlZj4/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="3ea3" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">？？还在困惑困惑矩阵？？</strong></p><blockquote class="ph"><p id="b0af" class="pi pj iq bd pk pl qf qg qh qi qj mk dk translated">解释二元或多类分类的混淆矩阵的简单方法是查看我们是否在从左到右的对角线像元中获得最大值，在其余像元中获得最小值。</p></blockquote><pre class="pr ps pt pu pv oc od oe of aw og bi"><span id="fb7d" class="kw kx iq od b gy oh oi l oj ok">from sklearn.metrics import classification_report,confusion_matrix<br/>cm = pd.DataFrame(data=confusion_matrix(test.classes, predictions, labels=[0, 1]),index=["Actual Normal", "Actual Pneumonia"],<br/>columns=["Predicted Normal", "Predicted Pneumonia"])</span><span id="6e4d" class="kw kx iq od b gy op oi l oj ok">import seaborn as sns<br/>sns.heatmap(cm,annot=True,fmt="d")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/4b0fa93036d9c475e95cb2bfc3f07d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*-rcYqa10_jZKqKyK4YJKLw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">混淆矩阵</p></figure><p id="3a38" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">分类报告</strong></p><ul class=""><li id="afd3" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk nm mz na nb bi translated">精度=真阳性/(真阳性+假阳性)</li><li id="b251" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">回忆=真阳性/(真阳性+假阴性)</li><li id="a794" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk nm mz na nb bi translated">F1 = (2 *精度*召回)/(精度+召回)</li></ul><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="7fb1" class="kw kx iq od b gy oh oi l oj ok">print(classification_report(y_true=test.classes,y_pred=predictions,target_names =['NORMAL','PNEUMONIA']))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/a3a45282b1d4b27c52c33b7c7b306aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*d-NoZhoZ-Wmyh9lhf4CPxA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类报告</p></figure><p id="b103" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">让我们用百分比% </strong>可视化一些预测图像</p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="e6d2" class="kw kx iq od b gy oh oi l oj ok">test.reset()<br/>x=np.concatenate([test.next()[0] for i in range(test.__len__())])<br/>y=np.concatenate([test.next()[1] for i in range(test.__len__())])<br/>print(x.shape)<br/>print(y.shape)</span><span id="074a" class="kw kx iq od b gy op oi l oj ok">#this little code above extracts the images from test Data iterator without shuffling the sequence</span><span id="7485" class="kw kx iq od b gy op oi l oj ok"># x contains image array and y has labels </span><span id="6dfa" class="kw kx iq od b gy op oi l oj ok">dic = {0:'NORMAL', 1:'PNEUMONIA'}<br/>plt.figure(figsize=(20,20))<br/>for i in range(0+228, 9+228):<br/>  plt.subplot(3, 3, (i-228)+1)<br/>  if preds[i, 0] &gt;= 0.5: <br/>      out = ('{:.2%} probability of being Pneumonia case'.format(preds[i][0]))<br/>      <br/>      <br/>  else: <br/>      out = ('{:.2%} probability of being Normal case'.format(1-preds[i][0]))</span><span id="6f5f" class="kw kx iq od b gy op oi l oj ok">plt.title(out+"\n Actual case : "+ dic.get(y[i]))    <br/>  plt.imshow(np.squeeze(x[i]))<br/>  plt.axis('off')<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qm"><img src="../Images/fe32434c08e2eded56db573381a96b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXufnAZddeMUQWJD0wvt_Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试数据预测</p></figure><p id="2de2" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">此代码块给出了单个图像的百分比预测，可以通过指定其路径直接从您的驱动器加载。</p><p id="7868" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">在导入图像后，我们必须在这里重新创建所有的数据预处理步骤，就像我们之前将测试集输入模型以获得预测一样。对于预处理，我们需要导入<code class="fe om on oo od b">tensorflow.keras.preprocessing.image</code>类。</p><ol class=""><li id="73a4" class="mt mu iq lu b lv nh ly ni lf nj lj nk ln nl mk my mz na nb bi translated">导入图像，定义尺寸为<code class="fe om on oo od b">(500,500)</code>，颜色通道为灰度。</li><li id="3475" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">将图像转换为数组，将它除以255进行缩放，将维度扩展到轴= 0，因为我们的模型需要4个维度，如前所述。</li><li id="9f08" class="mt mu iq lu b lv nc ly nd lf ne lj nf ln ng mk my mz na nb bi translated">最后我们来预测一下案情！</li></ol><p id="401d" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir">让我们用我的X射线对我们的模型做一些实地测试</strong></p><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="8d64" class="kw kx iq od b gy oh oi l oj ok"># Testing with my own Chest X-Ray<br/>hardik_path = '/content/drive/My Drive/unsegregated /IMG_20201023_204205928~2.jpg'</span><span id="3813" class="kw kx iq od b gy op oi l oj ok">from tensorflow.keras.preprocessing import image</span><span id="dc5b" class="kw kx iq od b gy op oi l oj ok">hardik_img = image.load_img(hardik_path, target_size=(500, 500),color_mode='grayscale')</span><span id="dfc4" class="kw kx iq od b gy op oi l oj ok"># Preprocessing the image<br/>pp_hardik_img = image.img_to_array(hardik_img)<br/>pp_hardik_img = pp_hardik_img/255<br/>pp_hardik_img = np.expand_dims(pp_hardik_img, axis=0)</span><span id="db36" class="kw kx iq od b gy op oi l oj ok">#predict<br/>hardik_preds= cnn.predict(pp_hardik_img)</span><span id="f3d5" class="kw kx iq od b gy op oi l oj ok">#print<br/>plt.figure(figsize=(6,6))<br/>plt.axis('off')<br/>if hardik_preds&gt;= 0.5: <br/>    out = ('I am {:.2%} percent confirmed that this is a Pneumonia case'.format(hardik_preds[0][0]))<br/>    <br/>else: <br/>    out = ('I am {:.2%} percent confirmed that this is a Normal case'.format(1-hardik_preds[0][0]))</span><span id="7d88" class="kw kx iq od b gy op oi l oj ok">plt.title("Hardik's Chest X-Ray\n"+out)  <br/>plt.imshow(np.squeeze(pp_hardik_img))<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/10401c711ac87c52ab2c777baf83b797.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*MxBdNcgyCmQdkegJQfevXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">哈迪克的胸部x光片</p></figure><p id="2057" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated"><strong class="lu ir"> Phew </strong>。我的胸部x光检查似乎一切正常。现在轮到你诊断胸透了。</p><h1 id="a638" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">干得好！我们刚刚创建了一个CNN模型，可以以91%的准确率将X射线图像分类为肺炎病例或正常病例。</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qa or l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gif via <a class="ae kv" href="https://media.giphy.com/media/XreQmk7ETCak0/giphy.gif" rel="noopener ugc nofollow" target="_blank"> GIPHY </a></p></figure><p id="d8ac" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">感谢你在这个漫长的旅程中陪伴我，我们刚刚为₹省下了466万卢比x 4年的放射学家学位，现在我们能够对x光进行分类。</p><h2 id="949b" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">关注我的社交活动</h2><p id="269f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">链接到我关于这个项目的笔记本:<a class="ae kv" href="https://colab.research.google.com/drive/1J6nM1LlGE-DW93QO-yFkeBSGv9OQoHSC?usp=sharing" rel="noopener ugc nofollow" target="_blank">colab.research.google.com</a></p><p id="89b4" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">我的LinkedIn个人资料:<a class="ae kv" href="https://www.linkedin.com/in/hardik-deshmukh/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/hardik-deshmukh/</a></p><p id="1b4c" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">我的其他媒体文章:<a class="ae kv" href="https://medium.com/@smarthardik10" rel="noopener">https://medium.com/@smarthardik10</a></p><p id="7afc" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">我的GitHub:<a class="ae kv" href="https://github.com/smarthardik10" rel="noopener ugc nofollow" target="_blank">https://github.com/smarthardik10</a></p><p id="c85c" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">部署在streamlit上的应用程序:</p><p id="66c3" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">https://share . streamlit . io/smarthardk 10/Xray-classifier/main/web app . py</p><h1 id="d90c" class="nq kx iq bd ky nr os nt lb nu ot nw le jw ou jx li jz ov ka lm kc ow kd lq oa bi translated">参考</h1><p id="6943" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">[1]<a class="ae kv" href="https://stackoverflow.com/questions/61060736/how-to-interpret-model-summary-output-in-cnn" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/61060736/how-to-interpret-model-summary-output-in-CNN</a></p><p id="7660" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[2]<a class="ae kv" rel="noopener" target="_blank" href="/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7">https://towards data science . com/a-guide-to-a-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42 efca 01 e5d 7</a></p><p id="9dcc" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[3]<a class="ae kv" href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148#:~:text=Strides,with%20a%20stride%20of%202" rel="noopener">https://medium . com/@ RaghavPrabhu/understanding-of-convolutionary-neural-network-CNN-deep-learning-99760835 f148 #:~:text = Strides，with%20a%20stride%20of%202 </a>。</p><p id="2bcc" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[4]<a class="ae kv" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/rectified-linear-activation-function-for-deep-learning-neural-networks/</a></p><p id="342b" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[5]<a class="ae kv" href="https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/37674306/what-the-difference-than-same-and-valid-padding-in-TF-nn-max-pool-of-t</a></p><p id="0f69" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[6]<a class="ae kv" href="https://deeplizard.com/learn/playlist/PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU" rel="noopener ugc nofollow" target="_blank">https://deep lizard . com/learn/playlist/plzbt5o _ S2 Q7 lw i2y 8 _ qtvuxzedl 6 qu</a></p><p id="fa0d" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[7]<a class="ae kv" rel="noopener" target="_blank" href="/adam-latest-trends-in-deep-learning-optimization-6be9a291375c">https://towards data science . com/Adam-latest-trends-in-deep-learning-optimization-6be 9a 291375 c</a></p><p id="9f40" class="pw-post-body-paragraph ls lt iq lu b lv nh jr lx ly ni ju ma lf nn mc md lj no mf mg ln np mi mj mk ij bi translated">[8]<a class="ae kv" rel="noopener" target="_blank" href="/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253">https://towardsdatascience . com/everything-you-need-known-to-know-about-activation-functions-in-deep-learning-models-84ba 9f 82 c 253</a></p></div></div>    
</body>
</html>