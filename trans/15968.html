<html>
<head>
<title>Statistical Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计语言模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistical-language-models-4e539d57bcaa?source=collection_archive---------23-----------------------#2020-11-03">https://towardsdatascience.com/statistical-language-models-4e539d57bcaa?source=collection_archive---------23-----------------------#2020-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="01c0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从简单到++包含用例、示例和代码片段</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/423c9e422852012c8dc4f82f33f7d22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BAzB5eaH2Ugnj7VbzHYmA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com/s/photos/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="72d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在NLP中，语言模型是字母表上字符串的<em class="lv">概率分布</em>。在形式语言理论中，语言是字母表上的一组字符串。NLP版本是形式语言理论的一个软变体。</p><p id="e4b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP版本更适合建模自然语言，如英语或法语。没有硬性的规则规定语言中应该包含哪些字符串，不包含哪些字符串。相反，我们有观察工作。人们写作。人们议论纷纷。他们的话语是这种语言的特征。</p><p id="4339" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是，NLP统计版本有利于<em class="lv">从示例中的字符串学习</em>语言。考虑学习一个识别产品名称的模型。训练集可能包含iPhone 4和iPhone 5，但不包含iPhone 12。它应该会将iPhone 12识别为产品名称。</p><p id="8b62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将从简单到详细地介绍统计语言模型。涵盖的模型包括:独立模型、一阶马尔可夫模型、k阶马尔可夫模型、隐马尔可夫模型、条件马尔可夫模型和条件随机场。每个都用真实的例子和用例来说明。</p><p id="aaff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">下一个令牌概率</strong></p><p id="ba0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将语言定义为字符串的概率分布。在许多用例中，我们真正想要的是给定当前字符串下一个符号的概率。事实证明，这两者是等价的，如[1]中所述。</p><p id="ff61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑字母表上的一个字符串。(字母表可以由字符或单词或其他记号组成。)表示这个字符串<em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv"> x </em> n .我们可以把<em class="lv"> P </em> ( <em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv"> xn </em>)写成<em class="lv">P</em>(<em class="lv">x</em>1)*<em class="lv">P</em>(<em class="lv">x【T33</em></p><p id="7752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用例</strong></p><p id="1e2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能用语言模型做什么？相当多。</p><p id="f5e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">建议自动完成</strong>。智能手机会在我们打字时自动给出建议。当我们开始输入查询时，网络搜索引擎会提供自动完成的建议。在引擎盖下，这些是由语言模型驱动的。</p><p id="3ec0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">辨认笔迹。</strong>想象一下，把你的智能手机指向你手写的笔记，并要求它们被识别。也许是数字化和可搜索的。或者至少让它们清晰可辨！</p><p id="dd61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">手写识别具有挑战性。甚至人们经常会弄错。有时甚至是他们自己写的！</p><p id="48ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑尝试识别书写糟糕的文本中的单词。纯粹的图像处理方法会产生很多错误。增加一个语言模型可以减少很多。语言模型提供了一个有用的上下文。</p><p id="b38d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例子1 </strong>:让我们用一个简单的例子来看一下。想象一下OCR认为下一个单词是<em class="lv">数据库</em>。<em class="lv"> e </em>被误认为是<em class="lv"> c </em>。他们看起来很相似。</p><p id="fb3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们添加一个根据英语单词训练的语言模型。特别是将高概率分配给看起来像英语单词的字符串，而将低概率分配给不像英语单词的字符串。<em class="lv"> bath </em>会获得大概率，但<em class="lv"> axbs </em>不会。</p><p id="e82d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个语言模型会知道<em class="lv">数据库</em>比<em class="lv"> databasc </em>更有可能。因此添加它将有助于检测和纠正OCR的错误。</p><p id="7926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过添加多词语言模型，我们可以进一步提高错误检测和纠正的准确性。这个模型的字母表是词汇。它的高概率字符串模拟高概率单词序列。字母表变得越来越大。字母表里有每一个不同的单词。因此，在避免成本(模型变得过于复杂)的同时，需要小心谨慎地获取其好处。稍后将详细介绍。</p><p id="24ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多词语言模型还可以帮助填充书面文本中缺失的词。</p><p id="f660" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">检测并纠正拼写错误</strong>。我们将重新解释同一个例子<em class="lv">数据库</em>。最后一个字符，<em class="lv"> c </em>，现在是拼写错误。</p><p id="ef7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">识别语音。类似的推理在这里也成立。除了模态不同。所表达的基本语言是相同的。这并不是说准确的手写或语音识别很容易。只是添加语言模型会有所帮助。</strong></p><p id="a157" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">识别多标记命名实体</strong>。多标记命名实体通常具有语言结构。例如，在一个人的名字中，名字通常出现在姓字之前。夹在两者之间的可能是中间名词。作为另一个例子，在美国街道地址中，街道号码通常出现在街道名称之前。正如我们将在后面看到的，这样的实体通常是通过潜在的语言模型来识别的，比如隐马尔可夫模型。</p><p id="734b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经看到了一些用例，让我们深入了解</p><p id="f5a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">型号</strong></p><p id="e60c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将字符串表示为<strong class="lb iu"> </strong> <em class="lv"> x </em> 1，<em class="lv"> x </em> 2，…，<em class="lv"> x </em> n。</p><p id="c353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">独立。</strong>这是最简单的方法。它假设字符串中的所有字符都是独立生成的。</p><p id="d3c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv">xn</em>)=<em class="lv">Q</em>(<em class="lv">x</em>1)*<em class="lv">Q</em>(<em class="lv">x</em>2)*…*<em class="lv">Q</em>(<em class="lv">xn</em>)。</p><p id="d908" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> Q </em>是字母表上的概率分布。</p><p id="a867" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法通常对大型字母表中的长字符串有效。比如有很多单词的文档。文本被看作是单词的序列。字母表是词汇。</p><p id="9f6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，更复杂的方法会很快变得复杂，因此它们需要令人信服地工作，并显著地更好地证明它们增加的复杂性。</p><p id="6f8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法可以有效地检测文档的语言。对于每种语言，我们将训练一个单独的Q，它是在该语言的训练集中看到的单词的分布。对于表示为单词序列W1 W2 … Wn的新文档W，我们将为我们已经建模的每种语言L计算PL(W)= QL(W1)* QL[W2]*…QL[Wn]。我们认为L* = argmax_L PL(W)是W的语言。</p><p id="2e45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">举例I1: </strong>按语序训练<em class="lv">狗在摇猫尾巴</em>。我们得到Q(the)=2/7，Q(is) = 1/7，等等。</p><p id="7429" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Python代码I1 </strong>:没有测试，甚至没有运行。另外，需要一个传令兵。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="6e05" class="mb mc it lx b gy md me l mf mg">from collections import Counter<br/>import math</span><span id="2cc2" class="mb mc it lx b gy mh me l mf mg">class IndependentModel:<br/>    <br/>    <strong class="lx iu">def __init__(self):</strong><br/>        self.ctr = Counter()</span><span id="3aa9" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def train(self,words):</strong><br/>        for word in words:<br/>            self.ctr[word] += 1</span><span id="9e3a" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def Q(self,word):</strong><br/>        return float(self.ctr[word])/self.ctr.values()</span><span id="db2c" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def P(self, words):</strong><br/>        return math.prod(map(lambda word: self.Q(word), words))</span></pre><p id="46ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">练习I1 </strong>:将这段代码片段发展成一个语言识别器。说英语vs法语。</p><p id="1bcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一阶马尔可夫模型。</strong>这里，一个符号允许依赖于前一个符号。</p><p id="3f48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv">xn</em>)=<em class="lv">Q</em>(<em class="lv">x</em>1)*<em class="lv">Q</em>(<em class="lv">x</em>2 | x1)*…*<em class="lv">Q</em>(<em class="lv">xn | xn-1</em>)</p><p id="0811" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一阶马尔可夫模型由状态转移矩阵<em class="lv"> P </em>描述。<em class="lv"> pij </em>是从状态<em class="lv"> i </em>到状态<em class="lv"> j </em>的概率。表示<em class="lv"> xt </em>为<em class="lv"> i </em>，<em class="lv"> xt </em> +1为<em class="lv"> j </em>，<em class="lv"> Q </em> ( <em class="lv"> xt </em> +1，<em class="lv"> xt </em>)等于<em class="lv"> pij </em>。</p><p id="6ba6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个马尔可夫模型也可以表示为有向图。如果值<em class="lv"> i </em>后面可以跟一个值<em class="lv"> j </em>，则有一个弧<em class="lv"> i </em> → <em class="lv"> j </em>。这条弧线上有一个概率<em class="lv">p</em>(<em class="lv">j</em>|<em class="lv">I</em>)。图形表示仅仅明确了哪些状态转移概率为0。</p><p id="2d61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例1M1: </strong>一阶马尔可夫模型对短短语建模有效。假设我们想从大量的英语文档中发现突出的短语(每个短语包含2到4个单词)。一阶马尔可夫模型在足够富裕以至于能够做合理的工作，而不会变得太复杂之间取得了良好的平衡。</p><p id="6931" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">考虑文本数据挖掘是一个短语。数据挖掘是一种从大型数据集中提取有意义信息的方法</em>。</p><p id="96db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个文本，我们将建立马尔可夫图。该图的节点是出现在文本中的不同单词。如果在文本中单词<em class="lv"> u </em>后面至少有一次单词<em class="lv"> v </em>，则从节点<em class="lv"> u </em>到节点<em class="lv"> v </em>有一条弧线。下面显示了一些弧线。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="51ba" class="mb mc it lx b gy md me l mf mg">data → mining   mining → is    method → of  extracting → meaningful</span></pre><p id="c90c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到<em class="lv"> P </em> ( <em class="lv">数据</em>，<em class="lv">挖掘</em> ) = <em class="lv"> Q </em> ( <em class="lv">数据</em> )* <em class="lv"> P </em> ( <em class="lv">挖掘</em> | <em class="lv">数据</em> ) = (2/19)*1。相比之下，<em class="lv"> P </em> ( <em class="lv"> a </em>，<em class="lv">法</em>)=<em class="lv">q</em>(<em class="lv">a</em>)*<em class="lv">p</em>(<em class="lv">法</em>|<em class="lv">a</em>)=(3/19)*(⅓)= 1/19&lt;<em class="lv">p</em>(<em class="lv">数据</em>，<em class="lv">挖掘</em>可惜，<em class="lv"> P </em> ( <em class="lv">是</em>，a) = <em class="lv"> P </em> ( <em class="lv">数据</em>，<em class="lv">挖掘</em>)。所以(<em class="lv">是</em>，<em class="lv">是</em>)会是一个误报，因为它不是一个显著短语。</p><p id="f2b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，一阶马尔可夫模型本身虽然有些用处，但也有明显的假阳性。向这种方法添加语法信息，特别是单词的词性标签，可以显著提高其质量。也就是说，(大部分)减少了假阳性而(几乎)没有遗漏真阳性。直觉告诉我们，突出的短语是由有利于某些词类的词组成的，尤其是名词。考虑(<em class="lv">数据</em>，<em class="lv">挖掘</em>)。这两个词都是名词。考虑一下(<em class="lv">是</em>，<em class="lv">一个</em>)。‘is’是动词，<em class="lv">是冠词。</em></p><p id="e2ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不会详细讨论如何将这两种方法结合起来。主要是马尔可夫模型还是有用的。</p><p id="b931" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Python代码1M1 </strong>:未测试，甚至未运行。另外，需要一个传令兵。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="245f" class="mb mc it lx b gy md me l mf mg">from collections import defaultdict, Counter<br/>import math</span><span id="6119" class="mb mc it lx b gy mh me l mf mg">class FirstOrderMarkovModel:</span><span id="8839" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def __init__(selfs):</strong><br/>       self.ctr2 = defaultdict(lambda:defaultdict(int))<br/>       self.ctr1 = Counter()</span><span id="ee59" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def train(self,words):</strong><br/>       words_with_sentinel = [‘_B_’] + words<br/>       for i in range(len(words_with_sentinel)-1):<br/>          self.ctr2[words[i][words[i+1] += 1<br/>          self.ctr1[words[i]] += 1</span><span id="64db" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def Q(self,word,previous_word):</strong><br/>       return float(self.ctr2[previous_word[word])/<br/>       self.ctr1[previous_word])</span><span id="06b6" class="mb mc it lx b gy mh me l mf mg">    <strong class="lx iu">def P(words):</strong><br/>       p = self.Q(words[0],’_B’)<br/>       for i in range(len(words)-1):<br/>          p *= self.Q(words[i+1],words[i])<br/>       return p</span></pre><p id="1d05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">k阶马尔可夫模型</strong>。在这个模型中，一个字符被允许依赖于前面的<em class="lv"> K </em>个字符。</p><p id="1d36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv">xn</em>)=<em class="lv">Q</em>(<em class="lv">x</em>1)*<em class="lv">Q</em>(<em class="lv">x</em>2 | x1)*…*<em class="lv">Q</em>(<em class="lv">xn | xn-1、xn-1</em></p><p id="6335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过将<em class="lv"> K </em>设置为2或3，这种方法可以发现比一阶马尔可夫模型更好的显著短语，而不会导致复杂性的巨大增加。我们说的“更好”是什么意思？考虑一个长度为3的短语。称之为<em class="lv"> x </em> 1、<em class="lv"> x </em> 2、<em class="lv"> x </em> 3。一阶马尔可夫模型丢失了信息，因为它假设<em class="lv"> x3 </em>独立于给定<em class="lv"> x2 </em>的<em class="lv"> x1 </em>。二阶模型没有。<em class="lv">P</em>(<em class="lv">x</em>1)*<em class="lv">P</em>(<em class="lv">x</em>2 |<em class="lv">x</em>1)*<em class="lv">P</em>(<em class="lv">x</em>3 |<em class="lv">x</em>2，<em class="lv"> x </em> 1)等于<em class="lv"> P </em> ( <em class="lv"> x </em> 1，<em class="lv"> x </em></p><p id="615c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和以前一样，仍然需要把统计方法和语法方法结合起来。</p><p id="a7a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例KM1: </strong>我们想对三个单词的产品名称建立一个二阶马尔可夫模型。为什么是二阶？考虑这些编造的例子:<em class="lv"> 4k电视三星</em>，<em class="lv"> 3k电视索尼</em>，…想象一下索尼不提供4k选项。如果我们使用一阶马尔可夫模型，我们将失去第一个单词(4k或3k或…)和品牌名称之间的相互作用。二阶模型将捕捉这种相互作用。换句话说，产品名称不仅会受到第二个词(在我们的例子中是tv)的影响，还会受到第一个词(4k或3k或……)的影响。</p><p id="1cde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">隐马尔可夫模型</strong></p><p id="3978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，字符串是从具有潜在变量(即隐藏状态)的模型中概率性地生成的。</p><p id="5af3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用符号表示，让<em class="lv"> X </em> = <em class="lv"> x </em> 1、…、<em class="lv"> xn </em>表示一串长度为<em class="lv"> n </em>和<em class="lv"> S </em> = <em class="lv"> s1 </em>、…、<em class="lv"> sn </em>表示一串与<em class="lv"> X </em>相关的状态。符号<em class="lv"> xi </em>由状态<em class="lv"> si </em>产生。</p><p id="0e91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> X </em>称为观测序列；产生<em class="lv"> X </em>的(隐藏)状态序列。</p><p id="aff3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在结构上，HMM是状态集上的一阶马尔可夫模型。一些州通过弧线与其他州相连。此外，每个状态在可观察的字母表上都有一个概率分布。</p><p id="0262" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型生成一对(<em class="lv"> X </em>，<em class="lv"> S </em>)如下。首先，它生成s1。然后它以一定概率从<em class="lv"> s </em> 1发出<em class="lv"> x </em> 1。接下来，它以一定的概率移动到状态<em class="lv"> s </em> 2。这个概率只取决于它来自哪里，即<em class="lv"> s </em> 1。发射的是什么，即<em class="lv"> x </em> 1，是无关紧要的。接下来，它以一定的概率从<em class="lv"> s </em> 2生成<em class="lv"> x </em> 2。事情就是这样。</p><p id="0809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，HMM生成器在生成状态和从这些状态生成可观测量之间交替。状态转移概率只取决于前一个状态。发射跃迁概率只取决于当前状态，即可观测物发射的状态。</p><p id="a287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例子HMM1 :假设我们想要生成与真实句子相似的英语句子。特别考虑具有两种结构之一的句子</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="4f45" class="mb mc it lx b gy md me l mf mg">Article Noun Verb Adverb<br/>Pronoun Verb</span></pre><p id="0015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个结构的例句是<em class="lv">男孩跑得快</em>。第二个结构的例句是<em class="lv">她唱</em>。</p><p id="2e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个真实的句子生成器可以容纳更多的结构。我们选择两个，因为最大的洞察力来自于从一个到两个。</p><p id="acf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模拟这两种结构的HMM看起来像什么？首先，它有助于添加一个<em class="lv">开始</em>状态和一个<em class="lv">结束</em>状态。HMM总是从<em class="lv">开始</em>状态开始，并在<em class="lv">结束</em>状态停止。<em class="lv">开始</em>和<em class="lv">结束</em>状态称为静音。他们不发射任何令牌。</p><p id="1989" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在来看结构，也就是连接各州的弧线。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="e820" class="mb mc it lx b gy md me l mf mg">begin → article → noun → verb → adverb → end<br/>begin → pronoun → verb → end</span></pre><p id="7292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，有些状态会显示两次。这仅仅是由于呈现的限制。举个例子。由于HMM具有弧线<em class="lv"> begin </em> → <em class="lv">冠词</em>和<em class="lv"> begin </em> → <em class="lv">代词</em>，这实际上意味着HMM可以从<em class="lv"> begin </em>状态以某个概率移动到<em class="lv">冠词</em>状态，并以另一个概率移动到<em class="lv">代词</em>状态。两个概率之和为1。从状态<em class="lv">开始</em>HMM必须移动到某个地方。</p><p id="c4c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，从状态<em class="lv">动词</em>我们可以移动到状态<em class="lv">advor</em>b<em class="lv">T39】或者移动到状态<em class="lv"> end </em>。注意，HMM没有跟踪我们如何到达状态<em class="lv">动词</em>，这意味着状态序列<em class="lv">开始</em>文章</em>名词T50】动词T52】结束也是可能的，尽管我们没有要求这样做。如果我们希望HMM能够生成诸如男孩吃了之类的句子，这种类型的概括是好的，否则就是坏的。</p><p id="e4bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练</strong>:我们可以很容易的训练出这个车型的排放参数。我们可以利用这样一个事实，即我们的状态是命名的实体，它们的训练集很容易获得。很容易收集构成冠词的词，构成名词的词等等。</p><p id="4336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们研究训练状态转移概率。例如，我们需要估计从<em class="lv">开始</em>状态到<em class="lv">结束</em>状态的概率。这是小于1的as从状态<em class="lv">开始</em>我们也可以用状态<em class="lv">代词</em>来代替。</p><p id="05f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练转换，我们假设我们可以访问单词附有词性标签的句子。这种类型的丰富训练集很容易组合。很容易得到大量的句子。通过在每个句子上运行合适的词性标注器，也很容易得到这些句子的词性标注序列。</p><p id="492f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">HMM的转移概率很容易从这样的训练集中训练出来。事实上，我们只需要每个句子的词性标签序列。让我们来说明这一点。弧线<em class="lv"> begin </em> → <em class="lv"> article </em>上的概率估计为第一个标记为<em class="lv"> article </em>的词性标签序列数除以第一个标记为<em class="lv"> article </em>或<em class="lv">代词</em>的词性标签序列数。</p><p id="4a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">两个生成的句子</strong></p><p id="5371" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来看看几个。第一个在下面。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d261" class="mb mc it lx b gy md me l mf mg">          The     boy     is     fast<br/>           ^       ^      ^       ^<br/>           |       |      |       |<br/>begin → article → noun → verb → adverb → end</span></pre><p id="e338" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从状态<em class="lv">开始</em>开始，走到<em class="lv">条</em>，从它发出<em class="lv">条</em>，走到<em class="lv">名词</em>，从它发出<em class="lv">男孩</em>，等等。t</p><p id="ecb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个在下面。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="c290" class="mb mc it lx b gy md me l mf mg">         She     sings<br/>          ^        ^<br/>          |        |<br/>begin → pronoun → verb → end</span></pre><p id="995f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从<em class="lv">开始</em>，走到<em class="lv">代词</em>，从中发出<em class="lv">她</em>，移到<em class="lv">动词</em>，从中发出<em class="lv">唱</em>，最后走到<em class="lv">结束</em>并停留在那里。</p><p id="bc9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与独立</strong>:相比之下，想象一下独立模型生成的句子。单词会根据它们的概率被吐出来，而不考虑想要的结构。</p><p id="8322" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">特定位置独立模型aka链HMM </strong></p><p id="b7a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在某些用例中，生成的符号会受到它们在字符串中的位置的显著影响。举个例子，考虑产品名称，比如<em class="lv"> iPhone 11 </em>、<em class="lv">佳能相机</em>和<em class="lv">索尼电视</em>。显然有一些连续的结构。在这些例子中，品牌名称(<em class="lv">佳能</em>、<em class="lv">索尼</em>)先于产品类型(<em class="lv">相机</em>、<em class="lv">电视</em>)。</p><p id="6d98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">独立模型的位置特定的一般化适合于这种建模。</p><p id="79a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> x </em> 1、<em class="lv"> x </em> 2、…、<em class="lv">xn</em>)=<em class="lv">Q1</em>(<em class="lv">x</em>1)*<em class="lv">Q2</em>(<em class="lv">x</em>2)*…*<em class="lv">Qn</em>(<em class="lv">xn</em></p><p id="b87c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> Qi </em>是字母表上特定于位置的概率分布。所以对于<em class="lv"> n </em>个位置，我们有<em class="lv"> n </em>个分布<em class="lv"> Q </em> 1，…，<em class="lv"> Qn </em>。</p><p id="9e2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特定于位置的独立模型可以被视为HMM，其状态<em class="lv"> 1 </em>、<em class="lv"> 2 </em>、…、<em class="lv"> n </em>分别表示令牌位置1至<em class="lv"> n </em>。这样一个HMM的图是一个单向路径1 → 2 → 3 → … → <em class="lv"> n </em>。因此，所有弧上的转移概率都是1。HMM的参数是特定位置的发射概率。我们将称这样的模型为<em class="lv">链嗯</em>。</p><p id="3626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">链式HMM几乎不是HMM，因为它没有任何马尔可夫性。也就是说，将它作为HMM调用是有用的。它根据需要提供增强模型的途径。</p><p id="5381" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种增强版本的一个例子通常用于生物分子序列分析。它的名字叫做<em class="lv">简介嗯</em>【3】。简档HMM是一个链式HMM，其中添加了一些明智选择的状态和转换，以在某些位置偏离链。迂回路径在某些其他位置合并回链。迂回路径模拟位置特异性突变。在生物分子序列中，这种突变经常发生。捕获它们的模型更准确地识别模型家族中生物分子序列的成员。</p><p id="9eab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与位置特定链HMM不同的增强路径是进入所谓的条件马尔可夫模型(CMM)。我们将在本帖的后面讨论CMM。在下面的例子中，我们还将说明从链式HMM迁移到CMM的好处。</p><p id="2ce0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">CHMM公园名称示例</strong>:假设我们想要模仿美国的国家公园或州立公园名称。比如黄石国家公园、约塞米蒂国家公园、城堡石国家公园……连锁嗯是个不错的选择。</p><p id="3d66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个HMM将从国家公园和州立公园的名称列表中训练出来。我们将设置<em class="lv"> n </em>为列表中一个条目的最大字数。链式HMM的特定位置发射概率易于估计。公园名称的标记化版本揭示了其中每个单词的位置。比如<em class="lv">约塞米蒂国家公园</em>，<em class="lv">约塞米蒂</em>是第一个字，<em class="lv">国家</em>第二，<em class="lv">公园</em>第三。因此，从公园名称中的一个标记，我们可以知道要训练哪个州的排放概率。</p><p id="2bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的模型因其简单性和灵活性而吸引人。只要在一系列公园名称上训练它。随着列表变得更加丰富，模型会自行改进。</p><p id="c443" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样说，似乎有点不自然。我们心目中公园名称的自然模式是</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d404" class="mb mc it lx b gy md me l mf mg">word+ ( state | national) park</span></pre><p id="6469" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是意味着一个公园名称有一两个单词后跟<em class="lv">国家</em>或<em class="lv">州</em>后跟<em class="lv">公园</em>。</p><p id="3025" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的模型更接近我们寻求的自然模型。</p><p id="c3c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例子HMM-Park-Names </strong>:这里我们会用到三种状态:<em class="lv">前缀_字</em>、<em class="lv">地区_字</em>和<em class="lv">公园</em>。state <em class="lv"> regional_word </em>会发出<em class="lv"> state </em>和<em class="lv"> national </em>(暂时)，概率相等(暂时)。state <em class="lv"> prefix_word </em>会发出一个出现在regional_word之前的公园名称中的单词。状态<em class="lv"> park </em>将以概率1(目前)发出单词<em class="lv"> park </em>。</p><p id="f72c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">状态序列的训练集很容易构造吗？我们只需要一些:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0163" class="mb mc it lx b gy md me l mf mg">prefix_word regional_word park<br/>prefix_word prefix_word regional_word park<br/>…<br/>prefix_word prefix_word prefix_word regional_word park</span></pre><p id="4a30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那为什么不使用正则表达式呢？HMM更准确。考虑一下某些文本中的短语“<em class="lv">国家公园</em>”<em class="lv"/>。这个短语不是一个真正的国家公园。HMM原则上可以通过分配从状态<em class="lv">前缀_字</em>发出<em class="lv">的非常低的概率来对此建模。</em></p><p id="b529" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其实这就把我们带到了如何训练<em class="lv">前缀_单词</em>的发射概率这个话题上了？(目前，其他州的排放概率已经确定。)这里有一个简单的方法。列出国家公园和州公园的名称。去掉每个公园名称的最后两个单词。剩下的单词是前缀词。</p><p id="e709" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">国家<em class="lv">公园</em>可以扩展到放射<em class="lv">海滩</em>、<em class="lv">森林</em>、<em class="lv">纪念碑、</em>等。(在加州，州立海滩通常归入州立公园。)</p><p id="8aff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解链式HMM和基于实体的HMM之间的权衡，让我们看另一个涉及两者的例子。</p><p id="0875" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">CHMM产品示例:</strong>考虑一个链式HMM来对产品名称进行建模。(我们所说的“产品名称”是指特定的产品和产品类别。)我们可以使用一个经过训练的版本来识别非结构化文本中的产品名称，有些甚至还没有经过训练。</p><p id="81a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个HMM将从产品名称列表中训练出来。培训类似于公园连锁嗯。记住，我们只需要学习(I)状态的数量和(ii)不同状态的排放概率。</p><p id="68b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种HMM结构简单，易于训练。也就是说，我们下面描述的基于实体的版本可能会更准确。</p><p id="cb16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例HMM-产品</strong>:</p><p id="43b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以选择<em class="lv">品牌令牌</em>、<em class="lv">产品类型令牌</em>、<em class="lv">产品版本令牌</em>和<em class="lv">产品基本名称令牌</em>作为我们的实体。示例值有<em class="lv"> brand_token </em> = <em class="lv">佳能</em>，<em class="lv">product _ type _ token</em>=<em class="lv">相机</em>，<em class="lv">product _ version _ token</em>= 12，<em class="lv">product _ base _ name _ token</em>=<em class="lv">iphone</em>。我们的HMM的状态就是这些实体。</p><p id="8c8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么每个实体的名字都以单词<em class="lv">结尾token </em>？因为实体适用于单个令牌。因此，与“<em class="lv">智能手机</em>”<em class="lv"/>相关联的状态序列将是[ <em class="lv">产品类型令牌</em>，<em class="lv">产品类型令牌</em> ]。</p><p id="85f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练这个HMM，我们需要各种实体的训练集。这些训练集可以来自品牌、产品类型等的列表。我们说“派生”是因为如果列表包含多单词条目，我们就不能照原样使用它们。考虑将<em class="lv">智能手机</em>列为产品类型。由此我们会衍生出两个例子:<em class="lv">智能</em> → <em class="lv">产品类型令牌</em>和<em class="lv">手机</em> → <em class="lv">产品类型令牌</em>。</p><p id="7c56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要状态序列来捕获标记化产品名称中的实体序列。我们可以手动构建这样的训练集。这并不难，因为产品名称往往遵循一些常见的约定。例如，在许多双字产品名称中，第一个名称是品牌，第二个是产品类型。我们已经看到一个例子:<em class="lv">佳能相机</em>。因此[<em class="lv">品牌_令牌T23，T24产品_类型_令牌T25应该在状态序列的训练集中。</em></p><p id="b414" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">从产品名称自动导出状态序列</strong></p><p id="087f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">手动构造状态序列会让我们走得很远。几个州序列涵盖了很多产品名称。也就是说，手动方法存在无法扩展到涵盖数百万种产品的强大工业强度模型的风险。产品名称可能跨越州序列的长尾。考虑状态序列[ <em class="lv">产品类型令牌</em>，<em class="lv">品牌令牌</em> ]。一些产品名称遵循这一惯例。</p><p id="32a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，我们可以从标记化的产品名称中自动构造状态序列。这样，当我们向训练集添加越来越多的产品时，状态序列会自动从中提取出来。</p><p id="2261" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基本版本</strong>:基本思想是获取标记化的产品名称，并为其中的每个单词找到最可能的实体。我们可以这样做，因为我们有各种实体的(单词，实体)对的训练集。</p><p id="7ac8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个简单的例子。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="c888" class="mb mc it lx b gy md me l mf mg">canon camera ⇒ [canon,camera] ⇒ [brand_token,product_type_token]</span></pre><p id="a6bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">精炼版</strong>:我们可以把这个基本思路精炼如下。和以前一样，我们对产品名称进行标记。然后，我们通过HMM运行令牌序列，以找到最适合它的状态序列。</p><p id="4907" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种改进使用了来自实体训练集和HMM状态转换的信息。因此，它可以更准确。</p><p id="d3da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我喜欢把这种方法称为“引导训练”。我们手动初始化HMM。这包括播种我们选择的任何状态序列。然后我们可以通过它运行标记化的产品名称，希望发现更多的状态序列。</p><p id="5a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了能够发现新的状态序列，在初始化HMM时，我们应该允许从所有状态到所有状态的转换。(除了从状态<em class="lv">开始</em>到状态<em class="lv">结束</em>以及从状态<em class="lv">结束</em>的任何外出。)我们可以自动初始化这些转换的训练集，因此这些转换的初始概率非常低，尽管不是零。我们称之为伪训练。</p><p id="990d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过“引导训练”发现的一些新的状态序列可能存在错误。所以这些应该由人类来审核。尽管如此，尽管有人在循环中，我们可能还是受益了。手动发现新的状态序列比管理自动发现的状态序列要耗时得多。</p><p id="8a8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">条件马尔可夫模型</strong></p><p id="95c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像HMM一样，条件马尔可夫模型(CMM)对(<em class="lv">令牌序列</em>、<em class="lv">状态序列</em>)对进行操作。与HMM不同，CMM被优化用于为给定的令牌序列寻找最佳状态序列。这到底是什么意思？这将在下面的例子中逐渐变得清晰。</p><p id="d1cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，在本文中，我们不会讨论如何为给定的令牌序列找到最佳状态序列。我们将讨论如何对给定的令牌序列的特定状态序列进行评分。这种评分将暴露CMM中的“有条件的”部分。</p><p id="8fbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例CMM1(第一部分)</strong>:设想在(<em class="lv">记号化的完整人名</em>，<em class="lv">记号实体</em>)对上训练一个语言模型，目的是用它来解析人名。解析意味着把一个人的名字分解成各个组成部分。如名字和姓氏。</p><p id="07fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想想约翰·K·史密斯这个名字。模型应该可以推断出<em class="lv"> K </em>是中间名词。即使训练集不包含从<em class="lv"> middle_name_word </em>发出的<em class="lv"> K </em>。</p><p id="1499" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CMM直接将其建模为</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ccee" class="mb mc it lx b gy md me l mf mg">P(state[2] = middle_name_word | state[1] = first_name_word, token[2] = K)</span><span id="7de5" class="mb mc it lx b gy mh me l mf mg">P(state[2] = middle_name_word | state[1] = first_name_word)*P(token[2] = K | state[2] = middle_name_word)</span></pre><p id="03ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与HMM的建模方式相比，这有什么好处。</p><p id="e4ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了回答这个问题，首先让我们抽象出这个例子的细节。我们有CMM的<em class="lv">P</em>(<em class="lv">S</em>[<em class="lv">I</em>+1】|<em class="lv">S</em>[<em class="lv">I</em>]，<em class="lv"> X </em> [ <em class="lv"> i </em> +1])对<em class="lv">P</em>(<em class="lv">S</em>[<em class="lv">I</em>+1】|<em class="lv">S</em>]这里<em class="lv"> X </em> [ <em class="lv"> i </em> ]是第<em class="lv"> i </em>个令牌，而<em class="lv"> S </em> [ <em class="lv"> i </em>是它被发出的状态。</p><p id="f931" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CMM方法可以被视为多项式分类器，其输入是一对(<em class="lv"> S </em> [ <em class="lv"> i </em>，<em class="lv"> X </em> [ <em class="lv"> i </em> +1])，其输出是各种值<em class="lv"> S </em> [ <em class="lv"> i </em> +1】的概率分布。这个公式有助于从输入中提取我们认为合适的任何特征。可能重叠。可能考虑到<em class="lv"> S </em> [ <em class="lv"> i </em> ]和<em class="lv"> X </em> [ <em class="lv"> i </em> +1]之间的相互作用。</p><p id="1597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以在这个问题上使用任何多项式分类器算法。例如(多项式)逻辑回归、决策树或随机森林。</p><p id="2742" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，HMM不能捕捉到<em class="lv"> S </em> [ <em class="lv"> i </em> ]和<em class="lv"> X </em> [ <em class="lv"> i </em> +1]之间的交互。它也不能适应任意的特征。它也不能利用复杂的机器学习分类算法。另一方面，CMM不能利用实体训练集。我们不再学习排放概率。</p><p id="34a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例子CMM 1(已完成)</strong>:让我们完成这个例子。从特征开始。从输入(<em class="lv"> S </em> [ <em class="lv"> i </em> ]，<em class="lv"> X </em> [ <em class="lv"> i </em> +1])我们将提取特征<em class="lv">状态</em> = <em class="lv"> S </em> [ <em class="lv"> i </em>，<em class="lv">令牌</em> = <em class="lv"> X </em> [i+1】，<em class="lv">令牌长度</em> =在<em class="lv">中的字符数也就是我们有三个预测器:<em class="lv">状态</em>、<em class="lv">令牌</em>和<em class="lv">令牌长度</em>。每个都是绝对的。响应是<em class="lv"> S </em> [ <em class="lv"> i </em> +1]的值。也是绝对的。</em></p><p id="2afb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么有这些功能？特性<em class="lv">状态</em>让我们模拟当前状态对下一个状态的影响。特性<em class="lv">令牌</em>让我们使用令牌的实际值作为其状态的预测器。这是有用的。某些令牌有利于某些实体。例如，<em class="lv">约翰</em>更有可能是名字而不是姓字。作为额外的奖励，我们可以免费获得<em class="lv">状态</em>和<em class="lv">令牌</em>之间的交互，只要我们的学习算法能够利用它。(如果有互动那就是。)特性<em class="lv">令牌长度</em>很有用，因为我们知道令牌的长度有利于某些实体。名字或中间名通常只有一个字符。几乎没有姓。</p><p id="6034" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看几个训练的例子。我们将从(<em class="lv">令牌序列</em>、<em class="lv">状态序列</em>)对开始。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="747b" class="mb mc it lx b gy md me l mf mg">John   Smith                        John    K    Smith<br/>first  last                         first middle last</span></pre><p id="ef1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这些出发，让我们为CMM公式推导一些训练实例。(我们不全部展示。)前三列列出了预测值。最后一个列出了响应。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="1473" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">(previous)</strong> <strong class="lx iu">state</strong>    <strong class="lx iu">token</strong>   <strong class="lx iu">token's length</strong>             <strong class="lx iu">state</strong><br/>       first          K             1                  middle<br/>       begin         John           4                  first<br/>       first         Smith          5                  last<br/>         …             …            …                  …</span></pre><p id="e5d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的第一个训练实例读作“(<em class="lv">前一个状态</em>等于<em class="lv">第一个</em>，令牌等于<em class="lv"> K </em>，令牌长度等于1)”导致下一个状态是<em class="lv">中间的</em>”。</p><p id="503f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例CMM 2(草图)</strong>:考虑解析美国街道地址。为简单起见，假设它们具有以下形式</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="0a62" class="mb mc it lx b gy md me l mf mg">street_num_word (street_name_word)+ street_suffix [unit_prefix unit_num_word]</span></pre><p id="ffdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是几对标记化的美国街道地址及其实体。实体名称是缩写的，因此示例可以放在分配的空间中。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="16e5" class="mb mc it lx b gy md me l mf mg">123             Marine           Dr<br/>street_num_word street_name_word street_suffix</span><span id="f7f3" class="mb mc it lx b gy mh me l mf mg">203             Lake             Forest           Ave<br/>street_num_word street_name_word street_name_word street_suffix</span><span id="5cf5" class="mb mc it lx b gy mh me l mf mg">123             Main             St            Apt         22<br/>street_num_word street_name_word street_suffix unit_prefix unit_num</span></pre><p id="a951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们特性的设计，让我们从关注从令牌中提取的特性开始。令牌的哪些特征区分不同的实体？token的实际值可以预测是<em class="lv"> street_name_word </em>还是<em class="lv"> street_suffix </em>还是<em class="lv"> unit_prefix </em>。数字的存在预示着它是一个<em class="lv"> street_num_word </em>或者是一个<em class="lv"> unit_num_word </em>。</p><p id="dffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于此，我们将从input ( <em class="lv"> S </em> [ <em class="lv"> i </em> ]，<em class="lv"> X </em> [ <em class="lv"> i </em> +1])中提取特征<em class="lv">state</em>=<em class="lv">S</em>[<em class="lv">I</em>]，<em class="lv">token</em>=<em class="lv">X</em>【I+1】，<em class="lv">proportion _ of _ digits _ in _ token【中</em></p><p id="2c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">条件随机字段</strong></p><p id="ac31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像CMM一样，CRF对(<em class="lv"> S </em>，X)对进行操作，其中<em class="lv"> X </em>表示令牌序列，S表示其状态序列。两个模型<em class="lv">P</em>(<em class="lv">S</em>|<em class="lv">X</em>)，为给定<em class="lv"> X </em>寻找高概率状态序列<em class="lv"> S </em>的用例进行优化。</p><p id="9572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CMM认为这是因为</p><p id="062f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">P</em>(<em class="lv">S</em>|<em class="lv">X</em>)= product _<em class="lv">I</em><em class="lv">P</em>(<em class="lv">S</em>[<em class="lv">I</em>)| X【I-1】，<em class="lv">S</em>【I-1】)<strong class="lb iu">(CMM 1)</strong></p><p id="3dbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即状态<em class="lv"> S </em> [ <em class="lv"> i </em> ]的概率是以前一状态<em class="lv"> S </em> [ <em class="lv"> i </em> -1】和当前令牌<em class="lv"> X </em> [ <em class="lv"> i </em> ]为条件的。</p><p id="5aaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通用报告格式限制较少。也就是说，它容纳了更一般的P(S|X)。</p><p id="4933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CRF在无向图上操作，其节点模拟状态，其边模拟状态对之间的直接影响。这些影响是对称的。</p><p id="9166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将局限于线性链CRF。它有这样的结构</p><p id="ce3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">S</em>【1】—<em class="lv">S</em>【2】—<em class="lv">S</em>【3】—<em class="lv">S</em>【n】</p><p id="8525" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是模拟了一个状态受到前一个状态和下一个状态的影响。即S[i]直接受S[i-1]和S[i+1]的影响。</p><p id="e649" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在通用报告格式领域，线性通用报告格式是最接近CMM的模拟。所以让我们更详细地研究一下。</p><p id="2d4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性通用报告格式模型<em class="lv"> P </em> ( <em class="lv"> S </em> | <em class="lv"> X </em>)为</p><p id="769f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <em class="lv"> S </em> |X)正比于product _<em class="lv">I</em>product _ f<em class="lv">p</em>(<em class="lv">f</em>)*e^<em class="lv">f</em>(<em class="lv">s</em>[<em class="lv">I</em>-1】，<em class="lv"> S </em> [ <em class="lv"> i </em>，<em class="lv"> X </em> ) <strong class="lb iu"/></p><p id="0dd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们通过记录日志来定义一个分数<em class="lv"> C </em> ( <em class="lv"> S </em>，<em class="lv"> X </em>)，这种直觉会更容易传达。</p><p id="fee2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> C </em> ( <em class="lv"> S </em>，<em class="lv">X</em>)= sum _ I sum _ f w(f)* f(S[I-1]，S[i]，X) <strong class="lb iu"> (CRF 2) </strong></p><p id="8ef3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们称之为<em class="lv"> C </em>是因为我们认为它是一个兼容性函数。作为S的函数，C(S，X)在P(S|X)中是单调的。因此，为了找到得分高的状态序列，C(S，X)同样适用。</p><p id="9757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么是<em class="lv"> f </em>？这是一个特征函数，对三元组(S[i-1]，S[i]和X)在某个维度上的兼容性进行评分。正值越多，兼容性越好。<em class="lv"> w </em> ( <em class="lv"> f </em>)是<em class="lv"> f </em>的重量。它控制兼容性的强度和方向。</p><p id="0a78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以选择任意多的特征函数。</p><p id="418c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在线性条件随机场里，我们有一组可能重叠的特征函数。每个应用于每个边缘。也就是说，他们可以使用来自X中标记的任何子集的任何信息。相比之下，cmm只对(<em class="lv"> S </em> [ <em class="lv"> i </em> -1】，<em class="lv"> S </em> [ <em class="lv"> i </em> ]，<em class="lv"> X </em> [ <em class="lv"> i </em>)三元组的兼容性进行评分。</p><p id="2595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，最好看一个具体的例子，具体的特性函数。</p><p id="83d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">示例CRF 1 :考虑我们之前看到的一个示例的一般化版本。解析全局街道地址。概括地说，我们正在走向全球。</p><p id="9fe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">全球的街道地址有许多不同的格式。在某些情况下，街道编号在街道名称之前，在某些情况下在街道名称之后。有些以单位开始，有些以单位结束。(单元的一个例子是“Apt 30”。)这些只是其中的几个变种。</p><p id="c24a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">全球街道地址的质量也各不相同，取决于它们是否符合当地的格式。我们希望能够尽可能好地解析低质量的地址。</p><p id="a737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看两个例子</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ced6" class="mb mc it lx b gy md me l mf mg">123 St Francis St<br/>Rue du Vivier 15</span></pre><p id="0ca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个是美国的，第二个是比利时的。除了街道编号和名称的位置之外，街道关键字的位置也各不相同。(在这些例子中，“St”和“Rue”是街道关键字。)</p><p id="ba46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用线性链CRF。</p><p id="0487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们应该从标记化的街道地址中提取什么全局特征吗？想到两个:<em class="lv">国家</em>和<em class="lv">语言。</em></p><p id="4666" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语言和国家都会影响地址格式。这些因素相互重叠，但也有互补的影响。比利时和法国的地址就是这种情况，都是用法语写的。在某些方面，他们是相似的。在其他方面，他们不是。这是国家的问题。</p><p id="698b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有鉴于此，我们应该两者兼而有之。</p><p id="6315" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将围绕这些构建两个特定的特性函数。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="9745" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">F1</strong>: country-edge compatibilities: (S[i-1],S[i], country(X))<br/><strong class="lx iu">F2</strong>: language-edge compatibilities: (S[i-1],S[i], country(X))</span></pre><p id="8abe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将添加第三个特征函数来优先选择与其标记状态兼容的令牌。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="5c2c" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">F3</strong>: state-token compatibilities: (S[i], X[i])</span></pre><p id="7133" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练</strong> : F1和F2可以从标记有国家和语言的(<em class="lv">符号化街道地址</em>、<em class="lv">州序列</em>)对中训练。从<em class="lv"> S </em> [ <em class="lv"> i </em> -1】过渡到<em class="lv"> S </em> [ <em class="lv"> i </em>兼容国家和语言分数高的，不兼容的不要。可以从训练集中的(<em class="lv">状态</em>，<em class="lv">令牌</em>)对中训练F3。</p><p id="b6c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读</strong></p><ol class=""><li id="b61c" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated"><a class="ae ky" href="https://www.cl.cam.ac.uk/teaching/1718/R228/lectures/lec9.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cl . cam . AC . uk/teaching/1718/R228/lections/le C9 . pdf</a></li><li id="fbb7" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model" rel="noopener ugc nofollow" target="_blank">最大熵马尔可夫模型</a></li><li id="2c64" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated"><a class="ae ky" href="https://www.ebi.ac.uk/training-beta/online/courses/pfam-creating-protein-families/what-are-profile-hidden-markov-models-hmms/#:~:text=Profile%20HMMs%20are%20probabilistic%20models,the%20alignment%2C%20see%20Figure%202" rel="noopener ugc nofollow" target="_blank">https://www . ebi . AC . uk/training-beta/online/courses/pfam-creating-protein-families/what-are-Profile-hidden-Markov-models-hmms/#:~:text = Profile % 20 hmms % 20 are % 20 probability % 20 models，the % 20 alignment % 2C % 20 see % 20 figure % 202</a>。</li><li id="b08a" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated"><a class="ae ky" href="http://pages.cs.wisc.edu/~jerryzhu/cs838/CRF.pdf" rel="noopener ugc nofollow" target="_blank">cs 838–1高级NLP:条件随机字段</a></li><li id="5d9d" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated"><a class="ae ky" href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers" rel="noopener ugc nofollow" target="_blank">条件随机场:用于分割和标记序列数据的概率模型</a></li></ol></div></div>    
</body>
</html>