<html>
<head>
<title>How much of your Neural Network’s Prediction can be Attributed to each Input Feature?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">您的神经网络的预测有多少可以归因于每个输入特征？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-much-of-your-neural-networks-prediction-can-be-attributed-to-each-input-feature-62581efd9b38?source=collection_archive---------16-----------------------#2020-10-24">https://towardsdatascience.com/how-much-of-your-neural-networks-prediction-can-be-attributed-to-each-input-feature-62581efd9b38?source=collection_archive---------16-----------------------#2020-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ef91" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用PyTorch实现的具有集成梯度的深度神经网络。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9d155f3fb5a8bec3cb9ef6c09d523e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oy059fDE95fGX5Jsc8nhlA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@imgly?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> img.ly </a>在<a class="ae kv" href="https://unsplash.com/s/photos/gradient?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c131" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，神经网络是黑盒预测器，数据科学家通常不知道哪个特定输入特征对预测影响最大。如果我们想了解模型实际学到了什么，这可能是相当有限的。有了这种理解，我们就可以发现我们的学习算法或数据处理管道中的错误或弱点，从而能够改进它们。</p><p id="e32d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在本项目中实施的方法称为集成梯度，在以下文章中有所介绍:</p><ul class=""><li id="f83e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1703.01365" rel="noopener ugc nofollow" target="_blank">深度网络的公理归属</a></li></ul><p id="d567" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，作者列出了一个好的属性方法应该遵循的一些理想的公理，并证明了他们的方法<strong class="ky ir"> Integrated gradients </strong>满足这些公理。其中一些公理是:</p><ul class=""><li id="cc4d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">灵敏度:如果两个样本仅在一个特征上不同，并且通过神经网络具有不同的输出，那么这个特征的属性应该是非空的。相反，如果一个特征根本不影响输出，那么它的属性应该为零。</li><li id="54b7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">实现不变性:如果两个网络的所有输入都有相同的输出，那么它们的属性应该是相同的。</li></ul><p id="4f22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多的公理可以在上面链接的文章中详细找到。</p><p id="41fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">积分梯度很容易实现和使用，它只需要计算神经网络输出相对于其输入的梯度的能力。这在PyTorch中很容易做到，我们将在下面详细介绍如何做到这一点。</p><h1 id="8c4d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">该方法:</h1><p id="391b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们将我们的神经网络表示为函数F:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1c35dabc78c94d78c8f2b965fb5d8b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*bD9B49IYUbR-ANl0Z2XO8Q.png"/></div></figure><p id="47c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们对特征向量x的属性感兴趣，并且还引入了基线特征向量x’。该基线x '允许我们对原因的“不存在”进行建模，并且其通过神经网络的输出应该接近于零。</p><p id="c904" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">积分梯度法的计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/e3410b8c1697c825bb0d0c3e16546b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vahMPznkO0WHSt0xAYiLzg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="55ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中x_i是向量x的第I个特征。</p><h1 id="8885" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">示例:</h1><h2 id="237d" class="nf mh iq bd mi ng nh dn mm ni nj dp mq lf nk nl ms lj nm nn mu ln no np mw nq bi translated">合成示例:</h2><p id="0d7c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">让我们生成一个合成数据集，尝试更好地理解这种方法。</p><p id="6567" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将数据生成流程设定为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/55f1c1654adaded0092c133b109c6573.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*1_75EbJK1sXDRVuKNrs1fw.png"/></div></figure><p id="8401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以用python来实现，如下所示:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="fa77" class="nf mh iq nt b gy nx ny l nz oa"><strong class="nt ir">def </strong>build_dataset(size):<br/>    pos_size = 32<br/>    neg_size = 32<br/>    noise_size = 32<br/>    pos_cols = [<strong class="nt ir">"POS_%s" </strong>% i <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>range(pos_size)]<br/>    neg_cols = [<strong class="nt ir">"NEG_%s" </strong>% i <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>range(neg_size)]<br/>    noise_cols = [<strong class="nt ir">"NOISE_%s" </strong>% i <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>range(noise_size)]<br/><br/>    pos = {i: np.random.uniform(-1, 1, size=size) <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>pos_cols}<br/>    neg = {i: np.random.uniform(-1, 1, size=size) <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>neg_cols}<br/>    noise = {i: np.random.uniform(-1, 1, size=size) <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>noise_cols}<br/><br/>    df = pd.DataFrame({**pos, **neg, **noise})<br/><br/>    df[<strong class="nt ir">"target"</strong>] = df.apply(<br/>        <strong class="nt ir">lambda </strong>x: sum(<br/>            [x[k] * (i + 1) / pos_size <strong class="nt ir">for </strong>i, k <strong class="nt ir">in </strong>enumerate(pos_cols)]<br/>            + [-x[k] * (i + 1) / neg_size <strong class="nt ir">for </strong>i, k <strong class="nt ir">in </strong>enumerate(neg_cols)]<br/>        ),<br/>        axis=1,<br/>    )<br/><br/>    coefs = (<br/>        [(i + 1) / pos_size <strong class="nt ir">for </strong>i, k <strong class="nt ir">in </strong>enumerate(pos_cols)]<br/>        + [-(i + 1) / neg_size <strong class="nt ir">for </strong>i, k <strong class="nt ir">in </strong>enumerate(neg_cols)]<br/>        + [0 <strong class="nt ir">for </strong>i, k <strong class="nt ir">in </strong>enumerate(noise_cols)]<br/>    )<br/><br/>    <strong class="nt ir">return </strong>np.array(df[pos_cols + neg_cols + noise_cols]), np.array(df[<strong class="nt ir">"target"</strong>]), coefs</span></pre><p id="0ccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，所有特征的系数并不相同，有些是正的，有些是负的，有些是零。</p><p id="88d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在这个数据上训练一个多层感知器，如果模型正确地学习了数据模式，那么我们期望发现特征x_i的属性大约等于:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d31388eac6b35b9702c81aaff41dd6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*xx6en-OvHl0K5pAIz7OyrA.png"/></div></figure><p id="3e5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为它是与基线相比，我改变输出的特性的数量。</p><p id="52e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并且:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/16d866270e2e6264355457c6c22128c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*QZByok71ZlnPYbuRAmCRTw.png"/></div></figure><p id="7ed1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，让我们实施综合梯度，并检查我们的实证结果是否有意义。</p><p id="273c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们在PyTorch中通过拟合训练数据来训练回归模型。然后我们选择x '全为零。</p><p id="688f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了计算积分，我们使用一种近似法，通过从x到x’计算小间隔的dF值，然后对dF * size_of_interval求和。使用以下函数实现整个过程:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="3df5" class="nf mh iq nt b gy nx ny l nz oa"><strong class="nt ir">def </strong>compute_integrated_gradient(batch_x, batch_blank, model):<br/>    mean_grad = 0<br/>    n = 100<br/><br/>    <strong class="nt ir">for </strong>i <strong class="nt ir">in </strong>tqdm(range(1, n + 1)):<br/>        x = batch_blank + i / n * (batch_x - batch_blank)<br/>        x.requires_grad = <strong class="nt ir">True<br/>        </strong>y = model(x)<br/>        (grad,) = torch.autograd.grad(y, x)<br/>        mean_grad += grad / n<br/><br/>    integrated_gradients = (batch_x - batch_blank) * mean_grad<br/><br/>    <strong class="nt ir">return </strong>integrated_gradients, mean_grad</span></pre><p id="1fb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用torch.autograd.grad可以轻松计算梯度。在我们的函数中，所有要素的运算同时进行矢量化。</p><p id="c4ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经得到了积分梯度，让我们检查它是否符合我们的预期:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/19935e2f91d6b715c37750e8c209a1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Di-W5A0Rf-s8j9a_C4gtTA.png"/></div></div></figure><p id="6692" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，属性的估计值(橙色)与我们的预期值(蓝色)非常接近。该方法能够识别特征如何影响输出，以及哪些特征对目标没有影响。</p><h2 id="9f51" class="nf mh iq bd mi ng nh dn mm ni nj dp mq lf nk nl ms lj nm nn mu ln no np mw nq bi translated">图像示例:</h2><p id="6164" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">现在让我们做一个图像分类的例子，我们将使用在ImageNet上训练的resnet18应用于我的猫的照片。我们将使用与上述完全相同的过程，每个图像像素将被视为一个输入特征。我们将得到一个结果，其中每个像素由它对图像分类的影响程度表示为一只<strong class="ky ir">虎斑猫。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/01e52580f1def35ddd56bb8d56caaea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TdSN6dTSFYDeP3qkJbBOw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/4fbaa49f1c62f3bdcc3cefebea83f973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8yFFBU8q23aFSjkpctQUBA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="859f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，对“虎斑猫”输出神经元影响最大的像素位于猫的面部。</p><h1 id="150f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论:</h1><p id="a62d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">积分梯度是了解每个输入要素对神经网络输出的影响的好方法。该方法解决了现有方法的一些缺点，并满足一些公理，如敏感性和实现不变性。<br/>在处理神经网络时，这种方法可以成为一种很好的工具，以便更好地理解它们的预测，甚至检测训练算法或数据集是否存在一些问题。</p><p id="14d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代号:<a class="ae kv" href="https://github.com/CVxTz/IntegratedGradientsPytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/IntegratedGradientsPytorch</a></p></div></div>    
</body>
</html>