<html>
<head>
<title>Vanilla Neural Networks in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R中的香草神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vanilla-neural-networks-in-r-43b028f415?source=collection_archive---------30-----------------------#2020-11-06">https://towardsdatascience.com/vanilla-neural-networks-in-r-43b028f415?source=collection_archive---------30-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4bdf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">看看神经网络架构的引擎盖:从零开始，在<strong class="ak"> R </strong>中设计并构建一个神经网络，而不使用任何深度学习框架或包</h2></div><p id="0cdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特别感谢:<a class="ae lb" href="https://www.linkedin.com/in/alexjscriven/" rel="noopener ugc nofollow" target="_blank">亚历克斯·斯克里文</a></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/31ac6062d354b62ac6d6bce510d1a5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CXpXUW-LyfJEA7Fg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图片来源:<a class="ae lb" href="https://github.com/karanvivekbhargava/vanilla-neural-network" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></figure><h1 id="82ae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">内容:</h1><p id="f39a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">1.<a class="ae lb" href="#b4fb" rel="noopener ugc nofollow">简介</a> <br/> 2。<a class="ae lb" href="#fce7" rel="noopener ugc nofollow">背景</a>T11】3。<a class="ae lb" href="#9e09" rel="noopener ugc nofollow">语义</a> <br/> 4。<a class="ae lb" href="#7ed5" rel="noopener ugc nofollow">设置</a> <br/> 5。<a class="ae lb" href="#c655" rel="noopener ugc nofollow">获取数据</a> <br/> 6。<a class="ae lb" href="#556b" rel="noopener ugc nofollow">检查数据</a>7<br/>。<a class="ae lb" href="http://266e" rel="noopener ugc nofollow" target="_blank">准备数据</a> <br/> 8。<a class="ae lb" href="#6a04" rel="noopener ugc nofollow">实例化网络</a> <br/> 9。<a class="ae lb" href="#73a7" rel="noopener ugc nofollow">初始化网络</a> <br/> 10。<a class="ae lb" href="#fa55" rel="noopener ugc nofollow">正向传播</a>T35】11。<a class="ae lb" href="#485a" rel="noopener ugc nofollow">计算成本</a> <br/> 12。<a class="ae lb" href="#1c42" rel="noopener ugc nofollow">反向传播</a> <br/> 13。<a class="ae lb" href="#5208" rel="noopener ugc nofollow">更新模型参数</a>14<br/>。<a class="ae lb" href="#34e4" rel="noopener ugc nofollow">端到端运行模型</a> <br/> 15。<a class="ae lb" href="#4ed9" rel="noopener ugc nofollow">创建预测</a>16<br/>。<a class="ae lb" href="#9a35" rel="noopener ugc nofollow">结论</a> <br/> 17。<a class="ae lb" href="#cae5" rel="noopener ugc nofollow">帖子脚本</a></p><h1 id="b4fb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.介绍</h1><p id="9d58" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现代数据科学技术经常使用健壮的框架来设计和构建机器学习解决方案。在<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.r-project.org/" rel="noopener ugc nofollow" target="_blank">R</a></code>社区中，<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.tidyverse.org/" rel="noopener ugc nofollow" target="_blank">tidyverse</a></code>、<code class="fe mp mq mr ms b"><a class="ae lb" href="http://caret.r-forge.r-project.org/" rel="noopener ugc nofollow" target="_blank">caret</a></code>等包被频繁引用；并且在<code class="fe mp mq mr ms b"><a class="ae lb" href="http://127.0.0.1:17287/rmd_output/0/" rel="noopener ugc nofollow" target="_blank">Python</a></code>内，经常引用<code class="fe mp mq mr ms b"><a class="ae lb" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank">numpy</a></code>、<code class="fe mp mq mr ms b"><a class="ae lb" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">pandas</a></code>、<code class="fe mp mq mr ms b"><a class="ae lb" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank">sci-kit learn</a></code>等包。甚至有一些包已经被构建为可以在两种语言中使用，比如<code class="fe mp mq mr ms b"><a class="ae lb" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">keras</a></code>、<code class="fe mp mq mr ms b"><a class="ae lb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">pytorch</a></code>、<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">tensorflow</a></code>。然而，使用这些包的限制是'<em class="mt">黑盒</em> ' <em class="mt"> </em>现象，用户不理解幕后(或者说“引擎盖下”)发生了什么。用户知道如何使用这些功能，并且可以解释结果，但是不一定知道软件包是如何实现这些结果的。</p><p id="ff0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文的目的是创建一种“<em class="mt">回归基础</em>”的方法来设计深度学习解决方案。其意图不是创建最具预测性的模型，也不是使用最新最棒的技术(如<a class="ae lb" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积</a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归</a>)；但其意图是创建一个<em class="mt">基本的</em>神经网络，从<em class="mt">开始</em>，使用<em class="mt">没有</em>的框架，并向<em class="mt">走过</em>的方法论。</p><p id="f459" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">注意</strong>:“香草神经网络”中的单词“<em class="mt">香草</em>”只是指它是从零开始构建的，并且在其构建中不使用任何预先存在的框架。</p><h1 id="fce7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.背景</h1><h2 id="5b1f" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">2.1.语境</h2><p id="2a95" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">已经有很多网站和博客解释了这个过程是如何完成的。如<a class="ae lb" href="https://machinelearningmastery.com/about/" rel="noopener ugc nofollow" target="_blank"> Jason Brownlee </a>的文章<a class="ae lb" href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/" rel="noopener ugc nofollow" target="_blank">如何用Python编写一个带反向传播的神经网络(从头开始)</a>，以及<a class="ae lb" href="https://notebooks.azure.com/goldengrape/projects/deeplearning-ai" rel="noopener ugc nofollow" target="_blank"> DeepLearning.ai </a>的笔记本<a class="ae lb" href="https://notebooks.azure.com/goldengrape/projects/deeplearning-ai/html/COURSE%201%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/dnn_app_utils_v2.py" rel="noopener ugc nofollow" target="_blank"> dnn_app_utils_v2.py </a>(上<a class="ae lb" href="https://notebooks.azure.com/" rel="noopener ugc nofollow" target="_blank">微软Azure笔记本</a>网)。但是，这些源码都是用Python写的。这很好，如果这是所需要的，并且有一些非常合理的理由使用<code class="fe mp mq mr ms b">Python</code>而不是其他语言。但是这篇论文会写在<code class="fe mp mq mr ms b">R</code>里。</p><p id="5195" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选择<code class="fe mp mq mr ms b">R</code>语言有两个原因:</p><ol class=""><li id="d3fc" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">我熟悉这种语言。我会说<code class="fe mp mq mr ms b">Python</code>(还有其他语言)；我选择了<code class="fe mp mq mr ms b">R</code>来展示如何使用这种语言来实现。</li><li id="5e2e" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">证明有很多不同的方法可以达到同样的结果。因此，虽然有时选择一种语言比选择另一种语言有合理的限制(业务遗产、技术可用性、系统性能等)，但有时选择一种语言只是因为它在风格上更可取。</li></ol><p id="303b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们看看如何在<code class="fe mp mq mr ms b">R</code>中设计和构建一个香草神经网络。</p><h2 id="1a22" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">2.2.什么不是</h2><p id="d16d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">本文不涵盖<em class="mt">最新的</em>和<em class="mt">最伟大的</em>深度学习架构(如<a class="ae lb" href="https://www.tensorflow.org/tutorials/images/cnn" rel="noopener ugc nofollow" target="_blank">卷积</a>或<a class="ae lb" href="https://www.tensorflow.org/guide/keras/rnn" rel="noopener ugc nofollow" target="_blank">递归</a>)。因此，如果使用这些其他架构，最终性能可能不会像<em class="mt">可能</em>的那样好。</p><p id="9d35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章没有向读者讲授神经网络如何工作背后的理论数学概念。有很多其他的讲座教授这些信息(例如<a class="ae lb" href="https://www.youtube.com/watch?v=RiqWATOoos8" rel="noopener ugc nofollow" target="_blank">神经网络背后的数学</a>)。事实上，本文假设读者有很多关于编程、微积分和神经网络概念背后的基础知识。</p><p id="dda8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章不包括<em class="mt">为什么</em>神经网络以这样的方式工作，以及前馈结构背后的概念理解。有很多其他博客(例如<a class="ae lb" href="https://medium.com/@purnasaigudikandula/a-beginner-intro-to-neural-networks-543267bda3c8" rel="noopener">神经网络入门</a>)和视频(例如<a class="ae lb" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank">神经网络系列</a>)涵盖了这些信息。</p><p id="62ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文并没有向读者指出其他可能已经设置并运行这些信息的包和应用程序。像<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.rdocumentation.org/packages/tensorflow" rel="noopener ugc nofollow" target="_blank">tensorflow</a></code>和<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.rdocumentation.org/packages/nnet" rel="noopener ugc nofollow" target="_blank">nnet</a></code>这样的包已经包含了这一点。</p><p id="79ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章<em class="mt">实际上</em>是一个功能走查，如何创建一个香草神经网络(一个前馈网络)，从零开始，一步一步，用<code class="fe mp mq mr ms b">R</code>编程语言。它包含大量代码和技术细节。</p><h1 id="9e09" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.语义学</h1><h2 id="7745" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">3.1.布局</h2><p id="11be" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这篇文章就是以这样一种方式来描述神经网络是如何从头开始构建的。它将完成以下步骤:</p><ol class=""><li id="f5f1" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">访问并检查数据</li><li id="0125" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">实例化和初始化网络</li><li id="af19" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">向前传播</li><li id="4c6e" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">计算成本</li><li id="1cd6" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">反向传播</li><li id="f27d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">更新模型</li><li id="5870" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">建立一个训练方法来循环每一件事</li><li id="61b3" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">预测和评估绩效</li></ol><p id="bcf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简洁起见，这里定义的函数将不包括典型函数中应该包括的所有注释和验证。它们将只包括基本步骤和提示。然而，本文的源代码(位于<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">这里</a>)确实包含了所有适当的函数文档字符串和断言。</p><h2 id="ccf9" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">3.2.句法</h2><p id="0b30" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在很大程度上，本文中的语法保留了<code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.rdocumentation.org/packages/dplyr" rel="noopener ugc nofollow" target="_blank">dplyr</a></code>‘pipe’方法(使用了<code class="fe mp mq mr ms b">%&gt;%</code>符号)。然而，在某些部分使用了R <code class="fe mp mq mr ms b"><a class="ae lb" href="https://www.rdocumentation.org/packages/base" rel="noopener ugc nofollow" target="_blank">base</a></code>语法(例如，在函数声明行中)。</p><p id="eef5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在整篇文章中，编写了许多自定义函数。这些都带有前缀<code class="fe mp mq mr ms b">get</code>、<code class="fe mp mq mr ms b">let</code>和<code class="fe mp mq mr ms b">set</code>。每个的定义如下。</p><ul class=""><li id="5e65" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated"><code class="fe mp mq mr ms b">get_*()</code> : <br/> —它将<code class="fe mp mq mr ms b">get</code>将来自对象的元数据的某些属性解析到此函数。<br/> —或将使用解析到该函数的信息来导出和<code class="fe mp mq mr ms b">get</code>其他值或参数。</li><li id="b95b" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">set_*()</code> : <br/> —它将<code class="fe mp mq mr ms b">set</code>(或‘更新’)解析到该函数的对象。<br/> —通常用于在正向和反向传播过程中更新网络。</li><li id="9e8e" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">let_*()</code> : <br/> —与<code class="fe mp mq mr ms b">get</code>类似，它使用解析到该函数的其他值来导出结果，但是<code class="fe mp mq mr ms b">let</code>该值将被另一个对象或函数使用。<br/> —主要用于初始化和激活功能。</li></ul><h1 id="7ed5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.建立</h1><h2 id="fd19" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">4.1.加载包</h2><p id="e598" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">第一步是导入相关的包。此列表包括整个过程中使用的主要软件包；并列出其主要用途。</p><p id="a5f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意上面列出的关于不使用现有深度学习包的内容，然而<code class="fe mp mq mr ms b">tensorflow</code>包却包括在内。为什么？嗯，这仅用于访问数据，这将在下一节讨论。<code class="fe mp mq mr ms b">tensorflow</code>包不用于构建和训练任何网络。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="57e7" class="mu lt iq ms b gy nz oa l ob oc">library(tensorflow)  #&lt;-- Only used for getting the data<br/>library(tidyverse)   #&lt;-- Used for accessing various tools<br/>library(magrittr)    #&lt;-- Extends the `dplyr` syntax<br/>library(grDevices)   #&lt;-- For plotting the images<br/>library(assertthat)  #&lt;-- Function assertions<br/>library(roxygen2)    #&lt;-- Documentation is important<br/>library(caret)       #&lt;-- Doing data partitioning<br/>library(stringi)     #&lt;-- Some string manipulation parts<br/>library(DescTools)   #&lt;-- To properly check `is.integer`<br/>library(tictoc)      #&lt;-- Time how long different processes take<br/>library(docstring)   #&lt;-- Makes viewing the documentation easier<br/>library(roperators)  #&lt;-- Conveniently using functions like %+=%<br/>library(plotROC)     #&lt;-- For plotting predictions</span></pre><h1 id="c655" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.检索数据</h1><h2 id="64db" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">5.1.下载数据</h2><p id="e41f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">要使用的数据集是<a class="ae lb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>数据集。选择它有很多原因，包括:</p><ol class=""><li id="0683" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">数据在图像上，非常适合深度学习目的；</li><li id="a040" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">包含了相当数量的图像(总共60，000张图像)；</li><li id="ff8a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">所有图像都是相同的大小(32x32像素)；</li><li id="2569" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">这些图像被分为10个不同的类别；和</li><li id="1d38" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">通过<code class="fe mp mq mr ms b">TensorFlow</code>包可以轻松访问它。</li></ol><p id="26e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下代码块具有以下过程步骤:</p><ol class=""><li id="c891" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">获取数据<br/> —为了导入日期，通过<code class="fe mp mq mr ms b">keras</code>元素访问，该元素包含<code class="fe mp mq mr ms b">datasets</code>的套件，包括<code class="fe mp mq mr ms b">cifar10</code>部分。<br/>—<code class="fe mp mq mr ms b">load_data()</code>函数从在线GitHub存储库中检索数据。</li><li id="f7d4" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取第二个元素<br/> —这里的<code class="fe mp mq mr ms b">load_package()</code>返回两个不同的对象:<br/> — — 1。训练数据集(包含50，000幅图像)；<br/> — — 2。测试数据集(包含10，000张图像)。<br/> —提取第二个元素(通过使用<code class="fe mp mq mr ms b">extract2(2)</code>功能),因为只需要10，000张图像。<br/> —本文将展示创建香草神经网络的<em class="mt">过程</em>；如果以后需要更多数据，可以在这里轻松访问。</li><li id="f92c" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">说出零件的名称<br/> —下载的数据包含另外两个元素:<br/> — — 1。图像本身(以4维阵列的形式)；<br/>———2。图像标签(以二维单列数组的形式)。<br/> —该数据没有任何名称，因此使用<code class="fe mp mq mr ms b">set_names()</code>函数设置名称。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="b9b5" class="mu lt iq ms b gy nz oa l ob oc"># Download Data<br/># NOTE:<br/># - The first time you run this function, it download everything.<br/># - Next time you run it, TensorFlow will load from Cache.<br/>cifar &lt;- tf$keras$datasets$cifar10$load_data() %&gt;% <br/>    extract2(2) %&gt;% <br/>    set_names(c("images","classes"))</span></pre><h2 id="d49c" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">5.2.获取类定义</h2><p id="a1be" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">从<code class="fe mp mq mr ms b">TensorFlow</code>包中访问这些数据的一个挑战是，对于每种类型的图像，这些类都只是数值(<code class="fe mp mq mr ms b">0</code>到<code class="fe mp mq mr ms b">9</code>)。这些图像的定义可以在GitHub上找到(<a class="ae lb" href="https://github.com/EN10/CIFAR#classes" rel="noopener ugc nofollow" target="_blank">GitHub&gt;EN10&gt;CIFAR</a>)。这些类在下面的代码块中定义。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="9c9c" class="mu lt iq ms b gy nz oa l ob oc"># Define classes<br/>ClassList &lt;- c(<br/>    "0" = "airplane",<br/>    "1" = "automobile",<br/>    "2" = "bird",<br/>    "3" = "cat",<br/>    "4" = "deer",<br/>    "5" = "dog",<br/>    "6" = "frog",<br/>    "7" = "horse",<br/>    "8" = "ship",<br/>    "9" = "truck" <br/>)</span></pre><h1 id="556b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">6.检查数据</h1><h2 id="4df8" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">6.1.检查对象</h2><p id="15eb" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">检查数据是很重要的，以确保数据是正确生成的，并且所有信息看起来都没问题。为此，编写了一个自定义函数(<code class="fe mp mq mr ms b">get_ObjectAttributes()</code>)，其源代码可以在<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如下面的代码块所示，<code class="fe mp mq mr ms b">images </code>对象是一个四维数字数组，包含<code class="fe mp mq mr ms b">10,000</code>个图像，每个<code class="fe mp mq mr ms b">32</code> x <code class="fe mp mq mr ms b">32</code>个像素，以及<code class="fe mp mq mr ms b">3</code>个颜色通道。整个物体超过<code class="fe mp mq mr ms b">117 Mb</code>大。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="b787" class="mu lt iq ms b gy nz oa l ob oc"># Check Images<br/>cifar %&gt;% <br/>    extract2("images") %&gt;% <br/>    get_ObjectAttributes("cifar$images") %&gt;% <br/>    cat()</span></pre><p id="310b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="ad80" class="mu lt iq ms b gy nz oa l ob oc">Name : cifar$images<br/> - Size : 117.2 Mb<br/> - Clas : array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 10000x32x32x3</span></pre><p id="39e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当检查<code class="fe mp mq mr ms b">classes </code>对象时，它是一个2维数字数组(只有1列)，但是具有与<code class="fe mp mq mr ms b">images </code>对象相同数量的图像(这是预料中的)，每个类标签的频率恰好都具有<code class="fe mp mq mr ms b">1000</code>图像。总尺寸小于<code class="fe mp mq mr ms b">40 Kb</code>。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="330a" class="mu lt iq ms b gy nz oa l ob oc"># Check classes<br/>cifar %&gt;% <br/>    extract2("classes") %&gt;% <br/>    get_ObjectAttributes(name="cifar$classes", print_freq=TRUE) %&gt;% <br/>    cat()</span></pre><p id="cad3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="1ce6" class="mu lt iq ms b gy nz oa l ob oc">Name : cifar$classes<br/> - Size : 39.3 Kb<br/> - Clas : matrix,array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 10000x1<br/> - Freq :<br/>      label Freq<br/>   1  0     1000<br/>   2  1     1000<br/>   3  2     1000<br/>   4  3     1000<br/>   5  4     1000<br/>   6  5     1000<br/>   7  6     1000<br/>   8  7     1000<br/>   9  8     1000<br/>   10 9     1000</span></pre><h2 id="5d00" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">6.2.检查图像</h2><p id="26e3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在了解了内存中对象的大小之后，就有必要检查实际的图像本身了。作为人类，我们理解实际的图像和颜色，胜过理解数字。</p><p id="66ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了可视化图像，编写了两个自定义函数，如下面的代码块所示。这些函数接收数据(作为一个4维数组)，并将图像可视化为一个图。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="9ea9" class="mu lt iq ms b gy nz oa l ob oc">set_MakeImage &lt;- function(image, index=1) {<br/>    <br/>    # Extract elements<br/>    image.r &lt;- image[,,1]<br/>    image.g &lt;- image[,,2]<br/>    image.b &lt;- image[,,3]<br/>    <br/>    # Make rgb<br/>    image.rgb &lt;- rgb(<br/>        image.r, <br/>        image.g, <br/>        image.b, <br/>        maxColorValue=255<br/>    )<br/>        <br/>    # Fix dimensions<br/>    dim(image.rgb) &lt;- dim(image.r)<br/>    <br/>    # Return<br/>    return(image.rgb)<br/>    <br/>}</span><span id="1894" class="mu lt iq ms b gy od oa l ob oc">plt_PlotImage &lt;- function(images, classes, class_list, index=1) {<br/>    <br/>    # Slice images<br/>    image &lt;- images[index,,,]<br/>    image %&lt;&gt;% set_MakeImage(index)<br/>    lbl &lt;- classes %&gt;% <br/>        extract(index) %&gt;% <br/>        as.character() %&gt;% <br/>        class_list[[.]]<br/>    <br/>    # Create plot<br/>    plot &lt;- ggplot() + <br/>        ggtitle(lbl) +<br/>        draw_image(image, interpolate=FALSE)<br/>    <br/>    # Return<br/>    return(plot)<br/>    <br/>}</span></pre><p id="2a1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对前16幅图像运行该功能时，将显示以下内容。如图所示，这些图像非常像素化(这是意料之中的，因为它们每个只有<code class="fe mp mq mr ms b">32</code> x <code class="fe mp mq mr ms b">32</code>像素)，你可以看到每个图像是如何分类的。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="7bec" class="mu lt iq ms b gy nz oa l ob oc"># Set list<br/>lst &lt;- list()</span><span id="e798" class="mu lt iq ms b gy od oa l ob oc"># Loop 16 images<br/>for (index in 1:16) {<br/>    lst[[index]] &lt;- plt_PlotImage(<br/>        cifar$images, <br/>        cifar$classes, <br/>        ClassList,<br/>        index)<br/>    }</span><span id="2624" class="mu lt iq ms b gy od oa l ob oc"># View images<br/>plt &lt;- gridExtra::grid.arrange(grobs=lst, ncol=4)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/f84c3cb31b37085f96a2617f4a774436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GvXjGdk2x_YTm_buQRL8A.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图1 </strong>:初始图像</p></figure><h1 id="266e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">7.准备数据</h1><p id="bec1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">准备数据有四个步骤:</p><ol class=""><li id="5002" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">把…重新分类</li><li id="7d90" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">裂开</li><li id="0414" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">使再成形</li><li id="5ece" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">使标准化</li></ol><h2 id="1d22" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">7.1.把…重新分类</h2><p id="363d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">出于本文的目的，让我们假设我们正在尝试预测图片是<code class="fe mp mq mr ms b">car</code>还是<code class="fe mp mq mr ms b">not</code>。这将需要将数据转换为二进制分类问题，其中神经网络将从数据中预测<code class="fe mp mq mr ms b">1</code>或<code class="fe mp mq mr ms b">0</code>。这将意味着模型输出将是分数的概率分布，通过改变截止变量可以容易地对其进行分类。</p><p id="f1b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步是对数据进行重新分类，使所有汽车的值都是<code class="fe mp mq mr ms b">1</code>，其他的都是<code class="fe mp mq mr ms b">0</code>。我们从之前定义的类中知道，汽车已经有了值<code class="fe mp mq mr ms b">1</code>，这意味着只需要对所有其他类进行转换。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="64a1" class="mu lt iq ms b gy nz oa l ob oc"># Implement within a pipe<br/>cifar[["classes"]] &lt;- cifar %&gt;%<br/>    extract2("classes") %&gt;% <br/>    (function(classes){<br/>        <br/>        # View initial classes<br/>        classes %&gt;% as.vector %&gt;% head(40) %&gt;% print<br/>        <br/>        # Reclassify<br/>        classes &lt;- ifelse(classes==1,1,0)<br/>        <br/>        # View reclassified classes<br/>        classes %&gt;% as.vector %&gt;% head(40) %&gt;% print<br/>        <br/>        # Return<br/>        return(classes)<br/>    })</span></pre><p id="377d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="b00a" class="mu lt iq ms b gy nz oa l ob oc">[1] 3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6 7 0 4 9 5 2 4 0 9 6<br/>[1] 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span></pre><h2 id="b95f" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">7.2.分割数据</h2><p id="59ff" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下一个任务是将数据分成训练集和测试集。这样做的原因在其他地方有所涉及(如<a class="ae lb" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29" rel="noopener ugc nofollow" target="_blank">维基百科的交叉验证</a>和<a class="ae lb" href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data" rel="noopener ugc nofollow" target="_blank">谷歌的机器学习速成班:训练和测试集分割数据</a>)。</p><p id="46c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，为了理解当前的数据分割，下面的代码块使用<code class="fe mp mq mr ms b">ggplot2</code>包可视化了这些数据。如图所示，数据目前分布在<code class="fe mp mq mr ms b">90%</code>和<code class="fe mp mq mr ms b">0</code>类别中，剩余的<code class="fe mp mq mr ms b">10%</code>和<code class="fe mp mq mr ms b">1</code>类别中。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="0741" class="mu lt iq ms b gy nz oa l ob oc"># Print Plot<br/>cifar %&gt;% <br/>    extract("classes") %&gt;% <br/>    table(dnn="classes") %&gt;% <br/>    data.frame() %&gt;% <br/>    ggplot(aes(classes, Freq, fill=classes)) + <br/>        geom_col(colour="black") +<br/>        geom_label(<br/>            aes(label=Freq),<br/>            show.legend=FALSE<br/>        ) +<br/>        scale_y_continuous(breaks=seq(0,10000,1000)) +<br/>        theme(panel.grid.minor.y=element_blank()) +<br/>        labs(<br/>            title="Count of Each Class"<br/>        )</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/3161ecafca60ce9cce1c2f8ed4bb0b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89nGze_b10rr-k0u_NzuUg.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图2 </strong>:每一类的计数</p></figure><p id="d6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现这种数据分割，我们使用了<code class="fe mp mq mr ms b">caret::createDataPartition()</code>函数。这将创建一个<code class="fe mp mq mr ms b">partition</code>对象，然后使用它来相应地分离<code class="fe mp mq mr ms b">cifar</code>数据。在<code class="fe mp mq mr ms b">70%</code>任意选择拆分比例用于训练，其余用于测试。然而，这可能是合理的<code class="fe mp mq mr ms b">80%</code>；这是一个超参数，可以在以后的阶段调整。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="8819" class="mu lt iq ms b gy nz oa l ob oc"># Set seed for reproducibility<br/>set.seed(1234)</span><span id="5ddf" class="mu lt iq ms b gy od oa l ob oc"># Create partition<br/>partition &lt;- createDataPartition(cifar$classes, p=0.7, list=FALSE)</span><span id="25ed" class="mu lt iq ms b gy od oa l ob oc"># Split data<br/>trn_img &lt;- cifar$images[partition,,,]<br/>tst_img &lt;- cifar$images[-partition,,,]<br/>trn_cls &lt;- cifar$classes[partition]<br/>tst_cls &lt;- cifar$classes[-partition]</span></pre><p id="c740" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分割后，数据被重新绘制，很容易看出训练/测试分割在两个类别上实现了均匀的<code class="fe mp mq mr ms b">70%</code>分布。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="b828" class="mu lt iq ms b gy nz oa l ob oc"># Print Plot<br/>rbind(<br/>    trn_cls %&gt;% table,<br/>    tst_cls %&gt;% table<br/>) %&gt;% <br/>    set_rownames(c("train","test")) %&gt;% <br/>    data.frame() %&gt;% <br/>    rename_all(str_remove_all, "X") %&gt;% <br/>    rownames_to_column("data") %&gt;% <br/>    pivot_longer(2:3, names_to="classes", values_to="Freq") %&gt;% <br/>    mutate(<br/>        label=paste(data, classes, sep=": "),<br/>        data=factor(data, levels=c("train","test"))<br/>    ) %&gt;% <br/>    ggplot(aes(classes, Freq, fill=data), position="dodge") + <br/>        geom_col(colour="black", position="dodge") +<br/>        geom_label(<br/>            aes(label=Freq), <br/>            position=position_dodge(width=0.9),<br/>            show.legend=FALSE<br/>        ) +<br/>        scale_y_continuous(breaks=seq(0,10000,1000)) +<br/>        theme(panel.grid.minor.y=element_blank()) +<br/>        labs(<br/>            title="Count of Each Class",<br/>            subtitle="Split by Train/Test"<br/>        )</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/b2b9795a67405832f0c46325e653fd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8uYX6CZn_1eNmyccZSgrjQ.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图3 </strong>:每类计数，按训练/测试划分</p></figure><p id="78d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">检查数据是否被正确分割的另一种方法是再次运行<code class="fe mp mq mr ms b">get_ObjectAttributes()</code>函数，如下面的代码块所示。这里显示的信息与上面的图一致。有趣的是，训练图像数组<code class="fe mp mq mr ms b">82 Mb</code>很大，这对于以后检查正向传播的性能很重要。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="7c56" class="mu lt iq ms b gy nz oa l ob oc">for (name in c("trn_img","tst_img","trn_cls","tst_cls")) {<br/>    name %&gt;% <br/>        get() %&gt;% <br/>        get_ObjectAttributes(<br/>            name, <br/>            if (name %in% c("trn_cls","tst_cls")) TRUE else FALSE<br/>        ) %&gt;% <br/>        cat()<br/>    if (name != "tst_cls") cat("\n")<br/>}</span></pre><p id="c1dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="c5f1" class="mu lt iq ms b gy nz oa l ob oc">Name : trn_img<br/> - Size : 82 Mb<br/> - Clas : array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 7000x32x32x3</span><span id="e11e" class="mu lt iq ms b gy od oa l ob oc">Name : tst_img<br/> - Size : 35.2 Mb<br/> - Clas : array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 3000x32x32x3</span><span id="0f46" class="mu lt iq ms b gy od oa l ob oc">Name : trn_cls<br/> - Size : 54.7 Kb<br/> - Clas : numeric<br/> - Type : double<br/> - Mode : numeric<br/> - Dims : 7000<br/> - Freq :<br/>     label Freq<br/>   1 0     6309<br/>   2 1      691</span><span id="677d" class="mu lt iq ms b gy od oa l ob oc">Name : tst_cls<br/> - Size : 23.5 Kb<br/> - Clas : numeric<br/> - Type : double<br/> - Mode : numeric<br/> - Dims : 3000<br/> - Freq :<br/>     label Freq<br/>   1 0     2691<br/>   2 1      309</span></pre><h2 id="a2b2" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">7.3.重塑数据</h2><p id="c8df" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于我们神经网络的第一个<code class="fe mp mq mr ms b">input</code>层，我们希望是一个一维的节点。因此，有必要将数据从一个4维数组调整为一个2维数组。这个过程叫做展平，更多信息可以在这里找到:<a class="ae lb" href="https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays" rel="noopener ugc nofollow" target="_blank">多维数组的内存布局</a>。</p><p id="bb5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<code class="fe mp mq mr ms b">array()</code>函数可以很容易地实现该方法，因为它有<code class="fe mp mq mr ms b">dim=</code>参数，可以用来指定所需的尺寸。</p><p id="c8fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">期望的矩阵尺寸应该使每个<em class="mt">图像</em>在新的一行，每个<em class="mt">像素</em>在不同的一列。因为每个<em class="mt">像素</em>由第2、第3和第4维组成，我们需要取这三个数的<em class="mt">乘积</em>，并使用它来指定所需的列数。实际上，我们正在运行这个等式:<code class="fe mp mq mr ms b">32</code> × <code class="fe mp mq mr ms b">32</code> × <code class="fe mp mq mr ms b">3</code>，这相当于拥有<code class="fe mp mq mr ms b">3072</code>列。这个等式是在下一个代码块中以编程方式内嵌实现的。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="e325" class="mu lt iq ms b gy nz oa l ob oc"># Reshape data<br/>trn_img %&lt;&gt;% array(dim=c(<br/>    dim(.) %&gt;% extract(1),<br/>    dim(.) %&gt;% extract(2:4) %&gt;% prod()<br/>))</span><span id="726a" class="mu lt iq ms b gy od oa l ob oc">tst_img %&lt;&gt;% array(dim=c(<br/>    dim(.) %&gt;% extract(1),<br/>    dim(.) %&gt;% extract(2:4) %&gt;% prod()<br/>))</span><span id="7ec0" class="mu lt iq ms b gy od oa l ob oc">trn_cls %&lt;&gt;% array(dim=c(<br/>    length(.),<br/>    1<br/>))</span><span id="d471" class="mu lt iq ms b gy od oa l ob oc">tst_cls %&lt;&gt;% array(dim=c(<br/>    length(.),<br/>    1<br/>))</span></pre><p id="5b82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当再次检查对象属性时，您将看到图像数据已经被正确地处理，以行数作为图像的数量，以列数作为像素的数量。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="f050" class="mu lt iq ms b gy nz oa l ob oc">for (name in c("trn_img","tst_img","trn_cls","tst_cls")) {<br/>    name %&gt;% <br/>        get() %&gt;% <br/>        get_ObjectAttributes(name, FALSE) %&gt;% <br/>        cat()<br/>    if (name != "tst_cls") cat("\n")<br/>}</span></pre><p id="5ad4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="c2a7" class="mu lt iq ms b gy nz oa l ob oc">Name : trn_img<br/> - Size : 82 Mb<br/> - Clas : matrix,array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 7000x3072</span><span id="ad85" class="mu lt iq ms b gy od oa l ob oc">Name : tst_img<br/> - Size : 35.2 Mb<br/> - Clas : matrix,array<br/> - Type : integer<br/> - Mode : numeric<br/> - Dims : 3000x3072</span><span id="58c3" class="mu lt iq ms b gy od oa l ob oc">Name : trn_cls<br/> - Size : 54.9 Kb<br/> - Clas : matrix,array<br/> - Type : double<br/> - Mode : numeric<br/> - Dims : 7000x1</span><span id="0c2c" class="mu lt iq ms b gy od oa l ob oc">Name : tst_cls<br/> - Size : 23.6 Kb<br/> - Clas : matrix,array<br/> - Type : double<br/> - Mode : numeric<br/> - Dims : 3000x1</span></pre><h2 id="6190" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">7.4.标准化数据</h2><p id="bdc3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">准备数据的最后一步是标准化数据，以便所有元素都在<code class="fe mp mq mr ms b">0</code>和<code class="fe mp mq mr ms b">1</code>之间。这样做的原因是为了防止在后面的步骤中出现爆炸和消失的梯度，因为神经网络将试图拟合所有的波峰和波谷，这是由于数据在<code class="fe mp mq mr ms b">0</code>到<code class="fe mp mq mr ms b">255</code>的值范围内引起的。</p><p id="8348" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如<a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>网站所述，<a class="ae lb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR10 </a>数据集由RGB图像数据组成。并且，正如<a class="ae lb" href="https://en.wikipedia.org/wiki/RGB_color_model" rel="noopener ugc nofollow" target="_blank">维基百科</a>上记载的，RGB数据都是在<code class="fe mp mq mr ms b">0</code>和<code class="fe mp mq mr ms b">255</code>之间的值。</p><p id="8654" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以要做的就是把所有的元素除以<code class="fe mp mq mr ms b">255</code>，必然会产生一个在<code class="fe mp mq mr ms b">0</code>和<code class="fe mp mq mr ms b">1</code>之间的值。由于图像数据当前在一个数组中，下面代码块中的函数将作为一个<em class="mt">矢量化函数</em>在整个数组中运行，相应地将所有元素除以<code class="fe mp mq mr ms b">255</code>。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="c225" class="mu lt iq ms b gy nz oa l ob oc">trn_img &lt;- trn_img/255<br/>tst_img &lt;- tst_img/255</span></pre><p id="3654" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据现已准备好，可用于网络。下一步是建立网络。</p><h1 id="6a04" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">8.实例化网络</h1><h2 id="1d27" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">8.1.定义架构</h2><p id="e04b" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">关于网络<em class="mt">实际上</em>是什么的一些快速注释:</p><ul class=""><li id="d668" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated">整体架构是一个<code class="fe mp mq mr ms b">list</code>。</li><li id="42c1" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">主<code class="fe mp mq mr ms b">list</code>的每一个<code class="fe mp mq mr ms b">element</code>都是另一个<code class="fe mp mq mr ms b">list</code>，这些将构成整体<em class="mt">网络</em>的每一个<em class="mt">层</em>。</li><li id="be4a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">第一层永远是<code class="fe mp mq mr ms b">input</code>层。</li><li id="4080" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">最后一层将永远是<code class="fe mp mq mr ms b">output</code>层。</li><li id="17c8" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">中间的每一层都将是<code class="fe mp mq mr ms b">hidden</code>层，这些层的名称简单地用数字命名。</li><li id="76bc" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">每层的每个元素都将被标记为相同，定义如下:</li><li id="027b" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">nodz</code>:本层节点数。</li><li id="0d70" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">inpt</code>:输入矩阵。又名<code class="fe mp mq mr ms b">A_prev</code>。这是前一层激活的副本，因此对于大型网络，需要考虑这一点。</li><li id="7b89" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">wgts</code>:权重矩阵。又名<code class="fe mp mq mr ms b">W</code>。</li><li id="b238" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">bias</code>:偏置向量。又名<code class="fe mp mq mr ms b">b</code>。</li><li id="de26" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">linr</code>:线性矩阵。又名<code class="fe mp mq mr ms b">Z</code>。这是<code class="fe mp mq mr ms b">inpt</code>、<code class="fe mp mq mr ms b">wgts</code>和<code class="fe mp mq mr ms b">bias</code>之间的线性代数的结果。</li><li id="46b9" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">acti</code>:激活矩阵。又名<code class="fe mp mq mr ms b">A</code>。将激活函数应用于<code class="fe mp mq mr ms b">linr</code>矩阵的结果。</li><li id="55fa" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">acti_func</code>:使用的激活功能。</li><li id="535d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">cost</code>:车型的整体成本。这是一个单一的值(模型的总成本)，但被复制到模型的每一层。</li><li id="1631" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">back_cost</code>:成本向量的梯度。又名<code class="fe mp mq mr ms b">dA_cost</code>。</li><li id="e7ad" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">back_acti</code>:激活矩阵的梯度。又名<code class="fe mp mq mr ms b">dA</code>。应用反向传播后的微分结果。具有给定的成本函数。</li><li id="d12f" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">back_linr</code>:线性代数矩阵的梯度。又名<code class="fe mp mq mr ms b">dZ</code>。向后线性微分反向传播的结果。</li><li id="63d5" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">back_wgts</code>:权重矩阵的梯度。又名<code class="fe mp mq mr ms b">dW</code>。也是背撑的结果。</li><li id="e487" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">— <code class="fe mp mq mr ms b">back_bias</code>:偏置向量的梯度。又名<code class="fe mp mq mr ms b">db</code>。也是背撑的结果。</li></ul><h2 id="bdd7" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">8.1.设置实例化功能</h2><p id="a9db" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于下面的代码块，定义了函数<code class="fe mp mq mr ms b">set_InstantiateNetwork()</code>。它只有三个输入参数，用于指定每层中使用的节点数。基于这些信息，模型将被实例化并返回，为下一步的初始化做好准备。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="4cc6" class="mu lt iq ms b gy nz oa l ob oc">set_InstantiateNetwork &lt;- function(<br/>    input=50, <br/>    hidden=c(30,20,10), <br/>    output=1<br/>    ) {</span><span id="32e8" class="mu lt iq ms b gy od oa l ob oc">    # Set up<br/>    model = list()<br/>    layers = c(<br/>        "input",<br/>        1:length(hidden),<br/>        "output"<br/>    )<br/>    <br/>    # Loop<br/>    for (layer in layers) {<br/>        <br/>        # Make layer<br/>        model[[layer]] &lt;- list(<br/>            "nodz"      = "",<br/>            "inpt"      = "",<br/>            "wgts"      = "",<br/>            "bias"      = "",<br/>            "linr"      = "",<br/>            "acti"      = "",<br/>            "acti_func" = "",<br/>            "cost"      = "",<br/>            "back_cost" = "",<br/>            "back_acti" = "",<br/>            "back_linr" = "",<br/>            "back_wgts" = "",<br/>            "back_bias" = "" <br/>        )<br/>        <br/>        # Set nodes<br/>        if (layer=="input") {<br/>            model[[layer]][["nodz"]] &lt;- input<br/>        } else if (layer=="output") {<br/>            model[[layer]][["nodz"]] &lt;- output<br/>        } else {<br/>            layer_index &lt;- layer %&gt;% as.numeric()<br/>            model[[layer]][["nodz"]] &lt;- hidden[layer_index]<br/>        }<br/>        <br/>    }<br/>    <br/>    # Return<br/>    return(model)<br/>    <br/>}</span></pre><h2 id="6327" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">8.1.创建网络</h2><p id="6012" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下面的代码块实例化了网络。该模型将在每层中设置以下数量的节点:</p><ul class=""><li id="099e" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated">层将有T2节点，和上面计算的一样多。</li><li id="a9fe" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated">从<code class="fe mp mq mr ms b">100</code>到<code class="fe mp mq mr ms b">20</code>音符，每个<code class="fe mp mq mr ms b">hidden</code>层的节点数量会逐渐减少。</li><li id="5d38" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">output</code>层会有<code class="fe mp mq mr ms b">1</code>节点，因为这一个节点会是<code class="fe mp mq mr ms b">0</code>和<code class="fe mp mq mr ms b">1</code>之间的浮点数，用来预测相关图像是不是汽车。</li></ul><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="b17e" class="mu lt iq ms b gy nz oa l ob oc">network_model &lt;- set_InstantiateNetwork(<br/>    input=3072, <br/>    hidden=c(100,75,50,30,20), <br/>    output=1<br/>)</span></pre><h2 id="c46b" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">8.1.将网络可视化</h2><p id="9a7c" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有一个非常好的网站可以让神经网络可视化:<a class="ae lb" href="http://alexlenail.me/NN-SVG/AlexNet.html" rel="noopener ugc nofollow" target="_blank">http://alexlenail.me/NN-SVG/AlexNet.html</a>。下图是刚刚创建的网络的示意图。</p><p id="b330" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦网络完全初始化并向前传播，可以进行进一步的可视化。详见<a class="ae lb" href="#de5b" rel="noopener ugc nofollow"> <strong class="kh ir">章节检查模型形状</strong> </a>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi og"><img src="../Images/2e4a15e20d774bcc5fdd64c2e8a79102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1R57pDTlMUNLPAY4iTJJFw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图4 </strong>:网络的可视化</p></figure><h1 id="73a7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">9.初始化网络</h1><p id="edd5" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">初始化网络有四个步骤:</p><ol class=""><li id="63e5" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">设置重量初始化功能</li><li id="d9c8" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">设置层初始化功能</li><li id="e0c5" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">设置模型初始化功能</li><li id="e403" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">运行初始化</li></ol><h2 id="b28a" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.1.重量初始化</h2><p id="34b8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在其核心，权重初始化只是生成一个随机的正常数字(用<code class="fe mp mq mr ms b">μ=0</code>和<code class="fe mp mq mr ms b">σ=1</code>)。然而，通过仅使用这个随机生成的数字，当试图训练更深的神经网络时，发现模型梯度爆炸或消失。因此，这些权重在被初始化后需要被缩放，以便足够健壮以继续在更深的层被训练。</p><p id="ecca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有许多算法可用于重量初始化。两个比较常见的是<a class="ae lb" href="https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank"> Xavier </a>算法和<a class="ae lb" href="https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528" rel="noopener"> He </a>算法。理解这些算法背后的细节的一些好资源包括:</p><ul class=""><li id="0ce1" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">神经网络中的权重初始化</a></li><li id="ea4a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><a class="ae lb" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">了解训练深度前馈神经网络的难度</a></li><li id="e791" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><a class="ae lb" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">在ImageNet分类上超越人类水平的性能</a></li></ul><p id="6bc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">9.1.1。泽维尔算法</p><p id="7a16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Xavier初始化的公式为:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/53ec35ac7b33199cad3eb19088257c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/0*Ky01V2AVXfKtVDe5"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">等式1 </strong> : Xavier初始化算法</p></figure><p id="b612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中:</p><ul class=""><li id="cecf" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated"><em class="mt"> nᵢ </em>是进入这一层的节点数量。也称为“扇入”。</li><li id="f3d2" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><em class="mt"> nᵢ₊₁ </em>是从这一层出去的节点数。也称为“扇出”。</li></ul><p id="bdf8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">9.1.2。何算法</p><p id="3382" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">he初始化的公式为:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e9753bcc9880dfa0dc54b7beff0c6c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:104/0*EyhvlCoaEPCiCYsC"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">等式2 </strong>:初始化算法</p></figure><p id="f770" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中:</p><ul class=""><li id="52c1" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated">nᵢ是进入这一层的节点数量。</li></ul><h2 id="a35f" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.1.初始化功能</h2><p id="6970" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">出于编程目的，这些函数是用<code class="fe mp mq mr ms b">order</code>值作为函数参数的一部分编写的。这意味着方程的数量级可以在稍后阶段改变，并用作超参数。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="d773" class="mu lt iq ms b gy nz oa l ob oc">let_InitialiseXavier &lt;- function(nodes_in, nodes_out, order=6) {<br/>    <br/>    # Do work<br/>    numer &lt;- sqrt(order)<br/>    denom &lt;- sqrt(nodes_in + nodes_out)<br/>    output &lt;- numer/denom<br/>    <br/>    # Return<br/>    return(output)<br/>    <br/>}</span><span id="deb5" class="mu lt iq ms b gy od oa l ob oc">let_InitialiseHe &lt;- function(nodes_in, nodes_out, order=2) {<br/>    <br/>    # Do work<br/>    numer &lt;- order<br/>    denom &lt;- nodes_in<br/>    output &lt;- sqrt(numer/denom)<br/>    <br/>    # Return<br/>    return(output)<br/>    <br/>}</span></pre><h2 id="ad99" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.3.层初始化</h2><p id="7e04" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下一步是构建一个函数，该函数将初始化一个单独层的所有相关方面。这一步很重要，因为这是创建权重矩阵的地方，并且这些权重矩阵必须以某种方式构建，以确保维度允许成功的向前传播。</p><p id="f477" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">层构造的步骤如下:</p><ol class=""><li id="e5b6" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">确定当前层的层名(<code class="fe mp mq mr ms b">layer</code>)和前一层的层名(<code class="fe mp mq mr ms b">layer_prev</code>)。<br/> —用于从<code class="fe mp mq mr ms b">network_model</code>列表中访问相关配置。</li><li id="873f" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">确定馈入本层的节点数(<code class="fe mp mq mr ms b">nodes_in</code>)和馈出当前层的节点数(<code class="fe mp mq mr ms b">nodes_out</code>)。<br/> —用于解析初始化算法。</li><li id="b361" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">创建权重矩阵。<br/> —尺寸如下:<br/>———1。行数是前一层中的节点数。<br/>———2。列数是当前层中的节点数。<br/> —使用<code class="fe mp mq mr ms b">rnorm()</code>函数创建每个元素，该函数使用<code class="fe mp mq mr ms b">μ=0</code>和<code class="fe mp mq mr ms b">σ=1</code>在正常曲线上生成一个随机数。</li><li id="1a40" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">确定要使用的初始化算法。<br/> —解析到该函数中的value值是与相关算法相关的小写单词。<br/> —强制转换为标题大写，然后赋予前缀<code class="fe mp mq mr ms b">let_Initialise</code>。<br/> —该值然后被解析到<code class="fe mp mq mr ms b">get()</code>函数中，该函数然后调用该函数，并执行解析到该函数的参数。<br/> —这是一种灵活调用不同算法的编程方式，基于解析到函数的值。</li><li id="f507" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">缩放权重矩阵。<br/> —通过将每个元素乘以初始化算法。</li><li id="af75" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">创建偏差矩阵。<br/> —尺寸如下:<br/> — — 1。行数是当前层中的节点数。<br/>———2。只有一列。<br/> —每个元素都有值<code class="fe mp mq mr ms b">0</code>。</li><li id="1618" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将重量和偏差矩阵重新应用到<code class="fe mp mq mr ms b">network_model</code>上。</li><li id="1abf" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">返回更新后的<code class="fe mp mq mr ms b">network_model</code>对象。</li></ol><p id="16e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现这一点，函数使用的函数参数包括:</p><ol class=""><li id="70d4" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated"><code class="fe mp mq mr ms b">network_model</code>本身。</li><li id="4961" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">该层的<code class="fe mp mq mr ms b">layer_index</code>(其中<code class="fe mp mq mr ms b">1</code>为<code class="fe mp mq mr ms b">input</code>层，每个递增的数字为每个后续层)。</li><li id="d1ab" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">initialisation_algorithm</code>，或者是值<code class="fe mp mq mr ms b">NA</code>，或者是值<code class="fe mp mq mr ms b">xavier</code>或者是相关算法的值<code class="fe mp mq mr ms b">he</code>。</li><li id="f0c0" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">initialisation_order</code>，它是一个整数值，在相关算法中用作分子。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="acaf" class="mu lt iq ms b gy nz oa l ob oc">set_InitialiseLayer &lt;- function(<br/>    network_model,<br/>    layer_index,<br/>    initialisation_algorithm=NA,<br/>    initialisation_order=6<br/>    ) {<br/>    <br/>    # Get layer names<br/>    layer_prev &lt;- names(network_model)[layer_index-1]<br/>    layer &lt;- names(network_model)[layer_index]<br/>    <br/>    # Get number of nodes<br/>    if (layer_index == 1) {<br/>        nodes_in &lt;- 0 #First layer is 'input'<br/>    } else {<br/>        nodes_in &lt;- network_model %&gt;% <br/>            extract2(layer_prev) %&gt;% <br/>            extract2("nodz")<br/>    }<br/>    nodes_out &lt;- network_model %&gt;% <br/>        extract2(layer) %&gt;% <br/>        extract2("nodz")<br/>    <br/>    # Set the seed of reproducibility<br/>    set.seed(1234)<br/>    <br/>    # Initialise weight matrix<br/>    w_matrix &lt;- matrix(<br/>        data=rnorm(nodes_in * nodes_out), <br/>        nrow=nodes_in,<br/>        ncol=nodes_out<br/>    )<br/>    <br/>    # Get initialisation algorithm<br/>    if (!is.na(initialisation_algorithm)) {<br/>        algorithm &lt;- paste0(<br/>            "let_Initialise", <br/>            str_to_title(initialisation_algorithm)<br/>        )<br/>    }<br/>    <br/>    # Scale weights<br/>    if (layer_index != 1) {<br/>        if (is.na(initialisation_algorithm)) {<br/>            w_matrix &lt;- w_matrix<br/>        } else {<br/>            w_matrix &lt;- w_matrix * <br/>                get(algorithm)(<br/>                    nodes_in=nodes_in, <br/>                    nodes_out=nodes_out, <br/>                    order=initialisation_order<br/>                )<br/>        }<br/>    }<br/>    <br/>    # Initialise bias matrix<br/>    b_matrix &lt;- matrix(<br/>        data=network_model %&gt;% <br/>            extract2(layer) %&gt;% <br/>            extract2("nodz") %&gt;% <br/>            replicate(0),<br/>        nrow=network_model %&gt;% <br/>            extract2(layer) %&gt;% <br/>            extract2("nodz"),<br/>        ncol=1<br/>    )<br/>    <br/>    # Place data back in to the model<br/>    network_model[[layer]][["wgts"]] &lt;- w_matrix<br/>    network_model[[layer]][["bias"]] &lt;- b_matrix<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><h2 id="c864" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.4.模型初始化</h2><p id="cfb1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated"><code class="fe mp mq mr ms b">set_InitialiseModel()</code>功能的目的是循环通过<code class="fe mp mq mr ms b">network_model</code>对象中的每一层，根据模型本身的参数设置对其进行初始化。该功能将获取节点数(由<code class="fe mp mq mr ms b">set_InstantiateNetwork()</code>功能设置)。</p><p id="d4fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数将只接受三个输入参数:</p><ol class=""><li id="a01d" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated"><code class="fe mp mq mr ms b">network_model</code> : <br/> —由<code class="fe mp mq mr ms b">set_InstantiateNetwork()</code>函数实例化的网络。</li><li id="d42f" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">initialisation_algorithm</code> : <br/> —用于网络初始化的算法。<br/> —这应该是值<code class="fe mp mq mr ms b">NA</code>、<code class="fe mp mq mr ms b">xavier</code>或<code class="fe mp mq mr ms b">he</code>。</li><li id="3c93" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">initialisation_order</code> : <br/> —用于等式分子的顺序值。<br/> —这可以是一个数值，或者是字符串<code class="fe mp mq mr ms b">layers</code>，表示顺序应该是网络中隐藏层的数量。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="5499" class="mu lt iq ms b gy nz oa l ob oc">set_InitialiseModel &lt;- function(<br/>    network_model, <br/>    initialisation_algorithm="xavier", <br/>    initialisation_order="layers"<br/>    ) {<br/>    <br/>    # Redefine 'initialisation_order'<br/>    if (initialisation_order == "layers") {<br/>        initialisation_order &lt;- get_CountOfElementsWithCondition(<br/>            names(network_model), <br/>            function(x){is.integer(as.numeric(x))}<br/>        )<br/>    }<br/>    <br/>    # Initialise each layer<br/>    for (layer_index in 1:length(names(network_model))) {<br/>        network_model &lt;- set_InitialiseLayer(<br/>            network_model=network_model, <br/>            layer_index=layer_index, <br/>            initialisation_algorithm=initialisation_algorithm,<br/>            initialisation_order=initialisation_order<br/>        )<br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><p id="f1ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，该函数使用了用户自定义函数<code class="fe mp mq mr ms b">get_CountOfElementsWithCondition()</code>。该函数允许计算模型中隐藏层的数量。该功能的源代码可以在这里<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">找到。</a></p><h2 id="8f00" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.5.网络初始化</h2><p id="3912" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下面的代码块使用上面定义的函数初始化网络。使用的方法是<code class="fe mp mq mr ms b">%&lt;&gt;%</code>，它在<code class="fe mp mq mr ms b">magrittr</code>包中定义，说明它用于更新左侧的值，首先通过管道将它输入右侧的第一个参数位置，然后将结果赋回左侧的对象。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="d69d" class="mu lt iq ms b gy nz oa l ob oc">network_model %&lt;&gt;% set_InitialiseModel()</span></pre><h2 id="0f41" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">9.6.检查模型参数</h2><p id="b650" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了快速检查，下面的代码块检查已定义模型的参数数量。这再次使用了一个定制函数<code class="fe mp mq mr ms b">get_ModelParametersCount()</code>，它的定义可以在本文的源代码中找到(位于<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="e6aa" class="mu lt iq ms b gy nz oa l ob oc">network_model %&gt;% <br/>    get_ModelParametersCount() %&gt;% <br/>    format(big.mark=",")</span></pre><p id="c6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="3747" class="mu lt iq ms b gy nz oa l ob oc">[1] "320,846"</span></pre><p id="c27f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该神经网络中有超过<code class="fe mp mq mr ms b">320,000</code>个参数。这些参数中的每一个都需要训练，并且每一个都会对最终结果产生影响。在接下来的部分中，我们将继续介绍如何实现这一点。</p><h1 id="fa55" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">10.正向传播</h1><h2 id="61b8" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.1.该理论</h2><p id="b23d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有一个非常好的网站概述了矩阵乘法中实际发生的事情:<a class="ae lb" href="http://matrixmultiplication.xyz/" rel="noopener ugc nofollow" target="_blank">矩阵乘法</a>。</p><p id="4e77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正向传播方法的理论如下:</p><ol class=""><li id="ea38" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">应用矩阵乘法<br/> — 1。第一个矩阵是来自前一层的激活矩阵。<br/> — 2。第二个矩阵是当前层的权重矩阵。<br/> — 3。第三矩阵是当前层的(第一)线性激活矩阵。</li><li id="0934" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">应用偏置矩阵<br/> — 1。第一矩阵是当前层的(第一)线性激活。<br/> — 2。第二个矩阵是当前层的偏差矩阵。<br/> — 3。第三矩阵是当前层的(第二)线性激活矩阵。</li><li id="a0b0" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">应用激活算法<br/> — 1。第一矩阵是当前层的(第二)线性激活矩阵。<br/> — 2。激活功能由用户在功能运行期间确定。可以是<code class="fe mp mq mr ms b">relu</code>激活、<code class="fe mp mq mr ms b">sigmoid</code>激活或任何其他激活功能。<br/> — 3。第三个矩阵是当前层的激活矩阵。</li></ol><p id="e56a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 10.1.1。第一步</strong></p><p id="318c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了说明，下面的矩阵显示了正向传播的第一步，使用了虚拟数字。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/cd7cf5bc1ed4fbd648354896bab39f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*67D-5eqiGUxYSI_L"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图五</strong>:前进道具，第一步</p></figure><p id="1688" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码显示了这个过程是如何以编程方式实现的。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="23d7" class="mu lt iq ms b gy nz oa l ob oc"># Declare First matrix<br/>matrix_input &lt;- matrix(<br/>    data=1:24, <br/>    nrow=8, <br/>    ncol=3, <br/>    byrow=TRUE<br/>)</span><span id="624c" class="mu lt iq ms b gy od oa l ob oc"># Declare Weight matrix<br/>matrix_weight &lt;- matrix(<br/>    data=1:15, <br/>    nrow=3, <br/>    ncol=5, <br/>    byrow=TRUE<br/>)</span><span id="9244" class="mu lt iq ms b gy od oa l ob oc"># Apply matrix manipulation<br/>matrix_layer &lt;- matrix_input %*% matrix_weight</span></pre><p id="2299" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="9586" class="mu lt iq ms b gy nz oa l ob oc">matrix_input:<br/>     [,1] [,2] [,3]<br/>[1,]    1    2    3<br/>[2,]    4    5    6<br/>[3,]    7    8    9<br/>[4,]   10   11   12<br/>[5,]   13   14   15<br/>[6,]   16   17   18<br/>[7,]   19   20   21<br/>[8,]   22   23   24</span><span id="f772" class="mu lt iq ms b gy od oa l ob oc">matrix_weight:<br/>     [,1] [,2] [,3] [,4] [,5]<br/>[1,]    1    2    3    4    5<br/>[2,]    6    7    8    9   10<br/>[3,]   11   12   13   14   15</span><span id="0ed2" class="mu lt iq ms b gy od oa l ob oc">matrix_layer:<br/>     [,1] [,2] [,3] [,4] [,5]<br/>[1,]   46   52   58   64   70<br/>[2,]  100  115  130  145  160<br/>[3,]  154  178  202  226  250<br/>[4,]  208  241  274  307  340<br/>[5,]  262  304  346  388  430<br/>[6,]  316  367  418  469  520<br/>[7,]  370  430  490  550  610<br/>[8,]  424  493  562  631  700</span></pre><p id="3ad0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 10.1.2。第二步</strong></p><p id="7fb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了如何应用偏置矩阵。如图所示，偏置矩阵中的每个元素水平应用于初始矩阵的每个元素。这张图表显示了它是如何工作的。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e5e52f3446a7c8de5830b4ad177bdd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/0*CojT1k88KTtgsTC-"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图6 </strong>:前进支柱，第二步</p></figure><p id="47fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码显示了这个过程是如何以编程方式实现的。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="3ce0" class="mu lt iq ms b gy nz oa l ob oc"># Declare Bias matrix<br/>vector_bias &lt;- matrix(1:8, 8, 1)</span><span id="2c47" class="mu lt iq ms b gy od oa l ob oc"># Apply Bias matrix<br/>matrix_biased &lt;- sweep(matrix_layer, 1, vector_bias, "+")</span></pre><p id="4bee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="e42b" class="mu lt iq ms b gy nz oa l ob oc">matrix_layer:<br/>     [,1] [,2] [,3] [,4] [,5]<br/>[1,]   46   52   58   64   70<br/>[2,]  100  115  130  145  160<br/>[3,]  154  178  202  226  250<br/>[4,]  208  241  274  307  340<br/>[5,]  262  304  346  388  430<br/>[6,]  316  367  418  469  520<br/>[7,]  370  430  490  550  610<br/>[8,]  424  493  562  631  700</span><span id="c444" class="mu lt iq ms b gy od oa l ob oc">vector_bias:<br/>     [,1]<br/>[1,]    1<br/>[2,]    2<br/>[3,]    3<br/>[4,]    4<br/>[5,]    5<br/>[6,]    6<br/>[7,]    7<br/>[8,]    8</span><span id="b859" class="mu lt iq ms b gy od oa l ob oc">matrix_biased:<br/>     [,1] [,2] [,3] [,4] [,5]<br/>[1,]   47   53   59   65   71<br/>[2,]  102  117  132  147  162<br/>[3,]  157  181  205  229  253<br/>[4,]  212  245  278  311  344<br/>[5,]  267  309  351  393  435<br/>[6,]  322  373  424  475  526<br/>[7,]  377  437  497  557  617<br/>[8,]  432  501  570  639  708</span></pre><p id="5873" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">10.1.3。第三步</p><p id="7e36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">激活功能是在运行该功能时定义的功能。该算法应用于初始矩阵的每个元素。在这种情况下，使用初始矩阵的简单乘法。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ol"><img src="../Images/47848b847c327a7585cda35940bff677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/0*Xu5mU5oV_xoVe0m0"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图7 </strong>:前进道具，第三步</p></figure><p id="0633" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码显示了这个过程是如何以编程方式实现的。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="6944" class="mu lt iq ms b gy nz oa l ob oc"># Apply Activation function<br/>matrix_output &lt;- matrix_biased * (0.01 * matrix_biased)</span></pre><p id="01c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="35de" class="mu lt iq ms b gy nz oa l ob oc">matrix_biased:<br/>     [,1] [,2] [,3] [,4] [,5]<br/>[1,]   47   53   59   65   71<br/>[2,]  102  117  132  147  162<br/>[3,]  157  181  205  229  253<br/>[4,]  212  245  278  311  344<br/>[5,]  267  309  351  393  435<br/>[6,]  322  373  424  475  526<br/>[7,]  377  437  497  557  617<br/>[8,]  432  501  570  639  708</span><span id="b26e" class="mu lt iq ms b gy od oa l ob oc">matrix_output:<br/>        [,1]    [,2]    [,3]    [,4]    [,5]<br/>[1,]   22.09   28.09   34.81   42.25   50.41<br/>[2,]  104.04  136.89  174.24  216.09  262.44<br/>[3,]  246.49  327.61  420.25  524.41  640.09<br/>[4,]  449.44  600.25  772.84  967.21 1183.36<br/>[5,]  712.89  954.81 1232.01 1544.49 1892.25<br/>[6,] 1036.84 1391.29 1797.76 2256.25 2766.76<br/>[7,] 1421.29 1909.69 2470.09 3102.49 3806.89<br/>[8,] 1866.24 2510.01 3249.00 4083.21 5012.64</span></pre><h2 id="b38f" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.2.线性分量</h2><p id="8bc7" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当组合在一起时，线性代数函数只用三行代码就实现了，如下面的<code class="fe mp mq mr ms b">set_LinearForward()</code>函数中的代码块所示。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="52ef" class="mu lt iq ms b gy nz oa l ob oc">set_LinearForward &lt;- function(inpt, wgts, bias) {<br/>    <br/>    # Perform matrix multiplication<br/>    linr &lt;- inpt %*% wgts<br/>    <br/>    # Add bias<br/>    linr &lt;- sweep(linr, 2, bias, "+")<br/>    <br/>    # Return<br/>    return(linr)<br/>    <br/>}</span></pre><h2 id="fb45" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.3.非线性分量</h2><p id="21f4" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">神经网络的真正力量来自于它们的激活函数。现在，网络能够捕捉非线性方面，这就是强调其预测能力的原因。</p><p id="14b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据网络的目的，激活功能可以是许多不同算法中的一种。激活的选择可以是在稍后阶段选择的超参数。<a class="ae lb" href="https://www.desmos.com/" rel="noopener ugc nofollow" target="_blank"> Desmos </a>网站提供了一种极好的交互方式来查看不同类型的激活:<a class="ae lb" href="https://www.desmos.com/calculator/rhx5tl8ygi" rel="noopener ugc nofollow" target="_blank">激活功能</a>。</p><p id="7ab1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个激活函数都是单独定义的，并且每个函数只接受一个参数，这是一个将被激活的矩阵。为了简洁起见，这里提供了四种比较流行的激活方式；但是还有很多很多其他的方法可以使用。还提供了如何计算这些函数的资源和等式:</p><p id="5fcd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mp mq mr ms b"><strong class="kh ir">sigmoid</strong></code> <strong class="kh ir"> : </strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7b4dfea0d8480df1645d4365840a87cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/0*L6sMEfWE0wQIoQp5"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">方程式3 </strong>:乙状结肠激活</p></figure><p id="0556" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来源:<br/> 1。<a class="ae lb" href="https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python" rel="noopener ugc nofollow" target="_blank">如何在Python中计算一个logistic sigmoid函数</a> <br/> 2。<a class="ae lb" href="https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/" rel="noopener ugc nofollow" target="_blank">使用Numpy实现sigmoid函数</a></p><p id="1cf7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mp mq mr ms b"><strong class="kh ir">relu</strong></code> <strong class="kh ir"> : </strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/b9ecd6e4476ed425e6cb53e90465cda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/0*goiwHAH3wqhSs3cS"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">方程式4 </strong> : Relu激活</p></figure><p id="3a3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来源:<br/> 1。<a class="ae lb" href="https://medium.com/ai%C2%B3-theory-practice-business/a-beginners-guide-to-numpy-with-sigmoid-relu-and-softmax-activation-functions-25b840a9a272" rel="noopener">带Sigmoid、ReLu和Softmax激活功能的NumPy初学者指南</a></p><p id="c01e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mp mq mr ms b"><strong class="kh ir">softmax</strong></code>T25:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/56acc52fcd42337b5712cf7487197b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/0*cqDZtY_OE51HkKCA"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">方程式5: </strong> Softmax激活</p></figure><p id="b19c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来源:<br/> 1。<a class="ae lb" rel="noopener" target="_blank" href="/softmax-activation-function-explained-a7e1bc3ad60"> Softmax激活功能说明</a></p><p id="9c4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe mp mq mr ms b"><strong class="kh ir">swish</strong></code> <strong class="kh ir"> : </strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4380e11098d1294b15d94e0aaabc8057.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/0*BjHw-42zm4G8klKm"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">方程式6 </strong>:嗖嗖激活</p></figure><p id="3e34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">来源:<br/> 1。<a class="ae lb" href="https://arxiv.org/pdf/1710.05941.pdf" rel="noopener ugc nofollow" target="_blank">搜索激活功能</a> <br/> 2。<a class="ae lb" href="https://www.bignerdranch.com/blog/implementing-swish-activation-function-in-keras/" rel="noopener ugc nofollow" target="_blank">在Keras中实现Swish激活功能</a></p><p id="495c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些激活函数以编程方式定义如下:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="613d" class="mu lt iq ms b gy nz oa l ob oc">let_ActivateSigmoid &lt;- function(linr) {<br/>    <br/>    # Do work<br/>    acti &lt;- 1/(1+exp(-linr))<br/>    <br/>    # Return<br/>    return(acti)<br/>    <br/>}</span><span id="59d2" class="mu lt iq ms b gy od oa l ob oc">let_ActivateRelu &lt;- function(linr) {<br/>    <br/>    # Do work<br/>    acti &lt;- sapply(linr, max, 0) %&gt;% <br/>        structure(dim=dim(linr))<br/>    <br/>    # Return<br/>    return(acti)<br/>    <br/>}</span><span id="81b6" class="mu lt iq ms b gy od oa l ob oc">let_ActivateSoftmax &lt;- function(linr) {<br/>    <br/>    # Do work<br/>    expo &lt;- exp(linr)<br/>    expo_sum &lt;- sum(exp(linr))<br/>    acti &lt;- expo/expo_sum<br/>    <br/>    # Return<br/>    return(acti)<br/>    <br/>}</span><span id="d646" class="mu lt iq ms b gy od oa l ob oc">let_ActivateSwish &lt;- function(linr, beta=0.1) {<br/>    <br/>    # Do work<br/>    acti &lt;- linr * (beta * linr)<br/>    <br/>    # Return<br/>    return(acti)<br/>    <br/>}</span></pre><h2 id="c36e" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.4.设置正向传播函数</h2><p id="1c28" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated"><code class="fe mp mq mr ms b">set_ForwardProp()</code>函数集合了上面提到的所有组件。它执行以下步骤:</p><ol class=""><li id="7255" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">循环通过<code class="fe mp mq mr ms b">network_model</code>的每一层。</li><li id="15a6" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">获取当前图层的图层名称。</li><li id="918d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">对<code class="fe mp mq mr ms b">input</code>层实施一个“通过”过程。</li><li id="7d80" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取相关信息，包括:<br/> —1。<em class="mt">上一层</em>的层名<br/> —2。<em class="mt">上一层</em>的激活矩阵<br/> —3。<em class="mt">当前</em>层<br/> — 4的权重矩阵。<em class="mt">电流</em>层的偏置矩阵</li><li id="44e5" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">应用线性代数组件。</li><li id="5832" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">应用非线性激活组件。</li><li id="e796" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">请注意隐藏层的激活和最终层的激活之间的差异。</li><li id="c16d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将相关信息应用回网络。</li><li id="adc1" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">返回<code class="fe mp mq mr ms b">network_model</code>对象。</li></ol><p id="b6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了实现这个过程，这个函数只有四个参数:</p><ol class=""><li id="6589" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated"><code class="fe mp mq mr ms b">network_model</code>:待更新的网络模型。</li><li id="9b87" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">data_in</code>:训练图像的4维数组，如上定义。</li><li id="f08d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">activation_hidden</code>:应用于隐藏层的激活功能。</li><li id="54a3" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated"><code class="fe mp mq mr ms b">activation_final</code>:应用于最终(<code class="fe mp mq mr ms b">output</code>)层的激活功能。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="16ac" class="mu lt iq ms b gy nz oa l ob oc">set_ForwardProp &lt;- function(<br/>    network_model, <br/>    data_in, <br/>    activation_hidden="relu", <br/>    activation_final="sigmoid"<br/>    ) {<br/>    <br/>    # Do work<br/>    for (index in network_model %&gt;% names() %&gt;% length() %&gt;% 1:.) {<br/>        <br/>        # Define layer name<br/>        layr &lt;- network_model %&gt;% <br/>            names() %&gt;% <br/>            extract(index)<br/>        <br/>        if (layr=="input") {<br/>            <br/>            # Pass-thru for 'input' layer<br/>            network_model[[layr]][["inpt"]] &lt;- data_in<br/>            network_model[[layr]][["acti"]] &lt;- data_in<br/>            <br/>        } else {<br/>            <br/>            # Extract data<br/>            prev &lt;- names(network_model)[index-1]<br/>            inpt &lt;- network_model %&gt;% <br/>                extract2(prev) %&gt;% <br/>                extract2("acti")<br/>            wgts &lt;- network_model %&gt;% <br/>                extract2(layr) %&gt;% <br/>                extract2("wgts")<br/>            bias &lt;- network_model %&gt;% <br/>                extract2(layr) %&gt;% <br/>                extract2("bias")<br/>            <br/>            # Calculate<br/>            linr &lt;- set_LinearForward(inpt, wgts, bias)<br/>            <br/>            # Activate<br/>            if (layr=="output") {<br/>                func &lt;- activation_final %&gt;% <br/>                    str_to_title() %&gt;% <br/>                    paste0("let_Activate", .) %&gt;% <br/>                    get()<br/>                acti &lt;- func(linr)<br/>                network_model[[layr]][["acti_func"]] &lt;- activation_final<br/>            } else {<br/>                func &lt;- activation_hidden %&gt;% <br/>                    str_to_title() %&gt;% <br/>                    paste0("let_Activate", .) %&gt;% <br/>                    get()<br/>                acti &lt;- func(linr)<br/>                network_model[[layr]][["acti_func"]] &lt;- activation_hidden<br/>            }<br/>            <br/>            # Apply back to our model<br/>            network_model[[layr]][["inpt"]] &lt;- inpt<br/>            network_model[[layr]][["linr"]] &lt;- linr<br/>            network_model[[layr]][["acti"]] &lt;- acti<br/>            <br/>        }<br/>        <br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><h2 id="e07b" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.5.向前传播</h2><p id="7695" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">正向传播过程的最后一步是实际运行函数。在下面的代码块中，实现了<code class="fe mp mq mr ms b">tic()</code>和<code class="fe mp mq mr ms b">toc()</code>函数来计算进程运行的时间。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="8df1" class="mu lt iq ms b gy nz oa l ob oc">tic()<br/>network_model %&lt;&gt;% set_ForwardProp(trn_img, "relu", "sigmoid")<br/>toc()</span></pre><p id="f32d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="95fb" class="mu lt iq ms b gy nz oa l ob oc">7.05 sec elapsed</span></pre><p id="dc9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上面<a class="ae lb" href="#b95f" rel="noopener ugc nofollow"> <strong class="kh ir">分割数据</strong> </a>部分所述，<code class="fe mp mq mr ms b">trn_img</code>对象超过<code class="fe mp mq mr ms b">82 Mb</code>大，模型超过<code class="fe mp mq mr ms b">320,000</code>参数。向前传播的整个端到端过程仅用了<code class="fe mp mq mr ms b">7 seconds</code>来运行；令人印象深刻，证明了数学的力量。</p><h2 id="de5b" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">10.6.检查模型形状</h2><p id="7efd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在可以使用自定义功能<code class="fe mp mq mr ms b">get_PrintNetwork()</code>打印网络。这个函数在一个单独的盒子中返回每一层，包含关键信息，如矩阵的相关形状和使用的激活函数。该功能的源代码可以在<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="c8b7" class="mu lt iq ms b gy nz oa l ob oc"># Print the Network<br/>network_model %&gt;% <br/>    get_PrintNetwork() %&gt;% <br/>    cat()</span></pre><p id="f605" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="dd1a" class="mu lt iq ms b gy nz oa l ob oc">+--------------------------------+<br/>|      Layer : input             |<br/>|      Nodes : 3,072             |<br/>| Inpt Shape : 7,000 x 3,072     |<br/>| Wgts Shape :     0 x 3,072     |<br/>| Outp Shape : 7,000 x 3,072     |<br/>| Activation :                   |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : 1                 |<br/>|      Nodes : 100               |<br/>| Inpt Shape : 7,000 x 3,072     |<br/>| Wgts Shape : 3,072 x   100     |<br/>| Outp Shape : 7,000 x   100     |<br/>| Activation : relu              |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : 2                 |<br/>|      Nodes : 75                |<br/>| Inpt Shape : 7,000 x   100     |<br/>| Wgts Shape : 100   x    75     |<br/>| Outp Shape : 7,000 x    75     |<br/>| Activation : relu              |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : 3                 |<br/>|      Nodes : 50                |<br/>| Inpt Shape : 7,000 x    75     |<br/>| Wgts Shape : 75    x    50     |<br/>| Outp Shape : 7,000 x    50     |<br/>| Activation : relu              |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : 4                 |<br/>|      Nodes : 30                |<br/>| Inpt Shape : 7,000 x    50     |<br/>| Wgts Shape : 50    x    30     |<br/>| Outp Shape : 7,000 x    30     |<br/>| Activation : relu              |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : 5                 |<br/>|      Nodes : 20                |<br/>| Inpt Shape : 7,000 x    30     |<br/>| Wgts Shape : 30    x    20     |<br/>| Outp Shape : 7,000 x    20     |<br/>| Activation : relu              |<br/>+--------------------------------+<br/>               |<br/>               V<br/>+--------------------------------+<br/>|      Layer : output            |<br/>|      Nodes : 1                 |<br/>| Inpt Shape : 7,000 x    20     |<br/>| Wgts Shape : 20    x     1     |<br/>| Outp Shape : 7,000 x     1     |<br/>| Activation : sigmoid           |<br/>+--------------------------------+</span></pre><h1 id="485a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">11.计算成本</h1><p id="2346" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">一旦前向传播部分完成，就有必要测量模型的错误程度。这将用于在反向传播步骤中更新模型参数。</p><h2 id="277f" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">11.1.设置成本函数</h2><p id="e266" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">第一步是编写用于获取模型成本的函数。最终，第一轮训练的结果有多差并不重要；记住，模型是用随机数初始化的。重要的是，成本函数应该确定网络成本的单个<em class="mt">值</em>，并且该单个函数将被用于反向传播步骤中使用的导数函数。</p><p id="a19f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，这里使用了一个非常小的ε值(<code class="fe mp mq mr ms b">epsi</code>)，它有效地调整了模型做出的完美预测。这样做的原因不是我们不希望模型预测一个完美值，而是我们希望模型预测一个完美值的概率。此外，不可能取一个<code class="fe mp mq mr ms b">0</code>的对数值，因此有必要将它调整为<em class="mt">稍微偏离零点一点。</em></p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="ddd7" class="mu lt iq ms b gy nz oa l ob oc">get_ComputeCost &lt;- function(pred, true, epsi=1e-10) {<br/>    <br/>    # Get number of samples<br/>    samp &lt;- length(true)<br/>    <br/>    # Instantiate totals<br/>    total_cost &lt;- 0<br/>    <br/>    # Loop for each prediction<br/>    for (i in 1:samp) {<br/>        <br/>        # Adjust for perfect predictions<br/>        if (pred[i]==1) {pred[i] %&lt;&gt;% subtract(epsi)}<br/>        if (pred[i]==0) {pred[i] %&lt;&gt;% add(epsi)}<br/>        <br/>        # Calculate totals<br/>        total_cost &lt;- total_cost - <br/>            (<br/>                true[i] * log(pred[i]) <br/>                + <br/>                (1-true[i]) * log(1-pred[i])<br/>            )<br/>        <br/>    }<br/>    <br/>    # Take an average<br/>    cost &lt;- (1/samp) * total_cost<br/>    <br/>    # Return<br/>    return(cost)<br/>    <br/>}</span></pre><p id="e3fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，必须将成本应用回网络。为此，将完全相同的值应用于网络的每一层。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="dc18" class="mu lt iq ms b gy nz oa l ob oc">set_ApplyCost &lt;- function(network_model, cost) {<br/>    <br/>    # Apply back to the model<br/>    for (layer in network_model %&gt;% names) {<br/>        network_model[[layer]][["cost"]] &lt;- cost<br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>}</span></pre><h2 id="5539" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">11.2.运行成本函数</h2><p id="4e62" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下面的代码块运行成本函数，利用上面定义的函数。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="1adb" class="mu lt iq ms b gy nz oa l ob oc">network_model %&lt;&gt;% set_ApplyCost(<br/>    get_ComputeCost(network_model[["output"]][["acti"]], trn_cls)<br/>)</span></pre><h1 id="1c42" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">12.反向传播</h1><p id="a547" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">反向传播函数旨在获取模型的成本，然后区分网络中的每个权重和偏差矩阵，以确定网络中的每个参数对最终成本值的贡献程度。要做到这一点，流程从结束到开始反向工作，遵循以下步骤:</p><ol class=""><li id="c987" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">区分最终成本值</li><li id="9419" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">区分激活矩阵</li><li id="8928" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">线性代数矩阵的微分</li><li id="aa3e" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">继续上一层。</li></ol><p id="abca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">反向传播过程中的每一步都需要微积分来导出值，最终的函数在这里实现。由于本文并不打算展示如何推导方程，而是更多地展示如何运行函数，因此这里不包括必要的代数步骤。</p><h2 id="9c5d" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">12.1.区分成本</h2><p id="93ec" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">与用于获取成本的函数类似，首先计算成本差值，然后将其应用于网络的每一层。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="46ee" class="mu lt iq ms b gy nz oa l ob oc">get_DifferentiateCost &lt;- function(pred, true) {<br/>    <br/>    # Do work<br/>    diff_cost &lt;- -(<br/>        divide_by(true, pred) - divide_by(1-true, 1-pred)<br/>    )<br/>    <br/>    # Return<br/>    return(diff_cost)<br/>    <br/>}</span><span id="ac15" class="mu lt iq ms b gy od oa l ob oc">set_ApplyDifferentiateCost &lt;- function(<br/>    network_model, <br/>    cost_differential<br/>    ) {<br/>    <br/>    # Do work<br/>    for (layer in names(network_model)) {<br/>        network_model[[layer]][["back_cost"]] &lt;- cost_differential<br/>        if (layer=="output") {<br/>            network_model[[layer]][["back_acti"]] &lt;- network_model %&gt;% <br/>                extract2(layer) %&gt;% <br/>                extract2("back_cost") %&gt;% <br/>                t()<br/>        }<br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><h2 id="aa32" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">12.2.差异化激活</h2><p id="a24f" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">因为每个激活都有自己的函数，所以也有一个可以用微积分计算的函数的导数。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="6a64" class="mu lt iq ms b gy nz oa l ob oc">let_BackwardActivateRelu &lt;- function(diff_acti_curr, linr_curr) {<br/>    <br/>    # Do work<br/>    diff_linr_curr &lt;- diff_acti_curr<br/>    diff_linr_curr[linr_curr&lt;=0] &lt;- 0<br/>    <br/>    # Return<br/>    return(diff_linr_curr)<br/>    <br/>}</span><span id="95a7" class="mu lt iq ms b gy od oa l ob oc">let_BackwardActivateSigmoid &lt;- function(diff_acti_curr, linr_curr) {<br/>    <br/>    # Do work<br/>    temp &lt;- 1/(1+exp(-linr_curr))<br/>    diff_linr_curr &lt;- t(diff_acti_curr) * temp * (1-temp)<br/>    <br/>    # Return<br/>    return(t(diff_linr_curr))<br/>    <br/>}</span></pre><h2 id="7b01" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">12.3.微分线性</h2><p id="6aa0" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">到目前为止，已经定义了所有这些，下一步是组合成一个单一的函数，它可以在每层使用一次，运行必要的反向传播微分函数。</p><p id="66ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意这里的输出实际上是三个元素的列表。这是<code class="fe mp mq mr ms b">R</code>和<code class="fe mp mq mr ms b">python</code>的关键区别。在<code class="fe mp mq mr ms b">R</code>中，函数只能输出单个元素；相比之下<code class="fe mp mq mr ms b">python</code>能够从每个函数返回多个元素。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="778e" class="mu lt iq ms b gy nz oa l ob oc">get_DifferentiateLinear &lt;- function(<br/>    back_linr_curr,<br/>    acti_prev,<br/>    wgts,<br/>    bias<br/>    ) {<br/>    <br/>    # get number of samples<br/>    samp &lt;- acti_prev %&gt;% dim %&gt;% extract(2)<br/>    <br/>    # Differentiate weights<br/>    diff_wgts &lt;- 1/samp * (back_linr_curr %*% acti_prev)<br/>    <br/>    # Differentiate bias<br/>    diff_bias &lt;- 1/samp * rowSums(back_linr_curr, dims=1)<br/>    <br/>    # Differentiate activation<br/>    diff_acti_prev &lt;- wgts %*% back_linr_curr<br/>    <br/>    # Consolidate in to one list<br/>    list_linr &lt;- list(<br/>        diff_acti_prev, <br/>        diff_wgts, <br/>        diff_bias<br/>    )<br/>    <br/>    # Return<br/>    return(list_linr)<br/>    <br/>}</span></pre><h2 id="e212" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">12.4.反向传播</h2><p id="e9f9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在定义了微分函数之后，接下来需要将这些单独的函数组合成一个单一的组合函数，该函数可以每层运行一次。</p><p id="2fb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将设置函数，其次我们将运行它。</p><p id="e47f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 12.4.1。设置反向传播功能</strong></p><p id="1c5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">待定义的反向传播函数(<code class="fe mp mq mr ms b">set_BackwardProp()</code>)必须设计为通过以下步骤运行:</p><ol class=""><li id="05e3" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">沿<em class="mt">反</em>方向穿过每一层。这是必要的，因为从逻辑上讲，反向传播函数需要反向运行。</li><li id="87ac" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取图层名称。</li><li id="6bcc" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">跳过<code class="fe mp mq mr ms b">input</code>层。同样，这也很符合逻辑，因为这一层位于网络的起点，不需要反向传播。</li><li id="f0cf" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取上一层的名称。</li><li id="2cdb" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取相关矩阵用于后续计算。</li><li id="2664" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取该特定层的相关激活函数。</li><li id="b8f0" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">设置一些空矩阵，这些空矩阵将存放相关的微分矩阵。</li><li id="eb3a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">求<em class="mt">当前</em>层的<code class="fe mp mq mr ms b">activation</code>矩阵的微分</li><li id="1799" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">区分<code class="fe mp mq mr ms b">linear</code>矩阵，包括:当前层的<br/> — <code class="fe mp mq mr ms b">weight</code>矩阵。<br/> — <code class="fe mp mq mr ms b">bias</code>当前层的矩阵。<br/> — <code class="fe mp mq mr ms b">activation</code>前一层的矩阵。</li><li id="9009" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将信息应用回<code class="fe mp mq mr ms b">network_model</code>中的相关位置。</li><li id="8556" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">返回更新后的<code class="fe mp mq mr ms b">network_model</code>。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="32e5" class="mu lt iq ms b gy nz oa l ob oc">set_BackwardProp &lt;- function(network_model) {<br/>    <br/>    # Loop through each layer in reverse order<br/>    for (layr_indx in network_model %&gt;% names() %&gt;% length() %&gt;% 1:. %&gt;% rev) {<br/>        <br/>        # Get the layer name<br/>        layr_curr &lt;- network_model %&gt;% <br/>            names() %&gt;% <br/>            extract(layr_indx)<br/>        <br/>        # Skip the 'input' layer<br/>        if (layr_curr == "input") next<br/>        <br/>        # Get the previous layer name<br/>        layr_prev &lt;- network_model %&gt;% <br/>            names %&gt;% <br/>            extract(layr_indx-1)<br/>        <br/>        # Set up the existing matrices<br/>        linr_curr &lt;- network_model %&gt;% <br/>            extract2(layr_curr) %&gt;% <br/>            extract2("linr")<br/>        wgts_curr &lt;- network_model %&gt;% <br/>            extract2(layr_curr) %&gt;% <br/>            extract2("wgts")<br/>        bias_curr &lt;- network_model %&gt;% <br/>            extract2(layr_curr) %&gt;% <br/>            extract2("bias")<br/>        acti_prev &lt;- network_model %&gt;% <br/>            extract2(layr_prev) %&gt;% <br/>            extract2("acti")<br/>        diff_acti_curr &lt;- network_model %&gt;% <br/>            extract2(layr_curr) %&gt;% <br/>            extract2("back_acti")<br/>        <br/>        # Get the activation function<br/>        acti_func_back &lt;- network_model %&gt;% <br/>            extract2(layr_curr) %&gt;% <br/>            extract2("acti_func") %&gt;%<br/>            str_to_title %&gt;% <br/>            paste0("let_BackwardActivate", .)<br/>        <br/>        # Set up the empty matrices<br/>        diff_linr_curr &lt;- matrix()<br/>        diff_acti_prev &lt;- matrix()<br/>        diff_wgts_curr &lt;- matrix()<br/>        diff_bias_curr &lt;- matrix()<br/>        <br/>        # Differentiate activation<br/>        diff_linr_curr &lt;- get(acti_func_back)(<br/>            diff_acti_curr,<br/>            linr_curr<br/>        )<br/>        <br/>        # Differentiate linear<br/>        list_linr &lt;- get_DifferentiateLinear(<br/>            back_linr_curr=diff_linr_curr,<br/>            acti_prev=acti_prev,<br/>            wgts=wgts_curr,<br/>            bias=bias_curr<br/>        )<br/>        diff_acti_prev &lt;- list_linr %&gt;% extract2(1)<br/>        diff_wgts_curr &lt;- list_linr %&gt;% extract2(2)<br/>        diff_bias_curr &lt;- list_linr %&gt;% extract2(3)<br/>        <br/>        # Apply back to model<br/>        network_model[[layr_prev]][["back_acti"]] &lt;- diff_acti_prev<br/>        network_model[[layr_curr]][["back_linr"]] &lt;- diff_linr_curr<br/>        network_model[[layr_curr]][["back_wgts"]] &lt;- diff_wgts_curr<br/>        network_model[[layr_curr]][["back_bias"]] &lt;- diff_bias_curr<br/>        <br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><p id="2f10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">12.4.2。运行反向传播功能</p><p id="2901" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">定义了这个函数之后，下一步是运行这个函数。下面的代码块用<code class="fe mp mq mr ms b">tic()</code>和<code class="fe mp mq mr ms b">toc()</code>函数包装，以确定函数运行需要多少时间。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="aecd" class="mu lt iq ms b gy nz oa l ob oc">tic()<br/>network_model %&lt;&gt;% set_BackwardProp()<br/>toc()</span></pre><p id="a906" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="9142" class="mu lt iq ms b gy nz oa l ob oc">8.84 sec elapsed</span></pre><p id="119a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图所示，运行这个函数大约需要9秒钟。考虑到有超过<code class="fe mp mq mr ms b">320,000</code>个参数需要更新(参见<a class="ae lb" href="#0f41" rel="noopener ugc nofollow"> <strong class="kh ir">检查模型参数</strong> </a>一节)，这是相当可观的。</p><h1 id="5208" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">13.更新模型参数</h1><h2 id="eca2" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">13.1.语境</h2><p id="05a1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在模型参数被差分后，在通过重新运行前向传播函数再次重新训练网络之前，需要更新网络的相关参数(权重和偏差)。</p><p id="31ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于更新这些参数的方法被称为<a class="ae lb" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>。更多信息，参见<a class="ae lb" href="https://leon.bottou.org/publications/pdf/nimes-1991.pdf" rel="noopener ugc nofollow" target="_blank">神经网络中的随机梯度学习</a>或<a class="ae lb" href="https://www.sciencedirect.com/science/article/abs/pii/092523129390006O" rel="noopener ugc nofollow" target="_blank">反向传播和随机梯度下降法</a>。</p><p id="7ddb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，还有其他实现神经网络优化的方法。文献在这方面已经花费了大量的精力。存在诸如<a class="ae lb" rel="noopener" target="_blank" href="/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a"> RMSProp </a>和<a class="ae lb" rel="noopener" target="_blank" href="/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"> Adam </a>的算法，这些算法实现了智能方法以实现更快的收敛和更精确的最终结果。这方面的一些好资料来源包括<a class="ae lb" href="https://heartbeat.fritz.ai/an-empirical-comparison-of-optimizers-for-machine-learning-models-b86f29957050" rel="noopener ugc nofollow" target="_blank">机器学习模型优化器的经验比较</a>和<a class="ae lb" href="https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3" rel="noopener">神经网络不同优化器的概述</a>。为了进一步增强和优化，探索这些优化选项非常重要。</p><h2 id="1496" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">13.2.过程</h2><p id="e638" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">尽管如此，实现随机梯度下降的过程实际上非常简单:</p><ol class=""><li id="87b6" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">在<code class="fe mp mq mr ms b">0</code>和<code class="fe mp mq mr ms b">1</code>之间指定一个给定的<em class="mt">学习率</em>(通常是很小的数字，例如<code class="fe mp mq mr ms b">0.001</code>)。</li><li id="3eb3" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">取差分后的<em class="mt">权重</em>和<em class="mt">偏差</em>矩阵，在<strong class="kh ir">负</strong>方向上乘以<em class="mt">学习率</em>。</li><li id="b629" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将更新的差分<em class="mt">权重</em>和<em class="mt">偏差</em>矩阵相加，并添加到原始的<em class="mt">权重</em>和<em class="mt">偏差</em>矩阵。</li><li id="d4d7" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">逐层重复这个过程。</li></ol><h2 id="6a84" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">13.3.设置更新模型功能</h2><p id="b2f0" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了设置用于更新模型参数的功能，使用以下步骤:</p><ol class=""><li id="54cf" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">指定<code class="fe mp mq mr ms b">learning_rate</code>作为函数的参数。</li><li id="fe8f" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">正向循环通过网络中的每一层。</li><li id="598a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">提取图层名称。</li><li id="b8f4" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">跳过<code class="fe mp mq mr ms b">input</code>层(因为它不需要更新)。</li><li id="6b18" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">定义<code class="fe mp mq mr ms b">back_wgts</code>和<code class="fe mp mq mr ms b">back_bias</code>矩阵的梯度步骤。</li><li id="1dba" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将梯度步骤应用于原始的<code class="fe mp mq mr ms b">wgts</code>和<code class="fe mp mq mr ms b">bias</code>矩阵。</li><li id="432c" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">返回更新的模式。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="6715" class="mu lt iq ms b gy nz oa l ob oc">set_UpdateModel &lt;- function(network_model, learning_rate=0.001) {<br/>    <br/>    # Do work<br/>    for (index in network_model %&gt;% names() %&gt;% length() %&gt;% 1:.) {<br/>        <br/>        # Get layer name<br/>        layr &lt;- network_model %&gt;% <br/>            names() %&gt;% <br/>            extract(index)<br/>        <br/>        # Skip 'input' layer<br/>        if (layr=="input") next<br/>        <br/>        # Define gradient steps for the weight<br/>        grad_step_wgts &lt;- -1 * <br/>            (<br/>                learning_rate * network_model[[layr]][["back_wgts"]]<br/>            )</span><span id="b398" class="mu lt iq ms b gy od oa l ob oc">        # Define gradient steps for the bias<br/>        grad_step_bias &lt;- -1 * <br/>            (<br/>                learning_rate * network_model[[layr]][["back_bias"]]<br/>            )<br/>        <br/>        # Take steps<br/>        network_model[[layr]][["wgts"]] &lt;- network_model %&gt;% <br/>            extract2(layr) %&gt;% <br/>            extract2("wgts") %&gt;% <br/>            add(t(grad_step_wgts))<br/>        network_model[[layr]][["bias"]] &lt;- network_model %&gt;% <br/>            extract2(layr) %&gt;% <br/>            extract2("bias") %&gt;% <br/>            add(grad_step_bias)<br/>        <br/>    }<br/>    <br/>    # Return<br/>    return(network_model)<br/>    <br/>}</span></pre><h2 id="43aa" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">13.4.运行更新模型功能</h2><p id="f1cd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">定义了函数之后，我们运行网络模型。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="bee2" class="mu lt iq ms b gy nz oa l ob oc">network_model %&lt;&gt;% set_UpdateModel(0.01)</span></pre><h1 id="34e4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">14.端到端运行模型</h1><p id="22d3" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在，是时候把这一切结合起来了。端到端运行模型实质上意味着:</p><ol class=""><li id="9d13" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">应用正向传播。</li><li id="c447" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">计算成本。</li><li id="75ab" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">运行反向传播。</li><li id="5bb9" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">更新模型参数。</li><li id="6c9b" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">重复…</li></ol><p id="abc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种重复的每一次被称为一个<em class="mt">时期</em>。对于网络来说，很典型的是经过许多时代才被更新。有时数百个，有时数千个纪元；运行的确切历元数由数据科学家根据众多变量自由决定。</p><p id="17e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里需要添加一个额外的步骤，那就是<em class="mt">批处理</em>数据。为此，我们可以考虑在每个历元内，将数据分批到可等分的组中，并用于后续处理。在这种情况下，模型参数将在每批之后更新。当全部数据都完整地通过模型时，这就被认为是一个时期。</p><h1 id="7ce2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">14.1.设置列车模型功能</h1><p id="0aac" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了以编程方式说明这一点，编写了以下函数。</p><p id="0021" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这里包含了许多自定义函数(每个函数的源代码可以在<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR/blob/master/Functions/functions.R" rel="noopener ugc nofollow" target="_blank">这里</a>找到)。这些功能包括:</p><ul class=""><li id="37c9" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nu nm nn no bi translated"><code class="fe mp mq mr ms b">get_Modulus()</code></li><li id="5e52" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">get_BatchIndexes()</code></li><li id="b773" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">get_VerbosityValues()</code></li><li id="62c6" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">get_TimeDifference()</code></li><li id="c750" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nu nm nn no bi translated"><code class="fe mp mq mr ms b">plt_PlotLearningCurve()</code></li></ul><p id="afdc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该功能的步骤包括:</p><ol class=""><li id="d914" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">开始计时。</li><li id="207a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">声明将返回哪些信息。</li><li id="a1eb" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">实例化网络。</li><li id="40de" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">初始化网络。</li><li id="8973" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">循环通过每个时期。</li><li id="a0eb" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">循环每个批次。</li><li id="eb7d" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">对该特定批次的数据进行子集划分。</li><li id="b819" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">向前传播。</li><li id="eabc" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">计算成本。</li><li id="4b35" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">将成本应用于网络。</li><li id="61f5" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">区分成本。</li><li id="ef83" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">运行反向传播。</li><li id="ea21" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">更新模型参数。</li><li id="9c53" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">运行下一批。</li><li id="e185" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">在纪元结束时节省总成本。</li><li id="f7c8" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">打印相关纪元编号处的更新(来自<code class="fe mp mq mr ms b">verbosity</code>参数)</li><li id="d3a7" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">贯穿下一个纪元。</li><li id="0098" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">保存更新的网络。</li><li id="6f49" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">打印学习曲线。</li><li id="9cd0" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">返回输出。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="51af" class="mu lt iq ms b gy nz oa l ob oc">let_TrainModel &lt;- function(<br/>    x_train,<strong class="ms ir"> <br/>    </strong>y_train,<br/>    input_nodes=dim(x_train)[2], <br/>    hidden_nodes=c(100, 50, 10), <br/>    output_nodes=1,<br/>    initialisation_algorithm="xavier",<br/>    initialisation_order="layers",<br/>    activation_hidden="relu", <br/>    activation_final="sigmoid",<br/>    batches=get_Modulus(dim(x_train)[1])[4], <br/>    epochs=500, <br/>    learning_rate=0.001,<br/>    verbosity=NA,<br/>    print_learning_curve=TRUE<br/>    ) {<br/>    <br/>    # Begin the timer<br/>    time_begin &lt;- Sys.time()<br/>    <br/>    # Set return values<br/>    output &lt;- list(<br/>        network_model=network_model,<br/>        results=list(<br/>            cost=vector()<br/>            # Can add more, such as accuracy or specificity.<br/>        )<br/>    )<br/>    <br/>    # Instantiate<br/>    network_model &lt;- set_InstantiateNetwork(<br/>        input=input_nodes,<br/>        hidden=hidden_nodes, <br/>        output=output_nodes<br/>    )<br/>    <br/>    # Initialise<br/>    network_model &lt;- set_InitialiseModel(<br/>        network_model=network_model, <br/>        initialisation_algorithm=initialisation_algorithm, <br/>        initialisation_order=initialisation_order<br/>    )<br/>    <br/>    # Loop each epoch<br/>    for (epoch in 1:epochs) {<br/>        <br/>        # Loop each batch<br/>        for (batch in 1:batches) {<br/>            <br/>            # Set indices<br/>            batch_indexes &lt;- get_BatchIndexes(<br/>                vector=1:dim(x_train)[1], <br/>                batches=batches, <br/>                batch=batch,<br/>                seed=1234<br/>            )<br/>            <br/>            # Set data<br/>            x_train_batch &lt;- x_train[batch_indexes,]<br/>            y_train_batch &lt;- y_train[batch_indexes]<br/>            <br/>            # Forward Prop<br/>            network_model &lt;- set_ForwardProp(<br/>                network_model = network_model, <br/>                data_in = x_train_batch, <br/>                activation_hidden = activation_hidden, <br/>                activation_final = activation_final<br/>            )<br/>            <br/>            # Get cost<br/>            cost &lt;- get_ComputeCost(<br/>                pred = network_model[["output"]][["acti"]], <br/>                true = y_train_batch, <br/>                epsi = 1e-10<br/>            )<br/>            <br/>            # Apply cost<br/>            network_model &lt;- set_ApplyCost(<br/>                network_model = network_model, <br/>                cost = cost<br/>            )<br/>            <br/>            # Differentiate cost<br/>            network_model &lt;- set_ApplyDifferentiateCost(<br/>                network_model = network_model, <br/>                cost_differential = get_DifferentiateCost(network_model[["output"]][["acti"]], y_train_batch)<br/>            )<br/>            <br/>            # Backprop<br/>            network_model &lt;- set_BackwardProp(network_model)<br/>            <br/>            # Update parameters<br/>            network_model &lt;- set_UpdateModel(<br/>                network_model = network_model, <br/>                learning_rate = learning_rate<br/>            )<br/>            <br/>        }<br/>            <br/>        # Save cost<br/>        output[["results"]][["cost"]] %&lt;&gt;% c(cost)<br/>        <br/>        # Print update<br/>        if (!is.na(verbosity)) {<br/>            if (epoch %in% get_VerbosityValues(epochs, verbosity)){<br/>                if (epoch == verbosity) {<br/>                    "Learning rate: {}\n" %&gt;% <br/>                        str_Format(learning_rate) %&gt;% <br/>                        cat()<br/>                }<br/>                "Epoch {}, Cost: {}, Time: {}\n" %&gt;% <br/>                    str_Format(<br/>                        epoch, <br/>                        round(cost, 5), <br/>                        get_TimeDifference(time_begin)<br/>                    ) %&gt;% <br/>                    cat()<br/>            }<br/>        }<br/>        <br/>    }<br/>    <br/>    # Re-apply back to the output list<br/>    output[["network_model"]] &lt;- network_model<br/>    <br/>    # Print the results<br/>    if (print_learning_curve == TRUE) {<br/>        <br/>        tryCatch(<br/>            expr={<br/>                output %&gt;% <br/>                    extract2("results") %&gt;% <br/>                    extract2("cost") %&gt;% <br/>                    plt_PlotLearningCurve(<br/>                        input_nodes=input_nodes, <br/>                        hidden_nodes=hidden_nodes, <br/>                        output_nodes=output_nodes,<br/>                        initialisation_algorithm=<br/>                            initialisation_algorithm, <br/>                        initialisation_order=initialisation_order,<br/>                        activation_hidden=activation_hidden, <br/>                        activation_final=activation_final,<br/>                        epochs=epochs, <br/>                        learning_rate=learning_rate, <br/>                        verbosity=verbosity,<br/>                        run_time=get_TimeDifference(time_begin)<br/>                    ) %&gt;% <br/>                    print()<br/>            },<br/>            warning=function(message){<br/>                writeLines("A Warning occurred:")<br/>                writeLines(message)<br/>                return(invisible(NA))<br/>            },<br/>            error=function(message){<br/>                writeLines("An Error occurred:")<br/>                writeLines(message)<br/>                return(invisible(NA))<br/>            },<br/>            finally={<br/>                #Do nothing...<br/>            }<br/>        )<br/>        <br/>    }<br/>    <br/>    # Return<br/>    return(output)<br/>    <br/>}</span></pre><h2 id="472c" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">14.2.运行列车模型功能</h2><p id="a586" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">已经设置了培训功能，现在让我们运行它。</p><p id="e06c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意:</p><ol class=""><li id="00d6" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">仅使用训练数据(参见<a class="ae lb" href="#b95f" rel="noopener ugc nofollow"> <strong class="kh ir">分割数据</strong> </a>部分)。</li><li id="d37b" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">使用he初始化算法，阶数为2。</li><li id="152a" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">relu激活算法用于隐藏层，sigmoid激活算法用于输出层。</li><li id="e1a1" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">有56个批次，50个纪元。</li><li id="35ff" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">每10个时期打印一次模型结果。</li><li id="a914" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">学习曲线打印在最后。</li></ol><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="98fb" class="mu lt iq ms b gy nz oa l ob oc">training_output &lt;- let_TrainModel(<br/>    x_train=trn_img, <br/>    y_train=trn_cls,<br/>    input_nodes=dim(trn_img)[2], <br/>    hidden_nodes=c(100,75,50,30,20), <br/>    output_nodes=1,<br/>    initialisation_algorithm="he", <br/>    initialisation_order=2,<br/>    activation_hidden="relu", <br/>    activation_final="sigmoid",<br/>    batches=56, epochs=50, <br/>    learning_rate=0.01,<br/>    verbosity=10, <br/>    print_learning_curve=TRUE<br/>)</span></pre><p id="e6da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="0ceb" class="mu lt iq ms b gy nz oa l ob oc">Learning rate: 0.01<br/>Epoch 10, Cost: 0.32237, Time: 4.86 mins<br/>Epoch 20, Cost: 0.30458, Time: 9.58 mins<br/>Epoch 30, Cost: 0.28903, Time: 12.23 mins<br/>Epoch 40, Cost: 0.29525, Time: 14.87 mins<br/>Epoch 50, Cost: 0.29884, Time: 17.43 mins</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/4c9e59b561c90b18ac64a9c5238e4e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2gp8hknOWkZh3nbttuL6Q.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图8 </strong>:神经网络学习曲线</p></figure><p id="7793" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有用！这个情节证明了模型是在训练的，随着时间的推移，它在继续学习，提高性能。</p><p id="cf38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，成本线大约从<code class="fe mp mq mr ms b">0.4</code>开始，并在20个时期后迅速下降到<code class="fe mp mq mr ms b">0.3</code>。为了完成全部50个纪元，这大约需要<code class="fe mp mq mr ms b">12 minutes</code>。这可以看作是一种成功。</p><h2 id="4af2" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">14.3.进一步的实验</h2><p id="1fb0" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">由于这些功能已经被设置的性质，进一步的实验和优化是非常容易的。</p><p id="17e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尝试以下方法可能是合理的:</p><ol class=""><li id="75cb" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">不同的隐藏层数或每层的节点数(改变<code class="fe mp mq mr ms b">hidden_nodes</code>参数)，</li><li id="5a53" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">不同的初始化算法或初始化顺序(改变<code class="fe mp mq mr ms b">initialisation_alghorithm</code>或<code class="fe mp mq mr ms b">initialisation_order</code>参数)，</li><li id="ade7" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">隐藏层上的不同激活功能(改变<code class="fe mp mq mr ms b">activation_hidden</code>参数)，</li><li id="9097" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">每个时期不同的批次数量(更改<code class="fe mp mq mr ms b">batches</code>参数)，</li><li id="81b0" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">不同数量的时期(改变<code class="fe mp mq mr ms b">epochs</code>参数)，</li><li id="2b91" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">不同的学习率(改变<code class="fe mp mq mr ms b">learning_rate</code>参数)，</li><li id="9eed" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">获取更多数据(回想在<a class="ae lb" href="#64db" rel="noopener ugc nofollow"> <strong class="kh ir">下载数据</strong> </a>部分，仅<code class="fe mp mq mr ms b">10,000</code>图像被下载；但是还有另一个<code class="fe mp mq mr ms b">50,000</code>图像可用于进一步训练)。</li></ol><p id="3bd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个尝试不同参数的实验。注意结果是如何变化的。这下一轮训练花了<code class="fe mp mq mr ms b">34 minutes</code>跑。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="27fe" class="mu lt iq ms b gy nz oa l ob oc">training_output &lt;- let_TrainModel(<br/>    x_train=trn_img, <br/>    y_train=trn_cls,<br/>    input_nodes=dim(trn_img)[2],<br/>    hidden_nodes=c(100,75,50,30,20), <br/>    output_nodes=1,<br/>    initialisation_algorithm="xavier", <br/>    initialisation_order="layers",<br/>    activation_hidden="relu", <br/>    activation_final="sigmoid",<br/>    batches=56, epochs=100, <br/>    learning_rate=0.001,<br/>    verbosity=20, <br/>    print_learning_curve=TRUE<br/>)</span></pre><p id="5eb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="2630" class="mu lt iq ms b gy nz oa l ob oc">Learning rate: 0.001<br/>Epoch 20, Cost: 0.33354, Time: 5.4 mins<br/>Epoch 40, Cost: 0.29891, Time: 10.26 mins<br/>Epoch 60, Cost: 0.30255, Time: 15.2 mins<br/>Epoch 80, Cost: 0.29968, Time: 20.84 mins<br/>Epoch 100, Cost: 0.29655, Time: 26.09 mins</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/63ad4ae59d4012c205610654b12554cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkfdY_f82ZdXZMHmhsDpTA.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图9 </strong>:神经网络的第二条学习曲线</p></figure><p id="2d88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，数据科学家的工作是确定这些参数的最佳配置，以实现更好的性能比率。参见<a class="ae lb" href="https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models" rel="noopener ugc nofollow" target="_blank">机器学习模型中的超参数优化</a>了解更多详情。</p><p id="19be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，由于这篇文章不是为了让<em class="mt">达到</em>最佳结果，而是为了让<em class="mt">展示</em>如何达到所述结果，以及所涉及的过程/方法。因此，本文不再做进一步的实验。</p><h1 id="4ed9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">15.创建预测</h1><p id="3422" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">训练完网络后，下一步是将它应用于<code class="fe mp mq mr ms b">test</code>数据，看看它能多准确地预测未知数据。</p><h2 id="636a" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">15.1.设置预测</h2><p id="8422" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">第一步是编写两个自定义函数。第一个将接受测试数据和训练模型，并将返回包含预测值和真实值的数据框。然后以一种<code class="fe mp mq mr ms b">ggplot</code>风格的输出漂亮地打印出混淆矩阵。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="a1ff" class="mu lt iq ms b gy nz oa l ob oc">get_Prediction &lt;- function(<br/>    x_test, <br/>    y_test, <br/>    network_model, <br/>    threshold=0.5<br/>    ) {<br/>    <br/>    # Create prediction<br/>    predic &lt;- set_ForwardProp(<br/>        network_model=network_model, <br/>        data_in=x_test, <br/>        activation_hidden="relu", <br/>        activation_final="sigmoid"<br/>    )<br/>    <br/>    # Extract probabilities<br/>    probas &lt;- predic %&gt;% <br/>        extract2("output") %&gt;% <br/>        extract2("acti")<br/>    <br/>    # Define results<br/>    result &lt;- data.frame(<br/>        probs=probas,<br/>        truth=y_test<br/>    )<br/>    <br/>    # Add prdic<br/>    result %&lt;&gt;% <br/>        mutate(prdic=ifelse(probas&gt;threshold, 1, 0))<br/>    <br/>    # Return<br/>    return(result)<br/>    <br/>}</span><span id="6c89" class="mu lt iq ms b gy od oa l ob oc">plt_ConfusionMatrix &lt;- function(confusion_matrix) {</span><span id="973a" class="mu lt iq ms b gy od oa l ob oc">    # Do work<br/>    plot &lt;- confusion_matrix %&gt;% <br/>        extract("table") %&gt;% <br/>        as.data.frame() %&gt;% <br/>        rename_all(str_remove_all, "table.") %&gt;% <br/>        rename("Prediction"=1, "Reference"=2) %&gt;% <br/>        mutate(<br/>            goodbad = ifelse(<br/>                Prediction == Reference, <br/>                "good", <br/>                "bad"<br/>            )<br/>        ) %&gt;%<br/>        group_by(Reference) %&gt;% <br/>        mutate(prop = Freq/sum(Freq)) %&gt;% <br/>        ungroup() %&gt;% <br/>        {<br/>            ggplot(., <br/>                aes(<br/>                    x = Reference, <br/>                    y = Prediction, <br/>                    fill = goodbad, <br/>                    alpha = prop<br/>                )) +<br/>                geom_tile() +<br/>                geom_text(<br/>                    aes(label = Freq), <br/>                    vjust = .5, <br/>                    fontface  = "bold", <br/>                    alpha = 1<br/>                ) +<br/>                scale_fill_manual(<br/>                    values=c(good="green", bad="red")<br/>                ) +<br/>                scale_x_discrete(<br/>                    limits=levels(.$Reference), <br/>                    position="top"<br/>                ) +<br/>                scale_y_discrete(<br/>                    limits=rev(levels(.$Prediction))<br/>                ) +<br/>                labs(<br/>                    title="Confusion Matrix",<br/>                    subtitle=paste0(<br/>                        "For: '", <br/>                        .$Prediction[1], <br/>                        "' vs '", <br/>                        .$Prediction[2], <br/>                        "'"<br/>                    )<br/>                )<br/>        }<br/>    <br/>    # Return<br/>    return(plot)<br/>    <br/>}</span></pre><h2 id="d2f7" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">15.2.运行预测</h2><p id="02b6" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下一步是实际运行预测。这一步是不言自明的。该函数的参数包括测试数据和训练模型。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="77ae" class="mu lt iq ms b gy nz oa l ob oc"># Create prediction<br/>Prediction &lt;- get_Prediction(<br/>    tst_img, <br/>    tst_cls, <br/>    training_output[["network_model"]], <br/>    0.1<br/>)</span></pre><h2 id="4cc3" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">15.3.视图预测</h2><p id="ac4d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">创建了这个预测之后，接下来需要可视化输出。与<a class="ae lb" href="#5d00" rel="noopener ugc nofollow"> <strong class="kh ir">检查图像</strong> </a>部分相同，以下代码块显示了前16个图像，并为每个图像返回一个标签。</p><p id="e505" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如所见，该模型显然能够识别一些，但它也犯了其他错误。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="92a2" class="mu lt iq ms b gy nz oa l ob oc"># Define classes<br/>ClassList &lt;- c("0"="Not", "1"="Car")</span><span id="ea29" class="mu lt iq ms b gy od oa l ob oc"># Set list<br/>lst &lt;- list()</span><span id="f601" class="mu lt iq ms b gy od oa l ob oc"># Loop 16 images<br/>for (image in 1:16) {<br/>    lst[[image]] &lt;- plt_PlotImage(<br/>        cifar$images[-partition,,,], <br/>        Prediction[["prdic"]], <br/>        ClassList,<br/>        image<br/>    )<br/>}</span><span id="71d7" class="mu lt iq ms b gy od oa l ob oc"># View images<br/>gridExtra::grid.arrange(grobs=lst, ncol=4)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/fae35c7b0b6aebab4fba9e0aea99c5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3bj6A_fjU_gBGwbLiyk0w.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图10 </strong>:预测图像</p></figure><h2 id="7906" class="mu lt iq bd lu mv mw dn ly mx my dp mc ko mz na me ks nb nc mg kw nd ne mi nf bi translated">15.4.检验预测</h2><p id="3f29" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">下一步是对数据进行统计测试，看看它有多准确。为此，使用了<code class="fe mp mq mr ms b">confusionMatrix()</code>功能(来自<code class="fe mp mq mr ms b">caret</code>包)。根据该输出，可以选择适当的度量，以便进一步优化神经网络。</p><p id="c031" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为这篇文章仅仅是关于<em class="mt">展示</em>这个方法，所以我们不会在这里进行更多的优化。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="3548" class="mu lt iq ms b gy nz oa l ob oc"># Set Up<br/>ConfusionMatrix &lt;- Prediction %&gt;% <br/>    mutate_at(c("truth","prdic"), ~ifelse(.==1,"car","not")) %&gt;% <br/>    select(prdic,truth) %&gt;% <br/>    table %&gt;% <br/>    caret::confusionMatrix()</span><span id="b687" class="mu lt iq ms b gy od oa l ob oc"># Print<br/>ConfusionMatrix %&gt;% print()</span></pre><p id="b99b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它打印:</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="2b35" class="mu lt iq ms b gy nz oa l ob oc">Confusion Matrix and Statistics</span><span id="1779" class="mu lt iq ms b gy od oa l ob oc">truth<br/>prdic  car  not<br/>  car  188 1118<br/>  not  121 1573<br/>                                             <br/>               Accuracy : 0.587              <br/>                 95% CI : (0.569, 0.605)     <br/>    No Information Rate : 0.897              <br/>    P-Value [Acc &gt; NIR] : 1                  <br/>                                             <br/>                  Kappa : 0.079              <br/>                                             <br/> Mcnemar's Test P-Value : &lt;0.0000000000000002<br/>                                             <br/>            Sensitivity : 0.6084             <br/>            Specificity : 0.5845             <br/>         Pos Pred Value : 0.1440             <br/>         Neg Pred Value : 0.9286             <br/>             Prevalence : 0.1030             <br/>         Detection Rate : 0.0627             <br/>   Detection Prevalence : 0.4353             <br/>      Balanced Accuracy : 0.5965             <br/>                                             <br/>       'Positive' Class : car</span></pre><p id="d70f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是绘制混淆矩阵。理解和解释混淆矩阵的一个很好的来源可以在这里找到:<a class="ae lb" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>。在这个网站中，它还包括一个图像(也复制在下面)，以获得非常好的视觉理解。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oq"><img src="../Images/90c55634f014cd99927318564a258dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbowRQggr1tsbRSI5Tkc0A.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图11 </strong>:混淆矩阵度量</p></figure><p id="ba06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们可视化我们的模型的混淆矩阵时，我们可以看到它成功地预测了大多数图像。然而，最大数量的坏分类是当它预测图像是一辆汽车时，而它实际上不是。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="642b" class="mu lt iq ms b gy nz oa l ob oc"># Plot Confusion Matrix<br/>ConfusionMatrix %&gt;% <br/>    plt_ConfusionMatrix()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/0765dd685c02ed07b7a8b7f5fbb1a4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MC30-X39bSHlA3Z_80-Qw.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图11 </strong>:生成混淆矩阵</p></figure><p id="4ebc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个好的分析和绘图工具是使用ROC曲线(接收机工作特性)。一条ROC曲线是一个图表，它说明了一个二元分类器系统的诊断能力，因为它的区分阈值是变化的(见<a class="ae lb" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank">接收器操作特性</a>)。</p><p id="1130" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本质上，一个创建完美预测的模型将使曲线完美地“拥抱”该图的左上角。由于我们的模型没有做到这一点，显然进一步的训练和优化是必要的。为此，有必要继续进行<a class="ae lb" href="#4af2" rel="noopener ugc nofollow"> <strong class="kh ir">进一步实验</strong> </a>一节中提到的实验。</p><pre class="ld le lf lg gt nv ms nw nx aw ny bi"><span id="1149" class="mu lt iq ms b gy nz oa l ob oc"># Print ROC<br/>Prediction %&gt;% <br/>    ggplot() +<br/>    plotROC::geom_roc(aes(m=probs, d=truth), n.cuts=0) +<br/>    plotROC::style_roc(<br/>        theme=theme_grey,<br/>        ylab="True Positive Rate",<br/>        xlab="False Positive Rate"<br/>    ) +<br/>    theme(<br/>        plot.title = element_text(hjust=0.5),<br/>        plot.subtitle = element_text(hjust=0.5)<br/>    ) + <br/>    labs(<br/>        title="ROC Curve",<br/>        y="True Positive Rate",<br/>        x="False Positive Rate"<br/>    )</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oe"><img src="../Images/df75a5d6d6a04ec57c7d893eeff7547d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*shxqCawaOvpG62dTp4eUHw.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><strong class="bd of">图12 </strong> : ROC曲线</p></figure><h1 id="9a35" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">16.结论</h1><p id="e0fa" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这是如何在<code class="fe mp mq mr ms b">R</code>中构建香草神经网络的有效表示。在这里，我们展示了如何:</p><ol class=""><li id="9c0b" class="ng nh iq kh b ki kj kl km ko ni ks nj kw nk la nl nm nn no bi translated">访问并检查数据</li><li id="ba66" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">实例化和初始化网络</li><li id="cb02" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">向前传播</li><li id="d6a6" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">计算成本</li><li id="1675" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">反向传播</li><li id="8065" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">更新模型</li><li id="b147" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">建立一个训练方法来循环每一件事</li><li id="b765" class="ng nh iq kh b ki np kl nq ko nr ks ns kw nt la nl nm nn no bi translated">预测和评估绩效</li></ol><p id="7ec1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它还展示了如何完全在<code class="fe mp mq mr ms b">R</code>中做到这一点，而不使用任何预定义的深度学习框架。当然，有其他的包可以执行所有的步骤，并且可能以一种计算效率更高的方式。但是这些包的使用不是本文的目标。在这里，目的是消除“黑箱”现象，并展示如何从头开始创建这些深度学习框架。</p><p id="92b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图所示，这些前馈神经网络的架构有效地使用了<a class="ae lb" href="https://en.wikipedia.org/wiki/Matrix_multiplication" rel="noopener ugc nofollow" target="_blank">矩阵乘法</a>和<a class="ae lb" href="https://en.wikipedia.org/wiki/Differential_calculus" rel="noopener ugc nofollow" target="_blank">微分</a>来调整网络的“权重”，并增加其预测能力。它确实需要大量的数据，也确实需要很长时间来训练和优化。此外，还有许多(许多，<em class="mt">许多</em>)不同的架构可供选择，所有这些都由应用该方法的数据科学家决定。</p><p id="97b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">说到底，这篇文章已经证明了深度学习绝对可以在<code class="fe mp mq mr ms b">R</code>中实现。从而破除了“为了做深度学习，你必须使用<code class="fe mp mq mr ms b">Python</code>”的迷思。当然，深度学习在<code class="fe mp mq mr ms b">Python</code>有很多成功的实现；比如:<a class="ae lb" href="https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b" rel="noopener ugc nofollow" target="_blank">用Python从零开始构建前馈神经网络</a>。在Swift中也有实现相同目标的例子(<a class="ae lb" href="https://www.tooploox.com/blog/deep-neural-networks-in-swift-lessons-learned" rel="noopener ugc nofollow" target="_blank">Swift中的深度神经网络，经验教训</a>)，在C++中也有实现相同目标的例子(<a class="ae lb" href="https://www.geeksforgeeks.org/ml-neural-network-implementation-in-c-from-scratch/" rel="noopener ugc nofollow" target="_blank">从零开始用C++实现神经网络</a>)，在Java中也有实现相同目标的例子(<a class="ae lb" href="https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114" rel="noopener">用纯Java实现人工神经网络(无外部依赖性)</a>)。因此，所使用的具体语言实际上是不相关的。重要的是用例、环境、业务工件以及数据科学家感到舒适的语言。</p><h1 id="cae5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">17.附言</h1><p id="ed91" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated"><strong class="kh ir">鸣谢</strong>:本报告是在他人的帮助下编写的。致谢:<br/> — <a class="ae lb" href="https://www.linkedin.com/in/alexjscriven/" rel="noopener ugc nofollow" target="_blank">亚历克斯·斯克里文</a></p><p id="9bdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">出版物</strong>:本报告同时在以下网站发布:<br/>—RPubs:<a class="ae lb" href="https://rpubs.com/chrimaho/VanillaNeuralNetworksInR" rel="noopener ugc nofollow" target="_blank">RPubs/chrimaho/VanillaNeuralNetworksInR</a><br/>—GitHub:<a class="ae lb" href="https://github.com/chrimaho/VanillaNeuralNetworksInR" rel="noopener ugc nofollow" target="_blank">GitHub/chrimaho/VanillaNeuralNetworksInR</a><br/>—Medium:<a class="ae lb" href="https://medium.com/@chrimaho/43b028f415?source=friends_link&amp;sk=f47b3d6f9f539e907d272966fa88bcb8" rel="noopener">Medium/chrimaho/VanillaNeuralNetworksInR</a></p><p id="b790" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">变更日志</strong>:本出版物在以下日期修改:<br/>—02/11/2020:原始出版日期。</p></div></div>    
</body>
</html>