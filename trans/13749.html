<html>
<head>
<title>Neural Network: Why Deeper Isn’t Always Better</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:为什么越深并不总是越好</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-why-deeper-isnt-always-better-2f862f40e2c4?source=collection_archive---------39-----------------------#2020-09-21">https://towardsdatascience.com/neural-network-why-deeper-isnt-always-better-2f862f40e2c4?source=collection_archive---------39-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8413" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个简单的例子来理解隐藏层如何工作</h2></div><p id="a150" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当创建神经网络时，人们不得不问:<strong class="kk iu">需要多少个隐藏层</strong>，以及<strong class="kk iu">每层</strong>需要多少个神经元？</p><p id="c6aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当创建神经网络时，人们不得不问:<strong class="kk iu">需要多少个隐藏层</strong>，以及<strong class="kk iu">每层</strong>需要多少个神经元？</p><p id="d21f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当涉及到复杂的真实数据时，吴恩达在Coursera的课程中建议:<strong class="kk iu"> <em class="le">改进深度神经网络</em> </strong>，这是一个高度迭代的过程，因此我们必须运行许多测试来找到最佳的超参数。</p><p id="50fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，<em class="le">隐藏层</em>和<em class="le">神经元</em>如何影响最终模型背后的<strong class="kk iu">直觉</strong>是什么？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/65ac1b8cbbbb5308dfb5602f16e35c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wcdhVNXlCXA19jnq"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae lv" href="https://unsplash.com/@moritz_photography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Moritz Kindler </a>在<a class="ae lv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="0f7f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">简单非线性可分数据</h1><p id="0690" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">让我们使用make_circles创建一个数据集。</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="7ba0" class="my lx it mu b gy mz na l nb nc">from sklearn import datasets</span><span id="710a" class="my lx it mu b gy nd na l nb nc">X,y=datasets.make_circles(n_samples=200,shuffle=True, noise=0.1,random_state=None, factor=0.1)</span></pre><p id="7765" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以将数据集可视化:</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="449d" class="my lx it mu b gy mz na l nb nc">import matplotlib.pyplot as plt<br/>plt.scatter(X[:, 0], X[:, 1], c=y,s=20)<br/>plt.axis(‘equal’)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ne"><img src="../Images/69a7eef75f947571903e1bd2ea628bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGyIlky6W1_hYnP6JtPLZw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><h1 id="d7c0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">有多少神经元和层数？</h1><p id="58d8" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">每个神经元创建一个线性决策边界。通过查看数据集，直觉是创建一个由3个神经元组成的层。我们应该能够创建一个完美的分类器。现在，如果只有两个神经元呢？这似乎还不够，如果我们尝试添加更多的层呢？</p><p id="908c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">带着这些问题，我们试着做一些测试。</p><h2 id="2bd7" class="my lx it bd ly nf ng dn mc nh ni dp mg kr nj nk mi kv nl nm mk kz nn no mm np bi translated">一个隐藏层中有3个神经元</h2><p id="4595" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果我们使用3个神经元，代码如下:</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="0bf6" class="my lx it mu b gy mz na l nb nc">from sklearn.neural_network import MLPClassifier</span><span id="2df0" class="my lx it mu b gy nd na l nb nc">clf =MLPClassifier(solver=’lbfgs’,hidden_layer_sizes(3,),<br/>activation=”tanh”,max_iter=1000)</span><span id="f648" class="my lx it mu b gy nd na l nb nc">clf.fit(X, y)<br/>clf.score(X,y)</span></pre><p id="ffb1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们能够在隐藏层内创建3个决策边界。并且我们可以注意到，创建这3条线有无限多种可能性(所以在这个神经网络的代价函数中有无限多种全局最小值)。</p><div class="lg lh li lj gt ab cb"><figure class="nq lk nr ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/65dfa47128dbb7526dc684a6bd291738.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*QLcjZDL7tjDyeTLd8-c7Xg.png"/></div></figure><figure class="nq lk nw ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/3a0d2e9b0973b2cc0a97c7bd70ec0286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*NJxRBh9I4cWcvn6c7ITLHw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk nx di ny nz translated">作者图片</p></figure></div><p id="d385" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了创建隐藏层的决策边界的可视化，我们可以使用隐藏层的系数和截距。</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="6df8" class="my lx it mu b gy mz na l nb nc">n_neurons=3</span><span id="f165" class="my lx it mu b gy nd na l nb nc">xdecseq=np.repeat(np.linspace(-1,1,100),n_neurons).reshape(-1,n_neurons)</span><span id="a42a" class="my lx it mu b gy nd na l nb nc">ydecseq=-(xdecseq*clf.coefs_[0][0]+clf.intercepts_[0])/clf.coefs_[0][1]</span></pre><p id="eb99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们可以用原始数据集绘制这三条线:</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="66e7" class="my lx it mu b gy mz na l nb nc">fig, ax = plt.subplots(figsize=(8,8))</span><span id="3564" class="my lx it mu b gy nd na l nb nc">ax.scatter(X[:, 0], X[:, 1], c=y,s=20)</span><span id="d3b2" class="my lx it mu b gy nd na l nb nc">ax.plot(xdecseq[:, 0], ydecseq[:, 0])<br/>ax.plot(xdecseq[:, 1], ydecseq[:, 1])<br/>ax.plot(xdecseq[:, 2], ydecseq[:, 2])</span><span id="ea89" class="my lx it mu b gy nd na l nb nc">ax.set_xlim(-1.5, 1.5)<br/>ax.set_ylim(-1.5, 1.5)</span></pre><h2 id="83d9" class="my lx it bd ly nf ng dn mc nh ni dp mg kr nj nk mi kv nl nm mk kz nn no mm np bi translated">一个隐藏层中的两个神经元</h2><p id="1fed" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果我们只使用2个神经元，很容易看出我们将不能完美地对所有观察结果进行分类。</p><p id="381d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用类似的代码，我们可以绘制出下图，其中包含隐藏层的两个决策边界。</p><div class="lg lh li lj gt ab cb"><figure class="nq lk oa ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/62623c95752c1774b9d463a01671be74.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*GZAtQ8KD_QJuOKefhxQghw.png"/></div></figure><figure class="nq lk ob ns nt nu nv paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/cf5256b59a6d9f3c76d2a6a4651dc7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*kX-GA1kr-U-_Cg4qkte0tQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk oc di od nz translated">作者图片</p></figure></div><h2 id="fc36" class="my lx it bd ly nf ng dn mc nh ni dp mg kr nj nk mi kv nl nm mk kz nn no mm np bi translated">2个神经元，每个神经元有许多层</h2><p id="0acb" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果我们在许多隐藏层中创建2个神经元会怎么样？为了更好地理解在第一个有2个神经元的隐藏层中发生了什么，我创建了这个可视化。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/d84854884e89ed67bd4881b89ab93890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/1*mVFFthmaMVzC93dskIcmEA.gif"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><blockquote class="of"><p id="4b95" class="og oh it bd oi oj ok ol om on oo ld dk translated">很明显，无论你添加多少层，你都无法对下面红圈里的数据进行分类。</p></blockquote><figure class="op oq or os ot lk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8b5ba3548b3fd783714e069b180bc76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*OSCu-zAQEubwzdccT5-x1g.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">作者图片</p></figure><p id="8365" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以自测一下:可以在参数hidden_layer_sizes中添加更多的图层。例如,( 2，3，2)表示3个隐藏层，每个层有2、3和2个神经元。</p><pre class="lg lh li lj gt mt mu mv mw aw mx bi"><span id="b026" class="my lx it mu b gy mz na l nb nc">from sklearn.neural_network import MLPClassifier</span><span id="8304" class="my lx it mu b gy nd na l nb nc">clf = MLPClassifier(solver=’lbfgs’,<br/>hidden_layer_sizes=(2,3,2),<br/>activation=”tanh”,max_iter=1000)</span><span id="722e" class="my lx it mu b gy nd na l nb nc">clf.fit(X, y)<br/>clf.score(X,y)</span></pre><h1 id="2096" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="19ff" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">从这个简单的例子中，我们可以用这个<em class="le">直觉</em>来结束:</p><ul class=""><li id="345f" class="ou ov it kk b kl km ko kp kr ow kv ox kz oy ld oz pa pb pc bi translated">第一个隐藏层中神经元的数量创建尽可能多的线性决策边界来对原始数据进行分类。</li><li id="500d" class="ou ov it kk b kl pd ko pe kr pf kv pg kz ph ld oz pa pb pc bi translated">如果第一层不包含必要数量的神经元，那么(理论上)创建更深的神经网络是没有帮助的。</li></ul></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><p id="189b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果想看其他动画了解神经网络是如何工作的，也可以看看这篇文章。</p><div class="pp pq gp gr pr ps"><a rel="noopener follow" target="_blank" href="/animations-of-neural-networks-transforming-data-42005e8fffd9"><div class="pt ab fo"><div class="pu ab pv cl cj pw"><h2 class="bd iu gy z fp px fr fs py fu fw is bi translated">神经网络转换数据的动画</h2><div class="pz l"><h3 class="bd b gy z fp px fr fs py fu fw dk translated">我们可以对神经网络为什么以及如何工作有更好的直觉</h3></div><div class="qa l"><p class="bd b dl z fp px fr fs py fu fw dk translated">towardsdatascience.com</p></div></div><div class="qb l"><div class="qc l qd qe qf qb qg lp ps"/></div></div></a></div></div></div>    
</body>
</html>