<html>
<head>
<title>Neural Style Transfer Using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch的神经类型转移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa?source=collection_archive---------14-----------------------#2020-11-09">https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa?source=collection_archive---------14-----------------------#2020-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="8d16" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">简介</strong></h1><p id="e023" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">神经风格转移是基于深层神经网络生成艺术图像的人工系统。这种方法使用两个随机图像，内容和样式图像。它从内容图像中提取结构特征，而从风格图像中提取风格特征。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/63253be15a752b5f9bff65f4ddc4e1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*4AN6AIMo5K3wRW86v38GNw.gif"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">神经风格转移(作者GIF)</p></figure><h1 id="735f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">内容和风格表现</strong></h1><h2 id="d72d" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">内容表示</h2><p id="ce9d" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">卷积神经网络沿着处理层次开发图像的表示。随着我们向网络的更深处移动，表示将更关心结构特征或实际内容，而不是详细的像素数据。为了获得这些表示，我们可以使用该层的特征图来重建图像。从较低层的重建将再现精确的图像。相反，较高层的重建将捕获高级内容，因此我们将来自较高层的特征响应称为<strong class="kn ir">内容表示。</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/1a0d6c03bb591f41cbad75e765ee1e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uP5x5K49IK3PUKCczq2dgA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">ConvNet不同层次的内容重构(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="bea1" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">上图显示了从层“conv1_1”、“conv2_1”、“conv3_1”、“conv4_1”和“conv5_1”重建输入图像。我们发现，从较低层重建的图像几乎与输入图像相同，但随着我们深入网络，详细的像素信息会丢失。相比之下，图像的高级内容被保留。</p><h2 id="e64e" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">风格表现</h2><p id="48b3" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了提取样式内容的表示，我们在每个网络层的过滤器响应的顶部构建了一个特征空间。它包括在特征图的空间范围内不同滤波器响应之间的相关性。不同层的滤波器相关性捕获输入图像的纹理信息。这在丢弃全局排列的信息的同时，创建了与给定图像的风格越来越匹配的图像。这种多尺度表示被称为<strong class="kn ir">风格表示。</strong></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/5b68c493615f47bf62bdd457c9261c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BY0_O23dei_VeWo-MCtRYQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">ConvNet不同层次的风格重构(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="ca9f" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">上图显示了代表CNN不同层中不同特征之间相关性的每个卷积层上方的特征空间。随着我们深入到网络中，我们可以看到全局排列或结构特征被丢弃了。</p><h1 id="4c2e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">模型架构</strong></h1><p id="99d0" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">论文《艺术风格的神经算法》中提出的模型的体系结构是</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/2ac249a4258487c5f96e0a58b0889ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p15iAAgqiCyVAbi4msgfeQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型架构(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="56ec" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">这里，我们使用预训练的VGG19网络的卷积神经网络，并执行内容和风格重建。通过将来自内容表示的结构信息和来自风格表示的纹理/风格信息纠缠在一起，我们生成了艺术图像。我们可以强调重建风格或内容。对风格的强烈强调将导致图像与艺术品的外观相匹配，有效地给出了它的纹理版本，但几乎没有显示照片的任何内容。当把重点放在内容上时，人们可以识别照片，但绘画风格并不匹配。我们对生成的图像执行梯度下降，以找到与原始图像的特征响应相匹配的另一个图像。</p><h1 id="4d2a" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">实施</strong></h1><h2 id="1396" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">导入库</strong></h2><p id="888b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们将从导入所需的库开始。我们将进口火炬，火炬视觉和PIL，以实现使用PyTorch的风格转移。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="b481" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">加载模型</strong></h2><p id="140a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这种情况下，我们将从<em class="mw"> torchvision.models()加载预训练的<a class="ae mm" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank"> VGG19 </a>模型。</em>vgg 19模型有三个组件特性，avgpool和分类器。</p><ul class=""><li id="4ac5" class="mx my iq kn b ko mn ks mo kw mz la na le nb li nc nd ne nf bi translated">该功能包含所有卷积层、最大池层和ReLu层</li><li id="d021" class="mx my iq kn b ko ng ks nh kw ni la nj le nk li nc nd ne nf bi translated">avgpool保存平均池层。</li><li id="731e" class="mx my iq kn b ko ng ks nh kw ni la nj le nk li nc nd ne nf bi translated">分类器保存密集层。</li></ul><p id="951a" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">我们将只使用卷积神经网络来实现风格转换，因此导入vgg19特性。如果可能的话，不要忘记使用GPU。这会节省训练时间。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="5355" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">图像预处理</strong></h2><p id="aa36" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了使图像与模型兼容，必须对图像进行预处理。使用<em class="mw"> torch.transforms() </em>我们将执行一些基本的预处理，包括以下步骤:</p><ul class=""><li id="ccc8" class="mx my iq kn b ko mn ks mo kw mz la na le nb li nc nd ne nf bi translated">调整大小:将所有图像的大小调整为512 x 512</li><li id="1e7e" class="mx my iq kn b ko ng ks nh kw ni la nj le nk li nc nd ne nf bi translated">将图像转换成张量</li></ul><p id="0a74" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">对于预训练的vgg19模型，还可以使用平均值(0.485，0.456，0.406)和标准差(0.229，0.224，0.225)对张量进行归一化。但是别忘了把它转换回原来的比例。因此，定义一个使用PIL库加载图像并对其进行预处理的函数。在第0个索引处添加一个额外的维度，使用<em class="mw"> unsqueeze() </em>表示批量大小，然后将其加载到设备并返回。</p><p id="0ca7" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">现在，使用image_loader函数从本地磁盘加载样式和内容图像。通常的做法是使用内容映像克隆作为输入基础映像或生成的映像。由于梯度下降会改变生成图像的像素值，我们将为<em class="mw"> require_grads()传递参数true。</em></p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="8bf0" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">获取特征表示</strong></h2><p id="fda4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们定义一个类，它将提供中间层的特征表示。使用中间层是因为这些层充当复杂的特征提取器。因此，这些可以描述输入图像的风格和内容。在本课程中，我们将通过删除vgg19模型中未使用的层(conv5_1以外的层)来初始化模型，并提取“conv1_1”、“conv2_1”、“conv3_1”、“conv4_1”和“conv5_1”层的激活或特征表示(索引值[0，5，10，19，28])。将5个卷积层的激活存储在一个数组中，并返回该数组。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="6f05" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">定义损失</strong></h2><p id="169f" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">风格转移的净损失定义为:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/999cb583dfd91586dd310049c292f845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*oVmMuCOgAAFx1ow9DSQV0A.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">全损(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">一种艺术风格的神经算法(2015) </a>)</p></figure><p id="c787" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">在上面的等式中，L <em class="mw"> ₜₒₜₐₗ </em>是总损失，l𝒸ₒₙₜₑₙₜ<strong class="kn ir">T9】是所有中间层的内容损失，lₛₜᵧ<em class="mw">ₗ</em>ₑ<strong class="kn ir">t13】是所有中间层的风格损失。这里，α和β分别是内容和风格损失的加权系数。p、a和x是内容图像、风格图像和生成图像或基本输入图像。我们对损失函数执行梯度下降，并且代替模型参数，我们更新输入图像x的像素值以最小化损失。这将使输入图像与内容和样式图像相似。我们可以通过改变α和β的值来强调风格或内容的损失。对风格的强烈强调将导致图像与艺术品的外观相匹配，有效地给出了它的纹理版本，但几乎没有显示照片的任何内容。当把重点放在内容上时，人们可以识别照片，但绘画风格并不匹配。</strong></strong></p><h2 id="5c6c" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">内容丢失</strong></h2><p id="4c5c" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">内容图像和输入基本图像被传递到我们的模型，中间层的输出(上面列为“conv1_1”、“conv2_1”、“conv3_1”、“conv4_1”和“conv5_1”)使用上面定义的类提取。然后，我们计算内容图像的中间表示和输入基础图像之间的欧几里德距离。因此，层l的内容损失由下式定义:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e23a49efe5cae754d0479bb066fc0553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*nBLe4TjtI-bZeATw5RxFdg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">内容丢失(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="cbef" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">在上面的等式中，我们计算内容图像(p)的特征表示和层(l)的输入基础图像(x)之间的平方误差。这里，nˡₕ、nˡ𝓌、nˡ𝒸是层l的高度、宽度和通道。为了计算内容损失，维度nˡₕ x nˡ𝓌 x nˡ𝒸的中间表示被展开成维度nˡ𝒸 x nˡₕ <strong class="kn ir"> * </strong> n的向量。展开要素制图表达不是强制步骤，但这是一个很好的做法。下图将有助于我们将这种转变形象化。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nn"><img src="../Images/310b0402f5d937bff19d328f6e92d56f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArNXe5tieIw6A3NyNBm0Ig.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">向2D展开3D特征地图(图片由作者提供)</p></figure><p id="f870" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">Fˡᵢⱼ <strong class="kn ir"> </strong>和Pˡᵢⱼ是代表输入基础图像和内容图像的中间表示的nˡ𝒸×nˡₕ<strong class="kn ir">*</strong>nˡ𝓌维度向量。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="d44f" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">风格丧失</strong></h2><p id="c84e" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们在网络的每一层上构建一个特征空间，表示不同滤波器响应之间的相关性。Gram matrix计算这些特征相关性<strong class="kn ir">。</strong></p><p id="ba1a" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">gram矩阵表示中间表示中每个过滤器之间的相关性。通过取展开的中间表示及其转置的点积来计算格拉姆矩阵。格拉姆矩阵g的维数是nˡ𝒸×nˡ𝒸，其中nˡ𝒸是层l的中间表示中的通道数</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f86f56a36102d00b13c3559db9e5a311.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*06aqmsLMuGFNwFGEIcrXVw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Gram Matrix(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">一种艺术风格的神经算法(2015) </a>)</p></figure><p id="0079" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">上式中，Gˡᵢⱼ <strong class="kn ir"> </strong>是l层的矢量化特征图I和j的内积，一个gram矩阵的矢量化方程如下图所示，其中g是中间表示a的gram矩阵</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi np"><img src="../Images/b09d73ad7f9246a8990efc8b5aaf82c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HsWvX7rmUU1VlXyeWfYK_A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Gram矩阵计算(图片由作者提供)</p></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/aa88f381f939d82fb307d47c13aca526.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*7J21Rl6kgQAIFSuw0ypjOg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Gram矩阵矢量化方程(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">一种艺术风格的神经算法(2015) </a>)</p></figure><p id="c8f3" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">层l的风格损失是风格图像的中间表示和输入基础图像的gram矩阵之间的平方误差。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ea80649a271e0d392afabf7127983ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*SqnS_a_UPy-WZyH_u96nLg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">l层的风格损失(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="3631" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">其中Eₗ <strong class="kn ir"> </strong>是层l的风格损失，Nₗ和Mₗ分别是层l的特征表示中的通道数和高度乘以宽度。Gˡᵢⱼ和Aˡᵢⱼ <strong class="kn ir"> </strong>分别是风格图像(a)和输入基础图像(x)的中间表示的克矩阵。</p><p id="97b8" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">总风格损失为:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5731e6e4966da586942ce6704e568e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Y311KhEcUv4DoaqXiMP1zA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">总体风格损失(图片来自<a class="ae mm" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">艺术风格的神经算法(2015) </a>)</p></figure><p id="2f70" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">其中wˡ是每层对总风格损失的贡献的权重因子。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="bd83" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">这里，我们没有将样式损失乘以常数，因为它使损失非常小。合计所有中间层的内容和样式损失，并计算总损失。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="8844" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">现在让我们继续训练模型。</p><h2 id="4514" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated"><strong class="ak">训练</strong></h2><p id="a35b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">初始化我们训练模型所需的变量。因此，在继续训练之前，我们需要设置超参数。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="91df" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">在这里，为类<em class="mw"> VGG创建一个对象。</em>初始化对象将调用构造函数，它将返回前29层的模型并将其加载到设备。Epoch为1000，学习率为0.01，alpha(内容损失加权系数)为1，beta(风格损失加权系数)为0.01。</p><p id="9723" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">Adam被用作优化器。生成的图像的像素数据将被优化，以将生成的图像作为优化器参数传递。</p><p id="b398" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">使用for循环迭代历元数。使用模型提取内容、风格和生成的图像的中间层的特征表示。在向模型传递图像时，它将返回一个长度为5的数组。每个元素对应于每个中间层的特征表示。</p><p id="bde3" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">使用上面定义的函数计算总损耗。用<em class="mw"> optimizer.zero_grads()，</em>将渐变设置为零，用<em class="mw"> total_loss.backward() </em>反向传播总损失，然后用<em class="mw"> optimizer.step()更新生成图像的像素值。</em>我们将在每100个历元后保存生成的图像，并打印总损失。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="08ad" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">经过训练，你可以在你当前的工作目录中找到艺术形象。你的图像会像这样。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nt"><img src="../Images/2a00b25a14b1841d473f61e7474d86a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jl2lHAFE_9kT185hnGzNxg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">内容图片(左:图片来自<a class="ae mm" href="https://simple.wikipedia.org/wiki/Nikola_Tesla" rel="noopener ugc nofollow" target="_blank">维基百科</a> ) +风格图片(中:图片来自<a class="ae mm" href="https://unsplash.com/@usgs" rel="noopener ugc nofollow" target="_blank">USGS</a>on U<a class="ae mm" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NSP lash</a>)=生成的图片(右:图片来自作者)</p></figure><p id="4cc1" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">请随意使用超参数。有时你可能会得到预期的结果，但有时你可能会努力实现预期的结果。玩得开心！！！</p><h1 id="d0d8" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">结论</h1><p id="e8f1" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在本教程中，您已经学习了神经类型转移的基础知识，并建立了一些直觉。您已经加载了一个预训练的vgg19模型，冻结了它的重量并定制了模型层。加载图像并执行一些基本的预处理。然后定义了内容损失和风格损失，两者结合起来计算总损失。最后你跑了模型，做了一个艺术形象，是内容和风格形象的交融。</p><p id="ec16" class="pw-post-body-paragraph kl km iq kn b ko mn kq kr ks mo ku kv kw mp ky kz la mq lc ld le mr lg lh li ij bi translated">你可以在这里找到完整的代码。</p><h1 id="6334" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h1><ol class=""><li id="dd43" class="mx my iq kn b ko kp ks kt kw nu la nv le nw li nx nd ne nf bi translated">莱昂·A·加蒂丝，亚历山大·s·埃克，马蒂亚斯·贝奇，<a class="ae mm" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank">一种艺术风格的神经算法</a> (2015)，arXiv</li></ol></div></div>    
</body>
</html>