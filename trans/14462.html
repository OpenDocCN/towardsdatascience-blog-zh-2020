<html>
<head>
<title>Big data, small box</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据，小盒子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/big-data-small-box-d9896310259f?source=collection_archive---------41-----------------------#2020-10-05">https://towardsdatascience.com/big-data-small-box-d9896310259f?source=collection_archive---------41-----------------------#2020-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8ffa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用VirtualBox: Spark、ElasticSearch &amp; Kibana构建和浏览大数据开发环境。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e37f30d730064dce5e57577a957ed412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMQun4oLQWF6w-6XH617tw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@fabioha?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">法比奥</a>在<a class="ae kv" href="https://unsplash.com/s/photos/big-data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="de21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">TL；博士</strong>:看看我们如何使用Vagrant来构建和配置一个大数据开发环境，它小到可以装进你的笔记本电脑。用一行代码启动这个环境。掌握ElasticSearch、Kibana和Spark的基础知识:将Spark数据帧索引到Elasticsearch，然后了解如何使用Python、Kibana或good ol' cURL查询索引。享受使用虚拟机的乐趣，提高您对大数据堆栈中一些关键技术的熟练程度。</p><p id="8157" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经尽可能地让这篇文章易于理解，你应该能够在不了解我们将使用的任何技术的情况下理解这篇文章。</p><h1 id="7fec" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.介绍</h1><p id="8aa0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们将把大数据堆栈中的关键技术联系在一起，探索数据如何在这些组件之间流动，以及每个组件发挥什么功能。我们将能够通过使用一种让我们构建和配置虚拟机，并为它们提供软件的技术，非常容易地做到这一点。事实上，如果你已经做到了这一步，你离拥有一个功能性的“大数据”开发环境只有几步之遥，尽管没有处理TB级数据集的资源。</p><p id="fef9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将能够使用此设置来学习如何在自己的时间内高效使用大数据技术堆栈(并且在没有人看到的情况下犯错误)，以便在真正使用这些技术时，您将拥有坚实的基础和一些实践经验！</p><h2 id="3854" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">内容</h2><ol class=""><li id="3c57" class="nb nc iq ky b kz mk lc ml lf nd lj ne ln nf lr ng nh ni nj bi translated"><a class="ae kv" href="#7fec" rel="noopener ugc nofollow">简介</a></li><li id="f070" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#f5f8" rel="noopener ugc nofollow">入门</a></li><li id="033a" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#28f3" rel="noopener ugc nofollow">弹性搜索和基巴纳</a></li><li id="5719" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#e4f3" rel="noopener ugc nofollow">火花</a></li><li id="2e60" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#9535" rel="noopener ugc nofollow"> ElasticSearch Hadoop </a></li><li id="0900" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#ebea" rel="noopener ugc nofollow">快捷方式</a></li><li id="4993" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><a class="ae kv" href="#e15a" rel="noopener ugc nofollow">进一步练习和阅读</a></li></ol><h1 id="f5f8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.入门指南</h1><p id="375e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">是时候开始运行了..travel允许我们根据写在travel文件中的指令初始化、供应和配置虚拟机。一个浮动文件包含虚拟机供应器的指令，以及(可能)在虚拟机上下载、安装和配置软件所需的所有其他内容(尽管指令块可以重构为在供应过程中执行的单独脚本)。这意味着，如果你幸运的话，有人已经创建了一个描述虚拟机的流浪文件，它有能力做你想做的事情，你可以直接进入有趣的部分。要开始使用我为本文创建的VM，请遵循下面的步骤。</p><ol class=""><li id="de25" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">下载并安装vagger(以及类似<a class="ae kv" href="https://www.virtualbox.org/wiki/Downloads" rel="noopener ugc nofollow" target="_blank">VirtualBox</a>)<a class="ae kv" href="https://www.vagrantup.com/intro/getting-started" rel="noopener ugc nofollow" target="_blank">https://www.vagrantup.com/intro/getting-started</a>这样的VM提供者)，检查你是否可以创建一个基本的裸机VM。</li><li id="2e06" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">从<a class="ae kv" href="https://github.com/kjarvis1905/vagrant-elk" rel="noopener ugc nofollow" target="_blank">这里</a>克隆我的GitHub库，这包含一个将被用来创建一个特定VM的流浪文件，以及一些安装脚本来为它提供我们想要使用的软件。</li><li id="9918" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">*进入克隆的git repo，打开命令控制台并键入<code class="fe np nq nr ns b">vagrant up</code>。</li></ol><p id="82b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">*作为设置虚拟机过程的一部分，Spark从<a class="ae kv" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">这里</a>下载并安装。实际上，在下载Spark之前，我已经包含了一个步骤，让安装脚本检查<code class="fe np nq nr ns b">spark-2.4.6-bin-hadoop2.7.tgz</code>文件是否已经存在于与流浪者文件相同的目录中，如果是，那么下载步骤被跳过。我建议手动下载这个文件，放在流浪汉文件目录下，因为下载这个<code class="fe np nq nr ns b">.tgz</code>的默认镜像站点会超级慢。</p><p id="14a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦你输入了<code class="fe np nq nr ns b">vagrant up</code>，就是这样！喝杯茶犒劳一下自己，因为安装和配置不会在瞬间完成。</p><p id="b144" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个简单的小命令只是触发了一系列事件的连锁反应，最终将创造一个完全成熟的大数据虚拟机供我们使用和探索。事情是这样的:</p><ol class=""><li id="d4bb" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">创建了一个虚拟机。</li><li id="f9a8" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">Python、Spark、ElasticSearch和Kibana都被安装在虚拟机上。</li><li id="5fc5" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">一个Python虚拟环境(虚拟机中的虚拟环境？矩阵多？)时，PySpark是根据Spark附带的源代码构建的，并安装到该虚拟环境中。</li></ol><p id="d87d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在了解如何使用我们的虚拟机探索这些技术之前，让我们简要了解一下它们各自扮演的角色，以及它们通常如何在生产大数据环境中一起使用。</p><ol class=""><li id="fb8c" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated"><strong class="ky ir">火花</strong>。Spark是我们的分布式计算引擎，接受使用Python或Scala API等指定的指令，并在我们的集群中优化和并行化所需的数据转换。Spark用于在内存中快速处理大型分布式数据集。</li><li id="1baa" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><strong class="ky ir">弹性搜索</strong>。ElasticSearch (ES)是一个搜索引擎，它允许我们对事物(文档)进行索引，这样我们可以非常快速地搜索和检索特定的信息。在本文的后面，我们将介绍如何直接从Spark数据帧中索引文档。可以使用一组由es服务器作为端点公开的API来搜索、更新和删除索引。这为我们提供了在非常大的数据集上构建面向用户的应用程序所必需的响应能力。</li><li id="d884" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated"><strong class="ky ir">基巴纳。</strong>基巴纳有多种用途。它可以作为一种工具，以一种用户友好的方式探索经济服务指数。我们还可以使用Kibana构建仪表板，并使用其开发工具来学习如何与ElasticSearch交互。</li></ol><p id="0928" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管ElasticSearch和Kibana是数据可能流向的“目的地”,但我们将通过查看这两个工具来开始探索我们闪亮的新虚拟机，因为服务器进程应该已经在运行了！</p><h2 id="7b0c" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">启动、停止和重新启动虚拟机</h2><p id="b71e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">您可以在方便的时候轻松地停止和启动VM。要停止VM，在游民文件所在目录的shell中键入<code class="fe np nq nr ns b">vagrant halt</code>(如果您已经通过ssh进入VM，请确保在执行该命令之前关闭ssh连接)。当您想重启VM，并从您离开的地方开始，只需运行<code class="fe np nq nr ns b">vagrant up</code>。谢天谢地，这将比我们第一次运行虚拟机时快得多！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/aaea8dd7e73b47bf6279be646ca67b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMXE4CCX0VEPM-kSzDWhVw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的虚拟机由vagger和VirtualBox构建，托管了多项大数据技术。</p></figure><h1 id="28f3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.弹性搜索和Kibana</h1><p id="a113" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">虚拟机启动并运行后，让我们执行一些基本的检查，以确保一切正常。您应该已经克隆了git存储库，并运行命令<code class="fe np nq nr ns b">vagrant up</code>来启动、配置和供应虚拟机。现在运行命令<code class="fe np nq nr ns b">vagrant ssh</code>将ssh导入正在运行的机器。使用以下命令检查ElasticSearch和Kibana是否已成功启动:</p><p id="ae33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">systemctl status elasticsearch</code> (2.a)</p><p id="9881" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">systemctl status kibana</code> (2.b)</p><p id="6493" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个命令应该向我们显示一些看起来令人放心的输出，告诉我们我们的服务正在运行。如果一切正常，运行以下代码应该没有问题:</p><p id="c9d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">curl -XGET http://127.0.0.1:9200/_cat/health?v</code> (3)</p><p id="89aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们刚刚向ElasticSearch REST API提交了一个HTTP GET请求，我们应该会收到一个响应，给出一些关于弹性集群状态的诊断信息。从ES索引中查询和检索信息同样简单！我们可以尝试的下一件事是切换到我们主机上的浏览器并导航到<code class="fe np nq nr ns b">127.0.0.1:5602</code>(注意，在<code class="fe np nq nr ns b">Vagrantfile</code>中，我们已经配置了来宾虚拟机的端口<code class="fe np nq nr ns b">5601</code>，这是Kibana正在侦听的端口，用于将流量转发到主机上的端口<code class="fe np nq nr ns b">5602</code>)。在我们的浏览器中，我们应该能够看到一个闪屏，告诉我们Kibana UI正在启动。加载后，导航至“开发工具”(左侧导航面板底部的扳手图标)。这使我们能够访问一个控制台，该控制台允许我们向es服务器发送请求，并帮助我们处理语法、自动完成以及探索可用的选项。响应显示在相邻的窗口中。尝试输入<code class="fe np nq nr ns b">GET /_cat/health?v</code>并将查询发送到ES服务器。在引擎盖下，发生了与我们执行(3)时非常相似的事情。</p><p id="bd05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很高兴知道我们的ES集群正在运行——但是更好的做法是完成文档的索引和搜索过程。导航到一个工作目录—可能是<code class="fe np nq nr ns b">/home/vagrant</code>，然后使用下面的命令下载并解压缩一些示例数据。</p><p id="f666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">wget https://download.elastic.co/demos/kibana/gettingstarted/accounts.zip</code> (4.a)</p><p id="9f84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">unzip accounts.zip</code> (4.b)</p><p id="63c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们将使用ElasticSearch <a class="ae kv" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html" rel="noopener ugc nofollow" target="_blank">批量上传API </a>来索引<code class="fe np nq nr ns b">accounts.json</code>中的文档。为此，我们需要向ES服务器发出一个<code class="fe np nq nr ns b">POST</code>请求。完整的命令是:</p><p id="19f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">curl -H "Content-Type: application/x-ndjson" -XPOST 127.0.0.1:9200/financial/_bulk?pretty --data-binary @accounts.json</code> (5)</p><p id="82ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意<code class="fe np nq nr ns b">accounts.json</code>是根据API <a class="ae kv" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html" rel="noopener ugc nofollow" target="_blank">文档</a>中概述的要求方便地格式化的。这就在<code class="fe np nq nr ns b">financial</code>索引中创建了1000个新文档。一个成功的请求将会得到一个由类似下面的单元组成的长响应:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="b242" class="mp lt iq ns b gy ob oc l od oe">"index" : {<br/>        "_index" : "financial",<br/>        "_type" : "_doc",<br/>        "_id" : "990",<br/>        "_version" : 4,<br/>        "result" : "updated",<br/>        "_shards" : {<br/>          "total" : 2,<br/>          "successful" : 1,<br/>          "failed" : 0<br/>        },<br/>        "_seq_no" : 3998,<br/>        "_primary_term" : 1,<br/>        "status" : 200<br/>      }</span></pre><p id="efe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们从索引中检索一些文档。为了增加多样性，切换回浏览器中打开的Kibana应用程序。转到控制台，键入<code class="fe np nq nr ns b">GET /financial/_doc/1</code>，然后“点击发送请求”。在相邻的窗口中，我们将看到响应，应该是成功的，以及带有<code class="fe np nq nr ns b">_id = 1</code>的文档的内容。将使用文档的内容填充<code class="fe np nq nr ns b">_source</code>属性。注意，我们可以通过使用不同的工具，比如<code class="fe np nq nr ns b">curl</code>，来发出请求，从而获得相同的结果。</p><p id="efa4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你已经做到了这一步，祝贺你。您已经拥有了使用ElasticSearch和Kibana进行探索和实验所需的一切。请记住，稍后您可以轻松地停止并重新启动您的虚拟机，以继续接下来的步骤。</p><h1 id="e4f3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.火花</h1><p id="aea9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们要使用的下一项技术是Spark。Spark实际上是高性能大数据处理的同义词。我想在这里介绍它的原因是，可以直接将Spark数据帧的行索引到一个弹性簇中。这使得直接从数据处理管道向Elastic写出步骤(我们的输出)变得非常容易，在Elastic中，可以通过API搜索索引和检索单个文档，或者使用Kibana仪表板浏览和可视化。在我更详细地介绍如何做到这一点之前，让我们先了解一下如何在我们的流浪者虚拟机中使用Spark。Spark有用多种语言编写的API，我们将使用Python API，也就是PySpark。</p><p id="9937" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从连接到虚拟机的shell中，导航到<code class="fe np nq nr ns b">/vagrant/examples</code>，并运行</p><p id="b013" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">pipenv shell</code>，(6)</p><p id="9a71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将激活已经使用<code class="fe np nq nr ns b">pipenv</code>设置好的Python虚拟环境。通过使用pip构建和安装Spark发行版中包含的PySpark源代码，PySpark已经被安装到这个虚拟环境中。如果您有兴趣更详细地理解这些步骤，您可以查看GitHub资源库中的<code class="fe np nq nr ns b">bootstrap/install/spark_install.sh</code>和<code class="fe np nq nr ns b">bootstrap/install/setup_demo_project.sh</code>脚本。</p><p id="7663" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此时，你有两个选择:一组捷径，或者绕远路。要查看可用的快捷方式，请查看快捷方式部分。</p><p id="100e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请继续阅读，寻找一种更为基础的方法，我们将一个接一个地分解中间步骤。</p><p id="b487" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该做的第一件事是启动Python解释器并运行以下命令，以检查我们是否可以正确启动Spark会话:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="2f3c" class="mp lt iq ns b gy ob oc l od oe">(examples) vagrant@vagrant:/vagrant/examples$ python<br/>Python 3.5.2 (default, Jul 17 2020, 14:04:10)<br/>[GCC 5.4.0 20160609] on linux<br/>Type "help", "copyright", "credits" or "license" for more information.<br/>&gt;&gt;&gt; import pyspark<br/>&gt;&gt;&gt; from pyspark.sql import SparkSession<br/>&gt;&gt;&gt; ss=SparkSession.builder.getOrCreate()</span></pre><p id="f8d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将为您提供一个SparkSession对象，它是底层Spark魔法的主要入口点，接下来我们应该使用它将一些数据加载到DataFrame中。为此，我们将重复使用上一节中获得的<code class="fe np nq nr ns b">accounts.json</code>。我们需要通过删除包含索引/id信息的行来快速转换这些数据，这样我们就可以将数据读入Spark数据帧。我们可以使用<code class="fe np nq nr ns b">sed</code>(流编辑器)来完成，您可以在单独的shell中使用它:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="e895" class="mp lt iq ns b gy ob oc l od oe">cat accounts.json | sed -n 's/\({"a.*}\)/\1/\p' &gt; accounts_edited.json</span></pre><p id="5acc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用<code class="fe np nq nr ns b">head -n 5 accounts_edited.json</code>检查结果，你会注意到我们用索引信息去掉了断续的行。你还会注意到我的正则表达式非常懒惰，在更一般的情况下会很快失效。我不找借口。</p><p id="3c8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">切换回我们的Python解释器，将json文件加载到Spark数据帧中(注意，如果您将数据下载到不同的位置，则必须在代码片段中设置适当的路径):</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="4559" class="mp lt iq ns b gy ob oc l od oe">&gt;&gt;&gt; import json<br/>&gt;&gt;&gt; import os<br/>&gt;&gt;&gt; data_file = os.path.join(os.environ['HOME'], 'accounts_edited.json')<br/>&gt;&gt;&gt; with open(data_file, 'r') as f:<br/>...   data = [Row(**json.loads(l)) for l in f]<br/>...<br/>&gt;&gt;&gt; df = ss.createDataFrame(data)<br/>&gt;&gt;&gt; df.printSchema()<br/>root<br/> |-- account_number: long (nullable = true)<br/> |-- address: string (nullable = true)<br/> |-- age: long (nullable = true)<br/> |-- balance: long (nullable = true)<br/> |-- city: string (nullable = true)<br/> |-- email: string (nullable = true)<br/> |-- employer: string (nullable = true)<br/> |-- firstname: string (nullable = true)<br/> |-- gender: string (nullable = true)<br/> |-- lastname: string (nullable = true)<br/> |-- state: string (nullable = true)</span></pre><p id="772e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个Spark数据帧与您可能在使用Spark的任何类型的数据处理管道的末端(或中间(或开始…))使用的对象完全相同。我们接下来要做的是看看如何将这个对象中包含的数据直接索引到ElasticSearch中。</p><h1 id="9535" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.ElasticSearch Hadoop</h1><p id="27cb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，为了能够将Spark数据框架直接索引到ElasticSearch中，我们需要付出一些努力来进行设置——即ElasticSearch和Spark之间的接口，该接口由<a class="ae kv" href="https://www.elastic.co/what-is/elasticsearch-hadoop" rel="noopener ugc nofollow" target="_blank"> ElasticSearch Hadoop </a>提供。同样，一些工作已经完成了:所需的归档文件已经在VM的供应期间下载并解压缩，相关的代码片段可以在这里找到<a class="ae kv" href="https://github.com/kjarvis1905/vagrant-elk/blob/238be02e36148a569254d3a1768663ad52eaa50c/bootstrap/install/spark_install.sh#L31-L47" rel="noopener ugc nofollow" target="_blank"/>。您还会注意到，我在<code class="fe np nq nr ns b">.bashrc</code>文件中添加了一个<code class="fe np nq nr ns b">export</code>语句，定义了一个名为<code class="fe np nq nr ns b">ES_HDP_JAR</code>的环境变量。将该变量的值回显到控制台，以查看实例化会话时需要传递给Spark的jar文件的路径。正是因为包含了这个jar文件，我们才可以直接使用PySpark DataFrameWriter对象中的索引数据。我们可以利用Python中的环境变量来轻松配置新的SparkSession:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="d492" class="mp lt iq ns b gy ob oc l od oe">(examples) vagrant@vagrant:/vagrant/examples$ python<br/>Python 3.5.2 (default, Jul 17 2020, 14:04:10)<br/>[GCC 5.4.0 20160609] on linux<br/>Type "help", "copyright", "credits" or "license" for more information.<br/>&gt;&gt;&gt; import os<br/>&gt;&gt;&gt; from pyspark import SparkConf<br/>&gt;&gt;&gt; from pyspark.sql import SparkSession<br/>&gt;&gt;&gt; sconf = SparkConf().set('spark.jars', os.environ['ES_HDP_JAR'])<br/>&gt;&gt;&gt; ss = SparkSession.builder.config(conf=sconf).getOrCreate()</span></pre><p id="8398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将索引使用前面步骤中的<code class="fe np nq nr ns b">accounts_edited.json</code>数据创建的数据帧。我们需要运行的命令是:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="a3d6" class="mp lt iq ns b gy ob oc l od oe">&gt;&gt;&gt; df.write\<br/>... .format('org.elasticsearch.spark.sql')\<br/>... .option('es.nodes', 'localhost')\<br/>... .option('es.port', 9200)\<br/>... .option('es.resource', 'sparkindex')\<br/>... .save()</span></pre><p id="f429" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的命令使用数据帧<code class="fe np nq nr ns b">df</code>的内容创建一个新的索引<code class="fe np nq nr ns b">sparkindex</code>。要查看这个新索引的内容，请切换回Kibana控制台(或使用等效的命令行),并使用带有空“match_all”子句的搜索API:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="57de" class="mp lt iq ns b gy ob oc l od oe">GET /sparkindex/_search<br/>{<br/>  "query": {<br/>    "match_all": {}<br/>  }<br/>}</span></pre><p id="d51c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，ElasticSearch文档包含了与上面类似的示例，带有“复制为cURL”选项，因此您可以看到如何使用cURL实现相同的结果。</p><p id="21b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">响应的正文将包含一些匹配的文档:</p><pre class="kg kh ki kj gt nx ns ny nz aw oa bi"><span id="95d6" class="mp lt iq ns b gy ob oc l od oe">"hits" : [<br/>      {<br/>        "_index" : "sparkindex",<br/>        "_type" : "_doc",<br/>        "_id" : "PFapu3MB4cJ_QglfiKLK",<br/>        "_score" : 1.0,<br/>        "_source" : {<br/>          "account_number" : 1,<br/>          "address" : "880 Holmes Lane",<br/>          "age" : 32,<br/>          "balance" : 39225,<br/>          "city" : "Brogan",<br/>          "email" : "<a class="ae kv" href="mailto:amberduke@pyrami.com" rel="noopener ugc nofollow" target="_blank">amberduke@pyrami.com</a>",<br/>          "employer" : "Pyrami",<br/>          "firstname" : "Amber",<br/>          "gender" : "M",<br/>          "lastname" : "Duke",<br/>          "state" : "IL"<br/>        }<br/>      }<br/>]</span></pre><p id="f385" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，因为我们没有指定一个列作为惟一的文档标识符，所以已经代表我们生成了<code class="fe np nq nr ns b">_id</code>字段。</p><p id="46aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#7" rel="noopener ugc nofollow">返回目录</a></p><h1 id="ebea" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">6.捷径</h1><p id="1c2b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在<code class="fe np nq nr ns b">/vagrant/examples</code>中输入<code class="fe np nq nr ns b">pipenv shell</code>，然后输入<code class="fe np nq nr ns b">spark-es-demo --help</code>，您会看到有几个命令可以运行:</p><p id="e250" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">spark-es-demo make-spark-dataframe</code> (7.a .)</p><p id="0931" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">spark-es-demo spark-to-es</code> (7.b)</p><p id="43e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">spark-es-demo search-es-index</code> (7。c)</p><p id="8d62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行这些并检查各自的代码(在<code class="fe np nq nr ns b">/vagrant/examples/examples/spark_demo.py</code>中可以找到)应该会给你一些有用的、有文档记录的例子来参考。</p><p id="60e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(7.a)将下载一些演示数据，并将其放入Spark数据帧中。</p><p id="3c13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(7.b)将带您了解通过Spark DataFrame将数据放入弹性集群的整个过程。</p><p id="915d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(7.c)将允许您编写一个基本的匹配查询并提交给一个ES集群。默认情况下，它会寻找一个名为<code class="fe np nq nr ns b">sparkindex</code>的索引，运行(7。b)将创建它，并将一些数据放入其中，以便该命令可以成功运行。运行带有<code class="fe np nq nr ns b">--match-all</code>选项的命令将提交一个匹配所有查询并返回一些结果，这样您就可以看到一些从es索引返回的文档示例。系统会提示您选择要搜索的字段，然后要求您输入要搜索的值。将返回实际结果的一些组合有:</p><p id="1df5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">0. firstname</code>接着是<code class="fe np nq nr ns b">Nanette</code>(感谢Nanette)</p><p id="96b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe np nq nr ns b">3. employer</code>接着是<code class="fe np nq nr ns b">Anocha</code>(感谢Anocha)</p><p id="ec20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">试着找出一些其他的。</p><h1 id="e15a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">7.进一步练习</h1><p id="585a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在前面的章节中，我们简要介绍了如何使用一些关键的大数据技术。我们已经通过示例逐步了解了如何:</p><ol class=""><li id="fbe7" class="nb nc iq ky b kz la lc ld lf nt lj nu ln nv lr ng nh ni nj bi translated">使用ElasticSearch REST API索引和查询数据。</li><li id="5a60" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">使用Kibana及其开发工具和控制台与ElasticSearch集群进行交互。</li><li id="a4c8" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">使用Python配置SparkSession，并使用ElasticSearch Hadoop接口将数据直接从Spark数据帧索引到ElasticSearch索引。</li><li id="27ba" class="nb nc iq ky b kz nk lc nl lf nm lj nn ln no lr ng nh ni nj bi translated">通过使用Python的urllib编写请求来搜索ES索引。</li></ol><p id="9939" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这里开始，您可以使用我们的vagger开发环境来磨练您的大数据技能:熟练使用Lucene，这是ElasticSearch用来编写搜索并返回与我们的查询匹配的文档的库，使用Kibana来构建仪表板以可视化和探索您最喜欢的数据集，构建ETL管道以将转换的数据集写入ElasticSearch索引，等等。</p></div></div>    
</body>
</html>