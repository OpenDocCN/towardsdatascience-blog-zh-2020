<html>
<head>
<title>Dimensionality Reduction Forensics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维取证</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-forensics-50014430767f?source=collection_archive---------23-----------------------#2020-10-24">https://towardsdatascience.com/dimensionality-reduction-forensics-50014430767f?source=collection_archive---------23-----------------------#2020-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="52cb" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="6454" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">何时应用哪种降维技术</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7c3251a8a477e9ff2232aca9d273e894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FsDYEgjvIZzzcPnGOWNsvg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由Pixabay上的<a class="ae le" href="https://pixabay.com/de/illustrations/tatort-band-szene-kriminalit%C3%A4t-999123/" rel="noopener ugc nofollow" target="_blank">路缘石</a>拍摄，编辑为me :D</p></figure><h1 id="b2a6" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="6052" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在我上一篇关于Gap-Statistics的博文中，我解释了大多数时候数据都有很多特征(数百个甚至有时数千个)。我们认为具有100个特征的数据集是100维的。所以每个特征代表数据集的一个维度。对于可视化数据来说，这是相当多的维度。我们不能只保留三个特征，不是吗？当然<strong class="lz ja">不是</strong>！一个例外是，100个特征中有98个高度相关(通过热图找出)，所以我们省略了97个，只取其中的一个。然后我们会得到一个三维数据集。<br/> <strong class="lz ja">但这种可能性极小！</strong></p><p id="3a58" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，我将介绍两个在处理数据时需要了解的重要主题:</p><ul class=""><li id="b8d7" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">1)维度的诅咒——更深入的解释</li><li id="6af8" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">2)降维取证</li></ul><h1 id="b835" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">1)维度的诅咒</h1><blockquote class="nm"><p id="f54a" class="nn no iq bd np nq nr ns nt nu nv ms dk translated">直观但重要的是要知道，所有模式分析或数据探索任务的复杂性随着维度的增加而增加。</p></blockquote><p id="ca68" class="pw-post-body-paragraph lx ly iq lz b ma nw ka mc md nx kd mf mg ny mi mj mk nz mm mn mo oa mq mr ms ij bi translated">我们称这种复杂性的增加为维度的诅咒。通常一些特征是完全无用的或者高度相关的。因此，可以应用降维技术来去除尽可能多的维度。我们真的想处理尽可能低维度的数据。</p><p id="57d8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在处理高维数据时，我们可以推断出三个问题:</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="56b2" class="oi lg iq bd lh oj ok dn ll ol om dp lp mg on oo lr mk op oq lt mo or os lv iw bi translated">第一期:</h2><p id="da03" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><strong class="lz ja">如何绘制三维以上的数据？</strong></p><blockquote class="nm"><p id="7d9b" class="nn no iq bd np nq nr ns nt nu nv ms dk translated">是的，我们甚至可以在4D或5D的空间里进行策划，但是升级很快。</p></blockquote><p id="ed7b" class="pw-post-body-paragraph lx ly iq lz b ma nw ka mc md nx kd mf mg ny mi mj mk nz mm mn mo oa mq mr ms ij bi translated">从技术上讲，有一些方法可以做到这一点。例如，我们可以改变数据点的大小来可视化第四维，或者我们可以额外引入一些颜色作为第五维。但是你可以看到升级的速度有多快，我们作为人类再也不能从5D的情节中读到任何有价值的东西了。所以这基本上是没用的，因为主要的一点是我们想从这些可视化中获取信息。</p><h2 id="d0b8" class="oi lg iq bd lh oj ok dn ll ol om dp lp mg on oo lr mk op oq lt mo or os lv iw bi translated">第二个问题:</h2><p id="35ed" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><strong class="lz ja">特征空间的细分随着维度的增加呈指数上升。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/e8e61ce31775794e4e554648c68d8de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hv-M5Y_9bDgRuznjxezh_w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1)特征空间</p></figure><p id="122f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在上图最右边的图中，我们看到，在三维空间中，我们已经有27个大小相等的细胞。对于15个特征(=15个维度)，我们最终将在特征空间中拥有1500万个大小相等的细胞(3^15)。随机森林需要大量的参数来适应这个高维特征空间。如果你知道随机森林是如何工作的，那么你就知道它将特征空间分割成不同的分类区域，如果它是随机森林分类器而不是随机森林回归器的话。但是其他分类方法也需要不可行的更多参数来适应这样的高维空间。</p><h2 id="ca2a" class="oi lg iq bd lh oj ok dn ll ol om dp lp mg on oo lr mk op oq lt mo or os lv iw bi translated"><strong class="ak">第三期:</strong></h2><p id="e337" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这可能是最重要的，但也是最令人惊讶的问题:</p><blockquote class="nm"><p id="f8a7" class="nn no iq bd np nq nr ns nt nu nv ms dk translated"><strong class="ak">维度越高，位于特征空间最外边界的数据点越多。</strong></p></blockquote><figure class="ov ow ox oy oz kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/d2f4b5228e3ede4827dc5e1967933fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8BwMfMk8xFI44GWOZu8Sg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2)边界行为|来自HTF第2.5节</p></figure><p id="2f55" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">从上面的图2中我们可以看到计算<strong class="lz ja">的公式，其中</strong>数据位于特征空间中。<strong class="lz ja"> D </strong>是尺寸的数量，而<strong class="lz ja"> ε </strong>表示边界壳的极限。如果选择<strong class="lz ja"> ε </strong>为0.01，表示有多少个数据点位于特征空间的外侧<strong class="lz ja"> 1% </strong>。ε = 0.75意味着有多少数据点位于特征空间的外部<strong class="lz ja"> 75% </strong>。上面的绿色圆圈代表特征空间，其中<strong class="lz ja"> ε </strong>表示它总是从边缘开始向圆圈的中心<strong class="lz ja">移动。</strong></p><p id="bdd7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，回到图1我们可以看到，对于<strong class="lz ja"> 200 </strong>的维度，我们有<strong class="lz ja"> 86% </strong>的数据点位于特征空间的外层<strong class="lz ja"> 1% </strong>。而在3D空间中，3%的数据位于1%的外壳上。</p><blockquote class="nm"><p id="2500" class="nn no iq bd np nq nr ns nt nu nv ms dk translated">这太疯狂了，不是吗？</p></blockquote><p id="77bc" class="pw-post-body-paragraph lx ly iq lz b ma nw ka mc md nx kd mf mg ny mi mj mk nz mm mn mo oa mq mr ms ij bi translated">特征空间的稀疏性随着维度的增加而显著增加。我们将需要大量的新数据样本来维持特征空间的恒定密度。但是，除非你使用一些非常奇特的生成模型，如甘的模型，否则你无法轻松地生成新的数据样本。</p><h1 id="8c1b" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">2)降维取证</h1><p id="4c91" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这一节中，我们将介绍四种降维技术。我对它们中的每一个都进行了简要的概述，但是仅仅是为了理解如何执行取证。不是每种维度技术都适合每种数据分布。在选择四种方法中的一种时需要非常小心，因为如果你选择了错误的技术，你会惊讶地发现它并没有像预期的那样起作用。我们需要区分全局和局部降维技术。这是我们将要介绍的四种技术:</p><ul class=""><li id="de92" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">1) PCA(全球)</li><li id="9f9a" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">2) MDS(全球)</li><li id="80db" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">3) ISOMAP(本地)</li><li id="475c" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">4)拉普拉斯特征映射(局部)</li></ul><p id="6516" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">有更多的方法，但让我们把重点放在我认为基本上是主要的方法上。最先进的t-SNE没有包括在内，因为它会打破这个帖子的限制。</p><h2 id="20f7" class="oi lg iq bd lh oj ok dn ll ol om dp lp mg on oo lr mk op oq lt mo or os lv iw bi translated">主成分分析</h2><p id="826d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">当您不知道需要使用哪种技术时，主成分分析(PCA)总是首选。对于许多数据科学家来说，这是一项繁重的工作，它通常会产生正确的结果，但也有失败的情况。我提到过PCA是一种全球性的技术。这意味着PCA仅通过矩阵乘法计算，然后将数据投影到新的轴上。</p><blockquote class="pa pb pc"><p id="d572" class="lx ly pd lz b ma mt ka mc md mu kd mf pe mv mi mj pf mw mm mn pg mx mq mr ms ij bi translated"><strong class="lz ja">重要提示</strong>:投影的每一个轴都是相互正交的。因此，您可以将图1中的基本笛卡尔坐标系想象为投影轴。</p></blockquote><p id="5faa" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">基本步骤是:</p><ol class=""><li id="1cc9" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms ph ne nf ng bi translated">将数据标准化为平均值0(第5行)</li><li id="38e3" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">对数据的协方差矩阵执行特征分解(第9行)</li><li id="ba04" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">按照特征值的大小降序排列特征向量(第11–15行)</li><li id="da8d" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">选择最大特征值的个数，将数据投影到特征向量上，将其作为新的轴。(第19–21行)</li></ol><p id="d4ea" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这四个步骤没什么特别的。它实际上只是几行Python代码:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pi pj l"/></div></figure><pre class="kp kq kr ks gt pk pl pm pn aw po bi"><span id="9fa1" class="oi lg iq pl b gy pp pq l pr ps">92.46+0.00j% explained variance in vector 0<br/>5.30+0.00j% explained variance in vector 1<br/>1.72+0.00j% explained variance in vector 2<br/>0.52+0.00j% explained variance in vector 3</span></pre><p id="c1e7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">你可以看到全局降维技术有另一个非常有用的特点，这是局部技术所没有的。全局方法揭示了在应用降维之后数据保留了多少差异。<em class="pd">虹膜数据集</em>是一个众所周知的尝试降维的数据集。它有四个维度，但只保留一个维度仍然保持数据总方差的<strong class="lz ja"> 92.46% </strong>(从上面的灰色输出框“92.46+0.00j”-&gt;可以看出，这是一个复数表示)。这意味着92.46%的信息仍然只保存在一个维度内。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/aa7848d5d0758749dc0c5c5bcd7cfb8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*cF2Fiu_ruPt0f4PCIigeaA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3)我制作的PCA示例</p></figure><p id="f08d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">左图显示了PCA的另一个例子。请注意每个红色数据点是如何投影到新的蓝色轴上的。这将保留红点所有位置信息的96.62%。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/5a8de764c1bd424df2411a8f317d2164.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*jtVOQGWlitfkcFWLHANCoQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4)我制作的1D主成分分析投影示例</p></figure><p id="0dd9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我使用了具有两个不同聚类的数据集。在减少了从2D到1D的维度后，即使将它们投射到蓝色箭头上，星团之间仍然保持着距离。</p><p id="88de" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">结论</strong>:我们只能用PCA将数据线性投影到特征向量轴上。请记住这一点。你能想象2D数据分布很难线性映射并保留大部分信息吗？</p><h1 id="4a73" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">2) MDS</h1><p id="3736" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">经典的多维标度(MDS)将更简单，因为它非常类似于PCA。MDS和PCA之间的唯一区别是，MDS将点之间的距离作为输入。如果欧几里德距离范数用于MDS，<strong class="lz ja">它的行为100%类似于PCA </strong>。因此，MDS也能够测量保留了多少差异。保存的公式是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pv"><img src="../Images/86019a9da6723a6d8a2ae195719d6b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTCu-ibxZItSFNjrHxvOyw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5)方差的最大保存</p></figure><p id="ecf5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">请注意，d’(d撇)是缩减的维度(例如1D)，d是初始维度(例如3D)。d* (d星)是最佳降维。我们可以将所有的目标维度方差相加，然后除以100，因为它从100%方差开始。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pw"><img src="../Images/78bea1128099c9fa697b5bc94030b166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYvfubKTqtH7wovjbj9cPw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6)弯头方法</p></figure><p id="3017" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">当你阅读我上一篇关于差距统计的博文时，你会注意到上面的这个数字是多么的相似。肘方法可用于可视化方差如何随着维度的减少而缩小。x轴表示维度的减少。例如，在x=2时，初始维度减少了两个维度，我们仍然保留了大约80%的总方差。最大的差距表明我们的最佳缩减结果可能是在<strong class="lz ja">只缩减两个维度，</strong>因为我们通过缩减三个维度的初始数据丢失了如此多的方差。</p><p id="24b1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">当对MDS使用不同的距离度量时</p><ul class=""><li id="f78f" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">欧几里德距离</li><li id="018e" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">曼哈顿距离</li><li id="683a" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">汉明距离</li><li id="c8fc" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">LogFC</li></ul><p id="aa64" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">很难知道最佳指标是什么。当您确定全局降维技术足以完成您的任务时，您可以尝试许多不同的距离度量，并绘制肘方法图，以查看哪种最小维度可以保持最大的差异。</p><p id="3173" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">结论:</strong>由于使用欧几里德距离时MDS与PCA相同，所以可以直接选择MDS而非PCA。然后你很快就能使用不同的测试距离指标。</p><h1 id="3ca8" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">PCA和MDS的线性取证</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi px"><img src="../Images/c4bf864fa38199c2bb4320b63f15360f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VmuMx7GLx1lJQ-koubeKuQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图7)线性取证</p></figure><p id="4f6d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">让我们看看上面图7中的数据分布。它看起来像蜗牛的壳。无论我们如何将数据投射到绿色的主成分线(或MDS线)上，我们都<strong class="lz ja">永远</strong>不能保留太多的方差。上图的右部显示了数据如何理想地投影到绿线上。这并没有给我们带来任何进展。</p><p id="9a9b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这同样适用于非常高维度的空间。我们真的无法想象100维的特征空间会是什么样子，但同样的原理也适用。有些分布不能用线性降维技术来解释。</p><p id="1bff" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，当我们看到降维揭示了对潜在数据聚类的零洞察时，如上所述，我们可以肯定PCA或MDS不是正确的选择。</p><h1 id="f2b3" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">3) ISOMAP</h1><p id="6788" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">先说局部降维。我会给你算法，然后我会解释它:</p><ol class=""><li id="3c5f" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms ph ne nf ng bi translated">使用K最近邻算法(kNN)定义局部邻域。</li><li id="b6a3" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">用欧几里得距离计算所有点到另一个点的距离，以得到我们的图在所有点之间的边。</li><li id="5f4e" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">使用Dijktra算法计算所有点到另一个点的最短距离。</li><li id="8e60" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">使用此最短距离矩阵作为MDS算法的距离度量。</li></ol><p id="22f7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">啊，我们亲爱的MDS。这么有用！kNN需要一个固定的超参数，即K的数量。问题是，我们不能真正知道K的最佳值是多少。因此，由于kNN的半径变得太大，局部邻域可能包括并不真正属于它的点，因为存在太稀疏的数据点。应用Dijkstras的算法并使用这个距离矩阵作为MDS的输入很简单，但是困难在于这四个步骤中的第一步。</p><h1 id="a258" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">ISOMAP的全球取证</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi py"><img src="../Images/7d91b2bb45dd2163c5984736c0ba5be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYdOu-CKFJxn0AWzjOVJvg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图8)全球取证第1部分</p></figure><p id="a5b0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">让我们考虑一下图8最左侧上方底部开放的蛋形数据分布。如果我们能沿着绿线投影数据，那将是最好的降维映射。数据分布的本地结构将被保留。紫色圆圈表示两个<strong class="lz ja">本地邻域</strong>的例子。</p><p id="5c65" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">你可以这样想象:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pz"><img src="../Images/24fd5445b775758733c85170d7a4f655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMwYm8XjKog65514r7Bjdw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图9)全球取证第2部分</p></figure><p id="e54d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这是最理想的情况。如果数据分布看起来像这样，它将完全工作。但是我已经提到了ISOMAP的问题行为。如果你仔细观察图8最右边的图，你会发现有两个新的蓝色数据点。这些可能是由于样本噪声造成的。这导致投射到绿线上的致命变化。</p><p id="a4bd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">结论</strong>:由于kNN，ISOMAP对噪声样本非常敏感。因为kNN对于每个局部邻域的半径是不同的，因为它想要例如在它的局部邻域中找到5个点，而不管这些点位于多远或多近。这就产生了问题，并成为图8中A点到B点的桥梁，尽管这些点应该相距很远。</p><h1 id="ec44" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">4)拉普拉斯特征映射</h1><p id="d91c" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我也将从编写LE的基本算法开始，然后我将解释它:</p><ol class=""><li id="92f2" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms ph ne nf ng bi translated">定义局部邻域，并根据所有数据点构建邻接图。</li><li id="6020" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">测量图中各边的权重</li><li id="892a" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">对该图进行特征分解(如PCA)。</li><li id="4762" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms ph ne nf ng bi translated">像PCA一样，通过选择最大特征值的数量来执行低维嵌入，并且相对于它们相应的特征向量来映射数据。</li></ol><p id="1df7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">邻接图基本上只是表示图中所有邻域点位置的矩阵(亲和矩阵),但是在一个矩阵内。这个<strong class="lz ja">亲和矩阵</strong>看起来像这样(它是二进制的，但是它也可以用除了二进制热核之外的其他核来计算):</p><h1 id="566a" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">拉普拉斯特征映射的全局取证</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qa"><img src="../Images/a67125998c5e6f1aa7682da7cedf5655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ow38swxTbUHdoUJFXaDSVg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图10)全球取证第3部分</p></figure><p id="29c3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> W </strong>表示二元亲和矩阵。上面的W是没有噪声的数据分布，而下面的<strong class="lz ja"> W </strong>包括一些绿色噪声样本点。你可以看到，嘈杂的点想要再次创建一个从A到B的桥梁，但亲和矩阵阻止了这种情况的发生，因为紫色的局部邻域键在对角线上太强了。噪声对拉普拉斯特征映射没有影响。优化的目的是使局部邻域尽可能靠近(因此邻域的距离尽可能小)。</p><p id="6a70" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，对于拉普拉斯特征映射会有最小化问题，但是对于PCA、MDS和ISOMAP会有最大化问题(这三个想要最大化数据的方差信息)。</p><p id="a953" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">结论</strong>:LE本质上更适合执行这种流形学习(=学习低维嵌入)，因为它对于噪声数据分布更鲁棒。正如我们所知，数据中经常存在噪声。在所有情况下，LE都优于ISOMAP。</p><h1 id="fc31" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">进一步的调查</h1><p id="98ce" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">最后，让我们考虑一些测试示例:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qb"><img src="../Images/6bd5b79563929bce585ab290326396d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aN5cqVUSWiqUEzj-BhFDMQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图11)进一步调查</p></figure><ul class=""><li id="7c20" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">在第一个图中，我们可以很容易地拟合一条线或一个邻居通过。</li><li id="8e76" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">对于第二个地块来说，附近可能会有问题，因此LE是安全的选择。</li><li id="b663" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">第三个图也将保留足够的方差，即使在拟合直线时也是如此。ISOMAP和LE也没有问题，因为没有样本噪声和容易的局部邻域</li><li id="dcca" class="my mz iq lz b ma nh md ni mg nj mk nk mo nl ms nd ne nf ng bi translated">最后一个例子用绿色显示地面实况。当我们想从中取样时，有噪声的样本可能会使地面实况向不希望的方向移动。这就是为什么我们真的想使用LE，因为ISOMAP可能会受到噪声的负面影响，从而产生错误的降维。</li></ul><p id="586c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我希望你发现这篇文章内容丰富！如果你有问题，可以给我发邮件。你可以在我的<a class="ae le" href="https://timloehr.me/" rel="noopener ugc nofollow" target="_blank">网站</a>上找到我的邮箱地址。</p><h1 id="d2c3" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><p id="325e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1] Trevor Hastie，Robert Tibshirani和Jerome Friedman，<a class="ae le" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">统计学习的要素:数据挖掘、推理和预测</a> (2009)</p><p id="d31e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这篇博文基于从弗里德里希·亚历山大大学埃尔兰根-纽伦堡的课程模式分析中获得的知识。我使用了Christian Riess博士演讲的部分内容来说明这篇文章的例子。所以所有权利归克里斯汀·里斯博士所有。</p></div></div>    
</body>
</html>