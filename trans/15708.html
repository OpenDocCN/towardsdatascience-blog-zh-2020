<html>
<head>
<title>SparkSession vs SparkContext vs SQLContext</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">spark session vs spark context vs SQLContext</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a?source=collection_archive---------1-----------------------#2020-10-29">https://towardsdatascience.com/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a?source=collection_archive---------1-----------------------#2020-10-29</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="1e4c" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">SparkSession、SparkContext和SQLContext有什么区别？</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/1fce2337acfff48e5c04bc7dbece4078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GRYb0dNBFx6RRHCL"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/photos/PC_lbSSxCZE" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kz" href="https://unsplash.com/@krisroller" rel="noopener ugc nofollow" target="_blank">克里斯托佛罗拉</a>拍摄的照片</p></figure><p id="eab7" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在大数据时代，Apache Spark可能是最受欢迎的技术之一，因为它提供了一个统一的引擎，可以在合理的时间内处理大量数据。</p><p id="f0a9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本文中，我将介绍Spark应用程序的各种入口点，以及这些入口点是如何随着发布的版本而发展的。在这样做之前，浏览一些基本概念和术语可能是有用的，这样我们就可以更容易地跳转到入口点，即SparkSession、SparkContext或SQLContext。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h1 id="90ed" class="md me iu bd mf mg mh mi mj mk ml mm mn ka mo kb mp kd mq ke mr kg ms kh mt mu bi translated">Spark基本架构和术语</h1><p id="bdd2" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">Spark应用程序由集群上的一个<strong class="lc iv">驱动程序</strong>和一组<strong class="lc iv">执行器</strong>组成。驱动程序是一个进程，它执行Spark应用程序的主程序，并创建协调作业执行的<strong class="lc iv"> SparkContext </strong>(稍后详细介绍)<strong class="lc iv">。</strong>执行器是运行在集群工作节点上的进程，负责执行驱动程序进程分配给它们的<strong class="lc iv">任务</strong>。</p><p id="5194" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">集群管理器(如Mesos或YARN)负责将物理资源分配给Spark应用程序。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj na"><img src="../Images/325fd5fd9fa8215d8de7c4eb88bfcf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*XwWZAbmHF75xWRDR.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图片取自<a class="ae kz" href="https://spark.apache.org/docs/latest/cluster-overview.html#components" rel="noopener ugc nofollow" target="_blank"> Apache Spark文档</a></p></figure><h1 id="c0b3" class="md me iu bd mf mg nb mi mj mk nc mm mn ka nd kb mp kd ne ke mr kg nf kh mt mu bi translated">入口点</h1><p id="c6ca" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">每个Spark应用程序都需要一个入口点，允许它与数据源通信并执行某些操作，比如读写数据。在Spark <strong class="lc iv"> 1.x </strong>中，引入了三个入口点:<strong class="lc iv"> SparkContext </strong>、<strong class="lc iv"> SQLContext </strong>和<strong class="lc iv"> HiveContext。</strong>自从<strong class="lc iv"> Spark 2.x </strong>以来，引入了一个名为<strong class="lc iv"> SparkSession </strong>的新入口点，该入口点基本上结合了上述三个上下文中的所有可用功能。请注意，即使在最新的Spark版本中，所有上下文仍然可用，主要是为了向后兼容。</p><p id="7fb9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在接下来的部分中，我将讨论上述入口点的目的以及它们之间的区别。</p><h1 id="4064" class="md me iu bd mf mg nb mi mj mk nc mm mn ka nd kb mp kd ne ke mr kg nf kh mt mu bi translated">SparkContext、SQLContext和HiveContext</h1><p id="68eb" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">如前所述，Spark的最早版本提供了这三个入口点，每个入口点都有不同的用途。</p><h2 id="3149" class="ng me iu bd mf nh ni dn mj nj nk dp mn lj nl nm mp ln nn no mr lr np nq mt nr bi translated"><strong class="ak">火花上下文</strong></h2><p id="2319" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">Spark应用程序的驱动程序进程使用<strong class="lc iv"> SparkContext </strong>来建立与集群和资源管理器的通信，以便协调和执行作业。SparkContext还允许访问其他两个上下文，即SQLContext和HiveContext(稍后将详细介绍这些入口点)。</p><p id="ad3e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了创建SparkContext，首先需要创建一个Spark配置(<em class="ns"> SparkConf </em>)，如下所示:</p><pre class="kk kl km kn gu nt nu nv nw aw nx bi"><span id="2204" class="ng me iu nu b gz ny nz l oa ob">// Scala</span><span id="2dee" class="ng me iu nu b gz oc nz l oa ob">import org.apache.spark.{SparkContext, SparkConf}</span><span id="0583" class="ng me iu nu b gz oc nz l oa ob">val sparkConf = new SparkConf() \<br/>    .setAppName("app") \<br/>    .setMaster("yarn")<br/>val sc = new SparkContext(sparkConf)</span></pre></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><pre class="nt nu nv nw aw nx bi"><span id="03a6" class="ng me iu nu b gz od oe of og oh nz l oa ob"># PySpark</span><span id="c298" class="ng me iu nu b gz oc nz l oa ob">from pyspark import SparkContext, SparkConf</span><span id="b352" class="ng me iu nu b gz oc nz l oa ob">conf = SparkConf() \<br/>    .setAppName('app') \<br/>    .setMaster(master)<br/>sc = SparkContext(conf=conf)</span></pre><p id="d16a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><em class="ns">注意，如果你使用spark-shell，SparkContext已经可以通过变量</em> <strong class="lc iv"> <em class="ns"> sc获得。</em>T9】</strong></p><h2 id="53ea" class="ng me iu bd mf nh ni dn mj nj nk dp mn lj nl nm mp ln nn no mr lr np nq mt nr bi translated"><strong class="ak"> SQLContext </strong></h2><p id="47ef" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated"><strong class="lc iv"> SQLContext </strong>是<em class="ns"> SparkSQL </em>的入口点，SparkSQL是用于结构化数据处理的Spark模块。一旦sqlContext被初始化，用户就可以使用它对数据集和数据帧执行各种“类似SQL”的操作。</p><p id="cd05" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了创建SQLContext，首先需要实例化SparkContext，如下所示:</p><pre class="kk kl km kn gu nt nu nv nw aw nx bi"><span id="41d2" class="ng me iu nu b gz ny nz l oa ob">// Scala</span><span id="77b5" class="ng me iu nu b gz oc nz l oa ob">import org.apache.spark.{SparkContext, SparkConf}<br/>import org.apache.spark.sql.SQLContext</span><span id="3a81" class="ng me iu nu b gz oc nz l oa ob">val sparkConf = new SparkConf() \<br/>    .setAppName("app") \<br/>    .setMaster("yarn")</span><span id="8aeb" class="ng me iu nu b gz oc nz l oa ob">val sc = new SparkContext(sparkConf)<br/>val sqlContext = new SQLContext(sc)</span></pre></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><pre class="nt nu nv nw aw nx bi"><span id="bcf0" class="ng me iu nu b gz od oe of og oh nz l oa ob"># PySpark</span><span id="b209" class="ng me iu nu b gz oc nz l oa ob">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SQLContext</span><span id="ab65" class="ng me iu nu b gz oc nz l oa ob">conf = SparkConf() \<br/>    .setAppName('app') \<br/>    .setMaster(master)</span><span id="e11e" class="ng me iu nu b gz oc nz l oa ob">sc = SparkContext(conf=conf)<br/>sql_context = SQLContext(sc)</span></pre><h2 id="d6c5" class="ng me iu bd mf nh ni dn mj nj nk dp mn lj nl nm mp ln nn no mr lr np nq mt nr bi translated"><strong class="ak"> HiveContext </strong></h2><p id="195d" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">如果您的Spark应用程序需要与Hive通信，并且您正在使用Spark &lt; 2.0 then you will probably need a <strong class="lc iv"> HiveContext </strong> if。对于Spark 1.5+，HiveContext还提供了对窗口函数的支持。</p><pre class="kk kl km kn gu nt nu nv nw aw nx bi"><span id="8569" class="ng me iu nu b gz ny nz l oa ob">// Scala</span><span id="f432" class="ng me iu nu b gz oc nz l oa ob">import org.apache.spark.{SparkConf, SparkContext}<br/>import org.apache.spark.sql.hive.HiveContext</span><span id="64e3" class="ng me iu nu b gz oc nz l oa ob">val sparkConf = new SparkConf() \<br/>    .setAppName("app") \<br/>    .setMaster("yarn")</span><span id="1fc8" class="ng me iu nu b gz oc nz l oa ob">val sc = new SparkContext(sparkConf)<br/>val hiveContext = new HiveContext(sc)</span><span id="ac69" class="ng me iu nu b gz oc nz l oa ob">hiveContext.sql("select * from tableName limit 0")</span></pre></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><pre class="nt nu nv nw aw nx bi"><span id="f269" class="ng me iu nu b gz od oe of og oh nz l oa ob"># PySpark</span><span id="5ee8" class="ng me iu nu b gz oc nz l oa ob">from pyspark import SparkContext, HiveContext</span><span id="a32f" class="ng me iu nu b gz oc nz l oa ob">conf = SparkConf() \<br/>    .setAppName('app') \<br/>    .setMaster(master)</span><span id="ef2a" class="ng me iu nu b gz oc nz l oa ob">sc = SparkContext(conf)<br/>hive_context = HiveContext(sc)<br/>hive_context.sql("select * from tableName limit 0")</span></pre><p id="cbbf" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">从Spark 2.x+开始，两个新增功能使得HiveContext变得多余:</p><p id="70cb" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">a)引入了SparkSession，它也提供了配置单元支持</p><p id="32f2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">b)发布了本机窗口函数，并且基本上用本机Spark SQL UDAFs替换了配置单元udaf</p><h1 id="49d4" class="md me iu bd mf mg nb mi mj mk nc mm mn ka nd kb mp kd ne ke mr kg nf kh mt mu bi translated">火花会议</h1><p id="e614" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">Spark 2.0引入了一个名为<strong class="lc iv"> SparkSession </strong>的新入口点，基本上取代了SQLContext和HiveContext。此外，它让开发人员可以立即访问SparkContext。为了创建一个支持Hive的SparkSession，您所要做的就是</p><pre class="kk kl km kn gu nt nu nv nw aw nx bi"><span id="2f48" class="ng me iu nu b gz ny nz l oa ob">// Scala</span><span id="c41c" class="ng me iu nu b gz oc nz l oa ob">import org.apache.spark.sql.SparkSession</span><span id="487a" class="ng me iu nu b gz oc nz l oa ob">val sparkSession = SparkSession \<br/>    .builder() \<br/>    .appName("myApp") \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()</span><span id="ec15" class="ng me iu nu b gz oc nz l oa ob">// Two ways you can access spark context from spark session<br/>val spark_context = <!-- -->sparkSession<!-- -->._sc<br/>val spark_context = <!-- -->sparkSession<!-- -->.sparkContext</span></pre></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><pre class="nt nu nv nw aw nx bi"><span id="7254" class="ng me iu nu b gz od oe of og oh nz l oa ob"># PySpark</span><span id="1705" class="ng me iu nu b gz oc nz l oa ob">from pyspark.sql import SparkSession</span><span id="26c4" class="ng me iu nu b gz oc nz l oa ob">spark_session = SparkSession \<br/>    .builder \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()</span><span id="e079" class="ng me iu nu b gz oc nz l oa ob"># Two ways you can access spark context from spark session<br/>spark_context = spark_session._sc<br/>spark_context = spark_session.sparkContext</span></pre><h1 id="9ee8" class="md me iu bd mf mg nb mi mj mk nc mm mn ka nd kb mp kd ne ke mr kg nf kh mt mu bi translated"><strong class="ak">结论</strong></h1><p id="301d" class="pw-post-body-paragraph la lb iu lc b ld mv jv lf lg mw jy li lj mx ll lm ln my lp lq lr mz lt lu lv in bi translated">在本文中，我们讨论了SparkContext、SQLContext和HiveContext这些在Spark早期版本中可用的旧入口点。</p><p id="be5f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们还看到了最新的入口点SparkSession如何使其他三个上下文的实例化变得多余。如果你使用的是Spark 2.x+，那么你真的不应该担心HiveContext、SparkContext和SQLContext。您所要做的就是创建一个SparkSession，为Hive和类似sql的操作提供支持。此外，如果您出于任何原因需要访问SparkContext，您仍然可以通过SparkSession来完成，正如我们在前一个会话的示例中看到的那样。另一个需要注意的重要事情是，Spark 2.x附带了最初在HiveContext中引入的原生窗口函数。</p><p id="11d3" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">PS:如果你还没有使用Spark 2.x，我强烈建议你开始使用。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><p id="cdfa" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="lc iv">成为会员</strong> </a> <strong class="lc iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。</strong></p></div></div>    
</body>
</html>