<html>
<head>
<title>Visualizing Linear, Ridge, and Lasso Regression Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化线性、脊形和套索回归性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-linear-ridge-and-lasso-regression-performance-6dda7affa251?source=collection_archive---------24-----------------------#2020-10-28">https://towardsdatascience.com/visualizing-linear-ridge-and-lasso-regression-performance-6dda7affa251?source=collection_archive---------24-----------------------#2020-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2db1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用黄砖可视化分析模型性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/58adcc0a566341cc043165e6ecec8495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ax5VDzTKXWeD2Wid"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亨特·哈里特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习是对通过经验自动改进的计算机算法的研究。根据问题和我们正在处理的数据集，有大量的机器学习算法。</p><p id="b50f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习模型性能是选择特定模型的最重要因素。为了选择机器学习模型，我们可以查看某些指标，这些指标可以帮助我们选择具有最高准确性和最小误差的最佳模型。除了所有这些因素，显示模型性能的最重要因素是不同类型的可视化。我们可以使用预测误差和残差图等可视化工具来选择性能最佳的模型。</p><p id="4f2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Yellowbrick是一个开源python库/包，它扩展了Scikit-Learn API，使模型选择和超参数调整更加容易。在引擎盖下，它使用的是Matplotlib。</p><p id="1b19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将探讨如何使用可视化技术(即使用Yellowbrick创建的残差图和预测误差)来可视化线性、岭和套索回归的模型性能。</p><h1 id="1b1f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">安装Yellowbrick</h1><p id="afc3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">像任何其他库一样，我们将使用pip安装yellowbrick。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b93e" class="mx lw it mt b gy my mz l na nb">pip install yellowbrick</span></pre><h1 id="f890" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">导入所需的库</h1><p id="f7d8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们将使用sklearn库下定义的线性、岭和套索回归模型，除此之外，我们将导入yellowbrick进行可视化，并导入pandas来加载我们的数据集。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3083" class="mx lw it mt b gy my mz l na nb">from sklearn.linear_model import LinearRegression, Lasso, Ridge<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import train_test_split<br/>from yellowbrick.regressor import PredictionError, ResidualsPlot<br/>import pandas as pd</span></pre><h1 id="2768" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">正在加载数据集</h1><p id="d890" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我们将探索一个包含跨国公司销售数据的简单数据集，您可以使用任何包含回归相关数据的数据集。让我们加载数据集，看看有哪些属性。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="fede" class="mx lw it mt b gy my mz l na nb">df = pd.read_csv("Advertsisng.csv')<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/be5d06335c02fab1603feb8f2f8b8b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*HvYyApsXZ-sX1U7wE4D3Bg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集(来源:作者)</p></figure><h1 id="4012" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分割测试和训练数据</h1><p id="d63c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了将数据集分为测试和训练，我们首先需要定义特征和目标变量。定义功能和目标后，我们将使用标准缩放器缩放数据，然后使用sklearn拆分数据。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="df31" class="mx lw it mt b gy my mz l na nb">X=df.iloc[:,0:3]<br/>y=df.iloc[:,3]<br/>#Scaling The data<br/>scaler = StandardScaler()<br/>scaler.fit(X)<br/>X = scaler.transform(X)<br/>#Splitting the data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)</span></pre><h1 id="1012" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">创建模型可视化</h1><p id="b61f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们将创建模型并使用该模型来创建可视化。我们将创建的可视化是:</p><p id="77f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a.<strong class="lb iu">预测误差</strong></p><p id="029b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预测误差图显示了数据集的实际目标与模型生成的预测值的对比。这让我们可以看到模型中的方差有多大。</p><p id="20df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b.<strong class="lb iu">残差图</strong></p><p id="582a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回归模型的上下文中，残差是目标变量的观测值(y)和预测值(ŷ)之间的差，即预测的误差。残差图显示了垂直轴上的残差和水平轴上的因变量之间的差异。</p><h2 id="2cff" class="mx lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">1.线性回归</h2><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9e35" class="mx lw it mt b gy my mz l na nb">model1 = LinearRegression()<br/>visualizer = PredictionError(model1)<br/>visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/eb12ac6b0b1b4f2c4b55c40836bc1a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*WAs7tAtCH86eqfQwWEA94A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测误差(来源:作者)</p></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6250" class="mx lw it mt b gy my mz l na nb">visualizer = ResidualsPlot(model1)</span><span id="abca" class="mx lw it mt b gy np mz l na nb">visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/4eac7b91ffbedcfd75c69272d47a8eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*eYWuhjoEKu9SeDlKjDWLDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">残差图(来源:作者)</p></figure><h2 id="c4eb" class="mx lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">2.套索回归</h2><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ecf3" class="mx lw it mt b gy my mz l na nb">model2 = Lasso()<br/>visualizer = PredictionError(model2)<br/>visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c2eda7a8336b98eadd00c333e99e965f.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*moRyWVaCKk6MgMV-OHsqsQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">套索预测(来源:作者)</p></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="031a" class="mx lw it mt b gy my mz l na nb">model2 = Lasso()<br/>visualizer = ResidualsPlot(model2)<br/>visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3487e0c0b5370f60a45f904b246a6dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*9OOptsmolFDaNw3IXqWO2Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">套索残差(来源:作者)</p></figure><h2 id="7c14" class="mx lw it bd lx nd ne dn mb nf ng dp mf li nh ni mh lm nj nk mj lq nl nm ml nn bi translated">3.里脊回归</h2><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3be4" class="mx lw it mt b gy my mz l na nb">model3 = Ridge()<br/>visualizer = PredictionError(model3)<br/>visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/628a3a4dd50b188c489fcedaff3e4fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*DvNp0eeQXcYN4YPCitFEzw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">山脊预测(来源:作者)</p></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="df91" class="mx lw it mt b gy my mz l na nb">model3 = Ridge()<br/>visualizer = ResidualsPlot(model3)<br/>visualizer.fit(X_train, y_train)  <br/>visualizer.score(X_test, y_test)  <br/>visualizer.poof()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f9abb0b72a6b88f30ee85467f67a2eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*iLC0gsRnucDrlCoPx7xk2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">岭残差(来源:作者)</p></figure><p id="51ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们如何为不同的模型创建残差图和预测误差图。通过分析这些图，我们可以选择性能最佳的模型。</p><p id="a61d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们数据集的上述图中，我们可以看到，根据我们创建的图，性能最好的模型是线性回归，因此对于这个给定的数据集，我们将使用线性回归进行预测，因为它的准确性高，误差小。</p><p id="f43e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，我们可以将yellowbrick用于不同的机器学习模型性能可视化。你可以尝试不同的数据集，找到最适合你的数据的模型，尝试一下，让我知道你对这个故事的反应。</p><h1 id="3a92" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">在你走之前</h1><p id="2195" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu"> <em class="nv">感谢</em> </strong> <em class="nv">的阅读！如果你想与我取得联系，请随时通过hmix13@gmail.com联系我或我的</em> <a class="ae ky" href="http://www.linkedin.com/in/himanshusharmads" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="nv"> LinkedIn个人资料</em> </strong> </a> <em class="nv">。可以查看我的</em><a class="ae ky" href="https://github.com/hmix13" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nv">Github</em></strong><em class="nv"/></a><em class="nv">简介针对不同的数据科学项目和包教程。还有，随意探索</em> <a class="ae ky" href="https://medium.com/@hmix13" rel="noopener"> <strong class="lb iu"> <em class="nv">我的简介</em> </strong> </a> <em class="nv">，阅读我写过的与数据科学相关的不同文章。</em></p></div></div>    
</body>
</html>