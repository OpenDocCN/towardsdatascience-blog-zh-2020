<html>
<head>
<title>Tensorflow multi-worker training on Google Cloud AI Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google Cloud AI平台上的Tensorflow多工培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-worker-distributed-tensorflow-training-on-google-cloud-ai-platform-64b383341dd8?source=collection_archive---------23-----------------------#2020-11-14">https://towardsdatascience.com/multi-worker-distributed-tensorflow-training-on-google-cloud-ai-platform-64b383341dd8?source=collection_archive---------23-----------------------#2020-11-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="47ac" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div class="gh gi jw"><img src="../Images/356f889e5d03e39d3667275e7a3248b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*jpxVgOS2_A1RFK-Ixzuzzw.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">图片由作者在<a class="ae kh" href="https://creativecommons.org/licenses/by-nc/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-NC 4.0 </a>下</p></figure><p id="0d52" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">几乎在每个深度学习项目中，都有一个决定性的时刻，海量的训练数据或缺乏处理能力成为在适当的时间完成训练的限制因素。因此，应用适当的缩放是不可避免的。虽然纵向扩展，即升级到更强大的硬件，可能会提供暂时的补救措施，但它很少提供正确的可扩展性，因为纵向扩展会很快达到其物理极限。因此，我们没有其他选择，只能扩大模型训练，即使用额外的机器。幸运的是，在云时代，横向扩展不再是一个障碍。</p><p id="2bf0" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">谈到向外扩展神经网络训练，有两种主要方法。有一种叫做<strong class="kk ja">模型并行</strong>的策略，在这种策略中，神经网络本身被分割到多个设备上。这种类型的分布主要用于模型由大量参数组成并且不适合特定设备的情况，或者输入样本的大小甚至会妨碍计算激活的情况。对于那些对Tensorflow的模型并行感兴趣的人，有一个官方的解决方案叫做<a class="ae kh" href="https://github.com/tensorflow/mesh" rel="noopener ugc nofollow" target="_blank"> Mesh </a>。</p><p id="53c1" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">另一种分布策略称为<strong class="kk ja">数据并行</strong>，其中模型在每个节点上复制。然后，数据以这样的方式分布，每个副本看到它的不同部分。这种分布策略对于具有大量训练数据的情况特别有用。</p><p id="c2e4" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">由于每个副本在每一步都接收到不同的训练数据片段，因此工作人员必须有序地传达梯度更新。有两种沟通方式。有<strong class="kk ja">异步训练</strong>、<strong class="kk ja">、</strong>工人独立训练自己的复制品。异步训练通常使用离散参数服务器来报告梯度更新。</p><p id="5837" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">另一种是<strong class="kk ja">同步训练</strong>，在每一个训练步骤之后，工人们分享参数更新。同步训练经常实现全归约算法，这意味着所有副本存储归约(更新)的参数，而不是将它们传递到中央位置。</p><p id="7a24" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">异步训练的主要好处体现在避免最慢的工人空闲，以及稍微好一点的机器容错能力。主要缺点是参数服务器需要大量的网络带宽来与工作人员进行有效的通信。运行多个参数服务器可能会解决这个问题，但也会增加培训成本。</p><p id="ae37" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">同步训练的权衡正好相反。不需要使用额外的机器来存储更新的模型参数。另一方面，等待最慢的工人肯定会拖延进程。选择业绩相似的员工可能会减少闲置时间。采用高效的网络拓扑来共享梯度更新同样至关重要。工人之间多余的交流会抵消同步培训的好处。</p><h1 id="6bba" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">多工人分布式张量流</h1><p id="90ef" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">Tensorflow通过<code class="fe mj mk ml mm b">tf.distribute.Strategy</code> API在他们的标准库中实现数据并行策略。它们支持同步和异步多工人培训，尽管后者支持有限。截至撰写本文时，异步的<code class="fe mj mk ml mm b">ParameterServerStrategy</code>只支持原始的<code class="fe mj mk ml mm b">tf.estimator.Estimator</code> API。然而，从2.4版本开始，人们开始努力使异步培训变得更加可用。</p><p id="1abb" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">自从Tensorflow 2.0发布以来，<code class="fe mj mk ml mm b">tf.keras.Model</code> API已经成为构建神经网络的主要方式，尤其是那些不需要定制训练循环的神经网络。新开发的分布式培训策略同样主要集中在Keras模型上。尽管已经为Keras模型实现了几种分布式培训策略，但是在撰写本文时，当前可用的多工人培训仅仅是同步的<code class="fe mj mk ml mm b">MultiWorkerMirroredStrategy</code>。</p><p id="6412" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">这个策略为我们增加了很多负担。它会根据所用的硬件和网络拓扑来优化选择合适的all-reduce实现。目前，可用的选项有ring all-reduce和NVidia的<a class="ae kh" href="https://developer.nvidia.com/nccl" rel="noopener ugc nofollow" target="_blank"> NCLL </a>。在训练Keras模型的情况下，该策略还自动在工人中分发训练数据集。通过文件或数据对数据集进行<a class="ae kh" href="https://www.tensorflow.org/tutorials/distribute/input#sharding" rel="noopener ugc nofollow" target="_blank">分片，可以进行分发。按文件分片是首选，因为每个副本只加载分配的文件。例如，它可用于</a><a class="ae kh" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank"> TFRecord </a>数据集。在其中存储训练样本是高效处理大型数据集的最有效方式。另一方面，如果没有办法按文件进行分片，那么所有的工作人员都会读取所有可用的数据，但是他们只处理分配给他们的分片。例如，在<a class="ae kh" href="https://www.tensorflow.org/io/tutorials/bigquery" rel="noopener ugc nofollow" target="_blank">直接从BigQuery </a>中读取数据的情况下，或者文件数量少于workers的情况下，可能会发生日期分片。验证和测试数据集也是如此。</p><p id="9962" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">该策略还可以通过名为<code class="fe mj mk ml mm b">BackupAndRestore</code>的Keras回调提供容错。<a class="ae kh" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/experimental/BackupAndRestore" rel="noopener ugc nofollow" target="_blank">该回调</a>在每个时期结束时备份模型。如果发生工人中断，所有其他工人也将重新开始，并且训练从最后完成的训练时期继续。模型训练成功后，存储的检查点将被删除。</p><h2 id="b479" class="mn lh iq bd li mo mp dn lm mq mr dp lq kt ms mt lu kx mu mv ly lb mw mx mc iw bi translated">多工人配置</h2><p id="2112" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">在Tensorflow中配置多工人培训最简单的方法是通过名为<code class="fe mj mk ml mm b">TF_CONFIG</code>的环境变量。它是一个JSON字符串，包含两个键，<code class="fe mj mk ml mm b">cluster</code>和<code class="fe mj mk ml mm b">task</code>。前者描述拓扑结构，包括节点的地址。它的值在所有节点上都是相同的。后者对于每个节点都是唯一的，并定义了它们在集群中各自的角色。一旦Tensorflow解析了环境变量，它就会根据配置启动gRPC服务器。</p><p id="fe71" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">以下示例描述了一个包含两个节点的集群，一个主节点和一个工作节点。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">TF_CONIFG</p></figure><p id="dce5" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">在这个集群中有一个指定的节点具有额外的职责，称为主节点。它可以满足诸如保存训练检查点、为TensorBoard编写摘要以及序列化训练模型等任务的需要。如果在配置中没有明确指定负责人，则索引为0的工作人员将担任该角色。当然，主节点也执行相同的代码。</p><h2 id="375d" class="mn lh iq bd li mo mp dn lm mq mr dp lq kt ms mt lu kx mu mv ly lb mw mx mc iw bi translated">多工人模型</h2><p id="3949" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">将<code class="fe mj mk ml mm b">MultiWorkerMirroredStrategy</code>与Keras模型集成很简单。只有几个要求。首先，模型的构建和编译必须在分销策略的范围内进行。其次，批量大小必须反映节点的数量。通常，批量大小通过乘以节点数来调整。保持原来的批量大小也是可行的。但是，在这种情况下，工人收到的批次较少。这些方法分别被称为弱缩放和强缩放。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">多员工战略范围</p></figure><p id="537b" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">与常规Keras训练相反的另一个区别是需要指定<code class="fe mj mk ml mm b">steps_per_epoch</code>参数，否则这是可选的。每个时期的步骤数很容易计算，并且是训练实例数除以调整后的全局批量的比率。函数<code class="fe mj mk ml mm b">create_model</code>包含了定义一个包含模型编译的模型的常用方法。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">创建模型助手方法</p></figure><p id="35e8" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">保存一个多工人模型有点复杂<a class="ae kh" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#model_saving_and_loading" rel="noopener ugc nofollow" target="_blank"/>。所有节点都必须保存模型，尽管保存到不同的位置。最后，除了主节点之外的所有节点都必须删除它们保存的版本。采取这些步骤的原因是所有-减少通信仍然可能发生。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">多工人模型保存</p></figure><h1 id="c62f" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">人工智能平台上的多人培训</h1><p id="265c" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">Google Cloud和Tensorflow和谐地协同工作。Tensorflow在AI平台中一直享有突出的地位。此外，谷歌云也一直在努力开发Tensorflow的新产品。例如，他们最近推出了带有补充支持和托管服务的<a class="ae kh" href="https://cloud.google.com/tensorflow-enterprise" rel="noopener ugc nofollow" target="_blank"> Tensorflow Enterprise </a>。</p><p id="432b" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">正如我们所见，正确设置<code class="fe mj mk ml mm b">TF_CONFIG</code>是Tensorflow多员工培训的重要组成部分。对于Tensorflow来说，在运行模型训练之前，环境变量必须存在。幸运的是，AI平台会根据给定的<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/training-jobs#configuring_the_job" rel="noopener ugc nofollow" target="_blank">培训作业配置</a>来负责<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/distributed-training-details" rel="noopener ugc nofollow" target="_blank">设置</a>。</p><p id="d7e9" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">有两种方式提供作业配置。功能是设置<a class="ae kh" href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training" rel="noopener ugc nofollow" target="_blank">提交命令</a>的各个选项。但是，对于不常改变的属性，将它们存储在YAML配置文件中更方便。对于成功的多人培训，需要的选项有<code class="fe mj mk ml mm b">runtimVersion</code>、<code class="fe mj mk ml mm b">pythonVersion</code>、<code class="fe mj mk ml mm b">scaleTier</code>、<code class="fe mj mk ml mm b">masterType</code>、<code class="fe mj mk ml mm b">worketType</code>、<code class="fe mj mk ml mm b">workerCount</code>。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">培训配置</p></figure><p id="3783" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">上面的配置文件将创建一个由三个节点组成的集群，包括一个首领和两个工人。机器的类型可以不同，但最好是在整个培训集群中保持相同。也有预定义的<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/machine-types" rel="noopener ugc nofollow" target="_blank">机器层级</a>，但是它们都带有参数服务器。因此，将scale-tier设置为custom是当前的当务之急。</p><p id="99b7" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">配置还指定了Tensorflow版本，它等于<code class="fe mj mk ml mm b">runtimeVersion</code>属性。<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/runtime-version-list" rel="noopener ugc nofollow" target="_blank">运行时</a>会定期更新，但支持更高版本的Tensorflow需要一些时间。需要注意的是，2.1版之前的运行时<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/distributed-training-details#chief-versus-master" rel="noopener ugc nofollow" target="_blank">没有正确设置主节点。因此，AI平台上的多工人培训仅适用于运行时2.1和更高版本。</a></p><p id="cd6e" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">另一方面，在某些情况下，能够使用尚不支持的Tensorflow版本是至关重要的。例如，有相关的新特性，或者它们需要在投入生产之前进行适当的测试。对于这种情况，Google AI平台提供了使用<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/custom-containers-training" rel="noopener ugc nofollow" target="_blank">自定义容器</a>进行训练的选项。提交定制容器进行培训与使用所提供的运行时非常相似。不过，还有一些小细节需要考虑。为了拥有正确的<code class="fe mj mk ml mm b">TF_CONFIG</code>，容器化张量流模型需要将选项<code class="fe mj mk ml mm b">useChiefInTfConfig</code>设置为<code class="fe mj mk ml mm b">true</code>。此外，培训配置必须为主管和工人指定图像URIs。当然，两种情况下的容器是相同的。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">自定义容器配置</p></figure><p id="b9a9" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">在官方运行时和自定义容器中，AI平台训练都自动使用<a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/custom-service-account#default" rel="noopener ugc nofollow" target="_blank">云ML服务代理</a>凭证。</p><h2 id="709b" class="mn lh iq bd li mo mp dn lm mq mr dp lq kt ms mt lu kx mu mv ly lb mw mx mc iw bi translated">本地测试</h2><p id="6edd" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">在向AI平台提交作业之前，在本地测试培训配置的正确性是可行的，因为失败的作业也会被计费。幸运的是，官方运行时在本地运行、执行</p><pre class="my mz na nb gt ne mm nf ng aw nh bi"><span id="b7d1" class="mn lh iq mm b gy ni nj l nk nl">gcloud ai-platform local train --distributed --worker-count $N ...</span></pre><p id="0ad8" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><a class="ae kh" href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/local/train" rel="noopener ugc nofollow" target="_blank">命令</a>为我们设置了一个正确的<code class="fe mj mk ml mm b">TF_CONFIG</code>。唉，它还在配置中添加了一个参数服务器。Tensorflow会发出警告，但拥有参数服务器不会妨碍训练。</p><p id="1e8a" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">对于容器，设置一个正确的<code class="fe mj mk ml mm b">TF_CONFIG</code>并运行容器稍微复杂一点，但并非不可行。实现本地测试的最简单的方法是使用<a class="ae kh" href="https://docs.docker.com/compose/" rel="noopener ugc nofollow" target="_blank"> docker-compose </a>来运行主容器和工作容器以及设置环境变量。</p><figure class="my mz na nb gt ka"><div class="bz fp l di"><div class="nc nd l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">docker-编写用于本地测试的配置</p></figure><p id="2136" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated"><code class="fe mj mk ml mm b">COMMAND</code>变量是一个字符串，包含训练任务的所有参数。一旦docker-compose配置完成，只需点击</p><pre class="my mz na nb gt ne mm nf ng aw nh bi"><span id="fffd" class="mn lh iq mm b gy ni nj l nk nl">docker-compose up</span></pre><p id="2567" class="pw-post-body-paragraph ki kj iq kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf ij bi translated">在这两种情况下，如果培训需要访问其他Google云服务，例如云存储，那么必须在运行本地培训之前定义指向正确的<a class="ae kh" href="https://cloud.google.com/docs/authentication/getting-started" rel="noopener ugc nofollow" target="_blank">服务帐户密钥JSON文件</a>的<code class="fe mj mk ml mm b">GOOGLE_APPLICATION_CREDENTIALS</code>。</p><h1 id="0dda" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">后续步骤</h1><p id="39b4" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">构建神经网络本身并不是一项简单的任务。在此基础上处理分布式计算最终会严重阻碍这一过程。幸运的是，现代深度学习框架是在考虑可扩展性的情况下构建的，因此能够实现更快的开发。此外，由于Tensorflow和Google Cloud之间的顺利兼容，开发这种可扩展性从未像现在这样容易。</p><h2 id="0178" class="mn lh iq bd li mo mp dn lm mq mr dp lq kt ms mt lu kx mu mv ly lb mw mx mc iw bi translated">简单的例子</h2><p id="3743" class="pw-post-body-paragraph ki kj iq kk b kl me kn ko kp mf kr ks kt mg kv kw kx mh kz la lb mi ld le lf ij bi translated">事实上，这种平稳性乍看起来可能并非如此。为此，我在Github上创建了一个简单的工作示例。查看实际细节。记住，最好的学习方法是实践。</p><div class="nm nn gp gr no np"><a href="https://github.com/kalosisz/tf-gcp-multi-worker-example" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd ja gy z fp nu fr fs nv fu fw iz bi translated">kalosisz/TF-GCP-多工人-示例</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">在Google Cloud AI平台上设置和运行Tensorflow多工人培训模型的简单示例。这个…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">github.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od kb np"/></div></div></a></div></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><h1 id="ba9f" class="lg lh iq bd li lj ol ll lm ln om lp lq lr on lt lu lv oo lx ly lz op mb mc md bi translated">资源</h1><ul class=""><li id="a07a" class="oq or iq kk b kl me kp mf kt os kx ot lb ou lf ov ow ox oy bi translated"><a class="ae kh" href="https://www.tensorflow.org/guide/distributed_training" rel="noopener ugc nofollow" target="_blank">使用TensorFlow进行分布式训练</a></li><li id="7487" class="oq or iq kk b kl oz kp pa kt pb kx pc lb pd lf ov ow ox oy bi translated"><a class="ae kh" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras" rel="noopener ugc nofollow" target="_blank">使用Keras进行多工人培训</a></li><li id="013a" class="oq or iq kk b kl oz kp pa kt pb kx pc lb pd lf ov ow ox oy bi translated"><a class="ae kh" href="https://cloud.google.com/ai-platform/training/docs/distributed-training-details" rel="noopener ugc nofollow" target="_blank"> TF_CONFIG和分布式培训</a></li><li id="d7ca" class="oq or iq kk b kl oz kp pa kt pb kx pc lb pd lf ov ow ox oy bi translated"><a class="ae kh" href="https://id2223kth.github.io/slides/2019/13_distributed_learning.pdf" rel="noopener ugc nofollow" target="_blank">Amir h . Payberah的分布式学习(幻灯片)</a></li></ul></div></div>    
</body>
</html>