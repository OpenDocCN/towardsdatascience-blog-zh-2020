<html>
<head>
<title>PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-f037256c5e3?source=collection_archive---------13-----------------------#2020-10-30">https://towardsdatascience.com/pyspark-f037256c5e3?source=collection_archive---------13-----------------------#2020-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="422c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python、SQL、Spark和分布式计算的结合使大数据上的机器学习成为可能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/373fea9b8c08bf7406b8e1a525d3bf68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Vejdutd2bWzVYAlu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@benearlweber?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">本·韦伯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="2dfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shilpa对她的第一份工作非常满意，她是一家很有前途的初创公司的数据科学家。她喜欢SciKit-Learn图书馆，尤其是熊猫。使用熊猫数据框架进行数据探索很有趣。一个初露头角的数据科学家无法企及类似SQL的接口和快速的内存数据处理。</p><p id="00f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着创业之旅的成熟，数据量也在增加，it开始追求用更大的数据库和更强的处理能力来增强他们的IT系统。Shilpa还通过会话池和多线程在她基于Python的ML程序中增加了并行性，然而，这还远远不够。很快，IT部门意识到他们无法继续增加更多的磁盘空间和内存，并决定采用分布式计算(也称为大数据)。</p><p id="71ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希尔帕现在该怎么办？他们是如何让熊猫用分布式计算工作的？</p><p id="2807" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事你看着眼熟吗？</p><p id="516f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我在本文中要带你经历的:Python在大数据中的困境。答案是<strong class="lb iu"> PySpark </strong>。</p><p id="0f61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark是什么？</p><p id="9fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我可以有把握地假设，您一定听说过Apache Hadoop:一种用于在计算机集群上分布式处理大型数据集的开源软件。Apache Hadoop只能以批处理模式处理数据集，缺乏实时流处理。为了填补这个空白，Apache推出了Spark <em class="lv">(实际上Spark是由UC Berkley amplab开发的)</em>:一个快如闪电的内存实时处理框架。Apache Spark是用Scala编程语言编写的。为了用Spark支持Python，Apache Spark社区发布了<strong class="lb iu"> PySpark </strong>。</p><p id="ff23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark被数据科学和机器学习专业人士广泛使用。看了PySpark提供的功能，我并不惊讶它已经被网飞、沃尔玛、Trivago、赛诺菲、Runtastic等组织使用。</p><p id="e8e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了Pyspark的特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/5080ab5507df1c3123e000c6d398c5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*0DduCSleektW3HiuMeYRGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="02f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将带您一步一步地在集群计算机上使用PySpark。</p><h2 id="ba73" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">环境准备</h2><p id="46de" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">要真正实践PySpark，您需要访问一组计算机。我建议通过下面的链接创建一个免费的计算机集群环境。</p><div class="mv mw gp gr mx my"><a href="https://community.cloud.databricks.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">数据块-登录</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">编辑描述</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">community.cloud.databricks.com</p></div></div></div></a></div><p id="75eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注册并确认电子邮件后，将显示“欢迎使用databricks”页面。单击常见任务列表中的新建集群。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/99bce749343080c5b9ff7ad9a464d595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbnNTq4xKTDi_eFzYRG3JQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ol class=""><li id="e2e1" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">在创建集群屏幕中输入详细信息。对于运行时版本，确保Scala版本高于2.5，Python版本为3及以上。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/c28098240e4773d48f93b76d005afd16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iJARl4jJcs-hWqJi"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6c87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.单击创建集群。群集开始运行需要几分钟时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/6b66fcb9ab0aae0c496e50024a226e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_64r2_F6b5VhNfYgBaFkxw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="55fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.单击集群名称可查看配置和其他详细信息。现在，不要对它做任何更改。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/bffaf7e90ce3dd7c1d1d41fbdc76467e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESg-56qw0slxg_SVKzlUsA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="95ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你！！您的计算机集群已准备就绪。是时候将数据上传到您的分布式计算环境中了。</p><h2 id="aa8b" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">数据源</h2><p id="6724" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">我将从下面的链接使用皮马-印第安人-糖尿病数据库。</p><div class="mv mw gp gr mx my"><a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">皮马印第安人糖尿病数据库</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">基于诊断方法预测糖尿病的发病</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">www.kaggle.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ks my"/></div></div></a></div><p id="dd6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">数据集包含几个医学预测变量(自变量)和一个目标变量(因变量)，</em><strong class="lb iu"><em class="lv"/></strong><em class="lv">。独立变量包括患者的怀孕次数、身体质量指数、胰岛素水平、年龄等。</em></p><p id="de8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它有一个diabetes.csv文件。将其放在您的本地文件夹中，然后上传到databrics文件系统(DBFS)。下面是将文件上传到DBFS的导航。</p><ol class=""><li id="b7ef" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">点击左侧菜单中的数据选项</li><li id="91a3" class="ni nj it lb b lc oa lf ob li oc lm od lq oe lu nn no np nq bi translated">点击添加数据按钮</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/540e53b3d6be0721fbdf98d5328f6dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*ThHyFaaNfdJ0qiSu0egNxA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.在创建新表格屏幕中，单击浏览。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2f15e76a95c1a70f7485ae160b80a390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*xRo2WIi9MoKamtYAF15RVw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="f53b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.它将指向本地磁盘上的目录路径。选择您从上面提到的Prima-Indian-diabetes链接下载的diabetes.csv文件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/75ee84e1189c69a56c60abac85ebf038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*cy72tLBsiMCZ6S4eCpFqaQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="03b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.点击DBFS。它将显示上传到databrics文件系统的文件(diabetes.csv)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/eb85857b22badeff8653f3d6cbfaa865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WofLMiNqUXyVSnWpTwDNAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="74ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">恭喜你！！您已成功将文件上传至databrics文件系统。现在，您可以通过pyspark将它保存在集群中的不同节点上。</p><h2 id="b5a4" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">笔记本</h2><p id="6856" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">Datbricks提供了一个编写pyspark代码的在线笔记本。点击新笔记本打开它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ca"><img src="../Images/2a13f9b5676a760351af00b1ab65a973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdsOCGBhiYOIqIy43JnRTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="b384" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">数据帧</h2><p id="7dca" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">到目前为止，文件只在DBFS。现在真正的行动开始了。在本文的这一部分，我将带您浏览Pyspark数据框架。</p><p id="aee0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们说dataframe时，很明显会想到熊猫。Pandas和Pyspark dataframe之间的主要区别在于Pandas将完整的数据放在运行它的一台计算机的内存中，Pyspark dataframe与集群中的多台计算机一起工作(分布式计算),并将数据处理分配给这些计算机的内存。Pyspark最大的增值是在多台计算机上并行处理一个巨大的数据集。</p><p id="1bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是主要原因，Pyspark在处理分布在不同计算机上的大型数据集时表现良好，Pandas在处理可以存储在单台计算机上的数据集时表现良好。</p><p id="5bf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但这并不是Pandas和Pyspark数据框之间的唯一区别。在Pandas和Pyspark之间，相同的操作在执行方式上有一些不那么细微的差异。</p><p id="753a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表显示了其中的一些差异</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="9570" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然熊猫和Pyspark的比较已经不在我们的讨论范围内，让我们来研究Pyspark数据框架。</p><p id="aa5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码行将根据DBFS的CSV数据创建一个Pyspark数据框，并显示前几条记录。注意如何</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/fee508e703c0e6f5beb96e0844180861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4jI8pmNwfu63xkrd43twg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上show命令的结果</p></figure><p id="abb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像Pandas一样，很多操作都可以在Pyspark数据帧上执行。下面是一些例子。</p><p id="e170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> printSchema: </strong>显示数据帧的结构，即列和数据类型，以及是否接受空值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1f9d68c50a21395d7b7155b304ce0791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*5vzXHPQODDsye3-haddy9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Pyspark数据帧上printSchema命令的结果</p></figure><p id="e857" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">列:</strong>显示<strong class="lb iu">列名称。</strong></p><pre class="kj kk kl km gt on oo op oq aw or bi"><span id="c66b" class="lx ly it oo b gy os ot l ou ov">df.columns</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/f4fe64d22ecd98e319361286b1095a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*gPHdYJ2HmlA7xZzyZDZ6oA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上columns命令的结果</p></figure><p id="bca4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">计数</strong>:显示行数。</p><pre class="kj kk kl km gt on oo op oq aw or bi"><span id="e519" class="lx ly it oo b gy os ot l ou ov">df.count()</span></pre><p id="31a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">产量:768</p><p id="a83f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> len( &lt; dataframe &gt;)。&lt;列&gt; ): </strong>显示数据帧中的列数。</p><pre class="kj kk kl km gt on oo op oq aw or bi"><span id="e9b9" class="lx ly it oo b gy os ot l ou ov">len(df.columns)</span></pre><p id="eba3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">产出:9</p><p id="7826" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> &lt;数据帧&gt;。描述(&lt;列名&gt;)。show(): </strong>描述提到的列。</p><p id="f0f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码描述了葡萄糖柱。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="e780" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出:显示葡萄糖值的统计值，如计数、平均值、标准差(stddev)、最小值(min)和最大值(max)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f88a6a4c927956245580fd5107de7177.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*_YqRe8lNXcks6l8YZ-1sTg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark描述命令的结果</p></figure><p id="e873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">选择:</strong>显示从数据框中选择的列。</p><p id="88c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码将仅从数据框中选择葡萄糖和胰岛素值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/204c67cef6952f25046e43add8a68c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*wVtIDOYbMdjDs4baJQDbsw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上select命令的结果</p></figure><p id="4115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> like: </strong>它的作用类似于SQL中的like过滤器。“%”可以用作通配符来筛选结果。但是，与基于like condition中提到的条件过滤结果的SQL不同，这里显示的是完整的结果，表明它是否满足like条件。</p><p id="f062" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码将显示数据帧中的怀孕和葡萄糖值，并指示单个行是否包含从33开始的身体质量指数值。</p><p id="45af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注意:通常，like条件用于分类变量。然而，我使用的数据源没有任何分类变量，因此使用了这个例子。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/eac4ff6a8cf1ba49c269db81a88f751f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30a8SU64a3DmWgVKGAr64g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据帧上相似条件的结果</p></figure><p id="308e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">过滤:</strong>根据上述条件过滤数据。</p><p id="0008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码过滤血压大于100的数据帧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/8f1d4e87e6e4738b0978e29f4d2ceafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAPLiXKhyOt8wvDOYvrUdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上过滤器命令的结果</p></figure><p id="22eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该筛选器可用于添加多个带有and (&amp;)或(|)条件的条件。</p><p id="3c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码片段过滤血压和胰岛素值大于100的数据帧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/22e13076f47982f9f55ab3438cc37118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ee1nZD5YvdhWR0HXg7qs0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上过滤器命令的结果</p></figure><p id="c4d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> orderBy: </strong>对输出进行排序。</p><p id="0c96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码过滤血压和胰岛素值大于100的数据帧，并基于葡萄糖值输出。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/6a19e430d0372beaadafbf6bbda69639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXRQhELkPjSbd4XzL6EzFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark数据帧上过滤器命令的结果</p></figure></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><p id="29b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Pyspark SQL </strong></p><p id="e664" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hive和Spark SQL可以用来查询Hadoop和Spark集群中存储的结构化数据。大多数情况下，Spark SQL至少比Hive快10倍。当Spark SQL在另一个编程接口中运行时，它将输出作为数据帧返回。</p><p id="9b7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的这一部分，我将带您了解pySpark内置的SQL函数pyspark.sql.function，它适用于dataframe。</p><p id="fc69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是一些主要功能:</p><p id="3331" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">when and other</strong>:评估特定列的值，并将值更改为更有意义的值。</p><p id="03be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码显示了如何评估血压水平并将其分类为高、低和正常。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/eac4ff6a8cf1ba49c269db81a88f751f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30a8SU64a3DmWgVKGAr64g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">当和否则命令时pyspak的结果</p></figure><p id="32a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">聚合函数:</strong>聚合函数，如平均值(avg)、最大值(max)、最小值(min)、总计(sum)、计数。</p><p id="5088" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码将显示糖尿病和非糖尿病患者的最大和最小葡萄糖值和计数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/01b63e0950176c99c6759365cf33f2a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*e0PAKGJAziWlPdrPNWhw1w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark SQL聚合函数的结果</p></figure><p id="6b49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html" rel="noopener ugc nofollow" target="_blank"> pySpark SQL page </a>获得内置函数的完整列表。</p><h2 id="83fb" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">以编程方式运行SQL查询</h2><p id="27e1" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">如果你像我一样有数据库经验并喜欢SQL查询，你会喜欢PySpark的这个特性。</p><p id="ce99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我告诉你，你可以在pySpark中直接运行任何SQL查询，结果将以dataframe的形式出现，那会怎么样？</p><p id="d7eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个两步走的过程。</p><ol class=""><li id="316e" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">使用数据帧中的完整数据创建临时或全局视图。</li><li id="60de" class="ni nj it lb b lc oa lf ob li oc lm od lq oe lu nn no np nq bi translated">在使用Oracle、MySQL等关系数据库时，可以像使用数据库视图一样在该视图上运行SQL查询。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/bd5248ca08b4b30e3aa096f523b726b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Py5yhTQGkkCMlszRa64k6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark SQL程序的结果</p></figure><p id="daa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，我同意这太简单了。让我们用聚合函数和group by、order by编写一个SQL。</p><p id="b6d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的SQL将打印糖尿病数据库中每个年龄组的平均血糖和血压。结果将按平均血压排序。它将显示5条记录。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/4f4cf87901ed6299b2c96c3aad5eb5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*eF4C5Rs197JIiZpUJCKk9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pyspark SQL编程的结果</p></figure><p id="53d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">createOrReplaceTempiew将创建一个临时视图，只要会话处于活动状态，该视图就会显示值。要创建跨会话的视图(类似于数据库实体化视图)，请使用createGlobalTempView。</p><p id="0b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在pySpark中，通过SQL编程可以做很多事情。在<a class="ae ky" href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank">Spark文档</a>中获取可能性列表。</p><h2 id="171d" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">用户定义的函数(UDF)</h2><p id="9120" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">上一节中的SQL包含预定义的SQL函数，如format_number、avg。在实际项目中，数据科学家经常会遇到这样的情况，他们想要实现一些没有内置函数的东西。那是他们使用UDF的时候。</p><p id="e70a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一个Python函数的片段，它根据一个人的身体质量指数来指示这个人是体重不足、超重、正常体重还是肥胖。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="3072" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在Spark SQL中使用这个Python函数，将其注册为UDF。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="62a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，UDF BMIInd可以在Spark SQL中使用，如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/66de03f9bf794b6b9c3951dfa69e3eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*MYk3AgIi97SPD4g24Ej4GQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用UDF输出触发SQL</p></figure></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><h2 id="a248" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">pySpak ML</h2><p id="881b" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">除非我们触及Spark的机器学习库MLlib，否则这篇文章是不完整的。</p><p id="92fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是来自<a class="ae ky" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark网页</a></p><blockquote class="po"><p id="79bd" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py"> MLlib是Spark的机器学习(ML)库。它的目标是让实用的机器学习变得可扩展和简单。在高层次上，它提供了一些工具，例如:</em></p><p id="7691" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py"> ML算法:分类、回归、聚类、协同过滤等常用学习算法</em></p><p id="b547" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py">特征化:特征提取、变换、降维、选择</em></p><p id="38ff" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py">管道:用于构建、评估和调整ML管道的工具</em></p><p id="61e6" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py">持久性:保存和加载算法、模型和管道</em></p><p id="00a1" class="pp pq it bd pr ps pt pu pv pw px lu dk translated"><em class="py">实用程序:线性代数、统计学、数据处理等。</em></p></blockquote><p id="6daa" class="pw-post-body-paragraph kz la it lb b lc pz ju le lf qa jx lh li qb lk ll lm qc lo lp lq qd ls lt lu im bi translated">我们将对糖尿病数据使用Spark MLlib来创建一个预测模型，该模型可以根据患者的身体质量指数、年龄、血糖水平和血压来预测他/她是否是糖尿病患者。</p><ol class=""><li id="7f1d" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">Vectorassembler:第一步是使用pyspark.ml.feature中的VectorAssemble方法将所有特征组合成一个向量</li><li id="281f" class="ni nj it lb b lc oa lf ob li oc lm od lq oe lu nn no np nq bi translated">然后将数据分成训练和测试数据集。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/8e0be643f60901e9fe7a5d410133f1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4wYhiyS9w6GEedP0cnU1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练和测试中特征组合和数据分割的结果</p></figure><p id="ff1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.为此，我将使用集成方法梯度引导分类器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/7ae472afa72c977cfcff3a5af961d4c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1n1nw14u3103orE5gW9GJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度增强分类器结果</p></figure><p id="09d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不错，梯度推进分类器返回75%的准确性。</p><p id="4c2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">浏览<a class="ae ky" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank">Spark ml lib</a>页面，获取Spark提供的机器学习模型列表。</p><h2 id="2ddc" class="lx ly it bd lz ma mb dn mc md me dp mf li mg mh mi lm mj mk ml lq mm mn mo mp bi translated">结论</h2><p id="ccbd" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">这是一篇大文章。我认为pySpark值得拥有这个，因为它是分布式计算上数据科学和机器学习可用的最强大的工具。我和我的学生已经成功地使用了本文档中解释的每一个特性和命令，它们对我们来说工作得很好。但是，如果您遇到任何问题，请让我知道。</p><p id="084f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我期待你的反馈。</p><p id="57be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考:</strong></p><p id="87fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.udemy.com/course/apache-spark-for-data-engineers/?referralCode=CA92888DA98AEA3315AC" rel="noopener ugc nofollow" target="_blank">https://www . udemy . com/course/Apache-spark-for-data-engineers/？referral code = ca 92888 da 98 AEA 3315 AC</a></p></div></div>    
</body>
</html>