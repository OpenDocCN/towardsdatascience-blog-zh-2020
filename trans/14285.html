<html>
<head>
<title>Multi-Armed Bandits: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多种武器的强盗:第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697?source=collection_archive---------9-----------------------#2020-10-02">https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697?source=collection_archive---------9-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7445" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a>，<a class="ae ep" href="https://towardsdatascience.com/tagged/baby-robot-guide" rel="noopener" target="_blank">一个婴儿机器人的强化学习指南</a></h2><div class=""/><div class=""><h2 id="018c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数学框架和术语</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2c3f3b833d97daa5873b6728df4dc832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WUKevSQJrdR5Xjwr"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@flowforfrank?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">费伦茨·阿尔马西</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="1d9a" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">概述</strong></h1><blockquote class="ma mb mc"><p id="b444" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">当面对各种选项的选择时，每个选项给你不同程度的回报，你如何找到哪个是最好的？</p><p id="07a9" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">这类问题通常被称为<strong class="mg jd">多臂强盗</strong>。在这一系列文章中，我们将看看用于解决这一难题的理论和算法。</p><p id="2210" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated"><strong class="mg jd">多臂土匪</strong>问题是全面强化学习道路上的第一步。</p></blockquote><p id="d7a4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这是一个六集系列的第一集，讲述了多种武装的强盗。要涵盖的内容相当多，因此需要将所有内容分成六个部分。即便如此，我们真的只打算看看多臂强盗的主要算法和理论。刚好够充当强化学习的垫脚石。</p><p id="5b29" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">在第1部分中，我们将涵盖Bandit问题的所有细节，定义将在后续部分中使用的术语和基本方程。这大部分也直接适用于一般的强化学习。然而，如果你已经熟悉了基础知识，那么你可能会想要跳到真正涉及到Bandit算法的部分。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="aa01" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">索引</h1><p id="6f97" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">本系列中关于多武装匪徒的全套文章如下:</p><ul class=""><li id="8965" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/multi-armed-bandits-part-1-b8d33ab80697"> <strong class="mg jd">第一部分:数学框架和术语</strong></a><strong class="mg jd"><br/></strong>——入门所需的全部基础信息</li><li id="2a79" class="nu nv it mg b mh od mk oe na of nb og nc oh mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/multi-armed-bandits-part-2-5834cb7aba4b"> <strong class="mg jd">第二部分:土匪框架</strong> </a> <br/> <em class="mf"> - </em>代码和测试框架的描述</li><li id="984f" class="nu nv it mg b mh od mk oe na of nb og nc oh mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/bandit-algorithms-34fd7890cb18"> <strong class="mg jd">第三部分:土匪算法</strong> </a> <br/> <em class="mf"> - </em> <a class="ae lh" rel="noopener" target="_blank" href="/bandit-algorithms-34fd7890cb18"> <em class="mf">贪婪算法</em></a><em class="mf"><br/>-</em><a class="ae lh" rel="noopener" target="_blank" href="/bandit-algorithms-34fd7890cb18"><em class="mf">乐观-贪婪算法</em></a><em class="mf"><br/>-</em><a class="ae lh" rel="noopener" target="_blank" href="/bandit-algorithms-34fd7890cb18"><em class="mf">ε-贪婪算法</em></a><em class="mf"><br/>-</em><a class="ae lh" rel="noopener" target="_blank" href="/bandit-algorithms-34fd7890cb18"><em class="mf">后悔</em> </a></li><li id="2e16" class="nu nv it mg b mh od mk oe na of nb og nc oh mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f"> <strong class="mg jd">第四部分:置信上限(UCB) Bandit算法</strong> </a></li><li id="2b8c" class="nu nv it mg b mh od mk oe na of nb og nc oh mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/thompson-sampling-fc28817eacb8"> <strong class="mg jd">第五部分:汤普森采样</strong></a><strong class="mg jd"><br/></strong><em class="mf">-</em><a class="ae lh" rel="noopener" target="_blank" href="/thompson-sampling-fc28817eacb8"><em class="mf">伯努利汤普森采样</em></a><br/><em class="mf">-</em><a class="ae lh" rel="noopener" target="_blank" href="/thompson-sampling-fc28817eacb8"><em class="mf">高斯汤普森采样</em> </a></li><li id="7d60" class="nu nv it mg b mh od mk oe na of nb og nc oh mz nz oa ob oc bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/a-comparison-of-bandit-algorithms-24b4adfcabb"> <strong class="mg jd">第六部分:土匪算法比较</strong> </a></li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="f973" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">bandit算法和测试框架的所有代码都可以在github上找到:<a class="ae lh" href="https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits" rel="noopener ugc nofollow" target="_blank"> Multi_Armed_Bandits </a></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8e5cf328db32fb224d1c235cc37f2e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*rNK40-iO5cc2dEeTpimT0Q.gif"/></div></figure><p id="0bc5" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi oj translated">从前有一个婴儿机器人。一天，他和妈妈出去购物，一只机器狗跑了过去。机器人宝宝的妈妈正忙着买新的火花塞，所以没注意到他什么时候转身开始跟着狗。</p><p id="56f1" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">他追着狗穿过拥挤的购物中心，穿过几条过道，下了自动扶梯，上了自动扶梯，最后，狗躲进了墙上的服务舱口，不见了。不仅服务舱太小，机器人宝宝进不去，而且他还意识到自己迷路了。完全迷失了。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="dac5" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">电源插座问题</h1><p id="e94e" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">更糟糕的是，机器人宝宝意识到自己快没电了。如果他想再找到他的妈妈，他需要充电。而且要快。</p><p id="2cbd" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">幸运的是，正对面有一个充电站。他走过去，发现房间里有5个独立的电源插座。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/e0435f6e4ac23cab75344dd121102a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Ehq09I3ja_LzrVWvb56ObA.png"/></div></figure><p id="1644" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">他走到第一个电源插座前，插上电源。令人失望的是，他只收到了一股能让他运行2秒钟的能量。按照这种速度，他将永远无法完全充电。也许其他电源点会更好？</p><p id="263f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">考虑到这一点，他移动到下一个插座，在那里他获得了3秒钟的充电时间。这个插座似乎并不比第一个好多少，所以他转而尝试第三个插座。这一次他获得了6秒的价值。这似乎是给他充电最多的电源插座，尽管完全充电仍需要很长时间，但似乎比他试过的其他插座要好。所以，决定留在这个电源点，他再次插入。这一次，他收到了少得可怜的2秒钟的费用。</p><p id="802a" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">他必须找到一种更好的方法来发现哪个是最好的，并让他以最快的速度达到最大电量，而不是沿着插座随意移动。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e9116c0443886932fb8c45fa751f0028.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/1*YfFWWr7bbQJFbncSdM4Chg.gif"/></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><blockquote class="ma mb mc"><p id="8806" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated"><strong class="mg jd">多兵种土匪问题</strong></p><p id="0127" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">这个电源插座问题类似于标准的、多臂的、土匪问题，用来说明如何孤立地检查探索。</p><p id="60f4" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">在多臂强盗游戏中，你试图通过玩一组单臂强盗(也称为吃角子老虎机或水果机)赢得尽可能多的钱，每个强盗都可以给出不同的支付。你需要找到哪台机器支付的金额最大，这样你就可以在分配的时间内赚尽可能多的钱。</p><p id="f8cf" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated">机器的每一次游戏(或强盗手臂的拉动)对应一个时间段，你只能玩固定数量的时间段。</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="adeb" class="ou lj it bd lk ov ow dn lo ox oy dp ls na oz pa lu nb pb pc lw nc pd pe ly iz bi translated">探索与开发的困境</h2><p id="92db" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">婴儿机器人面临着不知道哪个电源插座给他充电最多的问题。因此他需要<strong class="mg jd"> <em class="mf">探索</em> </strong>可能的选择，以寻找最好的一个。</p><p id="fcdf" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">然而，因为他时间紧迫，他不能花太多时间来决定哪一个是最好的，也不能测试每个电源插座来准确知道它平均会给多少电荷。他需要<strong class="mg jd"> <em class="mf">利用</em> </strong>他获得的知识，这样他就不会浪费时间去尝试不好的电源插座，让他在最短的时间内获得最大的电量。</p><p id="f045" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这是一个经典的探索-开发困境的例子，在这种情况下，你想探索可能的选项以寻找最佳选项，同时又想利用已经获得的信息，以便获得最大可能的总体回报。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="34a8" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">数学框架和术语</h1><p id="300c" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">为了帮助机器人宝宝充电，并让他回来找妈妈，我们首先需要熟悉一些强化学习中使用的常用术语和数学符号。</p><p id="8edd" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">使用的符号很大程度上遵循萨顿和巴尔托的，来自他们的强化学习圣经，“<a class="ae lh" href="https://amzn.to/2RIZ9pc" rel="noopener ugc nofollow" target="_blank"> <em class="mf">强化学习:介绍</em> </a>”。</p><p id="ee4f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd"> <em class="mf">动作</em> </strong></p><p id="b74c" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">在强化学习中，在可用选项之间做出选择，并选择该选项，被称为采取<strong class="mg jd"> <em class="mf">动作</em> </strong>。例如，在电源插座问题中，一个操作是选择一个可用的电源插座。在多臂强盗问题中，它是选择和玩一套吃角子老虎机。</p><p id="19d4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">在简单的电源插座和bandit问题中，在离散的时间步骤中采取行动。换句话说，一个动作接一个动作，在这些问题中，有固定数量的总动作。</p><ul class=""><li id="a2c7" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">在时间步'<em class="mf"> t </em>'采取的行动表示为'<em class="mf"> Aₜ </em></li></ul><p id="4f17" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd"> <em class="mf">奖励</em> </strong></p><p id="6090" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">每采取一个动作就获得一个<strong class="mg jd"> <em class="mf">奖励</em> </strong>。在电源插座问题中，奖励是一定数量的电荷；然而，在强盗问题中，它是从机器中赢得的一笔钱。</p><p id="07b9" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">采取特定行动所获得的奖励是一个随机值，从特定于该行动的潜在概率分布中提取。因此，每次采取行动时，返回的奖励可能会有不同的值。如果同一个动作重复多次，那么就可以计算出一个更准确的真实回报平均值。</p><p id="02e9" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">采取特定行动获得奖励可能是强化学习的主要定义特征。正是这种奖励被用来指导学习，试图找到最好的行动，从而最大限度地提高总的整体奖励。这与监督学习形成对比，在监督学习中，最佳动作将作为训练数据的一部分提供。</p><ul class=""><li id="a97c" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">在采取行动'<em class="mf"> Aₜ </em>'之后，在时间步'<em class="mf"> t </em>'获得的奖励表示为'<em class="mf"> Rₜ </em></li></ul><p id="f98d" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd"> <em class="mf">动作次数</em> </strong></p><p id="7a74" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">可用动作的数量由字母'<em class="mf"> k </em>表示。因此，在电源插座示例中，'<em class="mf"> k </em>'将是可供选择的电源插座数量。因为，在这种情况下，有5个不同的套接字，'<em class="mf"> k </em>'将是5。在多臂土匪问题中，它是可供选择的吃角子老虎机的总数(实际上这个问题经常被称为'<em class="mf"> k臂土匪问题'</em>)。</p><ul class=""><li id="172a" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">可用动作的数量由字母'<em class="mf"> k </em>表示</li></ul><p id="bbc5" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd"> <em class="mf">预期奖励</em> </strong></p><p id="b06d" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">每个可用的'<em class="mf"> k </em>'动作都有一个<strong class="mg jd"> <em class="mf">预期</em> </strong>奖励，其中术语“预期”是指如果该动作重复多次将获得的平均值。因此，例如，如果投掷一枚公平的硬币，获得正面的预期概率将是0.5，因为平均而言，当投掷总数很大时，正面应该出现在所有硬币投掷的一半中。</p><ul class=""><li id="fc40" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">财产的预期价值用符号'𝔼'表示</li></ul><p id="2148" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><strong class="mg jd"> <em class="mf">值</em> </strong></p><p id="21a0" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">一个行动的期望回报被称为该行动的<strong class="mg jd"> <em class="mf">值</em> </strong>，并被表示为“<em class="mf"> q(a) </em>”，其中“<em class="mf"> a </em>”是在时间步长“<em class="mf"> t </em>”选择的具体行动(即<em class="mf"> Aₜ = a)。</em></p><ul class=""><li id="5fd9" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">因此，动作'<em class="mf"> a </em>'的值由下式给出:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/e04133d3815386f2a991d1fea592ab9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*9i5DsC_ZoLKaXW14CEVvfA.png"/></div></figure><p id="3c86" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这可以理解为:<em class="mf">假设在时间步长‘t’选择的行动是行动‘a’，行动‘a’的值等于预期(平均)奖励值。</em></p><p id="7edb" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">(注意:任何时候你在概率方程中看到符号“|”，就把它理解为‘给定’)。</p><p id="bf7a" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">实际上，这意味着，如果你不断重复动作'<em class="mf"> a </em>'，然后对你得到的奖励值进行平均，最终，你会知道'<em class="mf">q(a)【T29]'的真实值，即'<em class="mf"> a </em>'的平均奖励。因此，如果小机器人继续尝试同一个电源插座，他将获得该插座真实平均功率输出的越来越准确的估计，直到最终，如果他重复该动作足够长的时间，他将知道该插座输出的真实值。</em></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="23ee" class="ou lj it bd lk ov ow dn lo ox oy dp ls na oz pa lu nb pb pc lw nc pd pe ly iz bi translated">样本平均估计值</h2><p id="997e" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">显然，我们不知道行为回报的真正价值。如果我们这样做了，事情就简单了。我们只是利用现有的知识，而不必做任何探索。我们只要玩最好的吃角子老虎机，赢得最多的钱，机器人宝宝就会插上充电最多的插座。</p><p id="a64f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">然而，事实并非如此。我们不知道真正的回报，因此必须做一些探索，从每一个可能的行动中找到回报的比较。因此，为了跟踪哪一个动作是最好的，当我们探索一组可能的动作时，我们需要估计每个动作的价值。随着时间的推移，这种估计应该会越来越准确，并向真正的回报值靠拢。</p><p id="8bc8" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">因为一个行为的真实价值是该行为的平均回报，一个简单而有效的估计可以通过取该行为迄今为止回报的平均值来计算。</p><p id="d9e6" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">因此，在时间步长<em class="mf">‘t’</em>处，动作<em class="mf"> a </em>的估计值<em class="mf"> Qₜ(a)' </em>由下式给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/773ab94291fa9809f3efd9a8af287535.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*oioqnWvPzBbxU73QM0448Q.png"/></div></figure><p id="3376" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">其中'<em class="mf"> n' </em>是在时间'<em class="mf"> t </em>'之前采取行动'<em class="mf"> a </em>的次数，而'<em class="mf"> Rᵢ </em>'是采取行动'<em class="mf"> a </em>时在每个时间步获得的奖励。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="14c0" class="ou lj it bd lk ov ow dn lo ox oy dp ls na oz pa lu nb pb pc lw nc pd pe ly iz bi translated">计算样本平均值</h2><p id="872a" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">对任何行为来说，形成所有奖励总和的最简单的方法是存储每个奖励，然后在需要时添加它们。然而，从实际的角度来看，这不是很有效，无论是在存储和计算时间方面。有可能记录总的回报，但随着时间的推移，即使是这个值也会变得难以管理。</p><ul class=""><li id="6445" class="nu nv it mg b mh mi mk ml na nw nb nx nc ny mz nz oa ob oc bi translated">一个更好的解决方案是根据上一次的估计来计算新的估计奖励。</li></ul><p id="e298" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">对于一个行动'<em class="mf"> a </em>'，行动价值的<em class="mf"> n </em> ᵗʰ估计值'<em class="mf"> Qₙ </em>'，由该行动以前获得的所有奖励的总和除以该行动被选择的次数(即，它只是平均值)得出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/df0c03f8538b4d0e3f746a99f2c240c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*lmjOStEPiW5kdwPar9awmw.png"/></div></figure><p id="6227" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">因此，交换一下东西，在'<em class="mf"> n </em>'之前，奖励的总和由下式给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/92523b8d1ecf691a9b0b2b90b475be75.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*-SOamL53xHXbdHh_cHCCjA.png"/></div></figure><p id="b9e4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">当获得下一个奖励“rₙ”<em class="mf"/>时，可以通过将该奖励加到先前的奖励总和上并增加已经采取的行动的次数的计数来计算新的估计值。所以新的估计是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6641215b6078081e0b913c7523464b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*dD97coi-m9sKvSM-rL8BOA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/2d9057c1224cace51dc9b900bb80c280.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*sKGVvKNvLsLE2kFKlfFhSw.png"/></div></figure><p id="d07e" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">由于我们已经知道如何写奖励的总和，在'<em class="mf"> n </em>'之前，就最后的估计而言，我们可以简单地将它代入等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/ba2918ef1558054ba3a848a5c1efbe9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*0SWBrDaqgpuq2tofxmiCOw.png"/></div></figure><p id="5fa7" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">重新整理后，我们最终得到了新评估的可用形式，用上次评估'<em class="mf"> Qₙ </em>和新奖励'<em class="mf"> Rₙ </em>'来表示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/48d73c2b575b813e8efa4d06f1cb5391.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*HlP-rCQjT7SmPLSjo5Ae2A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">公式1:新的估计值，根据上次的估计值和新的奖励值计算得出。</p></figure><p id="10ef" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">虽然对某些人来说，这些等式可能有点令人生畏，但它们基本上可以归结为公式1中给出的最后一个等式，它让我们在给定行为的先前值和新奖励的情况下，计算行为的值。</p><p id="87ae" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这个公式很容易在代码中实现(我们将在下一部分中实现)，让我们监视在采取这些动作时，动作的估计值是如何变化的。然后，这些值可用于比较这些操作的相对性能，以找到最佳操作和选择这些操作的最佳方式。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="b534" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">摘要</h1><p id="06f7" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">在这里，我们对多武器匪徒调查的第一部分，我们已经定义了描述匪徒问题所需的所有基本术语和方程。</p><p id="965b" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">然而，到目前为止，我们只给出了土匪问题的一个高层次的概述，没有真正进入问题本身。我们将在接下来的部分中纠正这一点，届时我们将全面检查一些可用于解决这一问题的主要策略。</p><p id="1944" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">更重要的是，我们将使用这些策略让机器人宝宝充电，然后回到妈妈身边！</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="5832" class="ou lj it bd lk ov ow dn lo ox oy dp ls na oz pa lu nb pb pc lw nc pd pe ly iz bi translated">接下来:<a class="ae lh" rel="noopener" target="_blank" href="/multi-armed-bandits-part-2-5834cb7aba4b">第二部分:强盗框架</a></h2></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="5d51" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">参考</h1><p id="c9ae" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm na nr mp mq nb ns mt mu nc nt mx my mz im bi translated">[1]《<a class="ae lh" href="http://www.incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mf">强化学习:导论</em> </a>》，萨顿&amp;巴尔托(2018)</p></div></div>    
</body>
</html>