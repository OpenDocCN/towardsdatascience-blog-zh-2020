<html>
<head>
<title>Vice Presidential and Presidential Debate Analysis using Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用数据科学分析副总统和总统辩论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vice-presidential-and-presidential-debate-analysis-using-data-science-61d542f0c974?source=collection_archive---------29-----------------------#2020-10-24">https://towardsdatascience.com/vice-presidential-and-presidential-debate-analysis-using-data-science-61d542f0c974?source=collection_archive---------29-----------------------#2020-10-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6ffe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用数据科学的辩论分析:使用YouTube评论发现选民的真实意图</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5381291a1503540914c4319b8ad5bbfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-_-RyOvgu2Q3dBhf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/5DDYHjk_KMU" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><h1 id="78d8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><span class="l lo lp lq bm lr ls lt lu lv di"> I </span>简介</h1><blockquote class="lw lx ly"><p id="0856" class="lz ma mb mc b md me jr mf mg mh ju mi mj mk ml mm mn mo mp mq mr ms mt mu mv ij bi translated">我<!-- -->相信数据科学让我以我从未想象过的方式表达我的好奇心。数据科学中最酷的事情是，我不是将数据视为数字，而是将数据视为<strong class="mc ir">机会</strong>(业务问题)<strong class="mc ir">洞察力</strong>(预测建模、统计和数据争论)，以及<strong class="mc ir">改进</strong>(指标)。带着这个想法，我决定<strong class="mc ir">分析一下VP和总统辩论的YouTube评论。</strong></p></blockquote><p id="8df5" class="pw-post-body-paragraph lz ma iq mc b md me jr mf mg mh ju mi mw mk ml mm mx mo mp mq my ms mt mu mv ij bi mz translated">在从新闻来源得到混杂的结果后，我想用数据科学来分析副总统和总统的辩论。<br/>我们的想法是使用YouTube评论作为媒介，获取关于辩论的情绪，并从数据中获得见解。在这个分析中，我们<strong class="mc ir">绘制常用短语</strong>、<strong class="mc ir">常用词</strong>，我们还<strong class="mc ir">分析情绪</strong>，最后，我向我所有的数据科学从业者展示了一个<strong class="mc ir">完整的数据集，其中包含了YouTube上副总统的评论和总统辩论</strong>。</p><h1 id="461b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">如何以及为什么</h1><p id="f3b7" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">hy:在从新闻来源获得关于辩论结果的混合结果后，我决定使用数据科学来帮助我看到结果的结果。随着选举的临近，技术或者更准确地说是分析在塑造我们的思想和支持我们的假设方面发挥了关键作用。</p><p id="1640" class="pw-post-body-paragraph lz ma iq mc b md me jr mf mg mh ju mi mw mk ml mm mx mo mp mq my ms mt mu mv ij bi mz translated">为了分析YouTube评论，我们使用Python和其他各种NLP库，然后使用一些数据可视化工具。我们将使用被称为熊猫的令人敬畏的数据争论图书馆的奇迹，我们希望找到一些有趣的见解。</p><h1 id="704f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">要求</h1><p id="5968" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">对于这个项目，我们要求:</p><ul class=""><li id="27fb" class="nf ng iq mc b md me mg mh mw nh mx ni my nj mv nk nl nm nn bi translated">Python 3.8</li><li id="00ab" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">熊猫</li><li id="9e17" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">sci kit-学习</li><li id="13ef" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">Numpy</li><li id="3ce6" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">海生的</li><li id="1b3e" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">NLTK</li><li id="dc73" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">Wordcloud</li><li id="fe2e" class="nf ng iq mc b md no mg np mw nq mx nr my ns mv nk nl nm nn bi translated">文本Blob</li></ul><h1 id="51c7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据集的创建</h1><p id="bc5f" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">数据集包含YouTube上关于最受欢迎/最受关注的副总统和总统辩论的评论。我们使用YouTube数据API来获得所有评论(由于大小限制，我们每个视频只能获得100条评论)。这些视频是作者经过仔细审查挑选出来的，准确地说，我们关注的是最高浏览量和最高YouTube评论数。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="6c39" class="ny kx iq nu b gy nz oa l ob oc">def clean_text(string):<br/>    string = re.sub(r'[^\w\s]', '', string) <br/>    return ''.join(i for i in string if ord(i) &lt; 128)</span><span id="e635" class="ny kx iq nu b gy od oa l ob oc">def remove_stopwords(string):<br/>    stop_words = set(stopwords.words('english')) <br/>    word_tokens = word_tokenize(string) <br/>    filtered_sentence = [w for w in word_tokens if not w in stop_words] <br/>    filtered_sentence = [] <br/>  <br/>    for w in word_tokens: <br/>        if w not in stop_words: <br/>            filtered_sentence.append(w)</span><span id="3fc1" class="ny kx iq nu b gy od oa l ob oc">return ' '.join(filtered_sentence)</span></pre><blockquote class="oe"><p id="6263" class="of og iq bd oh oi oj ok ol om on mv dk translated">这些函数定义了文本的清理和停用词的删除。</p></blockquote><h2 id="6b83" class="ny kx iq bd ky oo op dn lc oq or dp lg mw os ot li mx ou ov lk my ow ox lm oy bi translated">YouTube评论提取</h2><p id="ec41" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">下面的函数定义了我们丢弃的video _ ids和一些基本的提取代码。我们以JSON的形式获取数据，然后执行预处理，最后将所有评论和情感评分(使用TextBlob)组合成一个数据帧。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="2b0d" class="ny kx iq nu b gy nz oa l ob oc">video_ids = {"vp_debate":['65twkjiwD90','t_G0ia3JOVs','xXE6I3gWiMc'], "prez_debate":[<br/>                                                                         'yW8nIA33-zY','wW1lY5jFNcQ','K8Z9Kqhrh5c']}</span><span id="3ea0" class="ny kx iq nu b gy od oa l ob oc">video_id = "&amp;videoId="<br/>content = None</span><span id="681d" class="ny kx iq nu b gy od oa l ob oc">def extract_comments(resp):<br/>    """<br/>        Get comments from the resp (json) yt comment<br/>    """<br/>    com = []<br/>    for i in resp['items']:<br/>        com.append(i['snippet']['topLevelComment']['snippet']['textOriginal'].lower())<br/>    return com</span><span id="6f8a" class="ny kx iq nu b gy od oa l ob oc">def get_videos_comments():<br/>    """<br/>        Extract video comments and store in the arrays<br/>    """<br/>    vp_debate_data = []<br/>    prez_debate_data = []<br/>    for i,j in video_ids.items():<br/>        print("Getting for: ",i)<br/>        for id in j:<br/>            video_id = "&amp;videoId=" + id<br/>            resp = requests.get(URL+video_id)<br/>            <br/>            print("Length: ",len(resp.content))<br/>            content = resp.json()<br/>            comments = extract_comments(content)<br/>            if i == "vp_debate":<br/>                vp_debate_data.extend(comments)<br/>            else:<br/>                prez_debate_data.extend(comments)<br/>    return vp_debate_data, prez_debate_data<br/>class Comment:<br/>    def __init__(self, text):<br/>        self.text = remove_stopwords(clean_text(text))<br/>        # set threshold as 0.5<br/>        self.sentiment = 0 if TextBlob(self.text).sentiment.subjectivity &lt;= 0.5 else 1<br/>    def return_comment(self):<br/>        return self</span><span id="00b4" class="ny kx iq nu b gy od oa l ob oc">class DataGenerator:<br/>    def __init__(self, vp_debate_data, prez_debate_data):<br/>        self.vp_data = {"comments":[], "sentiment":[]}<br/>        self.prez_data = {"comments":[], "sentiment":[]}<br/>        for i in vp_debate_data:<br/>            c = Comment(i)<br/>            self.vp_data['comments'].append(c.text)<br/>            self.vp_data['sentiment'].append(c.sentiment)<br/>        for i in prez_debate_data:<br/>            c = Comment(i)<br/>            self.prez_data['comments'].append(c.text)<br/>            self.prez_data['sentiment'].append(c.sentiment)<br/>        self.df_vp = pd.DataFrame(self.vp_data)<br/>        self.df_prez = pd.DataFrame(self.prez_data)<br/>    def return_df(self):<br/>        print("Loaded dataframe.")<br/>        return self.df_vp, self.df_prez<br/>    <br/>    # return corpus given the debate key<br/>    # to be used for getting the commonkeywords and plotting<br/>    def get_corpus(self, key="vp_debate"):<br/>        corpus = []<br/>        if key == "vp_debate":<br/>            corpus = [i for i in self.vp_data['comments']]<br/>        else:<br/>            corpus = [i for i in self.prez_data['comments']]  <br/>        <br/>        return corpus</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/83998a386a673722716170ca2d5a6ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*x68VZIHVgM9JpAhfNXYyYg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们数据的预览</p></figure><h1 id="5484" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">分析</h1><p id="62ea" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">在检查数据时，我们首先将单词小写，然后去掉所有的标点符号，最后去掉所有的停用词。我们使用流行的文本块(【https://textblob.readthedocs.io/en/dev/】)来执行快速情感分析。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/9987bcfc214f3d818dd78242ad6c7382.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*jeBtF8deS6IitwWj1Mi74Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代表评论情感分析的条形图。</p></figure><blockquote class="oe"><p id="1a04" class="of og iq bd oh oi oj ok ol om on mv dk translated">这些数据并没有提供太多的信息，但是如果我们试着用特定的词来分类或分组，那么我们可以得到更好的理解。</p></blockquote><h1 id="2606" class="kw kx iq bd ky kz la lb lc ld le lf lg jw pb jx li jz pc ka lk kc pd kd lm ln bi translated">最常见的单词</h1><p id="14b6" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">我们找到每场辩论中最常见的单词(即df_vp和df_prez)。我们使用称为NLTK的流行包来获取最常见的单词(经过预处理)</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="4288" class="ny kx iq nu b gy nz oa l ob oc">def most_common(corpus):<br/>    fd = nltk.FreqDist(corpus)<br/>    print(fd)<br/>    fd_t10=fd.most_common(10)<br/>    counter = dict(fd_t10)<br/>    # pd.Dataframe from dict to show bar plot<br/>    df = pd.DataFrame.from_dict(counter, orient='index')<br/>    names, values = zip(*fd_t10)<br/>    # plot method 1<br/>    fd.plot(10)<br/>    # plot method two<br/>    print("Bar Plot")<br/>    df.plot(kind='bar')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/4452d574e001bd106aa413c921653ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ascHiryahTWAb_YsOD_lnA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">副总统辩论视频中的词频分布</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/2e6e4c59438039999547e596551f805a.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*76wv9UAWqg2ApJ3R9_19rA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">总统辩论视频中的词频分布</p></figure><blockquote class="oe"><p id="cd10" class="of og iq bd oh oi oj ok ol om on mv dk translated">正如你在总统辩论视频中看到的，最常见的词是“特朗普”和“拜登”。</p></blockquote><h1 id="110c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw pb jx li jz pc ka lk kc pd kd lm ln bi translated">WordCloud</h1><blockquote class="oe"><p id="93ef" class="of og iq bd oh oi pg ph pi pj pk mv dk translated">词云或标签云是词频的图形表示，对在源文本中出现频率较高的词给予更大的重视。视觉效果中的单词越大，该单词在文档中就越常见。</p></blockquote><pre class="pl pm pn po pp nt nu nv nw aw nx bi"><span id="3608" class="ny kx iq nu b gy nz oa l ob oc">def plot_wordcloud(kind="vp_debate"):<br/>    words = get_word_list(kind)<br/>    wordcloud = WordCloud(width = 800, height = 800, <br/>                background_color ='white', <br/>                min_font_size = 10).generate(' '.join(words))<br/>    plt.imshow(wordcloud) <br/>    plt.axis("off") <br/>    plt.tight_layout(pad = 0) <br/>    plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/ec162f4e9ecff3035a4a972239b53144.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*EK6RnE2elj73KBMnIwDjIQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">vp _辩论和prez _辩论的词云表示</p></figure><h1 id="0a7e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">查找关键词:搜索特定单词的独特方式。</h1><p id="f206" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated">这个特性可以找到指定的关键字(最多前10个单词)。它作用于每个句子。</p><pre class="kg kh ki kj gt nt nu nv nw aw nx bi"><span id="5100" class="ny kx iq nu b gy nz oa l ob oc">def find_keywords(word, df):<br/>    """<br/>        Find the specified keyword (most prob top 10 words)<br/>        Acts on each sentence<br/>        Add to the corresponding dataframe<br/>        Returns: DataFrame<br/>        <br/>        <br/>        Used inconjunction with get_most_common_words()<br/>        <br/>    """<br/>    for i in df.comments:<br/>        if word in i.split():<br/>            c = Counter(i.split())<br/>            if c[word] &gt;= 2:<br/>                print(f'Word is {word} that occured {c[word]} times where sentence is {i}\n')<br/><a class="ae kv" href="http://twitter.com/interact" rel="noopener ugc nofollow" target="_blank">@interact</a>(keyword="biden", debates=['vp_debate','prez_debate'])<br/>def run(keyword, debates):<br/>    df = None<br/>    if debates == "vp_debate":<br/>        df = df_vp<br/>    else:<br/>        df = df_prez<br/>    find_keywords(keyword,df)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pr"><img src="../Images/cd83c83f28767358b68b833038b7143c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhN-MjWyqqBgJopcFQ9qcg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出的一瞥。(提供演示)</p></figure><h1 id="59fd" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="7d3e" class="pw-post-body-paragraph lz ma iq mc b md na jr mf mg nb ju mi mw nc ml mm mx nd mp mq my ne mt mu mv ij bi mz translated"><span class="l lo lp lq bm lr ls lt lu lv di">我们</span>看到，通过使用数据科学，我们能够收集关于数据的有趣见解。我们当然可以通过使用更好的情感分类器和测试不同的假设来改进我们的工作。我们也可以使用n-grams并比较它们的频率。</p><p id="a954" class="pw-post-body-paragraph lz ma iq mc b md me jr mf mg mh ju mi mw mk ml mm mx mo mp mq my ms mt mu mv ij bi translated">试试这里:https://gesis . my binder . org/binder/v2/GH/aaditkapoor/debate analysis/main dataset这里:<a class="ae kv" href="http://www.kaggle.com/dataset/43532333d82042a1287e00672b86a2c76e76ffbc4d85569715309714635172b0" rel="noopener ugc nofollow" target="_blank">http://www . ka ggle . com/dataset/43532333d 82042 a 1287 e 00672 b 86 a2 c 76 e 76 ffbc 4d 85569715309714635172 b 0</a></p></div></div>    
</body>
</html>