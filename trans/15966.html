<html>
<head>
<title>Introduction to the Deadly Triad Issue of Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">介绍强化学习的致命问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-the-deadly-triad-issue-of-reinforcement-learning-53613d6d11db?source=collection_archive---------21-----------------------#2020-11-03">https://towardsdatascience.com/introduction-to-the-deadly-triad-issue-of-reinforcement-learning-53613d6d11db?source=collection_archive---------21-----------------------#2020-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi gj"><img src="../Images/f8b23c06955c2101341d04eb4ec7a9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6t8dsYXBnDYXfGoGpAVBLw.jpeg"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">作者图片</p></figure><p id="93a3" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">当我了解深度强化学习时，我阅读了致命三合问题，但找不到任何令人满意的简单解释(除了科学论文)。因此，我写了两篇文章用直觉(因此很少用数学)解释它是什么，然后是如何处理的。</p><p id="24c5" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">这个问题表明，当一个人试图结合<em class="kz">TD-学习(或引导)</em>、<em class="kz">偏离策略</em>学习和<em class="kz">函数逼近</em>(如深度神经网络)时，可能会出现不稳定和发散。</p><h1 id="b315" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">强化学习问题</h1><p id="f305" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">我认为你熟悉强化学习问题的典型表述。如果没有，请查看以下一个(或多个)来源:</p><div class="md me gp gr mf mg"><a href="https://www.guru99.com/reinforcement-learning-tutorial.html" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ir gy z fp ml fr fs mm fu fw ip bi translated">强化学习:什么是，算法，应用，例子</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">强化学习被定义为一种机器学习方法，它关注的是软件代理应该如何采取…</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">www.guru99.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu jv mg"/></div></div></a></div><figure class="mv mw mx my gt jq"><div class="bz fp l di"><div class="mz na l"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">Arxiv Insights的视频，非常清晰完整</p></figure><figure class="mv mw mx my gt jq"><div class="bz fp l di"><div class="mz na l"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">大卫·西尔弗的精彩讲座(每次1.5小时)</p></figure><p id="13c4" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">我将使用通常的符号。</p><h1 id="4107" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">学习价值函数</h1><p id="f3c8" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">我们想要解决RL问题的方法是学习价值函数。特别是，我们对Q值函数感兴趣，对于我们所处的任何状态，以及我们选择的任何行动，都会给我们带来回报(行动有多好)。</p><p id="aa7d" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">Q值函数<strong class="kd ir">给出的不是执行这个动作后获得的奖励</strong>，而是执行这个动作后，如果你遵循你的策略，你将获得的所有奖励的总和(潜在折扣)，称为<strong class="kd ir">回报</strong>:</p><figure class="mv mw mx my gt jq gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/3563b29a2e292648e9c683311040ef8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*77zXybdJSIRXL5Hriv2PJg.png"/></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">回报方程</p></figure><h2 id="070d" class="nc lb iq bd lc nd ne dn lg nf ng dp lk km nh ni lo kq nj nk ls ku nl nm lw nn bi translated">使用随机ε-贪婪策略</h2><p id="4a75" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">如果代理人想知道一个动作有多好，他首先要尝试一下。这将把他带到另一个状态，在那里他可以在不同的行动之间再次选择。这个想法是:</p><ol class=""><li id="bed8" class="no np iq kd b ke kf ki kj km nq kq nr ku ns ky nt nu nv nw bi translated">探索:尝试许多动作</li><li id="3d05" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">利用:更多地使用积极的高回报行为，以获得最佳策略。</li></ol><p id="7b90" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">因此，一个经典的方法是首先尝试完全随机的行动，这样你就可以了解每个状态下哪个行动是好的，哪个是坏的，并逐步优先考虑给你最积极回报的行动。</p><p id="18e9" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">因此，策略在这里(在强化学习的大部分时间里)<strong class="kd ir">随机</strong>，这意味着它随机地行动。<br/>注意不要将其与一致随机混淆，我这里不是说代理完全随机地选择动作，它试图以更高的概率选择好的动作而不是坏的动作，但是它必须探索、测试它不知道结果的动作，因此不能有完全确定的策略。</p><p id="fdc6" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">基于贝尔曼方程，有不同的方法来学习该函数，例如动态编程、蒙特卡罗方法和自举。</p><h2 id="d386" class="nc lb iq bd lc nd ne dn lg nf ng dp lk km nh ni lo kq nj nk ls ku nl nm lw nn bi translated">动态规划</h2><p id="c213" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">如果你有一个精确的环境模型(例如，在一个特定的状态下执行一个特定的动作，你将获得什么样的回报和状态)，你可以使用动态编程来计算你的状态和动作的值。</p><p id="622e" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">然而，对于有许多状态和动作的环境，或者随机的环境，获得环境的模型是非常昂贵的。因此，人们更喜欢使用蒙特卡罗技术(比如AlphaGo的Deepmind)。</p><h2 id="b988" class="nc lb iq bd lc nd ne dn lg nf ng dp lk km nh ni lo kq nj nk ls ku nl nm lw nn bi translated">蒙特卡洛</h2><p id="ae45" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">使用蒙特卡罗，代理通过选择它来更新他所考虑的动作的值，然后遵循他的策略直到情节终止(它到达最终状态)。然后，它可以汇总他通过这一集获得的所有回报，并获得回报的近似值。重复这个过程足够多次将最终收敛到状态动作的真实Q值。</p><p id="7b9a" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">如果你有很长的一集，这也是非常昂贵的，因为你必须等待一集结束才能获得回报。</p><h2 id="e8ba" class="nc lb iq bd lc nd ne dn lg nf ng dp lk km nh ni lo kq nj nk ls ku nl nm lw nn bi translated">时差学习</h2><p id="ace3" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">为了找到正确的Q值，我们可以观察到，对于每个状态/动作对的<strong class="kd ir">,它应该收敛于回报(奖励的贴现总和)。状态动作/对的回报是:</strong></p><ol class=""><li id="4257" class="no np iq kd b ke kf ki kj km nq kq nr ku ns ky nt nu nv nw bi translated">这次行动获得的报酬(R)</li><li id="eb34" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">下一个状态/动作的返回，在下一个状态中选择下一个动作后获得，由其Q值近似</li></ol><figure class="mv mw mx my gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi oc"><img src="../Images/e9333643c71818a9e1c73099dba943f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELlsnnzFRGYumUlYWL-vNQ.png"/></div></div></figure><p id="980a" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">为什么经纪人要等剧集结束？他可以做以下事情:</p><ol class=""><li id="a9a6" class="no np iq kd b ke kf ki kj km nq kq nr ku ns ky nt nu nv nw bi translated">从状态S，根据Q值(以ε-贪婪的方式)选择一个动作(<em class="kz"> a </em></li><li id="f06a" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">观察奖励(<em class="kz"> R </em>)和新状态(<em class="kz">S’</em>)</li><li id="fefe" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">再次选择一个动作(<em class="kz"> a' </em>)，观察它的Q值</li><li id="c782" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">使用这个新状态和动作的Q值作为剩余回报的近似<strong class="kd ir">:</strong></li></ol><figure class="mv mw mx my gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi od"><img src="../Images/bb6ef8c540f9d4cc6ba5efe5bd393cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PY5iX48TbYGJGp9JIggySA.png"/></div></div></figure><p id="1e93" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">这里，出于稳定性原因，该值用α因子逐步更新。</p><p id="f5c5" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">这是对<em class="kz"> SARSA算法</em>的描述。它使用自举技术，该技术在于使用下一个值的估计来估计该值(而不是等待整个剧集)。</p><p id="7e37" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">下图说明了这三种技术是如何工作:</p><figure class="mv mw mx my gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi oe"><img src="../Images/f3c67f3ea25f18374140909506bcf6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgP82t7OhBjvocxoQm-tjA.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated"><em class="of">大卫·西尔弗的RL课程</em> <a class="ae og" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="of">第四讲</em> </a> <em class="of">:“无模型预测”</em></p></figure><p id="761d" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">TD-learning和Monte Carlo混合存在，它们被归入<em class="kz"> TD( λ) </em>族。</p><h1 id="e279" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">q学习</h1><p id="40f8" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">SARSA是一种基于策略的学习技术，这意味着它遵循自己的策略来学习价值函数。另一个想法是直接使用next的Q值的最大值来计算回报。<br/>这被称为Q-Learning，具体如下:</p><ol class=""><li id="3a5a" class="no np iq kd b ke kf ki kj km nq kq nr ku ns ky nt nu nv nw bi translated">从状态S，根据Q值(以ε-贪婪的方式)选择一个动作(<em class="kz"> a </em>)</li><li id="a494" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">观察奖励(<em class="kz"> R </em>)和新状态(<em class="kz">S’</em>)</li><li id="417d" class="no np iq kd b ke nx ki ny km nz kq oa ku ob ky nt nu nv nw bi translated">使用下一状态的最大Q值来更新当前Q值:</li></ol><figure class="mv mw mx my gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi oc"><img src="../Images/9f9df28ae461531092267f53dd30706e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBm3t5ClSLmcLVcfcehHjw.png"/></div></div></figure><p id="b574" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">因此，代理使用了最佳的可能Q值，即使他没有采取相应的行动。这是连贯的，因为他总是选择最佳行动来结束。因此，这有助于加快学习过程。</p><p id="5f5b" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">这被称为偏离策略学习，因为你不再使用当前遵循的策略来更新你的Q值函数(而是一个贪婪的策略)。</p><p id="4e2c" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">政策外学习有优点也有缺点，你可以在这里查阅<a class="ae og" href="https://datascience.stackexchange.com/questions/13029/what-are-the-advantages-disadvantages-of-off-policy-rl-vs-on-policy-rl" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="a7f8" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">函数逼近(神经网络)</h1><p id="cfeb" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">当状态和动作的数量过多时，或者当在连续的环境中进化时(这意味着在计算机上是相同的)，人们不能为每个可能的状态或状态-动作对创建一个价值函数。</p><p id="ea2d" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">因此，这个想法是使用函数逼近，如神经网络。然后将状态输入神经网络，并获得每个动作的Q值作为输出。因此，这是容易处理的，然后我们可以使用梯度上升技术将这个神经网络变成一个良好的Q值函数近似值，告诉我们为每张图像按下哪个按钮(或按钮组合)。</p><figure class="mv mw mx my gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi oh"><img src="../Images/165947860785668e21b16246be43b0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mwPQwlESW5nrh45ZbmtPLw.png"/></div></div><p class="jx jy gj gh gi jz ka bd b be z dk translated">作者图片</p></figure><p id="b375" class="pw-post-body-paragraph kb kc iq kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky ij bi translated">使用近似法的一个明显的优点是，你可以得到相似状态的相似值(这似乎是一致的和有用的)，但是我们将在下一篇文章中看到这一点。</p><h1 id="7361" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">致命的黑社会问题</h1><p id="ee16" class="pw-post-body-paragraph kb kc iq kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky ij bi translated">致命的三位一体问题是这样一个事实，即自举、偏离策略学习和函数逼近的结合最终会导致很多不稳定，或者没有收敛。在下一篇文章中，我会解释为什么会发生这种情况以及如何处理。</p></div></div>    
</body>
</html>