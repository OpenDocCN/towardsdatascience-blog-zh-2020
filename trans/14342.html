<html>
<head>
<title>Unlocking the True Power of Support Vector Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">释放支持向量回归的真正力量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0?source=collection_archive---------0-----------------------#2020-10-03">https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0?source=collection_archive---------0-----------------------#2020-10-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bf806a1ee6e36f7f8b54b58ccf28c249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suieHavPXiz0SAYKSYwS2Q.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2104445" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="0263" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">用支持向量机解决回归问题</h2></div><p id="8090" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">支持向量机是机器学习中处理分类问题的最流行和最广泛使用的算法之一。然而，支持向量机在回归中的使用并没有很好的记录。该算法承认数据中非线性的存在，并提供了一个熟练的预测模型。</p><p id="6385" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我将首先通过深入研究算法背后的理论，让您对算法有一个直观的理解。然后，我们将建立我们自己的SVM回归模型。最后，我们将探讨使用支持向量回归机的一些优点。</p><p id="8f0e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SVM回归算法被称为<strong class="kx jh">支持向量回归</strong>或<strong class="kx jh"> SVR </strong>。在开始学习算法之前，我们有必要对支持向量机有一个直观的了解。</p><h1 id="caea" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">支持向量机</h1><p id="50c2" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在机器学习中，支持向量机是具有相关学习算法的监督学习模型，这些算法分析用于分类和回归分析的数据。在支持向量回归中，拟合数据所需的直线被称为<strong class="kx jh">超平面</strong>。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/3cca15c97e905f49c11b9b9facca1bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ok387eN9Z0aFWRdz4ulYWg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://commons.wikimedia.org/wiki/File:Regressions_sine_demo.svg" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><p id="8492" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">支持向量机算法的目标是在n维空间中找到一个超平面，该超平面清楚地分类数据点。超平面两侧最接近超平面的数据点称为<strong class="kx jh">支持<em class="mt"> </em>向量</strong>。这些影响超平面的位置和方向，从而有助于建立SVM。</p><h1 id="e023" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">SVR中的超参数</strong></h1><p id="04b4" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">既然我们对什么是支持向量机有了直观的认识，我们将研究支持向量回归中使用的各种超参数。使用的一些关键参数如下所述:</p><h2 id="b588" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak"> 1。超平面:</strong></h2><p id="a5c4" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">超平面是用于预测连续输出的决策边界。超平面任一侧最接近该超平面的数据点称为支持<em class="mt"> </em>向量。这些用于绘制显示算法预测输出的所需线条。</p><h2 id="dcdb" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak"> 2。内核:</strong></h2><p id="5793" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">内核是一组数学函数，它将数据作为输入，并将其转换为所需的形式。这些通常用于在高维空间中寻找超平面。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/81559a4a2371eab9ac7f52a9e988cae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*yVbqz60gtXoMwhdLZ34hfQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank"> Sci Kit Learn </a></p></figure><p id="9fff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">应用最广泛的核包括<strong class="kx jh">线性</strong>、<strong class="kx jh">非线性</strong>、<strong class="kx jh">多项式</strong>、<strong class="kx jh">径向基函数</strong>和<strong class="kx jh"> Sigmoid </strong>。默认情况下，RBF用作内核。这些核的使用取决于数据集。</p><h2 id="d88a" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak"> 3。</strong> <strong class="ak">边界线:</strong></h2><p id="c8fb" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">这是在超平面周围距离<strong class="kx jh">ε(ε)</strong>处画的两条线。它用于在数据点之间创建边距。</p><h1 id="33e7" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">支持向量回归</strong></h1><p id="9fd4" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">支持向量回归是一种监督学习算法，用于预测离散值。支持向量回归机使用与支持向量机相同的原理。支持SVR的基本思想是找到最佳拟合线。在SVR中，最佳拟合线是具有最大点数的超平面。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/ec235a635b38ca1fed598f4df955e02b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Gr72C6I-u0ZWdgBxI47JBw.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="http://www.semspirit.com/artificial-intelligence/machine-learning/regression/support-vector-regression/support-vector-regression-in-r/" rel="noopener ugc nofollow" target="_blank"> Semspirit </a></p></figure><p id="62ad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与试图最小化实际值和预测值之间的误差的其他回归模型不同，SVR试图在阈值内拟合最佳直线。阈值是超平面和边界线之间的距离。SVR的拟合时间复杂度大于样本数量的二次方，这使得它很难扩展到具有超过10000个样本的数据集。</p><p id="17c7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于大型数据集，使用<strong class="kx jh">线性SVR </strong>或<strong class="kx jh"> SGD回归器</strong>。线性SVR提供了比SVR更快的实现，但是只考虑线性核。由支持向量回归产生的模型仅依赖于训练数据的子集，因为成本函数忽略了预测接近其目标的样本。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/19900273a9ef91670d62021557d91a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*Me9NLgr9v2xNn51yReaixQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://blogs.mathworks.com/loren/2011/01/13/data-driven-fitting/" rel="noopener ugc nofollow" target="_blank"> MathWorks博客</a></p></figure><p id="f989" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们有了什么是支持向量回归机的要点，我们将尝试构建我们自己的SVR回归机。构建这个回归模型的代码和其他资源可以在这里找到<a class="ae jd" href="https://github.com/ashwinraj-in/MachineLearningTutorials/blob/master/SupportVectorRegression.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="7a42" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak">第一步:导入所需的库</strong></h2><p id="b5ff" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">我们的第一步是导入构建模型所需的库。没有必要在一个地方导入所有的库。Python给了我们在任何地方导入库的灵活性。首先，我们将导入Pandas、Numpy、Matplotlib和Seaborn库。</p><pre class="mp mq mr ms gt nj nk nl nm aw nn bi"><span id="d6c1" class="mu ls jg nk b gy no np l nq nr">#Import the Libraries and read the data into a Pandas DataFrame</span><span id="f448" class="mu ls jg nk b gy ns np l nq nr">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="c846" class="mu ls jg nk b gy ns np l nq nr">test = pd.read_csv("california_housing_test.csv")<br/>train = pd.read_csv("california_housing_train.csv")</span></pre><p id="a6ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦导入了这些库，我们的下一步将是获取数据集并将数据加载到我们的笔记本中。对于这个例子，我使用了加州住房数据集。</p><h2 id="11c4" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak">第二步:可视化数据</strong></h2><p id="437b" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">成功加载数据后，我们的下一步是可视化这些数据。<strong class="kx jh">海滨</strong>是一个优秀的库，可以用来可视化数据。</p><pre class="mp mq mr ms gt nj nk nl nm aw nn bi"><span id="307e" class="mu ls jg nk b gy no np l nq nr">#Visualise the data</span><span id="324e" class="mu ls jg nk b gy ns np l nq nr">plt.figure()<br/>sns.heatmap(data.corr(), cmap='coolwarm')<br/>plt.show()</span><span id="9e71" class="mu ls jg nk b gy ns np l nq nr">sns.lmplot(x='median_income', y='median_house_value', data=train)<br/>sns.lmplot(x='housing_median_age', y='median_house_value', data=train)</span></pre><h2 id="8b67" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak">第三步:特征工程</strong></h2><p id="a704" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">特征工程是利用领域知识通过数据挖掘技术从原始数据中提取特征的过程。对于这个模型，我选择了只有数值的列。为了处理分类值，应用了标签编码技术。</p><pre class="mp mq mr ms gt nj nk nl nm aw nn bi"><span id="70f0" class="mu ls jg nk b gy no np l nq nr">#Select appropriate features</span><span id="b295" class="mu ls jg nk b gy ns np l nq nr">data = data[[‘total_rooms’, ‘total_bedrooms’, ‘housing_median_age’, ‘median_income’, ‘population’, ‘households’]]<br/>data.info()</span><span id="0ea6" class="mu ls jg nk b gy ns np l nq nr">data['total_rooms'] = data['total_rooms'].fillna(data['total_rooms'].mean())<br/>data['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].mean()</span></pre><p id="9939" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要素缩放基本上有助于在特定范围内归一化数据。通常，几个常见的类类型包含特征缩放功能，以便自动进行特征缩放。</p><h2 id="3f10" class="mu ls jg bd lt mv mw dn lx mx my dp mb le mz na md li nb nc mf lm nd ne mh nf bi translated"><strong class="ak">第四步:拟合模型</strong></h2><p id="9c33" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">选择所需参数后，下一步是从sklearn库中导入train_test_split，该库用于将数据集拆分为训练和测试数据。</p><pre class="mp mq mr ms gt nj nk nl nm aw nn bi"><span id="739d" class="mu ls jg nk b gy no np l nq nr">#Split the dataset into training and testing data</span><span id="4c55" class="mu ls jg nk b gy ns np l nq nr">import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 0)</span><span id="0832" class="mu ls jg nk b gy ns np l nq nr">y_train = y_train.reshape(-1,1)<br/>y_test = y_test.reshape(-1,1)</span></pre><p id="b710" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在此之后，从<strong class="kx jh"> sklearn.svm </strong>导入<strong class="kx jh"> SVR </strong>，并且模型适合训练数据集。</p><pre class="mp mq mr ms gt nj nk nl nm aw nn bi"><span id="e097" class="mu ls jg nk b gy no np l nq nr"># Fit the model over the training data</span><span id="6992" class="mu ls jg nk b gy ns np l nq nr">from sklearn.svm import SVR<br/>regressor = SVR(kernel = 'rbf')<br/>regressor.fit(X_train, y_train)</span></pre><p id="d6b9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，在这个特殊的例子中，我使用了RBF核。模型的其他参数保留其默认配置。一旦模型适合训练数据，我们的模型就可以使用了。</p><h1 id="dfb3" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">支持向量回归的优势</strong></h1><p id="805d" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">尽管支持向量回归很少使用，但它具有如下优点:</p><ol class=""><li id="2531" class="nt nu jg kx b ky kz lb lc le nv li nw lm nx lq ny nz oa ob bi translated">它对异常值是鲁棒的。</li><li id="3bd5" class="nt nu jg kx b ky oc lb od le oe li of lm og lq ny nz oa ob bi translated">决策模型可以很容易地更新。</li><li id="e7ec" class="nt nu jg kx b ky oc lb od le oe li of lm og lq ny nz oa ob bi translated">它具有良好的泛化能力，预测精度高。</li><li id="fab4" class="nt nu jg kx b ky oc lb od le oe li of lm og lq ny nz oa ob bi translated">它的实现很容易。</li></ol><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1f7b72730b2ec116039b7061d8a0c3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*Z6vm0LZuxqeEo5j5pnLl9Q.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由戴尔·阮</p></figure><h1 id="1b71" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">支持向量回归的缺点</h1><p id="d3db" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">支持向量机在处理回归问题时面临的一些缺点如下所述:</p><ol class=""><li id="9a3c" class="nt nu jg kx b ky kz lb lc le nv li nw lm nx lq ny nz oa ob bi translated">它们不适合大型数据集。</li><li id="a214" class="nt nu jg kx b ky oc lb od le oe li of lm og lq ny nz oa ob bi translated">如果每个数据点的特征数量超过了训练数据样本的数量，则SVM将表现不佳。</li><li id="e86a" class="nt nu jg kx b ky oc lb od le oe li of lm og lq ny nz oa ob bi translated">当数据集具有更多噪声时，即目标类重叠时，决策模型的表现不是很好..</li></ol><p id="fbe7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">至此，我们已经到了这篇文章的结尾。我希望这篇文章能够帮助您了解SVR算法背后的思想。如果你有任何问题，或者如果你认为我有任何错误，请联系我！您可以通过<a class="ae jd" href="http://rajashwin812@gmail.com/" rel="noopener ugc nofollow" target="_blank">邮箱</a>或<a class="ae jd" href="http://linkedin.com/in/rajashwin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>