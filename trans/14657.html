<html>
<head>
<title>Linear Regression by Hand</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手工线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-by-hand-python-and-r-79994d47f68?source=collection_archive---------32-----------------------#2020-10-09">https://towardsdatascience.com/linear-regression-by-hand-python-and-r-79994d47f68?source=collection_archive---------32-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f057" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Python和R中</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bddce841ffcb20ad2d940028daf0eb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eTsRHfmFWlMI7ujZ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@chuttersnap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> CHUTTERSNAP </a>拍摄</p></figure><p id="199e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性回归背后的基本思想很简单。用数学术语来说，我们希望用自变量<strong class="ky ir"> X </strong>来预测因变量<strong class="ky ir"> Y </strong>。假设两个变量以线性方式相关，我们可以用简单的线性公式预测<strong class="ky ir"> Y </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/ea69ede848355c1d9cb3b6b44863ece7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*KHRX3hqHIjsNe5ubo2udMg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的线性方程</p></figure><p id="ada9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(波浪等号表示“大约”)。简单地说，一旦我们对两个系数之间的关系有所了解，即我们已经对两个系数<strong class="ky ir"> α </strong>和<strong class="ky ir"> β </strong>进行了近似，我们就可以(有一定把握地)预测y。α<strong class="ky ir">α</strong>代表截距(值<strong class="ky ir"> y </strong>与<strong class="ky ir"> f(x = 0) </strong>)，β<strong class="ky ir">β</strong>是斜率。</p><p id="3a0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了线性回归的帮助，我们可以回答很多问题；例如</p><ul class=""><li id="9c6a" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">"海平面的上升与气温上升有联系吗？",</li><li id="5dd3" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">"有三间卧室的房子会有多贵？"</li><li id="fa4f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">"如果我们增加20%的营销预算，我们能卖出多少产品？"</li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h2 id="8794" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">背后的数学原理</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/c38da304d5b93b2855fa8660c03fba6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ftgPR4GySMUt-7iEwsicA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者10次观察的散点图</p></figure><p id="6be4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来看一些数据！尽管上图中的观察结果有很大偏差，但趋势是明显可见的。这表明<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>之间呈正相关。我们可以利用最小二乘法 的<a class="ae kv" href="https://en.wikipedia.org/wiki/Least_squares" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">方法来近似模拟两者之间关系的线性函数。让</strong></a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/44a99f80790df40a6caea6112ac9e672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*qtxjUfc-bq3-ZA6levVFJQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的一组观察结果</p></figure><p id="220a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是一组观察值，用散点图中的点表示。现在可以应用最小二乘法来近似系数<strong class="ky ir"> α </strong>和<strong class="ky ir"> β </strong>。</p><p id="e43f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法最小化线性回归线的误差平方和(用机器学习术语来说，这是线性回归的成本函数)。误差在下一个图中用橙色线表示。通过将方程微分为β，我们得到以下公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/8aacb6aed7540f41660b8eec4829e462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*-yze6gesrU3SkADCpH97xQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者对β的最小二乘法</p></figure><p id="5e5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">顶部带有<em class="nk">杆</em>的<strong class="ky ir"> x </strong>和<strong class="ky ir"> y </strong>代表<strong class="ky ir"> x </strong>和<strong class="ky ir"> y </strong>的样本平均值。</p><p id="25cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过将近似的<strong class="ky ir"> β </strong>代入线性方程，我们得到<strong class="ky ir"> α </strong>的近似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/4b92ec840d98a1daf33eaa4dd358c632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*H9_aMhlprdGnsPE-NUyLHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者给出的Alpha近似值</p></figure><p id="68ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者，你可以通过<a class="ae kv" href="https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point." rel="noopener ugc nofollow" target="_blank">梯度下降</a>来最小化成本函数。现在我们已经知道了线性回归的近似系数，我们可以绘制方程并进行第一次目视检查。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/4db48fb6ce08ea813c27a5e90270a4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5uP8WV4_8OOEA4ji6D-AQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者列出的带回归线的散点图</p></figure><p id="9ee7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图中的绿线代表线性方程，我们刚刚对其系数进行了近似。橙色箭头是误差(也称为残差)，它们告诉我们观察值与预测值的偏差。误差总和越小，回归线拟合得越好。总和为0将是一个完美的拟合，这意味着所有的观察都是完全线性的。</p><h2 id="d925" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">残留物分析</h2><p id="0a26" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">对误差/残差的解释是建立回归模型的重要步骤。通过观察残差，我们可以知道它们是如何分布的(平均值为0的正态分布是最好的)以及它们是否是同方差的，即残差的方差是否是常数。</p><p id="b91e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">残差平方和(RSS)和残差标准差(RSE) </strong></p><p id="cba3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">残差是预测值<strong class="ky ir"> y hat </strong>(即近似值)与观察值<strong class="ky ir"> y </strong>，<strong class="ky ir"> </strong>之间的差值，在上图中显示为橙色线。<em class="nk">残差平方和</em> <strong class="ky ir"> RSS </strong> <em class="nk"> </em>由以下公式定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/084093da49e035ea259b19dfb9b13630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*HNcjfQK_SYPFOn-Rxi7g3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的剩余平方和</p></figure><p id="1d2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir"> RSS、</strong>的帮助下，我们可以计算出剩余标准误差(<strong class="ky ir"> RSE </strong>)。RSE是真实回归线标准偏差的估计值。这意味着平均而言，y的值与真正的回归线相差一个RSE。真回归线也叫<em class="nk">人口回归线</em>，描述的是<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>的真实关系。我们如下计算<strong class="ky ir"> RSE </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/259e751e57107f8a0c3354ea98c9482d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dQ9dcc_rmd5XJyCpV3fnw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的剩余标准误差</p></figure><h2 id="c5e4" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">图基-安斯科姆图</h2><p id="25c6" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">残留物的直观表示可能会有所帮助。良好拟合的残差满足三个标准:</p><ol class=""><li id="dbb5" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nu lz ma mb bi translated">残留物的平均值为0</li><li id="9a5d" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nu lz ma mb bi translated">残基是独立的</li><li id="b1c4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nu lz ma mb bi translated">残差服从正态分布</li></ol><p id="3e80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">借助Tukey-Anscombe图和简单的直方图，我们可以检查标准上的残留物。Tukey-Anscombe图是一个散点图，它显示了残留物。绿线(<strong class="ky ir"> y=0 </strong>)代表回归线。现在我们可以看到残数是如何围绕回归线排列的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/20eac333a581e14a2abe419bd0d2cf7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqEpvuUxgyUrxnqM4iBOuw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="285a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目测表明，残差是独立的(它们不遵循一种模式，即没有系统误差)，残差的平均值为0，并且它们遵循正态分布。</p><p id="238d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">点击<a class="ae kv" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fstudentportalen.uu.se%2Fportal%2Fauthsec%2Fportal%2Fuusp%2Fstudent%2Ffilearea%2Ffilearea-window%3Bjsessionid%3DB12D1B2482594C80E800CE07AD487EC0%3Fmode%3Dview%26webwork.portlet.portletNamespace%3D%252Ffilearea%252Fview%26webwork.portlet.action%3D%252Ffilearea%252Fview%252Fopen%26action%3Dd%26entityId%3D159813%26toolAttachmentId%3D543079%26windowstate%3Dnormal%26nodeId%3D2279919%26webwork.portlet.mode%3Dview&amp;psig=AOvVaw2LogR2IrcVd6xnuYYDSKms&amp;ust=1601980213820000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCPCGxr-fnewCFQAAAAAdAAAAABAP" rel="noopener ugc nofollow" target="_blank">这里</a>查看一些不好的图。</p><h2 id="e09d" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">r平方</h2><p id="0566" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">评估回归模型的另一种方法是R平方值。它通过测量可以用<strong class="ky ir"> X </strong>解释的<strong class="ky ir"> Y </strong>的变化比例，告诉我们模型与数据的拟合程度。为此，我们需要<strong class="ky ir"> RSS </strong>和<em class="nk">平方和</em> ( <strong class="ky ir"> TSS) </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/e360cb628850c3d2858389d5c30ccd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tk8wM1ZkyWPkIEDmjtXLqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的r平方</p></figure><p id="26dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> TSS </strong>是<em class="nk">总平方和</em>的计算公式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/7cc111a5d2e97ef03c658c48db9e8d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AmtcNQB8XH9bGIsSUvJXKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的平方和合计</p></figure><p id="2666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">r的平方可以取0到1之间的值。接近1的结果意味着许多可变性可以通过回归来“解释”。接近0的结果可以表明我们的模型很差。这可能表明线性模型是错误的，或者误差的方差很高。</p><h2 id="6165" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">计算机编程语言</h2><p id="aa45" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">进行线性回归最简单的方法是利用sklearn库。</p><p id="0c86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先导入必要的库，然后定义我们的数据(<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>)。请注意，数据可以很容易地来自数据帧。下一步是调用<em class="nk"> LinearRegression()来拟合我们的模型。在我们的数据上拟合(x，y) </em>方法。这将返回一个包含我们的回归模型的对象，该模型现在可用于预测<strong class="ky ir"> x </strong>的<strong class="ky ir"> y </strong>。</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="4575" class="mo mp iq nz b gy od oe l of og">import numpy as np<br/>import seaborn as sns<br/>from sklearn.linear_model import LinearRegression<br/>import sklearn.metrics</span><span id="5dea" class="mo mp iq nz b gy oh oe l of og">x = np.array([4, 6, 8, 10, 12, 14, 18, 18, 21]).reshape(-1,1)<br/>y = np.array([2, 6, 4, 9, 5, 8, 6, 10, 10]).reshape(-1,1)</span><span id="d229" class="mo mp iq nz b gy oh oe l of og"># fitting the model<br/>reg = LinearRegression().fit(x,y)</span><span id="2658" class="mo mp iq nz b gy oh oe l of og"># predicting values (x_new is an array of values you want to predict from)<br/>x_new = x<br/>y_new = reg.predict(x_new)</span><span id="0482" class="mo mp iq nz b gy oh oe l of og"># plot regression line<br/>ax = sns.scatterplot(x[:,0],y[:,0])<br/>sns.lineplot(x = x_new[:,0],y = y_new[:,0], color='green')<br/>ax.set(xlabel='X', ylabel='Y')<br/>plt.show()</span><span id="ec51" class="mo mp iq nz b gy oh oe l of og"># R Squared<br/>print(f'R Squared: {reg.score(x,y)}')</span></pre><p id="82c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这会导致以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/da78c0252e18511330a334e798ef0182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T63_S7WKQ8jfk0epd8whcA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者使用R平方值进行回归</p></figure><p id="ac03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关sk learning线性回归如何工作的更多信息，请访问<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h2 id="80e6" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">稀有</h2><p id="df25" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">r已经有一个内置的函数来做线性回归，叫做<em class="nk"> lm() </em> (lm代表线性模型)。我们通过插入<strong class="ky ir"> X </strong>和<strong class="ky ir"> Y </strong>的数据来拟合模型。summary() 返回我们模型的一个很好的概述。看那个:R的平方和我们用Python计算的一样。</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="e3f5" class="mo mp iq nz b gy od oe l of og"># observed data<br/>x &lt;- c(4, 6, 8, 10, 12, 14, 18, 18, 21)<br/>y &lt;- c(2, 6, 4, 9, 5, 8, 6, 10, 10)</span><span id="a949" class="mo mp iq nz b gy oh oe l of og"># fit model<br/>reg &lt;- lm(y ~ x)</span><span id="c302" class="mo mp iq nz b gy oh oe l of og"># get summary of model<br/>summary(reg)</span><span id="ae03" class="mo mp iq nz b gy oh oe l of og"># Output<br/>Call:<br/>lm(formula = y ~ x)</span><span id="4621" class="mo mp iq nz b gy oh oe l of og">Residuals:<br/>    Min      1Q  Median      3Q     Max <br/>-2.6377 -1.5507  0.3188  1.3623  3.1449</span><span id="8f20" class="mo mp iq nz b gy oh oe l of og">Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)  <br/>(Intercept)   2.3768     1.6452   1.445   0.1918  <br/>x             0.3478     0.1217   2.858   0.0244 *<br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="2ddd" class="mo mp iq nz b gy oh oe l of og">Residual standard error: 2.022 on 7 degrees of freedom<br/>Multiple R-squared:  0.5386, Adjusted R-squared:  0.4727 <br/>F-statistic:  8.17 on 1 and 7 DF,  p-value: 0.02439</span><span id="8c29" class="mo mp iq nz b gy oh oe l of og"># plotting the regression line<br/>plot(x,y)<br/>abline(reg)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/75ab80ac244aab39209f30421a2b7c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mOlFMibeUmEgBp1fI_QRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者用R绘制的回归线</p></figure><h2 id="6d29" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">来源</h2><p id="4060" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">詹姆斯、加雷思、丹妮拉·威滕、特雷弗·哈斯蒂和罗伯特·蒂布拉尼。2013.103 <em class="nk">统计学习入门</em>。纽约州纽约市:斯普林格纽约。<a class="ae kv" href="http://link.springer.com/10.1007/978-1-4614-7138-7" rel="noopener ugc nofollow" target="_blank">http://link.springer.com/10.1007/978-1-4614-7138-7</a>(4。2020年啤酒节)。</p><p id="4758" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“最小二乘法”。2020.<em class="nk">维基百科</em>。https://en.wikipedia.org/w/index.php?title=Least_squares&amp;oldid = 978615667(4。2020年啤酒节)。</p><p id="b1e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sklearn.linear_model。线性回归-sci kit-学习0.23.2文档”。<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。LinearRegression.html</a>(5。2020年啤酒节)。</p></div></div>    
</body>
</html>