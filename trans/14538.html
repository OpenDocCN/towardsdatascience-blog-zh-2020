<html>
<head>
<title>QRNN: A Potential Competitor to the Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">QRNN:变形金刚的潜在竞争对手</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/qrnn-a-potential-competitor-to-the-transformer-86b5aef6c137?source=collection_archive---------23-----------------------#2020-10-07">https://towardsdatascience.com/qrnn-a-potential-competitor-to-the-transformer-86b5aef6c137?source=collection_archive---------23-----------------------#2020-10-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a09f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用准RNN训练更快的RNNs</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cb3d517812abb6ecdbf73918d15b7cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gotH4l-64DRir1Gw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布拉登·科拉姆在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="9baa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">递归神经网络(RNNs)在序列建模业务中已经有很长时间了。但是RNNs很慢；他们一次处理一个令牌。此外，递归结构增加了对完整序列的固定长度编码向量的限制。为了解决这些问题，类似CNN-LSTM、Transformer、QRNNs这样的架构应运而生。</p><p id="7c23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论在论文“<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank">准递归神经网络</a>中提出的QRNN模型它本质上是一种把卷积加到递归上，把递归加到卷积上的方法。当你继续阅读这篇文章时，你会得到这个。</p><h1 id="98bf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">长短期记忆(LSTM)</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/da86079ab3aef538fbabe923f02c3e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DM5T5S6S1ebno9zw4iQqxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM via <a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a></p></figure><p id="b3c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM是RNNs最著名的变体。<strong class="lb iu">红色模块是线性函数或矩阵乘法，蓝色模块是无参数的元素式模块</strong>。LSTM单元应用门控功能(输入、遗忘、输出)来获得输出和称为隐藏状态的存储元件。这个隐藏状态包含整个序列的上下文信息。由于单个载体编码了完整的序列，LSTMs不能记住长期依赖性。此外，每个时间步长的计算取决于前一个时间步长的隐藏状态，即LSTM一次计算一个时间步长。因此，计算不能并行进行。</p><p id="d701" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，Colah的博客是对RNNs最好的解释之一(在我看来)。如果你有兴趣了解LSTM背后的数学，可以考虑读一读。</p><h1 id="a243" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">卷积神经网络(CNN)</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/e38b48c8fdba53f11ebdf92517e4ac56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9p1wMowqJwXweHGmG3_9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CNN via <a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a></p></figure><p id="21f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而CNN则捕捉空间特征(多用于图像)。<strong class="lb iu">红色块是卷积运算，蓝色块是无参数汇集运算</strong>。CNN使用核(或过滤器)通过滑动窗口捕捉特征之间的对应关系。这克服了固定长度的隐藏表示(以及因此的长期依赖性问题)以及rnn缺乏并行性的限制。但是，CNN没有考虑序列的时间性质，即时间不变性。池层只是减少了通道的维数，而没有考虑序列顺序信息。</p><p id="2204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1603.07285v1" rel="noopener ugc nofollow" target="_blank">深度学习卷积运算指南</a>是DL中涉及卷积运算最好的论文之一。值得一读！</p><h1 id="bd0d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">准递归神经网络</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/867180ea9ac0d3a300e0bf2580920b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*3-Tg-Sd_ABkPk0DtWHxtwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">QRNN via <a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a></p></figure><p id="f2c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">QRNN解决了这两种标准架构的缺点。它允许并行处理和捕获长期依赖关系，如CNN，还允许输出依赖于序列中的标记顺序，如RNN。</p><p id="5f87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，首先，QRNN架构有2个组件对应于CNN中的<strong class="lb iu">卷积(红色)和汇集(蓝色)组件</strong>。</p><h2 id="148a" class="mq lw it bd lx mr ms dn mb mt mu dp mf li mv mw mh lm mx my mj lq mz na ml nb bi translated">卷积分量</h2><p id="63a4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">卷积组件的工作原理如下:</p><ol class=""><li id="72f7" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated">形状的输入顺序:<strong class="lb iu"> <em class="nq">(批量_大小，顺序_长度，嵌入_尺寸)</em> </strong></li><li id="f6de" class="nh ni it lb b lc nr lf ns li nt lm nu lq nv lu nm nn no np bi translated"><strong class="lb iu"><em class="nq">【hidden _ dim】</em></strong>形状的一个‘bank’:<strong class="lb iu"><em class="nq">(batch _ size，kernel_size，embed_dim) </em> </strong>各一个。</li><li id="f7fa" class="nh ni it lb b lc nr lf ns li nt lm nu lq nv lu nm nn no np bi translated">输出的是一个形状序列:<strong class="lb iu"> <em class="nq"> (batch_size，sequence_length，hidden_dim) </em> </strong>。这些是序列的隐藏状态。</li></ol><p id="8ab8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卷积运算并行应用于序列和小批量。</p><p id="e668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持模型的因果关系(即，只有过去的表征应该预测未来)，使用了一个称为<strong class="lb iu">掩蔽卷积</strong>的概念。也就是说，输入序列通过'<strong class="lb iu"><em class="nq">kernel _ size-1 '</em></strong>零填充到左边。所以，只有'<strong class="lb iu"><em class="nq">sequence _ length-kernel _ size+1 '</em></strong>过去的记号才有可能预测给定的记号。为了更好的直观，请参考下图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/df54eb9597d7f0c02a57860bb2260050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/1*3VZ9hSFeNlmq_hnb66RZPw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者的掩蔽卷积动画</p></figure><p id="d4dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们基于我们的<strong class="lb iu">池函数</strong>(将在下一节讨论)使用额外的内核库，来获得像LSTM中那样的门控向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e569ed5cacd112bae6bd4769ad80f4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*23rVFZAFqytWqvwwjzcvNA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸</a>输出卷积组件</p></figure><p id="032c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu"> <em class="nq"> * </em> </strong>是卷积运算；<strong class="lb iu"> Z </strong>是上面讨论的输出(称之为'<strong class="lb iu">输入门</strong>输出)；<strong class="lb iu"> F </strong>是使用额外内核库<strong class="lb iu"> <em class="nq"> W_f </em> </strong>获得的“<strong class="lb iu">遗忘门</strong>”输出；<strong class="lb iu"> O </strong>是使用额外内核库<strong class="lb iu"> <em class="nq"> W_o </em> </strong>得到的'<strong class="lb iu">输出门</strong>输出。</p><blockquote class="ny nz oa"><p id="e382" class="kz la nq lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated"><strong class="lb iu">趣闻:</strong>如上所述，这些卷积仅应用于过去的'<strong class="lb iu">sequence _ length-kernel _ size+1<em class="it">'</em></strong>记号。因此，如果我们取<strong class="lb iu">内核大小</strong> = 2，我们得到<strong class="lb iu">类似LSTM的</strong>方程:</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/8c1a0ba0c9a59b0c6574cf7b98d48219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSiOzyQmzVEn-4vUlB8OhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸张</a>实现类似LSTM的输出</p></figure><h2 id="1612" class="mq lw it bd lx mr ms dn mb mt mu dp mf li mv mw mh lm mx my mj lq mz na ml nb bi translated">共用组件</h2><p id="18cc" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">总的来说，池是一个无参数的函数，它从错综复杂的特性中捕捉重要的特性。在图像的情况下，通常使用最大池和平均池。然而，在序列的情况下，我们不能简单地取特征之间的平均值或最大值。它需要有一些复发。因此，QRNN论文提出了受传统LSTM单元中元件式门控结构启发的池功能。它本质上是一个无参数的函数，将在时间步长上混合隐藏状态。</p><p id="eca3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最简单的选项是“<strong class="lb iu">动态平均池</strong>”，它仅使用遗忘门(因此称为<strong class="lb iu"><em class="nq">f-池</em> </strong>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/41bdf4fe7a10b9ccd4e3432b1d529567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MjYpO562T-vt8kfV7oOhmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">动态平均池(<strong class="bd og"><em class="oh">f-池</em> </strong>)通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸</a></p></figure><p id="54f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> ⊙ </strong>是逐元素矩阵乘法。</p><p id="1e3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，它或多或少是以遗忘门为参数的输出的'<strong class="lb iu">移动平均值</strong>'。</p><p id="b435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种选择是使用遗忘门和输出门(因此，<strong class="lb iu"> <em class="nq"> fo-pooling </em> </strong>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/bb3618e3d1c25ba3b5345ae6b0f5cb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6IGkNubITahcK2EOnkjyfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd og"> <em class="oh"> fo-pooling </em> </strong>经<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸</a></p></figure><p id="466b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者池可以另外具有专用输入门(<strong class="lb iu"> <em class="nq"> ifo-pooling </em> </strong>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3e072ffa84ca9426ff064b082554a764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*tclUIPrZfeC_pMC8doPAvA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd og"><em class="oh">ifo-pooling</em></strong>via<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank">QRNN论文</a></p></figure><h1 id="4bb3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">正规化</h1><p id="5143" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在检查了各种经常性辍学方案后，QRNN使用了一个名为'<strong class="lb iu"> zone out '的扩展方案。</strong>'它本质上是在每个时间步长选择一个随机的通道子集进行丢弃，对于这些通道，它只是将当前通道值复制到下一个时间步长，而不做任何修改。</p><blockquote class="ny nz oa"><p id="c285" class="kz la nq lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">方便地说，这相当于将QRNN的遗忘门通道子集随机设置为1，或者对1 f施加压差。</p><p id="1c45" class="kz la nq lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">— <a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a></p></blockquote><p id="fea5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/6cf1cdee9050d50c37134020a237e8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYuxOfvN2AvTbCLjC5ol3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a>退学</p></figure><h1 id="67bd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">来自DenseNet的想法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/8a415b76fa36a219fb403bee2191c680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PUbHoG3_xWT8pniTdf-YzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DenseNet via <a class="ae ky" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet纸</a></p></figure><p id="5f64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DenseNet架构建议在每一层和它之前的每一层之间<strong class="lb iu">有跳跃连接</strong>，这与在后续层上有跳跃连接的惯例相反。因此，对于具有<strong class="lb iu"> <em class="nq"> L </em> </strong>层的网络，将有<strong class="lb iu"> <em class="nq"> L(L - 1) </em> </strong>个跳过连接。这有助于梯度流动和收敛，但会占用二次空间。</p><h1 id="48f3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">带QRNN的seq2seq</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/b21fe304f98ec581697dc540f20d1f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KESexGEFrgIwOokF3RULA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">QRNN seq2seq via <a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a></p></figure><p id="e308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在常规的基于RNN的seq2seq模型中，我们简单地用编码器的最后隐藏状态初始化解码器，然后针对解码器序列进一步修改它。好吧，我们不能对循环池层这样做，因为在这里，编码器状态不会对解码器的隐藏状态有太大贡献。因此，作者提出了一种改进的解码器结构。</p><p id="83bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自编码器的最后隐藏状态(最后令牌的隐藏状态)被线性投影(线性层)，并且在应用任何激活之前被添加(<a class="ae ky" href="https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm" rel="noopener ugc nofollow" target="_blank">广播</a>，因为编码器向量更小)到解码器层的每个时间步长的卷积输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/ee5a0969541ee358a69a862c35775077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R21SM_dWEU247MLcoImU5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解码器层通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸</a></p></figure><p id="0833" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nq"> ~ </em> </strong>表示属于编码器；<strong class="lb iu"> <em class="nq"> V </em> </strong>是应用于最后一个编码器隐藏状态的线性权重。</p><h2 id="e8cd" class="mq lw it bd lx mr ms dn mb mt mu dp mf li mv mw mh lm mx my mj lq mz na ml nb bi translated">注意力</h2><p id="ad1d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">注意力仅应用于解码器的最后隐藏状态。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/70109f63f86ca34db54bc0a1032a556d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDLy2jA69LbJCX0HLuJr1A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN论文</a>关注QRNN</p></figure><p id="07c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="nq"> s </em> </strong>是编码器的序列长度，<strong class="lb iu"> <em class="nq"> t </em> </strong>是解码器的序列长度，<strong class="lb iu"> <em class="nq"> L </em> </strong>表示最后一层。</p><p id="7a45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，将解码器的<strong class="lb iu"> <em class="nq">未选通</em> </strong>最后一层隐藏状态与最后一层编码器隐藏状态进行点积。这将产生一个形状为<strong class="lb iu"> <em class="nq"> (t，s) </em> </strong>的矩阵。Softmax接管了<strong class="lb iu"> <em class="nq"> s </em> </strong>，用这个<strong class="lb iu"> <em class="nq">分数</em> </strong>获取注意力总和，形状<strong class="lb iu"><em class="nq">【t，hidden _ dim】</em></strong>的<strong class="lb iu"> <em class="nq"> k_t </em>。<strong class="lb iu"> <em class="nq"> k_t </em> </strong>然后与<strong class="lb iu"> <em class="nq"> c_t </em> </strong>一起使用，为解码器获得<strong class="lb iu"> <em class="nq">门控</em> </strong>最后一层隐藏状态。</strong></p><h1 id="4dff" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结果</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/e6399efdfbc883cb2681585f3ddf28b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87ZEY6a6f1br1SwExn-9PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过<a class="ae ky" href="https://arxiv.org/abs/1611.01576" rel="noopener ugc nofollow" target="_blank"> QRNN纸</a>进行速度比较</p></figure><p id="48f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">QRNN的计算速度比LSTM架构快17倍，在某些情况下，其结果与前者相当，甚至略好于后者。</p><blockquote class="ny nz oa"><p id="e4e6" class="kz la nq lb b lc ld ju le lf lg jx lh ob lj lk ll oc ln lo lp od lr ls lt lu im bi translated">最近，基于QRNN的模型pQRNN仅用1.3M个参数就在序列分类方面取得了与BERT相当的结果(与BERT相反，它有440M个参数):</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b40fc59a4c8eac89d7c107336c6dcc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*lWP8_xD9nQJ47H6J.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">pQRNN vs BERT via <a class="ae ky" href="https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客</a></p></figure><h1 id="c98e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="432b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们深入讨论了新颖的QRNN架构。我们看到了它如何将递归添加到基于卷积的模型中，从而加速序列建模。QRNN的速度和性能肯定会让我们重新考虑一些NLP任务的变压器。</p><h1 id="3667" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><div class="oq or gp gr os ot"><a href="https://arxiv.org/abs/1611.01576" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">准递归神经网络</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">递归神经网络是一个强有力的工具，用于建模序列数据，但每个时间步的依赖…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>