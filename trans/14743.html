<html>
<head>
<title>Understanding Transformers, the Programming Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解变压器，编程方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2?source=collection_archive---------19-----------------------#2020-10-11">https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2?source=collection_archive---------19-----------------------#2020-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b18eb20e8b004c40f756072e8f0afb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sfu0EF2g-LdoAsmP4t_Q-Q.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://wallpaperaccess.com/transformers" rel="noopener ugc nofollow" target="_blank">壁纸访问</a></p></figure><div class=""/><div class=""><h2 id="35d5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">因为你只能理解它，如果你能编程的话</h2></div><p id="9f3f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如今，变形金刚已经成为NLP任务的事实标准。它们开始被用于自然语言处理，但现在被用于计算机视觉，有时也用于创作音乐。我相信你们都听说过GPT3变压器或其中的笑话。</p><p id="74bc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">但抛开一切不谈，他们还是一如既往地难以理解。在我的<a class="ae jg" rel="noopener" target="_blank" href="/understanding-transformers-the-data-science-way-e4670a4ee076">上一篇文章</a>中，我非常详细地谈到了变形金刚以及它们是如何在基本层面上工作的。我浏览了编码器和解码器架构，以及神经网络不同部分的整个数据流。</em></strong></p><p id="7268" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是正如我喜欢说的，在我们自己实施之前，我们并没有真正理解一些东西。所以在这篇文章中，我们将使用Transformers实现一个英语到德语的翻译器。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3fcc" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">任务描述</h1><p id="e49c" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们想创建一个使用变压器将英语转换成德语的翻译器。因此，如果我们把它看作一个黑盒，我们的网络接受一个英语句子作为输入，并返回一个德语句子。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/8aec0f7fa48d2919f4fde7037ee4f264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBCtrY02DTg9ZQiK7dxaKQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片:用于翻译的转换器</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9bad" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">数据预处理</h1><p id="55e9" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了训练我们的英德翻译模型，我们需要英语和德语之间的翻译句子对。</p><p id="5735" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，通过IWSLT(国际口语翻译研讨会)数据集，我们有一个非常标准的方法来获取这些数据，我们可以使用<code class="fe ne nf ng nh b">torchtext.datasets</code>来访问这些数据。这个机器翻译数据集是一种用于翻译任务的事实标准，包含不同语言的TED和TEDx演讲的翻译。</p><p id="0c49" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，在我们真正进入整个编码部分之前，让我们了解在训练时我们需要什么作为模型的输入和输出。我们实际上需要两个矩阵输入到我们的网络中:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/2c95b86132aec812968593473a2173a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UddDCpMGLBIz33AQZtaEZg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图像:输入到网络</p></figure><ul class=""><li id="1939" class="nj nk jj la b lb lc le lf lh nl ll nm lp nn lt no np nq nr bi translated"><strong class="la jk">源英文句子(Source): </strong>一个形状的矩阵(批量大小x源句子长度)。该矩阵中的数字对应于基于我们还需要创建的英语词汇的单词。例如，英语词汇中的234可能对应于单词“the”。还有，你有没有注意到很多句子都以一个单词结尾，这个单词在词汇中的索引是6？这是怎么回事？因为所有句子的长度都不一样，所以会用一个索引为6的单词填充。所以，6指的是<code class="fe ne nf ng nh b">&lt;blank&gt;</code>令牌。</li><li id="c00a" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated"><strong class="la jk">移位的目标德语句子(Target): </strong>一个形状的矩阵(批量x目标句子长度)。这里，这个矩阵中的数字也对应于我们还需要创建的基于德语词汇的单词。如果你注意到这个特殊的矩阵似乎有一个模式。所有的句子都以一个在德语词汇中索引为2的单词开头，并且总是以一种模式结尾[3和0或更多的1]。这是有意为之的，因为我们希望以某个开始标记开始目标句子(so 2代表<code class="fe ne nf ng nh b">&lt;s&gt;</code>标记)，以某个结束标记(so 3代表<code class="fe ne nf ng nh b">&lt;/s&gt;</code>标记)和一串空白标记(so 1代表<code class="fe ne nf ng nh b">&lt;blank&gt;</code>标记)结束目标句子。这一部分在我上一篇关于变形金刚的文章中有更详细的介绍，所以如果你对此感到困惑，我想请你看一看</li></ul><p id="ac9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们知道了如何预处理数据，我们将进入预处理步骤的实际代码。</p><p id="cb43" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">请注意，如果您也使用其他方法进行预处理，这真的无关紧要。最终重要的是，最终，您需要以一种转换器可以使用的方式将句子源和目标发送到您的模型。即，源句子应该用空白标记填充，目标句子需要有开始标记、结束标记和由空白标记填充的剩余部分。</em></p><p id="8cde" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先加载Spacy模型，该模型提供了标记器来标记德语和英语文本。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="9df7" class="ob md jj nh b gy oc od l oe of"># Load the Spacy Models<br/>spacy_de = spacy.load('de')<br/>spacy_en = spacy.load('en')</span><span id="766a" class="ob md jj nh b gy og od l oe of">def tokenize_de(text):<br/>    return [tok.text for tok in spacy_de.tokenizer(text)]</span><span id="81b0" class="ob md jj nh b gy og od l oe of">def tokenize_en(text):<br/>    return [tok.text for tok in spacy_en.tokenizer(text)]</span></pre><p id="3b8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还定义了一些特殊的记号，我们将使用它们来指定空白/填充词，以及如上所述的句子的开头和结尾。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="44aa" class="ob md jj nh b gy oc od l oe of"># Special Tokens<br/>BOS_WORD = '&lt;s&gt;'<br/>EOS_WORD = '&lt;/s&gt;'<br/>BLANK_WORD = "&lt;blank&gt;"</span></pre><p id="c06b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以使用torchtext中的<code class="fe ne nf ng nh b">data.field</code>为源句子和目标句子定义一个预处理管道。您可以注意到，虽然我们只指定了源句子的<code class="fe ne nf ng nh b">pad_token</code>，但是我们提到了目标句子的<code class="fe ne nf ng nh b">pad_token</code>、<code class="fe ne nf ng nh b">init_token</code>和<code class="fe ne nf ng nh b">eos_token</code>。我们还定义了使用哪些记号赋予器。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="ac98" class="ob md jj nh b gy oc od l oe of">SRC = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)<br/>TGT = data.Field(tokenize=tokenize_de, init_token = BOS_WORD, <br/>                 eos_token = EOS_WORD, pad_token=BLANK_WORD)</span></pre><p id="9c55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你注意到现在我们还没有看到任何数据。我们现在使用来自<code class="fe ne nf ng nh b">torchtext.datasets</code>的IWSLT数据来创建一个训练、验证和测试数据集。我们还使用<code class="fe ne nf ng nh b">MAX_LEN</code>参数过滤我们的句子，这样我们的代码运行得更快。请注意，我们正在获取带有<code class="fe ne nf ng nh b">.en</code>和<code class="fe ne nf ng nh b">.de</code>扩展名的数据。我们使用<code class="fe ne nf ng nh b">fields</code>参数指定预处理步骤。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="76fb" class="ob md jj nh b gy oc od l oe of">MAX_LEN = 20<br/>train, val, test = datasets.IWSLT.splits(<br/>    exts=('.en', '.de'), fields=(SRC, TGT), <br/>    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN <br/>    and len(vars(x)['trg']) &lt;= MAX_LEN)</span></pre><p id="784d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们已经获得了训练数据，让我们看看它是什么样子的:</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="b82a" class="ob md jj nh b gy oc od l oe of">for i, example in enumerate([(x.src,x.trg) for x in train[0:5]]):<br/>    print(f"Example_{i}:{example}")</span><span id="c6da" class="ob md jj nh b gy og od l oe of">---------------------------------------------------------------</span><span id="ab2a" class="ob md jj nh b gy og od l oe of">Example_0:(['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', "'m", 'Dave', 'Gallo', '.'], ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'])</span><span id="3faa" class="ob md jj nh b gy og od l oe of">Example_1:(['And', 'we', "'re", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.'], ['Wir', 'werden', 'Ihnen', 'einige', 'Geschichten', 'über', 'das', 'Meer', 'in', 'Videoform', 'erzählen', '.'])</span><span id="183d" class="ob md jj nh b gy og od l oe of">Example_2:(['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.'], ['Ich', 'denke', ',', 'das', 'Problem', 'ist', ',', 'dass', 'wir', 'das', 'Meer', 'für', 'zu', 'selbstverständlich', 'halten', '.'])</span><span id="58cb" class="ob md jj nh b gy og od l oe of">Example_3:(['When', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.'], ['Wenn', 'man', 'darüber', 'nachdenkt', ',', 'machen', 'die', 'Ozeane', '75', '%', 'des', 'Planeten', 'aus', '.'])</span><span id="d3a6" class="ob md jj nh b gy og od l oe of">Example_4:(['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.'], ['Der', 'Großteil', 'der', 'Erde', 'ist', 'Meerwasser', '.'])</span></pre><p id="44cb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能会注意到，虽然<code class="fe ne nf ng nh b">data.field</code>对象已经完成了标记化，但它还没有应用开始、结束和填充标记，这是有意的。这是因为我们还没有批处理，填充标记的数量本质上取决于特定批处理中句子的最大长度。</p><p id="93b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如开始提到的，我们还通过使用<code class="fe ne nf ng nh b">data.field</code>对象中的内置函数来创建源语言和目标语言词汇表。我们指定MIN_FREQ为2，这样任何至少不出现两次的单词都不会成为我们词汇表的一部分。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="f13f" class="ob md jj nh b gy oc od l oe of">MIN_FREQ = 2<br/>SRC.build_vocab(train.src, min_freq=MIN_FREQ)<br/>TGT.build_vocab(train.trg, min_freq=MIN_FREQ)</span></pre><p id="d7d2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们完成了这些，我们就可以简单地使用<code class="fe ne nf ng nh b">data.Bucketiterator</code>，它用于给出相似长度的批处理来得到我们的训练迭代器和验证迭代器。注意，我们使用1的<code class="fe ne nf ng nh b">batch_size</code>作为验证数据。这样做是可选的，但这样做是为了在检查验证数据性能时不进行填充或进行最小填充。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="23d8" class="ob md jj nh b gy oc od l oe of">BATCH_SIZE = 350</span><span id="0f7e" class="ob md jj nh b gy og od l oe of"># Create iterators to process text in batches of approx. the same length by sorting on sentence lengths</span><span id="cb96" class="ob md jj nh b gy og od l oe of">train_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, repeat=False, sort_key=lambda x: len(x.src))</span><span id="b6cb" class="ob md jj nh b gy og od l oe of">val_iter = data.BucketIterator(val, batch_size=1, repeat=False, sort_key=lambda x: len(x.src))</span></pre><p id="de20" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们继续之前，最好先看看我们的批处理是什么样子，以及我们在训练时作为输入发送给模型的是什么。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="7892" class="ob md jj nh b gy oc od l oe of">batch = next(iter(train_iter))<br/>src_matrix = batch.src.T<br/>print(src_matrix, src_matrix.size())</span></pre><p id="6709" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们的源矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/cda3acbef04b8b60206dbeacbfa2a059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-TGYIgyk8mgyiuhhX99sQ.png"/></div></div></figure><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="cfff" class="ob md jj nh b gy oc od l oe of">trg_matrix = batch.trg.T<br/>print(trg_matrix, trg_matrix.size())</span></pre><p id="25c3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们的目标矩阵:</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/b2b96bbf164f0809929a2890257ae561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8S6-VEks0t2cWYwHikCnQw.png"/></div></div></figure><p id="5134" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以在第一批中，<code class="fe ne nf ng nh b">src_matrix</code>包含350个长度为20的句子，而<code class="fe ne nf ng nh b">trg_matrix</code>是350个长度为22的句子。为了确保我们的预处理，让我们看看这些数字在<code class="fe ne nf ng nh b">src_matrix</code>和<code class="fe ne nf ng nh b">trg_matrix.</code>中代表什么</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="985c" class="ob md jj nh b gy oc od l oe of">print(SRC.vocab.itos[1])<br/>print(TGT.vocab.itos[2])<br/>print(TGT.vocab.itos[1])<br/>--------------------------------------------------------------------<br/>&lt;blank&gt;<br/>&lt;s&gt;<br/>&lt;blank&gt;</span></pre><p id="c45a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">果然不出所料。相反的方法，即字符串到索引也工作得很好。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="3c40" class="ob md jj nh b gy oc od l oe of">print(TGT.vocab.stoi['&lt;/s&gt;'])<br/>--------------------------------------------------------------------<br/>3</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="974f" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">变形金刚</h1><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/ae1949e3161519f70d228b06beb1901c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3orJQUKYY8xzTWJK1cjKQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片:应用程序架构</p></figure><p id="9dc3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，现在我们有了一种将源句子和转换后的目标发送到转换器的方法，我们可以开始创建转换器了。</p><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="d4e9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的很多积木都取自Pytorch <code class="fe ne nf ng nh b">nn</code>模块。事实上，Pytorch也有一个<a class="ae jg" href="https://pytorch.org/docs/master/generated/torch.nn.Transformer.html" rel="noopener ugc nofollow" target="_blank"> Transformer </a>模块，但它不包括论文中提到的许多功能，如嵌入层和位置编码层。所以这是一种更完整的实现，也从pytorch实现中吸取了很多东西。</p><p id="ca04" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们特别使用Pytorch nn模块中的各种模块来创建我们的变压器:</p><ul class=""><li id="54cf" class="nj nk jj la b lb lc le lf lh nl ll nm lp nn lt no np nq nr bi translated"><a class="ae jg" href="https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html" rel="noopener ugc nofollow" target="_blank">TransformerEncoderLayer</a>:单个编码器层</li><li id="20f7" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html" rel="noopener ugc nofollow" target="_blank"> TransformerEncoder </a>:一堆<code class="fe ne nf ng nh b">num_encoder_layers</code>层。在本文中，默认情况下保持为6。</li><li id="7b58" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html" rel="noopener ugc nofollow" target="_blank"> TransformerDecoderLayer </a>:单个解码器层</li><li id="ae9f" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated"><a class="ae jg" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html" rel="noopener ugc nofollow" target="_blank"> TransformerDecoder </a>:一堆<code class="fe ne nf ng nh b">num_decoder_layers</code>层。在本文中，默认情况下保持为6。</li></ul><p id="e282" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，请注意，无论层中发生什么，实际上只是矩阵函数，正如我在变压器的解释帖子中提到的那样。请特别注意解码器堆栈如何从编码器获取内存作为输入。我们还创建了一个位置编码层，让我们将位置嵌入添加到单词嵌入中。</p><p id="9043" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你愿意，你可以看看我已经链接的所有这些模块的源代码。我不得不多次亲自查看源代码，以确保我为这些层提供了正确的输入。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="da33" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">定义优化器和模型</h1><p id="11a4" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">现在，我们可以使用以下代码初始化转换器和优化器:</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="4633" class="ob md jj nh b gy oc od l oe of">source_vocab_length = len(SRC.vocab)<br/>target_vocab_length = len(TGT.vocab)</span><span id="aa80" class="ob md jj nh b gy og od l oe of">model = MyTransformer(source_vocab_length=source_vocab_length,target_vocab_length=target_vocab_length)</span><span id="2767" class="ob md jj nh b gy og od l oe of">optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)</span><span id="790e" class="ob md jj nh b gy og od l oe of">model = model.cuda()</span></pre><p id="aa8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在论文中，作者使用了一个具有预定学习率的Adam优化器，但是这里我只是使用一个普通的Adam优化器来简化事情。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="235a" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">培训我们的翻译</h1><p id="a773" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">现在，我们可以使用下面的训练函数来训练我们的变压器。我们在培训循环中必须做的是:</p><ul class=""><li id="becf" class="nj nk jj la b lb lc le lf lh nl ll nm lp nn lt no np nq nr bi translated">从批处理中获取src_matrix和trg_matrix。</li><li id="35ae" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">创建一个<code class="fe ne nf ng nh b">src_mask</code> —这是告诉模型关于src_matrix数据中填充单词的掩码。</li><li id="b64a" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">创建一个<code class="fe ne nf ng nh b">trg_mask</code>——这样我们的模型就不能在任何时间点查看未来的后续目标单词。</li><li id="4e5a" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">从模型中获得预测。</li><li id="afe4" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">使用交叉熵计算损失。(在论文中，他们使用了KL散度，但这也有助于理解)</li><li id="3e77" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">反向投影。</li><li id="7ac6" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">我们保存基于验证损失的最佳模型。</li><li id="0edc" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">我们还使用函数<code class="fe ne nf ng nh b">greedy_decode_sentence</code>预测我们选择的一些句子在每个时期的模型输出，作为调试步骤。我们将在结果部分讨论这个函数。</li></ul><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="6adb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以使用以下工具进行培训:</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="ee8b" class="ob md jj nh b gy oc od l oe of">train_losses,valid_losses = train(train_iter, val_iter, model, optim, 35)</span></pre><p id="9399" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是训练循环的输出(仅针对某些时期显示):</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="8f32" class="ob md jj nh b gy oc od l oe of"><strong class="nh jk">Epoch [1/35] complete.</strong> Train Loss: 86.092. Val Loss: 64.514<br/>Original Sentence: This is an example to check how our model is performing.<br/>Translated Sentence:  Und die der der der der der der der der der der der der der der der der der der der der der der der</span><span id="109a" class="ob md jj nh b gy og od l oe of"><strong class="nh jk">Epoch [2/35] complete.</strong> Train Loss: 59.769. Val Loss: 55.631<br/>Original Sentence: This is an example to check how our model is performing.<br/>Translated Sentence:  Das ist ein paar paar paar sehr , die das ist ein paar sehr Jahre . &lt;/s&gt;</span><span id="2a56" class="ob md jj nh b gy og od l oe of">.<br/>.<br/>.<br/>.</span><span id="7347" class="ob md jj nh b gy og od l oe of"><strong class="nh jk">Epoch [16/35] complete.</strong> Train Loss: 21.791. Val Loss: 28.000<br/>Original Sentence: This is an example to check how our model is performing.<br/>Translated Sentence:  Hier ist ein Beispiel , um zu prüfen , wie unser Modell aussieht . Das ist ein Modell . &lt;/s&gt;</span><span id="b758" class="ob md jj nh b gy og od l oe of">.<br/>.<br/>.<br/>.</span><span id="8c30" class="ob md jj nh b gy og od l oe of"><strong class="nh jk">Epoch [34/35] complete.</strong> Train Loss: 9.492. Val Loss: 31.005<br/>Original Sentence: This is an example to check how our model is performing.<br/>Translated Sentence:  Hier ist ein Beispiel , um prüfen zu überprüfen , wie unser Modell ist . Wir spielen . &lt;/s&gt;</span><span id="b779" class="ob md jj nh b gy og od l oe of"><strong class="nh jk">Epoch [35/35] complete.</strong> Train Loss: 9.014. Val Loss: 32.097<br/>Original Sentence: This is an example to check how our model is performing.<br/>Translated Sentence:  Hier ist ein Beispiel , um prüfen wie unser Modell ist . Wir spielen . &lt;/s&gt;</span></pre><p id="46eb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到我们的模型是如何从一个莫名其妙的翻译开始的——“然后在几次迭代结束时开始给我们一些东西。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d294" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结果</h1><p id="6849" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们可以使用Plotly express绘制训练和验证损失。</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="7557" class="ob md jj nh b gy oc od l oe of">import pandas as pd<br/>import plotly.express as px</span><span id="98fe" class="ob md jj nh b gy og od l oe of">losses = pd.DataFrame({'train_loss':train_losses,'val_loss':valid_losses})</span><span id="c420" class="ob md jj nh b gy og od l oe of">px.line(losses,y = ['train_loss','val_loss'])</span></pre><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/bb959fdc658b10b3ed23f896a134e31e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--mwu9F7t4uI3eQn__Gmhw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片:培训和验证损失</p></figure><p id="aa61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们想要部署这个模型，我们可以简单地使用:</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="5dc1" class="ob md jj nh b gy oc od l oe of">model.load_state_dict(torch.load(f”checkpoint_best_epoch.pt”))</span></pre><p id="ed4a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并使用<code class="fe ne nf ng nh b">greeedy_decode_sentence</code>函数对任何源句子进行预测，该函数为:</p><figure class="na nb nc nd gt iv"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/4224b45eca00632041f9d265e6e6432b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sdjUHzfL9arBGjMModT9vw.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片:使用变形金刚进行贪婪搜索预测</p></figure><p id="f864" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个函数做分段预测。贪婪搜索将从以下内容开始:</p><ul class=""><li id="26f4" class="nj nk jj la b lb lc le lf lh nl ll nm lp nn lt no np nq nr bi translated">将整个英语句子作为编码器输入，仅将开始标记<code class="fe ne nf ng nh b">&lt;s&gt; </code>作为移位输出(解码器的输入)传递给模型，并进行正向传递。</li><li id="9dab" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">该模型将预测下一个单词——<code class="fe ne nf ng nh b">der</code></li><li id="26b2" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">然后，我们将整个英语句子作为编码器输入，并将最后预测的单词添加到移位的输出(解码器的输入= <code class="fe ne nf ng nh b">&lt;s&gt; der</code>)中，并进行正向传递。</li><li id="17e6" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">该模型将预测下一个单词——<code class="fe ne nf ng nh b">schnelle</code></li><li id="2bac" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">将整个英语句子作为编码器输入，将<code class="fe ne nf ng nh b">&lt;s&gt; der schnelle</code>作为移位输出(解码器的输入)传递给模型，并进行正向传递。</li><li id="5bbe" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">依此类推，直到模型预测到结束标记<code class="fe ne nf ng nh b">&lt;/s&gt;</code>或者我们生成一些最大数量的标记(我们可以定义)，这样翻译就不会在任何中断的情况下无限期运行。</li></ul><p id="272e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以用这个来翻译任何句子:</p><pre class="na nb nc nd gt nx nh ny nz aw oa bi"><span id="e560" class="ob md jj nh b gy oc od l oe of">sentence = "Isn't Natural language processing just awesome? Please do let me know in the comments."</span><span id="d0f5" class="ob md jj nh b gy og od l oe of">print(greeedy_decode_sentence(model,sentence))</span><span id="7618" class="ob md jj nh b gy og od l oe of">------------------------------------------------------------------</span><span id="51cc" class="ob md jj nh b gy og od l oe of">Ist es nicht einfach toll ? Bitte lassen Sie mich gerne in den Kommentare kennen . &lt;/s&gt;</span></pre><p id="51a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于我手头没有德语翻译，所以我将使用退而求其次的方法来查看我们的模型表现如何。让我们借助谷歌翻译服务来理解这个德语句子的意思。</p><figure class="na nb nc nd gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/2757bc447f3e41ae59182148329779c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6quLQHvfRyWDdTJ_ChibA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片:谷歌翻译结果</p></figure><p id="5680" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">翻译中似乎有一些错误作为“自然语言处理”是没有的(讽刺？)但对我来说，这似乎是一个足够好的翻译，因为神经网络只需一个小时的训练就能理解这两种语言的结构。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="335c" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">vorbehalte/verbes rungen(警告/改进)</h1><p id="fcd4" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">如果我们按照报纸上的方法做每件事，我们可能会取得更好的结果:</p><ul class=""><li id="0137" class="nj nk jj la b lb lc le lf lh nl ll nm lp nn lt no np nq nr bi translated">全数据训练</li><li id="3284" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">字节对编码</li><li id="8f34" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">学习率调度</li><li id="ec58" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">KL发散损失</li><li id="6da1" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">光束搜索，和</li><li id="cd01" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated">检查点集合</li></ul><p id="62d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在我的<a class="ae jg" rel="noopener" target="_blank" href="/understanding-transformers-the-data-science-way-e4670a4ee076">上一篇文章</a>中讨论了所有这些，所有这些都很容易实现。但是这个简单的实现是为了理解转换器是如何工作的，所以我没有包括所有这些内容，以免混淆读者。事实上，在变形金刚的基础上已经有了相当多的进步，让我们有了更好的翻译模型。我们将在接下来的文章中讨论这些进步以及它们是如何实现的，在这篇文章中，我将谈论BERT，这是最受欢迎的NLP模型之一，其核心使用了转换器。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cf71" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">参考</h1><ul class=""><li id="4d04" class="nj nk jj la b lb mu le mv lh op ll oq lp or lt no np nq nr bi translated"><a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></li><li id="65c7" class="nj nk jj la b lb ns le nt lh nu ll nv lp nw lt no np nq nr bi translated"><a class="ae jg" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变压器</a></li></ul><p id="f2c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们使用transformer架构几乎从头开始创建了一个英语到德语的翻译网络。</p><p id="826c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更仔细地查看这篇文章的代码，请访问我的<a class="ae jg" href="https://github.com/MLWhiz/data_science_blogs/tree/master/transformers" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库，在那里你可以找到这篇文章以及我所有文章的代码。</p><p id="810c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想了解更多关于NLP的知识，我想从高级机器学习专业化中调出一门关于<a class="ae jg" href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">自然语言处理</strong> </a>的精品课程。一定要去看看。</p><p id="5923" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我以后也会写更多这样的帖子。让我知道你对他们的看法。我应该写技术性很强的主题还是更初级的文章？评论区是你的朋友。使用它。还有，在<a class="ae jg" href="https://medium.com/@rahul_agarwal" rel="noopener"> <strong class="la jk">中</strong> </a>关注我或者订阅我的<a class="ae jg" href="https://mlwhiz.ck.page/a9b8bda70c" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>。</p><p id="17d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，一个小小的免责声明——这篇文章中有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p><p id="b625" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个故事最初发表于<a class="ae jg" href="https://lionbridge.ai/articles/transformers-in-nlp-creating-a-translator-model-from-scratch/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>