<html>
<head>
<title>LSTM vs BERT — a step-by-step guide for tweet sentiment analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM vs伯特——推特情感分析的分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47?source=collection_archive---------5-----------------------#2020-11-04">https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47?source=collection_archive---------5-----------------------#2020-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59ee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最先进的NLP模型能比RNN更好地预测股票交易者情绪吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f9b5d5acefdd0f614915200899adc206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwNeM13D4NfNpALVLeWuCw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fa92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">注来自《走向数据科学》的编辑:</em> </strong> <em class="lu">虽然我们允许独立作者根据我们的</em> <a class="ae lv" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="lu">规则和指导方针</em> </a> <em class="lu">发表文章，但我们不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae lv" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="lu">读者术语</em> </a> <em class="lu">。</em></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="6f5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<a class="ae lv" rel="noopener" target="_blank" href="/nlp-in-the-financial-market-sentiment-analysis-9de0dda95dc?source=friends_link&amp;sk=9bb03bc7e108c125499bc9cf1996bd49">金融文本的情绪分析</a>中看到BERT的竞争结果后，我对更非正式的文本进行了另一项初步研究，因为最终目标是除了新闻情绪之外，还分析交易员在电话和聊天中的声音。在这篇文章中，我让LSTM和伯特分析了Stocktwit的一些推文。</p><h1 id="89c8" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">背景</h1><p id="4991" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">与正式的金融文本不同，交易者的声音和聊天包含了迄今为止的非正式语言。在我过去的研究中，传统的基于规则的模型或简单的矢量化技术(如BoW、Tfidf、word2vec)表现不佳，因为</p><ul class=""><li id="f61f" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">一个词在不同的上下文中通常有不同的意思</li><li id="8ecd" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">拼写和句子远不是语法正确的语言</li><li id="b1c1" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">通过词干化、词汇化、停用词移除等来平衡单词分组。保持原来的形式是很困难的</li><li id="c45c" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">对于基于规则的方法，字典必须专门建立在这种语言上，在这方面我没有足够的概括</li></ul><p id="c414" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LSTM是自然语言处理中最著名的RNN模型之一，表现很好。这在很大程度上归功于这样一个事实</p><ul class=""><li id="9a18" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt nf ng nh ni bi translated">句子结构相当简单——简单地从左到右处理就足够了</li><li id="1b33" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">每一个输入文本都很短，有利于记忆</li><li id="0d57" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt nf ng nh ni bi translated">这项任务相当简单的分类</li></ul><p id="345e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，我在这里使用的BERT是在维基百科上训练的，那里的语言非常不同。由于资源的限制，从零开始训练BERT并不是一个好的选择。在这种情况下，让伯特获得比LSTM更好的表现还值得吗？</p><h1 id="b83a" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">作为输入文本的推文</h1><p id="fa74" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">这里输入的文字取自Stocktwits，作为这里交易者声音的类似语言。大约有一百万条推文被手工标记为0(负面)到4(正面)，分别被加载为<code class="fe no np nq nr b">messages</code>和<code class="fe no np nq nr b">sentiments</code>列表。请注意，实际环境通常需要做更多的工作来准备输入，如声音识别、数据清理、流式传输。这篇文章跳过这些步骤，从数据加载的地方开始。</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="3ad3" class="nw me it nr b gy nx ny l nz oa">&gt; ##### Sample input messages ######</span><span id="9201" class="nw me it nr b gy ob ny l nz oa">&gt; print(messages)<br/>["$AMZN sick! they’re running a prime flash sale on shares too!", </span><span id="43e7" class="nw me it nr b gy ob ny l nz oa">"$AAPL has a good Piotroski-F score of 7.00. This indicates a good health and profitability. https://www.chartmill.com/analyze.php?utm_source=stocktwits&amp;amp;utm_medium=FA&amp;amp;utm_content=PROFITABILITY&amp;amp;utm_campaign=social_tracking#/AAPL?r=fa&amp;amp;key=bb853040-a4ac-41c6-b549-d218d2f21b32", "$FB got rid of this trash today, </span><span id="ad65" class="nw me it nr b gy ob ny l nz oa">i admit that bears were right", ...]</span><span id="c4e6" class="nw me it nr b gy ob ny l nz oa">&gt; print(sentiments)<br/>[4, 2, 0, ...]</span></pre><h2 id="880a" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">1.预处理</h2><p id="2e9c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在训练之前，输入文本需要进行预处理，如删除URL、标记符号、@提及、符号等。在这里，我简单地删除了它们，因为它们在语音中不可用，但也有一些有趣的研究，关于如何利用表情符号和标签等信息，而不是删除它们，如果最终目标是分析推文文本的话。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理输入消息</p></figure><p id="1d6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在输入已经被清理如下。</p><pre class="kj kk kl km gt ns nr nt nu aw nv bi"><span id="d5ce" class="nw me it nr b gy nx ny l nz oa">&gt; ###### Input messages after preprocessing ######</span><span id="45e6" class="nw me it nr b gy ob ny l nz oa">&gt; print(preprocessed)<br/>["sick they re running a prime flash sale on shares too", </span><span id="b66c" class="nw me it nr b gy ob ny l nz oa">"has a good piotroski f score of this indicates a good health and profitability", </span><span id="569d" class="nw me it nr b gy ob ny l nz oa">"got rid of this trash today i admit that bears were right", ...]</span></pre><h2 id="bb83" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">2.标记化</h2><p id="703d" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">下一步是标记文本。在这里，我使用python NLTK库，但也给出了使用不同方法的选项，以查看什么最适合输入。经过几次实验后，我决定使用nltk.word_tokenize()，而不进行词汇化和停用词移除。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="72cc" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">3.语料库和词汇库</h2><p id="54a8" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">一旦输入文本被标记化，我们就可以用下面的方式创建一个语料库和词汇库。词云或条形图是快速查看输入中的常用词的好方法。分布显示该标签不平衡，比其他情绪更中性。通过重采样(欠采样或过采样)来平衡数据是可能的，但这里按原样进行，因为这个比率将代表推文流中情感的实际发生。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/785153570de0be771ad925f416c6bcc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-le2i6mL-Sm6aPtqGH96g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文字云图像(由作者创建)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/4f72b49d9aaaa440fc014f59da38df5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SO2NMGGszn3IXx3BE_QGEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语料库中最常用的单词(由作者创建)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/88918760660dd9f7798de5b1cc3f3170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SF8OIoullsPVBuNk8psXPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签、字母和单词的分发(由作者创建)</p></figure><h1 id="f6fc" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">分类模型</h1><p id="2fbb" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">既然输入数据已经准备好了，就创建基于神经网络的模型，并为该模型标记化。</p><h2 id="0bf6" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">4.LSTM</h2><p id="655c" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">使用pytorch创建一个基于LSTM的模型。该类扩展了torch.nn.Module并将层定义为嵌入→ lstm →丢弃→密集(全连接)→输出(softmax)。LSTM的记号赋予器是将输入填充到右边或左边，直到指定的最大长度，如果输入超过最大长度，则截断，设计用于在每批的训练期间使用，而不是预处理所有输入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="0c28" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">5.伯特</h2><p id="c9bb" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">这里我使用BERT的拥抱脸实现。只需使用他们的变压器和预先训练的模型和记号化器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h1 id="39e0" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">培训过程</h1><h2 id="7349" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">6.资料组</h2><p id="21e1" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">为批处理创建数据集类和数据加载器。有许多不同的方法来定义它们，这只是一个非常简单的解决方案，与返回torch.tensor的已定义记号化器一起使用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="aa52" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">7.取样周期</h2><p id="2933" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">根据不同输入大小的准确度、f1和训练时间来测量性能。使用scikit-learn的分层混洗Split，它可以根据给定的训练和测试大小，通过保留标签分布来执行欠采样。在每个循环中，通过perf_counter()测量完成训练的持续时间。</p><p id="74a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，定义一个简单的函数来返回准确性和F1分数。</p><p id="a7ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，模型参数是在实例化模型类时定义的，可以根据输入数据进行更新。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="9f58" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">8.训练神经网络模型</h2><p id="66a5" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">如下定义培训流程:</p><ol class=""><li id="b67b" class="na nb it la b lb lc le lf lh nc ll nd lp ne lt os ng nh ni bi translated">循环训练批次</li><li id="8006" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt os ng nh ni bi translated">在整个数据的1/5处运行评估</li><li id="5b86" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt os ng nh ni bi translated">一个时期完成后，显示结果</li><li id="2bf3" class="na nb it la b lb nj le nk lh nl ll nm lp nn lt os ng nh ni bi translated">如果分数没有提高到超出忍耐限度，则结束，或者开始下一个纪元</li></ol><p id="a24b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AdamW优化器和线性时间表与学习率预热一起使用，但这些可以根据需要与其他选项交换。</p><p id="e30f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每个训练批次循环中，将输入消息标记化并移动到torch.tensor，执行前馈预测，计算损失并反向传播以更新权重。裁剪以避免在下一个批处理步骤之前爆发梯度问题。</p><p id="f224" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每个时期结束时，显示训练和验证数据的混淆矩阵和分数/损失。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h1 id="8d77" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">火车！</h1><p id="ff92" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">最后:)</p><h2 id="668a" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">9.运行LSTM模型</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="7070" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最小样本(n=1，000)的第一个时期之后，模型简单地将所有数据分类为“中性”，这是合理的，因为“中性”是多数类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/2f68d866e4fe7de0d746089168ec0510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RVU2bTS3QZFUsqgTrMelw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵(n=1，000，epoch=1)(由作者创建)</p></figure><p id="4ad2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成五个时代后，它看起来几乎是随机地对数据进行了分类。它从第三个时期(=验证周期= 10)开始过度拟合训练数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/09e62b27cdc2e7f488b675a95a2202db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTAf7HiAL1rji8atd5EAVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评估结果(n=1，000，epoch=5)(由作者创建)</p></figure><p id="96be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最大的数据集(n=500，000)包含比第一个周期多500倍的数据。它表现得更好，现在可以正确分类大多数标签。从第四个周期开始超配，所以三个纪元会是最好的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/d35db95a5937a68ea2530cf2079543b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1sVIvx3OGRZhsOdTmeE-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评估结果(n=500，000，epoch=5)(由作者创建)</p></figure><h2 id="6da7" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">10.运行伯特模型</h2><p id="06cb" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">三个时期的训练预训练的BERT花费的时间几乎与上述LSTM模型的五个时期相同。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="1e4f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT显示了类似的结果，但是它在最大数据集(n = 500，000)的第三个时期开始过拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/b4994ef2d1740238b7d8f9e7d8f456f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQ69fAJIyV5oOHZDc4d_7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评估结果(n=500，000，epoch=5)(由作者创建)</p></figure><h2 id="686c" class="nw me it bd mf oc od dn mj oe of dp mn lh og oh mp ll oi oj mr lp ok ol mt om bi translated">11.比较结果</h2><p id="7c65" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">如下所示，随着输入数据数量的增加，它的性能自然会更好，在大约100k的数据时达到75%以上的分数。伯特的表现略好于LSTM，但当模型接受相同时间的训练时，没有显著差异。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/5529a51d9570b3475d759963df24d310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwvwRSxTI4ezvQu-m_ayfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM与伯特的演奏比较(作者创作)</p></figure><h1 id="e954" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">结论</h1><p id="116d" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">在这篇文章中，来自stockswits的推文被清理、标记和分析，以通过LSTM模型和预训练的伯特模型预测情绪。</p><p id="f9ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定相同的资源和时间，预训练的BERT比LSTM稍好，但没有显著差异。</p><p id="f348" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">潜在地，对类似的推文从头训练BERT模型可以产生更好的结果，而所需的资源和成本超出了本研究的范围。</p><h1 id="f70a" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">(可选)推文流上的推理</h1><p id="0e65" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">下面是一段代码，它使用上面训练好的模型，将tweet流作为输入进行处理，并输出带有置信度的情绪。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="c7e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">示例输出如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure></div></div>    
</body>
</html>