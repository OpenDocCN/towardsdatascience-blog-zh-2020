<html>
<head>
<title>TernaryBERT: Quantization Meets Distillation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特里纳尔伯特:量子化与蒸馏相遇</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ternarybert-quantization-meets-distillation-1b902ac31bd6?source=collection_archive---------32-----------------------#2020-10-06">https://towardsdatascience.com/ternarybert-quantization-meets-distillation-1b902ac31bd6?source=collection_archive---------32-----------------------#2020-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bd1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">华为对BERTology的贡献</h2></div><p id="6a1f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated">建造像伯特和GPT-3这样越来越大的模型的趋势一直伴随着一种互补的努力，即以很少或没有精度成本来减小它们的尺寸。有效的模型要么通过蒸馏(<a class="ae ln" href="https://arxiv.org/abs/1908.08962" rel="noopener ugc nofollow" target="_blank">预训练蒸馏</a>、<a class="ae ln" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank">蒸馏伯特</a>、<a class="ae ln" href="https://arxiv.org/abs/2004.02984" rel="noopener ugc nofollow" target="_blank">移动伯特</a>、<a class="ae ln" href="https://arxiv.org/abs/1909.10351" rel="noopener ugc nofollow" target="_blank"> TinyBERT </a>)、量化(<a class="ae ln" href="https://arxiv.org/pdf/1909.05840.pdf" rel="noopener ugc nofollow" target="_blank"> Q-BERT </a>、<a class="ae ln" href="https://arxiv.org/pdf/1910.06188.pdf" rel="noopener ugc nofollow" target="_blank"> Q8BERT </a>)或者参数修剪来建立。</p><p id="8ae4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">9月27日，华为推出了<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank"> TernaryBERT </a>，这是一种利用蒸馏和量化来实现与原始BERT模型相当的精度的模型<strong class="kk iu">，尺寸缩小了约15倍</strong>。TernaryBERT真正值得注意的是，它的权重是<em class="lo">三进制的</em>，即具有三个值之一:-1、0或1(因此只能存储在两位中)。</p><p id="ab8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TernaryBERT巧妙地将现有的量化和提炼技术结合在一起。这篇论文大量引用了以前的工作，因此相当密集。本文的目标是提供一个自包含的演练，并在需要时提供额外的上下文。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/80fd8dea4e0e5dd4a210795a73293ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ip50gLEewIsjSFJi"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">马库斯·斯皮斯克在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="4e98" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">量化</h1><blockquote class="mx my mz"><p id="c2f4" class="ki kj lo kk b kl km ju kn ko kp jx kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated"><strong class="kk iu">量化</strong>是减少用于表示单个标量参数的位数的过程。</p></blockquote><p id="ff0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当成功时，量化相对容易，因为它允许模型设计者保持原始模型的体系结构和配置不变:通过从32位切换到8位参数表示，可以实现4倍的大小缩减，而不必重新访问像层数或隐藏大小这样的设置。量化一般把<em class="lo">实数值</em>映射到<em class="lo">整数值</em>，可以有效得多的相加相乘。</p><h2 id="47bf" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">量化方案</h2><p id="cd57" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated"><em class="lo">量化方案</em>是<em class="lo"> </em>确定一个<em class="lo"> </em>实值<em class="lo"> r </em>如何映射到一个可以由目标比特数(对于8比特:-128到127包括在内，或者0到255不包括在内)表示的整数(或量子)<em class="lo"> q </em>的算法。最常见的是，量化方案是线性的。<em class="lo"> r </em>和<em class="lo"> q </em>之间的关系可以用比例因子<em class="lo"> S </em>和零点<em class="lo"> Z </em>来表示:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nu"><img src="../Images/b40ab03747f21335ac322281c3c6c420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DEYJKhhp0yrcLv1mn_j_DA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">线性量化:用比例因子<strong class="bd nv"> S </strong>和零点<strong class="bd nv"> Z </strong>表示的真实值<strong class="bd nv"> r </strong>与其量化值<strong class="bd nv"> q </strong>之间的线性关系。方程式(1)来自Jacob等人[Q1]</p></figure><p id="27d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实值模型参数均匀分布在区间[-1，1]且量程从-127到127(暂时忽略-128桶)的最简单情况下，<em class="lo"> r = 1/127 * q. </em>换句话说，比例因子<em class="lo"> S </em>为<em class="lo"> 1/127 </em>，零点<em class="lo"> Z </em>为<em class="lo"> 0 </em>。当<em class="lo"> r </em>不均匀分布时(假设其值主要为负值)，零点会相应移动。例如，当<em class="lo"> Z=10 </em>时，我们将10个量子重新分配给负值，从而提高大多数<em class="lo"> r </em>值所在区域的精度。</p><p id="be07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TernaryBERT对其权重和激活都应用了线性量化(下面将详细介绍)。但就上下文而言，你应该知道还有其他方案。例如，在神经网络[Q2]的上下文中介绍了<em class="lo">三值化</em>(即量化为-1，0和1)的论文提出了一种随机算法，用于将实数<em class="lo"> r </em> ∈ [-1，1]转换为量化值<em class="lo"> q </em> ∈ {-1，0，1}:</p><ul class=""><li id="5933" class="nw nx it kk b kl km ko kp kr ny kv nz kz oa ld ob oc od oe bi translated">如果<em class="lo"> r </em> ∈ (0，1)<em class="lo">q =</em>1以概率<em class="lo"> r </em>和<em class="lo"> q </em> =0以概率<em class="lo"> 1-r. </em></li><li id="5bce" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">如果<em class="lo"> r </em> ∈ [-1，0]，<em class="lo"> q </em> =-1以概率<em class="lo"> -r </em>，<em class="lo"> q </em> =0以概率<em class="lo"> 1+r. </em></li></ul><h2 id="4e3f" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">量子化应该在什么时候发生？</h2><p id="374f" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">应用量化的最方便的时间是<strong class="kk iu">后训练</strong>:在用32位实值参数训练模型之后，应用标准量化方案之一，然后使用量化的模型进行有效的推断。然而，在实践中，这种天真的方法往往会导致精度大幅下降，尤其是针对超低精度(2或4位)时。即使我们在这种训练后量化之后执行额外的微调步骤，结果仍然不能令人满意；量化降低了模型参数及其梯度的分辨率(即可以表示多少不同的值)，从而阻碍了学习过程。</p><p id="1795" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，Jacob等人【Q1】提出了用<strong class="kk iu">模拟量子化</strong>进行训练:在正向传递过程中，模型的行为就好像它已经被量子化了一样。因此，损失和梯度是相对于这种位约束模型来计算的，但是反向传递照常在全精度权重上发生。这鼓励模型在量化正向传递期间表现良好(这是在推断时发生的)，同时继续利用较长位参数和梯度的更好的表示能力。Q8BERT [Q3]使用这种技术(也称为<strong class="kk iu">量化感知训练</strong>)将原始的BERT模型从32位减少到8位整数表示。TernaryBERT采用了类似的策略。</p><h2 id="6fab" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">三元伯特中的权重三元化</h2><p id="6482" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">TernaryBERT通过如上所述的线性量化方案将其32位实值权重转换成具有来自集合{-1，0，1}的值的2位三进制表示。当零点固定为<em class="lo"> Z=0 </em>时，比例因子<em class="lo"> S &gt; 0 </em>(从这里开始用<em class="lo"> α </em>表示)与模型参数一起被学习。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ok"><img src="../Images/ed45ff915ceb6f812e8a43f091d306d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9UaJ6sG7UKG94C8K"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">由<a class="ae ln" href="https://unsplash.com/@florenciaviadana?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Florencia Viadana </a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9341" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不用说，将参数从32位降级到2位会带来精度的巨大损失。为了恢复模型的一些失去的表达能力，通常的做法是使用多个比例因子<em class="lo"> αᵢ </em>(而不是用于整个网络的单个<em class="lo"> α </em>)，一个用于参数(矩阵、向量、核、层等)的每个自然分组<em class="lo"> i </em>。).例如，Rastegari等人[Q4]在其卷积神经网络的每一层上为每个滤波器使用了单独的比例因子。类似地，TernaryBERT为BERT的每个变换层添加一个<em class="lo"> αᵢ </em>，并为令牌嵌入矩阵添加单独的每行缩放因子。</p><p id="c259" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么这些比例因子是如何习得的呢？如上所述，比例因子<em class="lo"> αᵢ、</em>全精度权重<strong class="kk iu"> <em class="lo"> w </em> </strong>和量化权重<strong class="kk iu"> <em class="lo"> b </em> </strong>都是在训练过程中学习的。TernaryBERT比较了两种用于近似这些参数的现有方法:<a class="ae ln" href="https://arxiv.org/pdf/1605.04711.pdf" rel="noopener ugc nofollow" target="_blank">三元权重网络</a>(TWN)【Q5】和<a class="ae ln" href="https://arxiv.org/pdf/1802.08635.pdf" rel="noopener ugc nofollow" target="_blank">损失感知三元化</a>(LAT)【Q6】。虽然这两者具有看似不同的公式，但它们都归结为最小化量化前向传递的预测损失，并具有附加约束，其中在每个训练步骤期间，鼓励量化权重<em class="lo">α</em><strong class="kk iu"><em class="lo">b</em></strong><em class="lo"/>保持接近全精度权重<strong class="kk iu"> w </strong>。</p><p id="a42a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你时间不够，你可以直接跳到“激活量化”部分，确信TWN和拉特在<a class="ae ln" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"> GLUE基准</a>(主要包含分类任务)和<a class="ae ln" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> SQuAD </a>(一个流行的问题回答数据集)上得分相当。</p><h2 id="4744" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">选项1: TWN(三元加权网络)</h2><p id="8d6b" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated"><a class="ae ln" href="https://arxiv.org/pdf/1605.04711.pdf" rel="noopener ugc nofollow" target="_blank">三进制权重网络</a> (TWN)将问题公式化为<em class="lo">最小化全精度和量化参数</em>之间的距离:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d143541f62aa8fb7aab248ff61de0a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*9mKVIST8bTUSaEEmxW3dOA.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">“基于近似的量子化”。在训练步骤<strong class="bd nv"> t </strong>期间，选择量化参数<strong class="bd nv"> b </strong>和比例因子<strong class="bd nv"> α </strong>，使得它们最小化到全精度权重<strong class="bd nv"> w </strong>的距离。这是来自<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">三元伯特</a>的方程式(5)。</p></figure><p id="f069" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为这种最小化是在每个训练步骤上执行的，所以有效的实现是至关重要的。幸运的是，有一个近似的解析解，所以计算<strong class="kk iu"> <em class="lo"> b </em> </strong>和<em class="lo"> α </em> <strong class="kk iu"> </strong>就像应用一个依赖于<strong class="kk iu"> <em class="lo"> w </em> </strong>的公式一样简单(为简单起见，本文不包括)。</p><h2 id="15b4" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">选项#2: LAT(损失感知三值化)</h2><p id="1f38" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">另一种方法，<a class="ae ln" href="https://arxiv.org/pdf/1802.08635.pdf" rel="noopener ugc nofollow" target="_blank">损失感知三值化</a> (LAT)，是<em class="lo">直接最小化相对于量化权重</em>计算的损失；这个表达式完全避开了全精度参数(注意下面没有<strong class="kk iu"> <em class="lo"> w </em> </strong>):</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi om"><img src="../Images/eaeca0c75103d67b3ba6a9a970d62cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*Wqm5VW_Dnb8uk8W3_15zYQ.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">“有损量化”。在整个训练过程中，选择量化参数<strong class="bd nv"> b </strong>和比例因子<strong class="bd nv"> α </strong>，使得它们最小化训练数据上的损失<strong class="bd nv"> L </strong>(关于<strong class="bd nv"> L </strong>的定义，参见蒸馏部分)。来自<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">三元贝</a>的方程式(6)。</p></figure><p id="fcd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，上面的表达式应该看起来有些可疑:我们已经确定，以低位表示训练模型会受到精度损失的阻碍。当然，全精度权重<strong class="kk iu"> <em class="lo"> w </em> </strong>需要以某种方式参与进来。事实证明，该表达式可以根据每次迭代最小化子问题来重新表述，其中量化权重再次被鼓励接近全精度权重，但是以还考虑当前时间步长的损失的方式:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi on"><img src="../Images/1fda5af98b1bb19e2b3ea1266da28342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaAl5c9lWw4Jy3bRY_nemw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">每迭代损失感知量化。在训练步骤<strong class="bd nv"> t </strong>期间，选择量化参数<strong class="bd nv"> b </strong>和比例因子<strong class="bd nv"> α </strong>，使得它们最小化到全精度权重<strong class="bd nv"> w </strong>的特定距离。这是来自<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">三元伯特</a>的方程式(7)。注意与等式(5)的相似性。这里的距离是以<strong class="bd nv"> v </strong>表示的，即等式(6)中损耗<strong class="bd nv"> L </strong>的对角近似值。</p></figure><p id="3f15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该表达式与等式(5)非常相似，不同之处在于它包括<strong class="kk iu"> <em class="lo"> v </em> </strong>，这是等式(6)中的损失的统计。与等式(5)类似，它有一个近似的解析解，是<strong class="kk iu"> <em class="lo"> w </em> </strong>的函数，可以有效地计算(同样，为了简单起见，不包括在本文中)。</p><h2 id="8c8c" class="nd mg it bd mh ne nf dn ml ng nh dp mp kr ni nj mr kv nk nl mt kz nm nn mv no bi translated">激活量子化</h2><p id="d1e3" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">在<em class="lo">权重</em>量化之后，该模型可以被描述为被分组为逻辑单元(例如矩阵)的一组三元权重，它们中的每一个都具有其自己的实值缩放因子<em class="lo"> αᵢ </em>。因此，流经网络的值(层的输入和输出)，也称为<em class="lo">激活、</em>是实值。为了加速矩阵乘法，激活也可以被量化。然而，正如在以前的工作[Q7]中提到的，激活对量子化更敏感；这很可能是为什么TernaryBERT决定将激活量化到8位而不是2位的原因。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oo"><img src="../Images/78b9ccb6c4eaee4ad283f102e3ab29e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*95xQ6BVpdWe3GMKq"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">西蒙·哈默在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4f2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于对变压器激活倾向于负向多于正向的观察，TernaryBERT的作者选择了一种<em class="lo">非对称</em>量化算法，或者，根据高于<em class="lo">r = S(q-Z)</em>的线性量化表达式，零点<em class="lo"> Z </em>不固定为0，而是<em class="lo"> r的最小和最大可能值的中点</em></p><h1 id="db44" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">蒸馏</h1><p id="333e" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">从最初天真的提议到执行训练后量化，我们已经走了很长的路。我们确定量化需要成为训练的一部分，需要多个比例因子来衰减由从32位降级到2位引起的精度损失，并探索了使量化权重接近全精度权重的两种不同方法。但是TernaryBERT告诉我们，通过利用机器学习工具箱中的另一种技术，我们可以做得更好。</p><blockquote class="mx my mz"><p id="3080" class="ki kj lo kk b kl km ju kn ko kp jx kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated"><strong class="kk iu">提炼</strong>(或称知识提炼)是一个大而精确的模型(老师)将其知识转移到一个代表性力量较小的模型(学生)的过程。</p></blockquote><p id="119a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，蒸馏是一个两步走的过程:1)训练一个大的老师使用金标签，2)训练一个小的学生使用老师制作的标签，也称为<em class="lo">软标签</em>。辛顿等人[D1]解释说，蒸馏优于标准训练，因为软标签携带额外的信息，或<em class="lo">暗知识</em>。例如，在句子“我喜欢散步”中，考虑用单词“散步”的正确词性来标记它。形式为<em class="lo"> p(名词)=0.9 </em>的软标签比形式为<em class="lo"> p(名词)=1.0 </em>的硬标签更能提供信息，硬标签未能捕捉到“walk”在其他上下文中可能是动词的事实。TernaryBERT在微调过程中使用相同的技术，从一个大得多的教师产生的软分布中学习:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi op"><img src="../Images/25cd909efe35d50355cfffa05b67e3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZRb_qH8NY2EoBuIqwYXlg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">蒸馏损失第1部分:学生和教师预测之间的软交叉熵。来源:<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">特里纳尔伯特</a>。</p></figure><p id="05e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">诸如FitNets [D2]等更精细的提炼公式鼓励教师和学生的内部表征之间的直接一致。TernaryBERT也这样做，将完全精确的学生的隐藏层拉近教师的隐藏层，并鼓励两个转换网络的注意力分数相似:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oq"><img src="../Images/9130157902a20dc9ae6e5b4f48d12e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KVLStOVdXHWnEjgs14Rdg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">蒸馏损失第2部分:学生/教师隐藏权重和注意力分数之间的均方误差。来源:<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">三月伯特</a>。</p></figure><h1 id="10d2" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">端到端:蒸馏感知三值化</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi or"><img src="../Images/3fab31beb7afaeb2477c8f020e6dab10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7g7Tg7QjqmZkm5CRzxrZjA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">蒸馏感知三值化。图2来自<a class="ae ln" href="https://arxiv.org/pdf/2009.12812.pdf" rel="noopener ugc nofollow" target="_blank">三月伯特</a>。</p></figure><p id="5a11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TernaryBERT将来自量化和提炼的既定技术放在一个单一的端到端配方下，用于训练三值化模型。在每个训练步骤中:</p><ol class=""><li id="c43b" class="nw nx it kk b kl km ko kp kr ny kv nz kz oa ld os oc od oe bi translated">完全精确的学生是三位一体的。实际上，这相当于模拟量化，Jacob等人[Q1]介绍的方法:正向传递在低表示中执行，以<em class="lo">模拟</em>在推断期间将真正发生的事情。使用前面描述的两种方法之一(TWN或LAT)来执行三值化。</li><li id="fedc" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld os oc od oe bi translated">蒸馏损失(<em class="lo"> L_pred + L_trm </em>根据量化模型所做的预测进行计算，但是梯度更新(即反向传递)应用于全精度参数。</li></ol><p id="9a04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练结束时，量化模型已准备好进行推断，无需进一步调整。</p><h1 id="dcb7" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">量化参考</h1><ul class=""><li id="a990" class="nw nx it kk b kl np ko nq kr ot kv ou kz ov ld ob oc od oe bi translated">[Q1]雅各布等人，<a class="ae ln" href="https://arxiv.org/pdf/1712.05877.pdf" rel="noopener ugc nofollow" target="_blank">用于高效整数算术推理的神经网络的量化和训练</a> (2017)</li><li id="a271" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[Q2]林等，<a class="ae ln" href="https://arxiv.org/pdf/1510.03009.pdf" rel="noopener ugc nofollow" target="_blank">少乘法神经网络</a> (2016)</li><li id="4b7e" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[Q3] Zafrir等人，<a class="ae ln" href="https://arxiv.org/pdf/1910.06188.pdf" rel="noopener ugc nofollow" target="_blank"> Q8BERT:量化的8位BERT </a> (2019)</li><li id="3fb4" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[Q4] <a class="ae ln" href="https://arxiv.org/pdf/1603.05279.pdf" rel="noopener ugc nofollow" target="_blank">拉斯特加里等人，XNOR网络:使用二进制卷积神经网络的图像网络分类</a> (2016)</li><li id="1b64" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">李等，<a class="ae ln" href="https://arxiv.org/pdf/1605.04711.pdf" rel="noopener ugc nofollow" target="_blank">三元权重网络</a> (2016)</li><li id="51e1" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[Q6]侯，郭，<a class="ae ln" href="https://arxiv.org/pdf/1802.08635.pdf" rel="noopener ugc nofollow" target="_blank">深度网络的损失感知权重量化</a> (2018)</li><li id="0c81" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[Q7]周等，DoReFa-NET:用低位宽梯度训练低位宽卷积神经网络(2018)</li></ul><h1 id="8ec4" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">蒸馏参考</h1><ul class=""><li id="3250" class="nw nx it kk b kl np ko nq kr ot kv ou kz ov ld ob oc od oe bi translated">[D1]辛顿等，<a class="ae ln" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a> (2015)</li><li id="557c" class="nw nx it kk b kl of ko og kr oh kv oi kz oj ld ob oc od oe bi translated">[D2]罗梅罗等人，<a class="ae ln" href="https://arxiv.org/abs/1412.6550" rel="noopener ugc nofollow" target="_blank"> FitNets:提示薄深网</a> (2014)</li></ul></div></div>    
</body>
</html>