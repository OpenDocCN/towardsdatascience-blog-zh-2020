<html>
<head>
<title>What “no free lunch” really means in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“没有免费的午餐”在机器学习中真正意味着什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-no-free-lunch-really-means-in-machine-learning-85493215625d?source=collection_archive---------6-----------------------#2020-11-12">https://towardsdatascience.com/what-no-free-lunch-really-means-in-machine-learning-85493215625d?source=collection_archive---------6-----------------------#2020-11-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="731a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">揭开这个经常被误解的定理。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ebbf31f2ae1f4c209fa4672f700ae7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDbmArBjkmmia9gMQhZVpQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Riccardo Bergamini 在<a class="ae ky" href="https://unsplash.com/s/photos/free-lunch?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e2ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谁不爱免费的午餐？你不用做饭，也不用花你的任何血汗钱。对任何人来说都很划算！事实是，除非你算上研究生院承诺免费披萨的特殊讲座和演讲，否则在机器学习领域就没有<strong class="lb iu">免费的午餐。</strong></p><p id="d84b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">监督机器学习的“没有免费的午餐”(NFL)定理本质上暗示了<strong class="lb iu">没有单一的机器学习算法是普遍适用于所有问题的最佳算法</strong>。这是我在上一篇关于XGBoost局限性的文章中探讨的一个概念，XGBoost是一种算法，由于它在学术研究和机器学习竞赛中的表现，在过去五年中获得了巨大的流行。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/why-xgboost-cant-solve-all-your-problems-b5003a62d12a"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">为什么XGBoost不能解决你所有的问题。</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">XGBoost和其他基于树的算法的一个关键限制。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="71a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目标是利用这个经常被误解的定理并解释它，以便您可以欣赏这个定理背后的理论，并理解它对您作为机器学习实践者或数据科学家的工作的实际影响。</p><h1 id="9f76" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">归纳的问题</h1><p id="005f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">奇怪的是，启发NFL定理的想法是由18世纪的一位哲学家首先提出的。是的，你没看错！不是数学家或统计学家，而是哲学家。</p><p id="fa6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">18世纪中期，一位名叫大卫·休谟的苏格兰哲学家提出了他所谓的归纳 的<a class="ae ky" href="https://plato.stanford.edu/entries/induction-problem/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">问题。这个问题是一个哲学问题，问归纳推理是否真的引导我们走向真知。</strong></a></p><p id="f2f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归纳推理是一种推理形式，我们根据过去的观察得出关于世界的结论。奇怪的是，这正是机器学习算法所做的。如果神经网络看到100张白天鹅的图像，它可能会得出所有天鹅都是白色的结论。但是如果神经网络看到一只黑天鹅会怎么样呢？现在，算法学习的模式突然被一个反例推翻了。这个想法通常被称为<a class="ae ky" href="https://deepai.org/machine-learning-glossary-and-terms/black-swan-paradox" rel="noopener ugc nofollow" target="_blank">黑天鹅悖论</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/0c8a2155724c636d7014ac33828ebd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8ElZ3K3vOFax5ZkfOkJvQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Yuvraj Yadav 在<a class="ae ky" href="https://unsplash.com/s/photos/black-swan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c950" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">休谟用这种逻辑来强调归纳推理的局限性——我们不能将一组特定的观察结果应用到一组更普遍的观察结果上。</p><blockquote class="nl nm nn"><p id="f211" class="kz la no lb b lc ld ju le lf lg jx lh np lj lk ll nq ln lo lp nr lr ls lt lu im bi translated"><em class="it">“没有论证可以证明，那些我们没有经历过的实例，类似于那些我们经历过的实例。”——大卫·休谟在《人性论》中</em></p></blockquote><p id="8c44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的想法成为200多年后机器学习的NFL定理的灵感。</p><h1 id="76d0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Wolpert在机器学习中的应用</h1><p id="196b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在他1996年的论文<a class="ae ky" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">学习算法之间缺乏先验区分</a>中，Wolpert为监督机器学习引入了NFL定理，并在论文开头引用了David Hume的话。该定理指出，给定无噪声数据集，<strong class="lb iu">对于任何两个机器学习算法A和B，A和B的平均性能在从均匀概率分布中抽取的所有可能的问题实例上将是相同的。</strong></p><p id="eff6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么会这样呢？这又回到了归纳推理的概念。<strong class="lb iu">对于机器学习问题，每个机器学习算法都预先假设特征和目标变量之间的关系。</strong>这些假设通常被称为<em class="no">先验假设</em>。机器学习算法在任何给定问题上的性能取决于算法的假设与问题的现实相符程度。<strong class="lb iu">一种算法可能对一个问题表现得很好，但这并不意味着我们有理由相信，在同样的假设可能不起作用的另一个问题上，它也会表现得一样好。</strong>这个概念基本上就是机器学习背景下的黑天鹅悖论。</p><p id="98d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">你在选择任何算法时做出的限制性假设就像你为午餐支付的价格。</strong>这些假设将使你的算法在某些问题上自然更好，同时在其他问题上自然更差。</p><h1 id="a170" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">偏差-方差权衡</h1><p id="d246" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">统计学和机器学习中与NFL定理密切相关的一个关键思想是<strong class="lb iu">偏差-方差权衡</strong>的概念。这一概念探讨了任何模型的两种误差源之间的关系:</p><ul class=""><li id="27be" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">模型的<strong class="lb iu">偏差</strong>是来自模型中潜在错误的先验假设的<strong class="lb iu">误差。这些假设导致模型错过了关于机器学习问题的特征和目标之间的关系的重要信息。</strong></li><li id="84f2" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">模型的<strong class="lb iu">方差</strong>是来自模型对训练数据微小变化的敏感度<strong class="lb iu">误差。</strong></li></ul><p id="b23d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有<strong class="lb iu">高偏差的模型通常过于简单</strong>并导致<strong class="lb iu">欠拟合</strong>，而具有<strong class="lb iu">高方差的模型通常过于复杂</strong>并导致<strong class="lb iu">过拟合</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl og"><img src="../Images/3940ea98809147e2d78b7a2b2a0ec8cf.png" data-original-src="https://miro.medium.com/v2/0*lk7W93jW0wv2RPnZ"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">过度拟合与欠拟合。来源:<a class="ae ky" href="https://www.educative.io/edpresso/overfitting-and-underfitting" rel="noopener ugc nofollow" target="_blank"> Edpresso </a>，根据<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a>授权。</p></figure><p id="aea7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上图所示，具有高偏差的模型无法正确拟合训练数据，而具有高方差的模型则很好地拟合了训练数据，以至于它记住了这些数据，却无法正确地将学到的知识应用到新的真实数据中。给定问题的最优模型介于这两个极端之间。<strong class="lb iu">它有足够的偏差来避免简单地记忆训练数据，并且有足够的方差来实际拟合训练数据中的模式</strong>。这种优化模型是通过优化偏差-方差权衡来实现对给定问题的测试数据的最低预测误差的模型，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/962172cd392c5d368843ed511a0cde99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aOwocsnOKj3U2OV-.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">偏差-方差权衡。来源:<a class="ae ky" href="https://www.ncbi.nlm.nih.gov/books/NBK543534/figure/ch8.Fig3/" rel="noopener ugc nofollow" target="_blank">临床数据科学基础</a>，4.0 授权<a class="ae ky" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC下。</a></p></figure><p id="dc0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，每个机器学习问题都有一个不同的点，在这个点上，偏差-方差的权衡被优化，预测误差被最小化。正因如此，<strong class="lb iu">没有一个超级算法能比每一个其他算法更好地解决每一个机器学习问题</strong>。每种算法都做出假设，创造不同类型和水平的偏差，从而使它们更适合某些问题。</p><h1 id="8a0b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">“没有免费的午餐”对你意味着什么</h1><p id="695e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">所有这些理论都很棒，但对于作为数据科学家、机器学习工程师或只想开始学习机器的人来说，“没有免费的午餐”意味着什么？</p><p id="42f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是不是说所有算法都是平等的？不，当然不是。实际上，所有的算法并不都是平等的。这是因为整个机器学习问题集是NFL定理中的一个理论概念，它比我们实际尝试解决的实际机器学习问题集要大得多。在某些类型的问题上，一些算法可能通常比其他算法表现得更好，但是由于算法的先验假设，每个算法都有缺点和优点。</p><p id="024d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像XGBoost这样的算法可能会赢得数百场Kaggle比赛，但在预测任务中却惨败，因为基于树的模型中涉及到有限的假设。当涉及到图像分类和语音检测等复杂任务时，神经网络可能表现得非常好，但如果没有正确训练，就会因其复杂性而遭受过拟合。</p><p id="665c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，这就是“没有免费的午餐”对你的意义:</p><ul class=""><li id="c435" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated"><strong class="lb iu">没有一种算法会比其他算法更好地解决你所有的机器学习问题</strong>。</li><li id="edb7" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated"><strong class="lb iu">在选择要使用的算法之前，确保你完全理解一个机器学习问题和所涉及的数据。</strong></li><li id="335a" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">所有模型的好坏取决于创建它们时的假设以及用来训练它们的数据。</li><li id="993e" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated"><strong class="lb iu">逻辑回归等更简单的模型偏差更大，容易欠拟合，而神经网络等更复杂的模型方差更大，容易过拟合。</strong></li><li id="7616" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated"><strong class="lb iu">给定问题的最佳模型位于两个偏差-方差极值的中间。</strong></li><li id="963f" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated"><strong class="lb iu">为了找到解决问题的好模型，您可能需要尝试不同的模型，并使用稳健的交叉验证策略对它们进行比较。</strong></li></ul><h1 id="f61c" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">来源</h1><ol class=""><li id="4494" class="ns nt it lb b lc nf lf ng li oi lm oj lq ok lu ol ny nz oa bi translated">《斯坦福哲学百科全书》，<a class="ae ky" href="https://plato.stanford.edu/entries/induction-problem/" rel="noopener ugc nofollow" target="_blank">归纳的问题</a>，(2018)。</li><li id="5890" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ol ny nz oa bi translated">迪派，<a class="ae ky" href="https://deepai.org/machine-learning-glossary-and-terms/black-swan-paradox" rel="noopener ugc nofollow" target="_blank">黑天鹅悖论定义</a>，(2020)，deepai.org。</li><li id="d3ba" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ol ny nz oa bi translated">D.休谟，<a class="ae ky" href="https://www.gutenberg.org/files/4705/4705-h/4705-h.htm" rel="noopener ugc nofollow" target="_blank">人性的论述</a>，(1739)，古腾堡计划。</li><li id="baf6" class="ns nt it lb b lc ob lf oc li od lm oe lq of lu ol ny nz oa bi translated">D.H.Wolpert，<a class="ae ky" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.390.9412&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">学习算法之间缺乏先验区分</a>，(1996)，CiteSeerX。</li></ol></div></div>    
</body>
</html>