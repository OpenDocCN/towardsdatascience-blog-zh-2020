<html>
<head>
<title>Incorporate Text Feature Into Classification Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将文本特征并入分类项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/integrate-text-content-into-classification-project-eddd8e18a7e9?source=collection_archive---------41-----------------------#2020-11-01">https://towardsdatascience.com/integrate-text-content-into-classification-project-eddd8e18a7e9?source=collection_archive---------41-----------------------#2020-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e544" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">比较了波特、斯诺鲍和兰开斯特·斯特默斯。将评论整合到客户数据中，以提高模型的性能。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6cb15e35b6046ac1dedf13a300f91dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZmMRlezei4PhcY1CKQuHQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/lud4OaUCP4Q" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><h1 id="f49d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">内容表</h1><ul class=""><li id="f600" class="lo lp iq lq b lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><a class="ae kv" href="#acad" rel="noopener ugc nofollow">项目总结</a></li><li id="6495" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="#aba4" rel="noopener ugc nofollow">数据</a></li><li id="9db2" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="#e053" rel="noopener ugc nofollow">文本挖掘</a></li><li id="91de" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="#3206" rel="noopener ugc nofollow">建模</a></li><li id="7474" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="#dd58" rel="noopener ugc nofollow">结论</a></li></ul><h1 id="acad" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">项目摘要</h1><p id="ee2c" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">该项目旨在研究非结构化文本数据如何为机器学习分类项目增加价值。</p><p id="856b" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">我有来自一家公司的两个数据集。其中一个包含了所有客户的基本信息。customer数据集也有target列，它指示客户是持有成员资格还是已经取消了成员资格。另一个数据集包含客户留给公司的评论。我的任务是将顾客的评论与顾客现有的信息结合起来，预测他们是否会取消或保留会员资格。</p><p id="da70" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">在这个项目中，我研究了如何使用自然语言工具包(NLTK)将文本挖掘技术整合到机器学习模型中，以提高二元分类的性能。</p><p id="2e17" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">我在这个项目中使用了Porter stemmer、Snowball stemmer、Lancaster stemmer、word_tokenize、CountVectorizer、TfidfTransformer、LabelEncoder、XGBClassifier和CatBoostClassifier。我选择ROC分数作为主要的评估标准。</p><p id="8d2f" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">点击<a class="ae kv" href="#f49d" rel="noopener ugc nofollow">此处</a>返回顶部</p><h1 id="aba4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据</h1><p id="7310" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">客户数据集有2070条记录和17个变量，包括每个客户的ID。评论数据集包含公司从每个客户那里收集的信息。</p><p id="73c8" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">图1显示了目标列的频率。其中1266人是该公司的会员，804人已经取消了会员资格。我认为这是非常平衡的数据，所以我没有应用过采样技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c65298e93af619ac63984fdf38766348.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*WSfB5DH_wNUW-bg1QzJzsg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 —目标的频率</p></figure><p id="ef3d" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">因为我要计算ROC得分，所以我将目标值转换为二进制变量。代码如下:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="6460" class="nj kx iq nf b gy nk nl l nm nn"># Encoding the Target</span><span id="928a" class="nj kx iq nf b gy no nl l nm nn">le = LabelEncoder()</span><span id="b928" class="nj kx iq nf b gy no nl l nm nn">label = le.fit(customer["TARGET"]).transform(customer["TARGET"])</span></pre><p id="7387" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">点击<a class="ae kv" href="#f49d" rel="noopener ugc nofollow">此处</a>返回页首</p><h1 id="e053" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">文本挖掘</h1><p id="984a" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">首先，我必须标记注释。原始评论可能包含错别字、无关内容、冗余信息等，这些会增加计算负担并降低模型的效率。利用分词器将这些句子分解成单个的单词使我能够提取评论的核心信息。图2比较了原始注释和标记化注释。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2f40" class="nj kx iq nf b gy nk nl l nm nn"># First, tokenize the comments<br/>comment["TokenizedComments"]<br/>        =comment["Comments"].apply(word_tokenize)</span><span id="b784" class="nj kx iq nf b gy no nl l nm nn">comment.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/aa1656e6b27c3712f2e6e7037c071052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dH_efM7NHJnyG3yOO1BFbA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2 —标记化的注释</p></figure><p id="36dd" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">为了减少高维度，我应用了三种不同的词干分析器:Porter、Snowball (Porter 2)和Lancaster。图3显示了不同算法的输出。每一种都有利弊。我们可以看看这里:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/329564609a2c159fea521a38b5ddde41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-wsKRvkJqJzgWIOCCfRyw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3 —不同词干分析器的输出</p></figure><blockquote class="nr ns nt"><p id="ffc8" class="ml mm nu lq b lr my jr mn lt mz ju mo nv na mq mr nw nb mt mu nx nc mw mx mb ij bi translated">1.看起来Porter倾向于保存原始内容的大写或小写</p><p id="6aa0" class="ml mm nu lq b lr my jr mn lt mz ju mo nv na mq mr nw nb mt mu nx nc mw mx mb ij bi translated">2.兰卡斯特非常好斗。例如，对于ID 3034，当其他两个保留单词“nee”时，Lancaster将其转换为“nee”</p><p id="a47d" class="ml mm nu lq b lr my jr mn lt mz ju mo nv na mq mr nw nb mt mu nx nc mw mx mb ij bi translated">3.波特也很难识别简单的单词。例如，在大多数情况下，波特不知何故将“his”转换成了“hi”</p><p id="3ad8" class="ml mm nu lq b lr my jr mn lt mz ju mo nv na mq mr nw nb mt mu nx nc mw mx mb ij bi translated">4.因此，我选择了Snowball stemmer来完成项目的剩余部分</p></blockquote><p id="dc24" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">词干化之后，我构建了术语-文档矩阵并删除了停用词。术语文档矩阵的维数是2070×354。它可以告诉我们一个特定的单词是否存在于一个特定的文档中。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="82f9" class="nj kx iq nf b gy nk nl l nm nn"># Contruct term-document matrix<br/>count_vect = CountVectorizer(stop_words='english',lowercase=False)</span><span id="6643" class="nj kx iq nf b gy no nl l nm nn">TD_counts = count_vect.fit_transform(newTextData["Snow"])</span><span id="5ec1" class="nj kx iq nf b gy no nl l nm nn">DF_TD_Counts=pd.DataFrame(TD_counts.toarray())</span><span id="33bc" class="nj kx iq nf b gy no nl l nm nn">DF_TD_Counts.columns = count_vect.get_feature_names()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a2d5b90adc8fd2a9a5cab70db99b701f.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*n3-lD8fHKbLrkBM7nbypsg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4 —术语文档矩阵</p></figure><p id="41f1" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">然后，我从术语文档矩阵中构造了TF-IDF矩阵。有许多方法可以量化文档集合中的文本内容，比如可读性分数。在我之前的一篇博客中，我也写过关于计算<a class="ae kv" rel="noopener" target="_blank" href="/use-r-to-calculate-boilerplate-for-accounting-analysis-f4a5b64e9b0d">样板分数</a>的内容，这也是一种量化文本内容的方法。TF-IDF是最常见的量化方法之一，用于评估一个单词与文档集合中的一个文档的相关程度。图5是TF-IDF矩阵的一部分。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="08bd" class="nj kx iq nf b gy nk nl l nm nn">#Compute TF-IDF Matrix<br/>tfidf_transformer = TfidfTransformer()</span><span id="352a" class="nj kx iq nf b gy no nl l nm nn">tfidf = tfidf_transformer.fit_transform(TD_counts)</span><span id="1f5e" class="nj kx iq nf b gy no nl l nm nn">DF_TF_IDF=pd.DataFrame(tfidf.toarray())</span><span id="ce53" class="nj kx iq nf b gy no nl l nm nn">DF_TF_IDF.columns=count_vect.get_feature_names()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/68136031da2e417ba84a6f8d369e659f.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*pmm9Yktc6uIF1qDWp4Ty-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5 — TFIDF矩阵</p></figure><p id="bcf1" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">在构建了TF-IDF矩阵之后，我将该矩阵与原始客户数据集相结合。组合数据集有2070条记录和387列。</p><p id="9569" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">点击<a class="ae kv" href="#f49d" rel="noopener ugc nofollow">此处</a>返回页首</p><h1 id="3206" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">建模</h1><p id="c477" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">为了验证我构建的模型，比较原始数据和组合数据，并避免过度拟合，我将两个数据集分别以4:1的比例分为训练数据集和测试数据集。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="02ab" class="nj kx iq nf b gy nk nl l nm nn"># split data at 80% 20% for orginal data and combined data</span><span id="8172" class="nj kx iq nf b gy no nl l nm nn">data_train, data_test, label_train, label_test = <br/>     train_test_split (pd.get_dummies(data.drop(["ID"],axis=1), <br/>                       test_size = 0.2, random_state = 42)</span><span id="a584" class="nj kx iq nf b gy no nl l nm nn">X_train, X_test, y_train, y_test = train_test_split(EncodeData.drop(["ID"],axis=1), label, <br/>                 test_size = 0.2, random_state = 42)</span></pre><p id="f193" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">首先，我对没有TF-IDF矩阵的数据应用了CatBoost和XGBoost。输出如图6和图7所示。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="966f" class="nj kx iq nf b gy nk nl l nm nn">cat=CatBoostClassifier()<br/>cat.fit(data_train,label_train)<br/>cat_predictions = cat.predict_proba(data_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/23220c6a7b571dba1f3bddcf914537a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFqLGxWXOqITG0GGgICvPA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图CatBoostClassifier的分类报告</p></figure><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="d9fb" class="nj kx iq nf b gy nk nl l nm nn">xgb = XGBClassifier()<br/>xgb.fit(data_train,label_train)<br/>xgb_predictions = xgb.predict_proba(data_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/a6adbcb9a34169bb1670376addd4d9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iR4cBURlYzRDZUinRm3yNg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图XGBClassifier的分类报告</p></figure><p id="5251" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">然后，我把同样的模型应用到综合数据中。CatBoost和XGBoost的ROC得分分别为<strong class="lq ir"> <em class="nu"> 0.917173 </em> </strong>和<strong class="lq ir"> <em class="nu"> 0.911844 </em> </strong>。图8显示了CatBoost模型的对数损失如何随着学习轮次的增加而变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/28c264d1cf85110354ff72ea31c451cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*B9QZK6PU2LqMPhVX"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8-CatBoostClassifier学习曲线</p></figure><p id="4e1d" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">图9是CatBoostClassifier结合数据的分类报告，这也是我在这个项目中得到的最好的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d69f32eb2d491a2f4f4b5b7b963c16ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*mw_vwLseGveE983ISvm-Ww.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9-包含组合数据的catboost分类器的分类报告</p></figure><p id="386a" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">点击<a class="ae kv" href="#f49d" rel="noopener ugc nofollow">此处</a>返回顶部</p><h1 id="dd58" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="9cdf" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mn lt lu ju mo lv mp mq mr lx ms mt mu lz mv mw mx mb ij bi translated">我比较了三种不同的词干法:波特、雪球和兰卡斯特。兰开斯特比另外两个梗工更有侵略性。波特倾向于保留更多的信息，并且很难识别最基本的单词，比如“他的”斯诺鲍似乎落在了另外两个词干作者之间:不像兰开斯特那样咄咄逼人，但保持了基本的词。</p><p id="499e" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">量化文本特征并将其添加到机器学习模型中有助于捕捉原始数据中不会发现的信息。此外，TF-IDF并不是唯一有帮助的方法。通过改变量化文本内容的方式，可以进一步改进模型。</p><p id="0eca" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">你可以在这里找到代码和数据。</p><p id="40f4" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated">点击<a class="ae kv" href="#f49d" rel="noopener ugc nofollow">此处</a>返回页首</p><h2 id="028f" class="nj kx iq bd ky oe of dn lc og oh dp lg lv oi oj li lx ok ol lk lz om on lm oo bi translated">以前的文章:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://medium.com/analytics-vidhya/analyzing-disease-co-occurrence-using-networkx-gephi-and-node2vec-53941da35a0f"><div class="gh gi op"><img src="../Images/7c606ad187c3a36da673e5a8fcf3f88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AadPvtOrWswBT_-WqU_xNw.png"/></div></a></figure><p id="f7f3" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated"><a class="ae kv" href="https://medium.com/analytics-vidhya/simple-weighted-average-ensemble-machine-learning-777824852426" rel="noopener">简单加权平均集成|机器学习</a></p><p id="705c" class="pw-post-body-paragraph ml mm iq lq b lr my jr mn lt mz ju mo lv na mq mr lx nb mt mu lz nc mw mx mb ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/use-r-to-calculate-boilerplate-for-accounting-analysis-f4a5b64e9b0d">使用R计算用于会计分析的样板文件</a></p></div></div>    
</body>
</html>