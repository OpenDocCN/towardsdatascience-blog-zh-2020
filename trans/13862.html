<html>
<head>
<title>Strong Women Through the Lens of The New York Times</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">透过《纽约时报》的镜头看强势女性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/strong-women-through-the-lens-of-the-new-york-times-f7f7468a2645?source=collection_archive---------35-----------------------#2020-09-23">https://towardsdatascience.com/strong-women-through-the-lens-of-the-new-york-times-f7f7468a2645?source=collection_archive---------35-----------------------#2020-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4084" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">Python中的API数据收集和文本分析</h2><div class=""/><div class=""><h2 id="03dc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">历史版画中的性别平等与表现研究</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1cb2cabed07c00319be3e20ce8bc8e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5G4xjroZCb2pJyMtKFYYrw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@giaferroni?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">贾科莫·费乐理</a>在<a class="ae lh" href="https://unsplash.com/s/photos/suffragette?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b0b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个项目的<strong class="lk jd">目标是通过情感分析、频繁术语可视化和主题建模的方式，调查过去70年间纽约时报中女性的代表性。</strong></p><p id="7dc0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了这次调查，我通过纽约时报开发者门户的<a class="ae lh" href="https://developer.nytimes.com/docs/archive-product/1/overview" rel="noopener ugc nofollow" target="_blank">归档API </a>搜集了纽约时报的数据。首先，你必须在这里获得API密匙<a class="ae lh" href="https://developer.nytimes.com/" rel="noopener ugc nofollow" target="_blank"/>。免费的！NYT只是喜欢调节防洪闸门的概念。由于这种类型的API适合批量数据收集，它不允许有效的预先过滤。如果您希望重新创建实验，请遵循Github上发布的Jupyter <a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times" rel="noopener ugc nofollow" target="_blank">笔记本</a>中的说明。如果你喜欢这篇文章的视频版本，你可以点击进入<a class="ae lh" href="https://www.youtube.com/watch?v=rK-9t1IS0A4&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi me"><img src="../Images/c42b839885b6fc53a632d5dad0d3e377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhxZTmT84xBXDHgVS3DBNQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">分析管道。图片作者。弗里皮克的图标。</p></figure><p id="6617" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有的指令、代码笔记本和结果也可以通过<a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times" rel="noopener ugc nofollow" target="_blank">GitHub上的</a><a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times" rel="noopener ugc nofollow" target="_blank"> my project repository </a>进行访问，以实现更流畅的复制。</p><h2 id="b016" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">通过归档API收集数据，并使用SpaCy和Gensim进行主题建模</h2><p id="89bf" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在我进一步进行分析之前，我决定对2019年1月至2020年9月期间《纽约时报》的大部分文章运行<strong class="lk jd">主题建模</strong>，以分析标题、关键词和前导段落。我的目标是区分最普遍的问题和持久的话题，以确保我的研究符合NYT的使命宣言，我没有歪曲他们的新闻风格。</p><p id="fdcc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这部分分析的数据收集蓝图受到了<a class="ae lh" href="https://towardsdatascience.com/@brienna" rel="noopener" target="_blank">布蕾娜·赫罗尔德</a>的<a class="ae lh" rel="noopener" target="_blank" href="/collecting-data-from-the-new-york-times-over-any-period-of-time-3e365504004">教程</a>的启发。</p><p id="a159" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们导入必要的工具和库:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="c532" class="mf mg it nd b gy nh ni l nj nk"><strong class="nd jd">import</strong> <strong class="nd jd">os</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">pandas</strong> <strong class="nd jd">as</strong> <strong class="nd jd">pd</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">requests</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">json</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">time</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">dateutil</strong><br/><strong class="nd jd">import</strong> <strong class="nd jd">datetime</strong><br/><strong class="nd jd">from</strong> <strong class="nd jd">dateutil.relativedelta</strong> <strong class="nd jd">import</strong> relativedelta<br/><strong class="nd jd">import</strong> <strong class="nd jd">glob</strong></span></pre><p id="2299" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">确定分析的时间框架:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="64d3" class="mf mg it nd b gy nh ni l nj nk">end = datetime.date.today()<br/>start = datetime.date(2019, 1, 1)<br/>print('Start date: ' + str(start))<br/>print('End date: ' + str(end))</span></pre><p id="51d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据按月分组:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="d560" class="mf mg it nd b gy nh ni l nj nk">months_in_range = [x.split(' ') <strong class="nd jd">for</strong> x <strong class="nd jd">in</strong> pd.date_range(start, end, freq='MS').strftime("%Y %-m").tolist()]</span></pre><p id="040b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面一组<strong class="lk jd">辅助函数</strong>(参见<a class="ae lh" rel="noopener" target="_blank" href="/collecting-data-from-the-new-york-times-over-any-period-of-time-3e365504004">教程</a>)通过API提取NYT数据，并保存到特定的csv文件中:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2777" class="mf mg it nd b gy nh ni l nj nk"><strong class="nd jd">def</strong> send_request(date):<br/>    <em class="nl">'''Sends a request to the NYT Archive API for given date.'''</em><br/>    base_url = 'https://api.nytimes.com/svc/archive/v1/'<br/>    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + 'F9FPP1mJjiX8pAEFAxBYBg08vZECa39n'<br/>    <strong class="nd jd">try</strong>:<br/>        response = requests.get(url, verify=<strong class="nd jd">False</strong>).json()<br/>    <strong class="nd jd">except</strong> <strong class="nd jd">Exception</strong>:<br/>        <strong class="nd jd">return</strong> <strong class="nd jd">None</strong><br/>    time.sleep(6)<br/>    <strong class="nd jd">return</strong> response<br/><br/><br/><strong class="nd jd">def</strong> is_valid(article, date):<br/>    <em class="nl">'''An article is only worth checking if it is in range, and has a headline.'''</em><br/>    is_in_range = date &gt; start <strong class="nd jd">and</strong> date &lt; end<br/>    has_headline = type(article['headline']) == dict <strong class="nd jd">and</strong> 'main' <strong class="nd jd">in</strong> article['headline'].keys()<br/>    <strong class="nd jd">return</strong> is_in_range <strong class="nd jd">and</strong> has_headline<br/><br/><br/><strong class="nd jd">def</strong> parse_response(response):<br/>    <em class="nl">'''Parses and returns response as pandas data frame.'''</em><br/>    data = {'headline': [],  <br/>        'date': [], <br/>        'doc_type': [],<br/>        'material_type': [],<br/>        'section': [],<br/>        'keywords': [],<br/>        'lead_paragraph': []}<br/>    <br/>    articles = response['response']['docs'] <br/>    <strong class="nd jd">for</strong> article <strong class="nd jd">in</strong> articles: <em class="nl"># For each article, make sure it falls within our date range</em><br/>        date = dateutil.parser.parse(article['pub_date']).date()<br/>        <strong class="nd jd">if</strong> is_valid(article, date):<br/>            data['date'].append(date)<br/>            data['headline'].append(article['headline']['main']) <br/>            <strong class="nd jd">if</strong> 'section' <strong class="nd jd">in</strong> article:<br/>                data['section'].append(article['section_name'])<br/>            <strong class="nd jd">else</strong>:<br/>                data['section'].append(<strong class="nd jd">None</strong>)<br/>            data['doc_type'].append(article['document_type'])<br/>            <strong class="nd jd">if</strong> 'type_of_material' <strong class="nd jd">in</strong> article: <br/>                data['material_type'].append(article['type_of_material'])<br/>            <strong class="nd jd">else</strong>:<br/>                data['material_type'].append(<strong class="nd jd">None</strong>)<br/>            keywords = [keyword['value'] <strong class="nd jd">for</strong> keyword <strong class="nd jd">in</strong> article['keywords'] <strong class="nd jd">if</strong> keyword['name'] == 'subject']<br/>            data['keywords'].append(keywords)<br/>            <strong class="nd jd">if</strong> 'lead_paragraph' <strong class="nd jd">in</strong> article:<br/>                data['lead_paragraph'].append(article['lead_paragraph'])<br/>            <strong class="nd jd">else</strong>:<br/>                data['lead_paragraph'].append(<strong class="nd jd">None</strong>)<br/>    <strong class="nd jd">return</strong> pd.DataFrame(data) <br/><br/><br/><strong class="nd jd">def</strong> get_data(dates):<br/>    <em class="nl">'''Sends and parses request/response to/from NYT Archive API for given dates.'''</em><br/>    total = 0<br/>    print('Date range: ' + str(dates[0]) + ' to ' + str(dates[-1]))<br/>    <strong class="nd jd">if</strong> <strong class="nd jd">not</strong> os.path.exists('headlines'):<br/>        os.mkdir('headlines')<br/>    <strong class="nd jd">for</strong> date <strong class="nd jd">in</strong> dates:<br/>        print('Working on ' + str(date) + '...')<br/>        csv_path = 'headlines/' + date[0] + '-' + date[1] + '.csv'<br/>        <strong class="nd jd">if</strong> <strong class="nd jd">not</strong> os.path.exists(csv_path): <em class="nl"># If we don't already have this month </em><br/>            response = send_request(date)<br/>            <strong class="nd jd">if</strong> response <strong class="nd jd">is</strong> <strong class="nd jd">not</strong> <strong class="nd jd">None</strong>:<br/>                df = parse_response(response)<br/>                total += len(df)<br/>                df.to_csv(csv_path, index=<strong class="nd jd">False</strong>)<br/>                print('Saving ' + csv_path + '...')<br/>    print('Number of articles collected: ' + str(total))</span></pre><p id="6e24" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们仔细看看助手函数:</p><ul class=""><li id="b8ba" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated"><strong class="lk jd"> send_request(date) </strong>在给定的<em class="nl">日期</em>将请求发送到存档中，转换成json格式，返回<em class="nl">响应。</em></li><li id="babd" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated"><strong class="lk jd"> is_valid(article，date) </strong>检查文章是否在请求的时间范围内，确认标题的存在，并返回<em class="nl"> is_in_range </em>和<em class="nl"> has_headline </em>结论。</li><li id="f09d" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated"><strong class="lk jd">parse _ response(response)</strong>将响应转换成数据帧。<em class="nl"> data </em>是一个字典，它包含我们的数据帧的列，这些列最初是空的，但是会被这个函数追加。该函数返回最终的<em class="nl">数据帧</em>。</li><li id="ad8e" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated"><strong class="lk jd"> get_data(dates) </strong>，其中<em class="nl">date</em>对应于用户指定的范围，利用<em class="nl"> send_request() </em>和<em class="nl"> parse_response() </em>函数。将<em class="nl">标题</em>和其他信息保存到。csv文件，范围内每年每月一个文件。</li></ul><p id="191f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦我们获得了该范围内每年的每月csv文件，我们就可以将它们连接起来供将来使用。glob库是一个很好的工具。确保到<em class="nl"> headlines </em>文件夹的路径与代码中的路径匹配。我使用了相对路径，而不是我的绝对路径。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="b0fe" class="mf mg it nd b gy nh ni l nj nk"><em class="nl"># get data file names</em><br/>path = "headlines/"<br/>filenames = glob.glob("*.csv")<br/><br/>dfs = []<br/>print(filenames)<br/><strong class="nd jd">for</strong> filename <strong class="nd jd">in</strong> filenames:<br/>    dfs.append(pd.read_csv(filename))<br/><br/><em class="nl"># Concatenate all data into one DataFrame</em><br/>big_frame = pd.concat(dfs, ignore_index=<strong class="nd jd">True</strong>)</span></pre><p id="2317" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nl"> big_frame </em>是一个数据帧，它将标题文件夹中的所有文件连接成一个帧。这是预期的输出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/716cc51031e3a223ac361e38d45dc1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkr_E-7K31h34YZqcWFXxQ.png"/></div></div></figure><p id="1553" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">拉了135954篇文章及其数据。</p><p id="4f8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们为<strong class="lk jd">主题建模</strong>做好准备。以下分析的目的是对过去一年半《纽约时报》文章的标题、关键词和前导段落进行主题建模。我想确保标题与介绍性段落和关键词一致。</p><p id="6319" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">导入工具和库:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="5224" class="mf mg it nd b gy nh ni l nj nk"><strong class="nd jd">from</strong> <strong class="nd jd">collections</strong> <strong class="nd jd">import</strong> defaultdict  <br/><strong class="nd jd">import</strong> <strong class="nd jd">re</strong>, <strong class="nd jd">string   #regular expressions</strong><br/><strong class="nd jd">from</strong> <strong class="nd jd">gensim</strong> <strong class="nd jd">import</strong> corpora # this is the topic modeling library<br/><strong class="nd jd">from</strong> <strong class="nd jd">gensim.models</strong> <strong class="nd jd">import</strong> LdaModel</span></pre><p id="809b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们仔细看看:</p><ul class=""><li id="7c50" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated"><a class="ae lh" href="https://www.geeksforgeeks.org/defaultdict-in-python/" rel="noopener ugc nofollow" target="_blank"> <em class="nl"> defaultdict </em> </a>用于统计唯一单词及其出现的次数。</li><li id="248d" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated"><em class="nl"> re </em>和<em class="nl"> string </em>在我们寻找文本中的匹配时很有用，无论是完整的还是模糊的。如果你对文本分析感兴趣，正则表达式会经常出现；这里有一个方便的<a class="ae lh" href="https://regex101.com/" rel="noopener ugc nofollow" target="_blank">工具</a>来练习这些。</li><li id="95a5" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated"><a class="ae lh" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> <em class="nl"> gensim </em> </a>是我们要用来做主题建模的库。一旦你把必要的<a class="ae lh" href="https://www.tutorialspoint.com/gensim/gensim_getting_started.htm" rel="noopener ugc nofollow" target="_blank">依赖关系</a>整理出来，它就是用户友好的。</li></ul><p id="ea03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为我们正在查看数据帧的三个不同列，所以将实例化语料库的三个不同实例:保存标题的语料库、关键字的语料库和引导段落的语料库。这意味着是一个健全的检查，以确保标题和关键字和引导段落与文章的内容一致。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="9d0a" class="mf mg it nd b gy nh ni l nj nk">big_frame_corpus_headline = big_frame['headline']<br/>big_frame_corpus_keywords = big_frame['keywords']<br/>big_frame_corpus_lead = big_frame['lead_paragraph']</span></pre><p id="dbf8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了使文本数据可用，需要对其进行预处理。一般来说，它看起来像这样:小写和标点符号删除，词干，词汇化和标记化，然后停用词删除和矢量化。前四个操作显示为一个群集，因为这些操作的顺序通常取决于数据，在某些情况下，交换操作的顺序可能是有意义的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/07af4fb160640f568408a454ea03a4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrN4g8pxgm_RXQTbL_f3Rg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">文本预处理步骤。图片作者。Freepik的图标</p></figure><p id="6204" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">先说预处理。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="a1fb" class="mf mg it nd b gy nh ni l nj nk"><strong class="nd jd">from</strong> <strong class="nd jd">nltk.corpus</strong> <strong class="nd jd">import</strong> stopwords</span><span id="79ef" class="mf mg it nd b gy oc ni l nj nk">headlines = [re.sub(r'[^\w\s]','',str(item)) <strong class="nd jd">for</strong> item <strong class="nd jd">in</strong> big_frame_corpus_headline]</span><span id="8841" class="mf mg it nd b gy oc ni l nj nk">keywords = [re.sub(r'[^\w\s]','',str(item)) <strong class="nd jd">for</strong> item <strong class="nd jd">in</strong> big_frame_corpus_keywords]</span><span id="d33a" class="mf mg it nd b gy oc ni l nj nk">lead = [re.sub(r'[^\w\s]','',str(item)) <strong class="nd jd">for</strong> item <strong class="nd jd">in</strong> big_frame_corpus_lead]</span><span id="8adb" class="mf mg it nd b gy oc ni l nj nk">stopwords = set(stopwords.words('english')) <br/># please note: you can append to this list of pre-defined stopwords if needed</span></pre><p id="cb8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">更多预处理:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2a8b" class="mf mg it nd b gy nh ni l nj nk">headline_texts = [[word <strong class="nd jd">for</strong> word <strong class="nd jd">in</strong> document.lower().split() <strong class="nd jd">if</strong> word <strong class="nd jd">not</strong> <strong class="nd jd">in</strong> stopwords] <strong class="nd jd">for</strong> document <strong class="nd jd">in</strong> headlines]</span><span id="7444" class="mf mg it nd b gy oc ni l nj nk">keywords_texts = [[word for word in document.lower().split() if word not in stopwords] for document in keywords]</span><span id="c738" class="mf mg it nd b gy oc ni l nj nk">lead_texts = [[word for word in document.lower().split() if word not in stopwords] for document in lead]</span></pre><p id="0059" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">删除不常用的单词:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8716" class="mf mg it nd b gy nh ni l nj nk">frequency = defaultdict(int)<br/>for headline_text in headline_texts:<br/>    for token in headline_text:<br/>         frequency[token] += 1<br/>for keywords_text in keywords_texts:<br/>    for token in keywords_text:<br/>         frequency[token] += 1<br/>for lead_text in lead_texts:<br/>    for token in lead_text:<br/>         frequency[token] += 1<br/>            <br/>headline_texts = [[token for token in headline_text if frequency[token] &gt; 1] for headline_text in headline_texts]<br/>keywords_texts = [[token for token in keywords_text if frequency[token] &gt; 1] for keywords_text in keywords_texts]<br/>lead_texts = [[token for token in lead_text if frequency[token] &gt; 1] for lead_text in lead_texts]</span><span id="fba6" class="mf mg it nd b gy oc ni l nj nk">dictionary_headline = corpora.Dictionary(headline_texts)<br/>dictionary_keywords = corpora.Dictionary(keywords_texts)<br/>dictionary_lead = corpora.Dictionary(lead_texts)</span><span id="c29d" class="mf mg it nd b gy oc ni l nj nk">headline_corpus = [dictionary.doc2bow(headline_text) for headline_text in headline_texts]<br/>keywords_corpus = [dictionary.doc2bow(keywords_text) for keywords_text in keywords_texts]<br/>lead_corpus = [dictionary.doc2bow(lead_text) for lead_text in lead_texts]</span></pre><p id="94fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们决定我们案例的最佳主题数量:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="c022" class="mf mg it nd b gy nh ni l nj nk">NUM_TOPICS = 5  <br/>ldamodel_headlines = LdaModel(headline_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=12)<br/>ldamodel_keywords = LdaModel(keywords_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=12)<br/>ldamodel_lead = LdaModel(lead_corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=12)</span></pre><p id="5831" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果如下:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="3112" class="mf mg it nd b gy nh ni l nj nk">topics_headlines = ldamodel_headlines.show_topics()<br/>for topic_headlines in topics_headlines:<br/>    print(topic_headlines)</span><span id="ffb1" class="mf mg it nd b gy oc ni l nj nk">topics_keywords = ldamodel_keywords.show_topics()<br/>for topic_keywords in topics_keywords:<br/>    print(topic_keywords)</span><span id="81c2" class="mf mg it nd b gy oc ni l nj nk">topics_lead = ldamodel_lead.show_topics()<br/>for topic_lead in topics_lead:<br/>    print(topic_lead)</span></pre><p id="7952" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们将它们组织成数据帧:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="f361" class="mf mg it nd b gy nh ni l nj nk">word_dict_headlines = {};</span><span id="c352" class="mf mg it nd b gy oc ni l nj nk">for i in range(NUM_TOPICS):<br/>    words_headlines = ldamodel_headlines.show_topic(i, topn = 20)<br/>    word_dict_headlines['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words_headlines]<br/>pd.DataFrame(word_dict_headlines)</span><span id="846f" class="mf mg it nd b gy oc ni l nj nk">for i in range(NUM_TOPICS):<br/>    words_keywords = ldamodel_keywords.show_topic(i, topn = 20)<br/>    word_dict_keywords['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words_keywords]<br/>pd.DataFrame(word_dict_keywords)</span><span id="dec5" class="mf mg it nd b gy oc ni l nj nk">for i in range(NUM_TOPICS):<br/>    words_lead  = ldamodel_lead.show_topic(i, topn = 20)<br/>    word_dict_lead ['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words_lead]<br/>pd.DataFrame(word_dict_lead)</span></pre><p id="57b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请记住:即使算法可以将单词分类到相应的主题中，仍然要靠人类来解释和标记它们。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/c5b0de41b89819dede9226a5c42a90f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5P4jR14frykoHZAVNHJ-9Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">话题建模结果。图片作者。弗里皮克的图标。</p></figure><p id="ef6c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">出现了各种各样的话题。它们在我们的社会中都是非常严肃和重要的。在这项特殊的研究中，我们将调查性别表征。</p><h2 id="37d0" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">1950年至今:数据收集和关键词分析。</h2><p id="4ce5" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们将使用前面提到的助手函数来获取从1950年1月1日到现在(即2020年9月)的数据。我建议使用较小的时间增量，例如十年，以防止API超时。</p><p id="30c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据将被收集到<em class="nl"> headlines.csv </em>中，然后使用上述方法连接成一个数据帧。一旦你得到了你辛辛苦苦得到的数据框架，我建议把它腌制一下以备后用:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="318c" class="mf mg it nd b gy nh ni l nj nk">import pickle<br/>with open('frame_all.pickle', 'wb') as to_write:<br/>    pickle.dump(frame, to_write)</span></pre><p id="9aa5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是您提取腌制文件的方法:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8319" class="mf mg it nd b gy nh ni l nj nk">with open('frame_all.pickle', 'rb') as read_file:<br/>    df = pickle.load(read_file)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/60872fead914f8c0d23004b2c0daeee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4IpdBCVsxf5uWxO6w-3AQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">发现的文章总数与70年时间范围内的相关文章。图片作者。Slidesgo模板。</p></figure><p id="8109" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们将日期列转换成<em class="nl">日期时间</em>格式，这样就可以按时间顺序对文章进行排序。我们也将删除空值和重复。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="9d9b" class="mf mg it nd b gy nh ni l nj nk">df['date'] = pd.to_datetime(df['date'])</span><span id="b2d6" class="mf mg it nd b gy oc ni l nj nk">df = df[df['headline'].notna()].drop_duplicates().sort_values(by='date')</span><span id="70af" class="mf mg it nd b gy oc ni l nj nk">df.dropna(axis=0, subset=['keywords'], inplace = True)</span></pre><p id="2dee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看<strong class="lk jd">相关关键词</strong>:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="98f3" class="mf mg it nd b gy nh ni l nj nk">import ast<br/>df.keywords = df.keywords.astype(str).str.lower().transform(ast.literal_eval)</span><span id="f86b" class="mf mg it nd b gy oc ni l nj nk">keyword_counts = pd.Series(x for l in df['keywords'] for x in l).value_counts(ascending=False)</span><span id="df0b" class="mf mg it nd b gy oc ni l nj nk">len(keyword_counts)</span></pre><p id="0ff0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">58298</strong>唯一关键字。</p><p id="083e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我用自己的个人判断来确定哪些关键词与强势女性及其代表性的话题相关:政治、社会活动、企业家精神、科学、技术、军事成就、体育突破和女性领导力。这种分析绝不意味着将任何群体或个人排除在女强人的概念之外。我乐于接受补充和建议，所以请不要犹豫，如果你认为有什么事情可以做，使这个项目更加全面。快速提醒如果您发现单元格中的代码由于格式问题难以复制，请参考我的项目<a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times" rel="noopener ugc nofollow" target="_blank">资源库</a>中的代码和说明。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="6d8c" class="mf mg it nd b gy nh ni l nj nk">project_keywords1 = [x for x in keyword_counts.keys() if 'women in politics' in x <br/>                 or 'businesswoman' in x  <br/>                 or 'female executive' in x <br/>                 or 'female leader' in x <br/>                 or 'female leadership' in x <br/>                 or 'successful woman' in x <br/>                 or 'female entrepreneur' in x<br/>                 or 'woman entrepreneur' in x <br/>                 or 'women in tech' in x <br/>                 or 'female technology' in x <br/>                 or 'female startup' in x <br/>                 or 'female founder' in x ]</span></pre><p id="fce0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面是一个相关关键字的示例查询。关于相关关键词搜索和文章标题提取的更详细说明可以在<a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times/blob/master/project5_data_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">本笔记本</a>中找到。</p><p id="aee7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们来看看与从政女性有关的头条新闻。</p><p id="d460" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们通过小写来规范化它们:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="c263" class="mf mg it nd b gy nh ni l nj nk">df['headline'] = df['headline'].astype(str).str.lower()</span></pre><p id="acf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">检查包含诸如女性、政治和权力等词汇的标题:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e3a6" class="mf mg it nd b gy nh ni l nj nk">wip_headlines = df[df['headline'].str.contains(('women' or 'woman' or 'female')) &amp; df['headline'].str.contains(('politics' or 'power' or 'election'))]</span></pre><p id="dd5d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">‘WIP’代表‘从政的妇女’。</p><p id="1dcd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的搜索只返回了185个标题。让我们看看关键字来补充这一点。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="b67e" class="mf mg it nd b gy nh ni l nj nk">df['keywords'].dropna()<br/>df['keywords_joined'] = df.keywords.apply(', '.join)<br/>df['keywords_joined'] = df['keywords_joined'].astype(str)<br/>import re<br/>wip_keywords = df[df['keywords_joined'].str.contains(r'(?=.*women)(?=.*politics)',regex=True)]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/644b20f5fbb59b678c81e42fe863d242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9OxqdNsw2AEr27IxlDeHRQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">妇女参政:结果数据框架</p></figure><p id="b4a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的数据框架包含了基于相关关键词的2579篇文章。我们将对关键字和标题数据帧执行外部连接，以获得更全面的数据:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="989a" class="mf mg it nd b gy nh ni l nj nk">wip_df = pd.concat([wip_headlines, wip_keywords], axis=0, sort = True)</span></pre><p id="5e68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">利用同样的技术，我们将能够获得更多关于女性在军事、科学、体育、创业和其他领域成就的数据。例如，如果我们要寻找关于女权主义的文章:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8f73" class="mf mg it nd b gy nh ni l nj nk">feminist_keywords = df[df['keywords_joined'].str.contains(r'(?=.*women)(?=.*feminist)',regex=True)]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/d37dec1682982e803b52e965602a1d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3L5f4MNKERyCaYX--CM6RA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">基于关键词搜索的文章:女权主义</p></figure><p id="0778" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">#metoo机芯:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="db3d" class="mf mg it nd b gy nh ni l nj nk">metoo_keywords = df[df['keywords_joined'].str.contains(r'(?=.*women)(?=.*metoo)(?=.*movement)',regex=True)]</span></pre><p id="1f44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正则表达式和模糊匹配允许几乎无限的可能性。您可以在本笔记本中查看更多查询。</p><p id="59f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有查询完成后，最终的数据框架将在GitHub和本文的代码笔记中被进一步称为<em class="nl"> project_df </em>。</p><p id="af77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们来看一下<strong class="lk jd">历年文章分布</strong>:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="ba74" class="mf mg it nd b gy nh ni l nj nk">ax = df.groupby(df.date.dt.year['headline'].count().plot(kind='bar', figsize=(20, 6))<br/>ax.set(xlabel='Year', ylabel='Number of Articles')<br/>ax.yaxis.set_tick_params(labelsize='large')<br/>ax.xaxis.label.set_size(18)<br/>ax.yaxis.label.set_size(18)<br/>ax.set_title('Total Published Every Year', fontdict={'fontsize': 24, 'fontweight': 'medium'})<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/3f044905a04370f1c208bdaf2f170a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdN52DzJ-eHaXvCBurHpDw.png"/></div></div></figure><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2c14" class="mf mg it nd b gy nh ni l nj nk">ax = project_df.groupby('year')['headline'].count().plot(kind='bar', figsize=(20, 6))<br/>ax.set(xlabel='Year', ylabel='Number of Articles')<br/>ax.yaxis.set_tick_params(labelsize='large')<br/>ax.xaxis.label.set_size(18)<br/>ax.yaxis.label.set_size(18)<br/>ax.set_title('Articles About Strong Women (based on relevant keywords) Published Every Year', \<br/>             fontdict={'fontsize': 20, 'fontweight': 'medium'})<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/a1255f1c24d59a35f00921bd9a68d3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Tmpy2cGgxlBo5I2CPSW0Q.png"/></div></div></figure><p id="7fc9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们把这两张图叠加起来，蓝色的几乎消失了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/fd55f857674d9d35679b0da23791646e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YytZMjbH9prQNrnp_1XHUA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">与随着时间的推移而发表的大量文章相比，基于关键词和标题的相关出版物几乎是不可见的。</p></figure><p id="38e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对妇女问题的报道似乎不多。我认为这可能是由于这些关键字并不总是正确编码的事实:有些不是丢失了就是误导了，因此使得研究人员更难通过Archive API找到想要的资料。</p><p id="eda2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过我的分析，我有了一个<strong class="lk jd">有趣的发现</strong>。根据对n-grams的分析，在20世纪50年代初，多次提到妇女的职业机会。他们中的许多人从大学毕业成为医生，以便加入海军。我把这种宣传的激增归因于第二次世界大战的后果:妇女被鼓励加入劳动大军，以补充军队的努力。还记得铆钉工罗西的海报吗？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/b898fb97a15cab6eabc74b6dd71d39e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAa874vKpUcH913RXcyA9w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">这些剪报是通过NYT出版物档案馆获得的。作者利用这些剪报创作了这幅图像。</p></figure><p id="133b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管在那个没有太多机会向女性敞开的时代，看到女性可以获得这样的机会令人温暖和振奋，但我真的希望这不是因为战争。</p><h2 id="7920" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">N-grams，WordCloud和情感分析。</h2><p id="3f14" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">要了解标题中的总体术语频率:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8606" class="mf mg it nd b gy nh ni l nj nk">from sklearn.feature_extraction.text import CountVectorizer</span><span id="5ae0" class="mf mg it nd b gy oc ni l nj nk">word_vectorizer = CountVectorizer(ngram_range=(1,3), analyzer='word')<br/>sparse_matrix = word_vectorizer.fit_transform(corpus)<br/>frequencies = sum(sparse_matrix).toarray()[0]<br/>ngram_df_project = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])</span><span id="d839" class="mf mg it nd b gy oc ni l nj nk">from wordcloud import WordCloud, STOPWORDS<br/>all_headlines = ' '.join(project_df['headline'].str.lower())</span><span id="ed31" class="mf mg it nd b gy oc ni l nj nk">stopwords = STOPWORDS<br/>stopwords.add('will')<br/># Note: you can append your own stopwords to the existing ones.</span><span id="6012" class="mf mg it nd b gy oc ni l nj nk">wordcloud = WordCloud(stopwords=stopwords, background_color="white", max_words=1000, width = 480, height = 480).\<br/>generate(all_headlines)</span><span id="d324" class="mf mg it nd b gy oc ni l nj nk">plt.figure(figsize=(20,10))<br/>plt.imshow(wordcloud)<br/>plt.axis("off");</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/e3f2e9a7cb02e41747c88ae0f483770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_wc0zAxIUqgXflkwEJu3tw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">上面的代码创建的WordCloud:最常用的术语以更大的字体显示。</p></figure><p id="b4bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还可以根据各种时间框架或特定关键字等特征创建单词云。参考<a class="ae lh" href="https://github.com/sasha-talks-tech/New-York-Times/blob/master/project5_data_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>获取更多视觉效果。</p><p id="92fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">先说<strong class="lk jd">情绪分析</strong>。我们将使用<a class="ae lh" href="https://www.nltk.org/_modules/nltk/sentiment/vader.html" rel="noopener ugc nofollow" target="_blank"> NLTK的Vader库</a>来分析与标题相关联的<strong class="lk jd">情感</strong> <strong class="lk jd">。我们真的能在写文章的时候了解记者们对某个问题的感受吗？</strong></p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="32b2" class="mf mg it nd b gy nh ni l nj nk">import nltk <br/>nltk.download('vader_lexicon')</span><span id="37a4" class="mf mg it nd b gy oc ni l nj nk">from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA</span><span id="b39d" class="mf mg it nd b gy oc ni l nj nk">sia = SIA()<br/>results = []</span><span id="f24b" class="mf mg it nd b gy oc ni l nj nk">for line in project_df.headline:<br/>    pol_score = sia.polarity_scores(line)<br/>    pol_score['headline'] = line<br/>    results.append(pol_score)</span><span id="53c7" class="mf mg it nd b gy oc ni l nj nk">print(results[:3])</span></pre><p id="0b8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输出:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="bddc" class="mf mg it nd b gy nh ni l nj nk">[{'neg': 0.0, 'neu': 0.845, 'pos': 0.155, 'compound': 0.296, 'headline': 'women doctors join navy; seventeen end their training and are ordered to duty'}, {'neg': 0.18, 'neu': 0.691, 'pos': 0.129, 'compound': -0.2732, 'headline': 'n.y.u. to graduate 21 women doctors; war gave them, as others, an opportunity to enter a medical school'}, {'neg': 0.159, 'neu': 0.725, 'pos': 0.116, 'compound': -0.1531, 'headline': 'greets women doctors; dean says new york medical college has no curbs'}]</span></pre><p id="423c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为数据框架的情感:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="9d8a" class="mf mg it nd b gy nh ni l nj nk">sentiment_df = pd.DataFrame.from_records(results)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/50d9042a27b3658d6ad01ad38250dda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q79NYHuZOQTMo8GandCpDw.png"/></div></div></figure><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="fac0" class="mf mg it nd b gy nh ni l nj nk">dates = project_df['year']<br/>sentiment_df = pd.merge(sentiment_df, dates, left_index=True, right_index=True)</span></pre><p id="fc99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的代码允许我们为自己的情绪设定一个时间表。为了简化情感分析，我们将为积极、消极和中立创建一些新的类别。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="455c" class="mf mg it nd b gy nh ni l nj nk">sentiment_df['label'] = 0<br/>sentiment_df.loc[sentiment_df['compound'] &gt; 0.2, 'label'] = 1<br/>sentiment_df.loc[sentiment_df['compound'] &lt; -0.2, 'label'] = -1<br/>sentiment_df.head()</span></pre><p id="ca09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要可视化整体情绪分布:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="dcf2" class="mf mg it nd b gy nh ni l nj nk">sentiment_df.label.value_counts(normalize=True) * 100</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/8f9af3131fd2f0988e80bd6b5d87a2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7HMB6KDVVwiXJRtInPmng.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片作者。Slidesgo模板。</p></figure><p id="f338" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要可视化情绪随时间的变化:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="c8f3" class="mf mg it nd b gy nh ni l nj nk">sns.lineplot(x="year", y="label", data=sentiment_df) <br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/62a5b5c2bb42272b32d52a4e7e046fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*co_Nr_egecA-m6rza6Yd4w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由于问题的复杂性，情绪波动很大</p></figure><p id="61e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如你所见，情绪波动。这一点都不意外，因为女性问题通常包含沉重的主题，如暴力和虐待。在这些情况下，我们预计市场情绪会偏向负面。</p><p id="8625" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我创建了一个<a class="ae lh" href="https://public.tableau.com/profile/alexandra.prokhorova#!/vizhome/NYT_16003912659750/Dashboard1" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Tableau仪表板</strong> </a>，观众可以在那里与可视化互动。可通过<a class="ae lh" href="https://public.tableau.com/profile/alexandra.prokhorova#!/" rel="noopener ugc nofollow" target="_blank">我的Tableau公众号</a>获取。这个仪表板展示了几十年来关键词的分布情况。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/057d98e299ad9ee6741943a4973aaf6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZU62swvVSnQknZS3Nl2ClQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片作者。</p></figure><h2 id="10fc" class="mf mg it bd mh mi mj dn mk ml mm dp mn lr mo mp mq lv mr ms mt lz mu mv mw iz bi translated">结论</h2><p id="c822" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">多年来,《纽约时报》在性别平等代表性方面取得了显著进步。如果让我提一个建议，我会推荐增加关键词列表。当我们进一步探究存档API的过去时，更全面、更强大的关键字会有助于搜索。</p><p id="987a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">重要的是继续展示<em class="nl">女性领导力</em>，直到它变成<em class="nl">正义领导力</em>。想象一下这个世界，不再需要形容词“女性”来描述成就，因为它变得多余了。想象一下这个世界，没有“女医生”或“女工程师”:只有医生和工程师。创始人和政治家。企业家、科学家和开拓者。作为一个社团，我们的目标是为不同群体持有的这些头衔建立一个坚实的心智模型。通过不断提醒我们自己和我们周围的社会，任何性别或国籍都不能被排除在这些机会之外，我们可以共同实现这一目标。</p></div></div>    
</body>
</html>