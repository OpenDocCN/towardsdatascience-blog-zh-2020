<html>
<head>
<title>All the ways to initialize your neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初始化你的神经网络的所有方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52?source=collection_archive---------17-----------------------#2020-10-30">https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52?source=collection_archive---------17-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="df3a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本文中，我评估了权重初始化的许多方法和当前的最佳实践。</h2></div><h1 id="5a18" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">零初始化</h1><p id="c553" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">将权重初始化为零不起作用。</strong>那为什么我在这里提到了？为了理解权重初始化的需要，我们需要理解为什么将权重初始化为零<strong class="kz ir">不会</strong>工作。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/b4056b168f51ddd9318237c0f9518a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*C-Q_WOX7y58CplVET_QVcg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图一。简单的网络。图片由作者提供。</p></figure><p id="ed51" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">让我们考虑一个如上所示的简单网络。每个输入只是一个定标器X₁，X₂，X₃.每个神经元的权重是W₁和W₂.每次重量更新如下:</p><p id="c6cb" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">out₁=x₁*w₁+x₂*w₁+x₃*w₁<br/>out₂=x₁*w₂+x₂*w₂+x₃*w₂</p><p id="39dd" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">正如你现在看到的，如果权重矩阵W = [W₁ W₂]被初始化为零，那么out1和out2完全相同。</p><p id="b7e2" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">即使我们向两者添加非零随机偏置项，权重也被更新为非零，然而，它们将保持相同，因此隐藏单元的两个神经元正在计算相同的东西。换句话说，它们是<strong class="kz ir"> <em class="mk">对称</em> </strong>。</p><p id="62d5" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">这是非常不希望的，因为这是浪费计算。这就是零初始化不起作用的原因。</p><h1 id="700a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">随机初始化</h1><p id="7879" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在我们知道权重必须不同，下一个想法是随机初始化这些权重。随机初始化比零初始化好很多，但是这些随机数可以是<strong class="kz ir"> <em class="mk">任意</em> </strong>数吗？</p><p id="cd2b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">让我们假设您使用的是sigmoid非线性。函数如下图所示。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/8dda7e706b625ebdc56ece92006aa426.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*tRUSpGBEXsFh8nwE8qgJuQ.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">乙状结肠。图片由作者提供。</p></figure><p id="9df5" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们可以看到，即使对于大到6的值，sigmoid的值也几乎是1，而对于小到-6的值，sigmoid的值是0。这意味着如果我们的权重矩阵被初始化为太大或太小的值，所有有用的信息都会在sigmoid函数中丢失。</p><p id="efb6" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">如果我们使用ReLu非线性，这并不重要，但是当权重被初始化为大或小的值时，会有其他问题。有更好的方法来初始化我们的权重。</p><h1 id="b78a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">Xavier初始化</h1><p id="92d8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae mm" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> Xavier初始化</a>由Xavier Glorot和Yoshua Bengio在2010年提出。本文的主要目的是初始化权重，使激活的平均值为零，标准差为1。考虑如下所示计算的函数。</p><blockquote class="mn"><p id="a5cb" class="mo mp iq bd mq mr ms mt mu mv mw ls dk translated">Z = WX + b</p></blockquote><p id="6758" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">这里W是权重矩阵，X是来自前一层的输入，b是偏差。z是由也称为激活的层计算的输出。我们希望Z的平均值为0，标准差为1。(从技术上讲，Z是类似ReLu的非线性激活后的结果)</p><p id="6612" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">为什么均值为零和标准差为1很重要？<br/>考虑一个有100层的深度神经网络。在每一步，权重矩阵与来自前一层的激活相乘。如果每一层的激活数大于1，当它们重复相乘100次时，它们将继续变大，并且<strong class="kz ir">将</strong>爆炸到无穷大。类似地，如果激活少于一次，它们将<strong class="kz ir">消失</strong>到零。这就是所谓的<strong class="kz ir"> <em class="mk">爆炸和消失渐变</em> </strong>问题。我们可以在下图中看到这一点。稍大于1的值会爆炸成非常大的数字，稍小于1的值会消失为零。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/b3843810f6c738bf2808404cb9ea630c.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*rQnzf2cZsRIwX6gPusQhxg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">图片由作者提供。</p></figure><p id="750f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">为了避免渐变和激活的爆炸和消失，我们希望激活的平均值为0，标准差为1。我们可以通过精心选择砝码来实现这一目标。</p><p id="9b99" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">在本文发布期间，权重的最佳实践是从[-1，1]的<a class="ae mm" href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> <em class="mk">均匀分布</em> </strong> </a>中随机选择，然后除以输入维度的平方根。事实证明，这不是一个好主意，梯度消失，训练非常缓慢，如果可能的话。</p><p id="3138" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">Xavier初始化解决了这个问题，他建议我们从均匀分布中随机初始化权重，如下所示。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/227ffb1071df41c4daa1251f661324ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*Yd4GwOVX1c2Y-6hDZdOZFQ.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">泽维尔初始化均匀分布。图片由作者提供。</p></figure><p id="75c2" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">如今，Xavier初始化是通过从标准正态分布<a class="ae mm" href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"><em class="mk"/></strong></a>中选择权重来完成的，并且每个元素被除以输入维度大小的平方根。在PyTorch中，代码如下。</p><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="8bdc" class="nj kg iq nf b gy nk nl l nm nn">torch.randn(n_inp, n_out)*math.sqrt(1/n_inp)</span></pre><p id="80dc" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">Xavier初始化非常适合sigmoid和Tanh等对称非线性。然而，对于现在最流行的非线性项ReLu来说，它就不那么适用了。</p><h1 id="e95d" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">明凯初始化</h1><p id="82a7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">何等人在2015年写了一篇名为《深入研究<a class="ae mm" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mk">整流器:在ImageNet分类</em> </a> <em class="mk"> </em>上超越人类水平的性能》的论文，他们在论文中介绍了现在广为人知的明凯Init。</p><p id="4d82" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">但是我们为什么需要明凯呢？Xavier Init在ReLu非线性方面有什么问题？</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/491a3a4028d829370c1ded7ccac3d59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*5LBPVw7XiVrM6YDXqqNkvg.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">雷鲁。图片由作者提供。</p></figure><p id="5f2c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">从上图可以看出，对于所有X &lt;0 and Y=X for all X&gt; 0，ReLu给出0。ReLu没有很好地定义为0，但是大多数现代程序赋予它一个接近于0的近似值，就像<a class="ae mm" href="https://en.wikipedia.org/wiki/Machine_epsilon#:~:text=Formal%20definition,-Rounding%20is%20a&amp;text=For%20a%20number%20system%20and,of%20the%20chosen%20rounding%20procedure.&amp;text=%2C%20so%20machine%20epsilon%20also%20is,rounding%20to%20the%20unit%20value%22." rel="noopener ugc nofollow" target="_blank">机器ε</a>。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/b8d948a87c95cca166575a34308f2445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_CmkCZTworAcAqioMTtNhg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">左图:平均值为0、标准差为1的正态分布。右图:经过ReLu后的正态分布。图片由作者提供。</p></figure><p id="601c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">上面我们可以看到两个散点图，左边是ReLu之前的数据，右边是ReLu之后的数据。从图像中可以清楚地看到，在ReLu之后，方差几乎是一半，平均值略高。这改变了激活，方差减半，所以我们需要将方差加倍，以获得Xavier Init的原始效果。因此，我们将权重乘以额外的<br/> <strong class="kz ir"> √2 </strong>。所以在PyTorch中，明凯Init如下所示。</p><pre class="lu lv lw lx gt ne nf ng nh aw ni bi"><span id="a834" class="nj kg iq nf b gy nk nl l nm nn">torch.randn(n_inp, n_out)*math.sqrt(2/n_inp)</span></pre><blockquote class="nt nu nv"><p id="4a2e" class="kx ky mk kz b la mf jr lc ld mg ju lf nw mh li lj nx mi lm ln ny mj lq lr ls ij bi translated">如果你仍然感到困惑:<br/>方差=(标准差)<br/>因此，如果你想将方差加倍，你可以将数据乘以<strong class="kz ir"> √2 </strong></p></blockquote><h1 id="98f0" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">修复初始化</h1><p id="4fbd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae mm" href="https://arxiv.org/abs/1901.09321" rel="noopener ugc nofollow" target="_blank"> Fixup </a>是张等人在2019年提出的一种初始化。他们的观察是，明凯初始化和其他标准初始化对于具有剩余分支的网络(又名<a class="ae mm" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">剩余网络</a>)来说效果不佳。他们发现使用标准初始化<strong class="kz ir">的残差网络仅在使用</strong> <a class="ae mm" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir">批处理</strong> </a>时工作良好。</p><p id="5f13" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">让我们看看为什么明凯Init在剩余网络上不起作用。考虑下图所示的跳过连接。<strong class="kz ir">X2 = f(X1)</strong><strong class="kz ir">X3 = f(X2)+X1</strong>。我们知道，明凯初始化选择权重，使得每层之后的激活具有0均值和1方差。所以我们知道X1的方差为1，X2的方差为1。但是明凯初始化不考虑跳过连接。因此，根据总方差定律，X3的方差是双倍的。剩余分支增加的额外差异在明凯初始化中没有考虑。因此，剩余网络不能很好地与标准init一起工作，除非它们有BatchNorm。如果没有BatchNorm，输出方差会随深度呈指数增长。</p><blockquote class="mn"><p id="6080" class="mo mp iq bd mq mr ms mt mu mv mw ls dk translated">Var[Xₗ₊₁] ≈ 2Var[Xₗ ]</p></blockquote><figure class="oa ob oc od oe ly gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nz"><img src="../Images/12bb7f01161a006d9675a2db7bc4b490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oCKA9vEqldBDjZSlgkvQA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">跳过剩余网络中的连接。图片由作者提供。</p></figure><p id="8857" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">论文中的作者进行了重要的观察，即<a class="ae mm" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> SGD </a>对每个剩余分支的权重的更新以高度相关的方向更新了网络输出。这意味着，如果剩余分支权重全部由X更新，网络输出也在权重更新的相同方向上与X成比例地变化。</p><p id="cc60" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">作者将所需的网络输出变化定义为θ(η)。如我们所知，平均而言，每个剩余分支更新对输出更新的贡献是相等的，如果我们将剩余分支的数量称为L，则平均而言，每个剩余分支应该将输出改变θ(η/L ),以实现输出的总改变θ(η)。</p><p id="b5d3" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">接下来，作者展示了他们如何初始化m层的剩余分支，以便它的SGD更新将输出改变θ(η/L)。作者表明，这可以通过以下方式重新调整这些权重层的标准初始值来实现:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1051dd8c9b80e40478eb9b1c435f003e.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*e-cD-sDO4b1qNge6gfq20g.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">重量比例因子。图片由作者提供。</p></figure><p id="a197" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">作者还讨论了偏差和乘数的效用。他们发现，在每次卷积之前添加一个初始化为0的偏置层，线性层和逐元素激活可以显著改善训练。他们发现，在每个剩余分支上增加一个乘法定标器，有助于模拟标准化网络的权重范数动态。</p><p id="c69f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">总结一下修正:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi og"><img src="../Images/86886e43b7fe3aed7d6228835f22f82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oO7DWZITmJfxsiLF3xclwg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">修复摘要。图片摘自张等人的修图论文。</p></figure><blockquote class="nt nu nv"><p id="3443" class="kx ky mk kz b la mf jr lc ld mg ju lf nw mh li lj nx mi lm ln ny mj lq lr ls ij bi translated">Fixup有点令人困惑，所以如果你有任何问题，请在评论中自由提问，我将很乐意尽我所能回答。</p></blockquote><h1 id="d1a1" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">LSUV初始化</h1><p id="d644" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">LSUV是由Mishkin等人在2016年的一篇名为<a class="ae mm" href="https://arxiv.org/abs/1511.06422" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> <em class="mk">的论文中介绍的，你需要的只是一个好的Init </em> </strong> </a>。LSUV Init是一种数据驱动的方法，具有最小的计算量和非常低的计算开销。初始化是一个两部分的过程，首先初始化正交矩阵的权重(与高斯噪声相反，高斯噪声只是近似正交的)。下一步是用一个小批量进行迭代，并调整权重，使激活的方差为1。作者断言，在很大范围内，小批量对方差的影响可以忽略不计。</p><p id="3776" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">在本文中，作者列出了如下初始化步骤。</p><ol class=""><li id="f8c7" class="oh oi iq kz b la mf ld mg lg oj lk ok lo ol ls om on oo op bi translated">将权重初始化为具有单位方差的高斯噪声。</li><li id="b6af" class="oh oi iq kz b la oq ld or lg os lk ot lo ou ls om on oo op bi translated">用<a class="ae mm" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank"> SVD </a>或<a class="ae mm" href="https://en.wikipedia.org/wiki/QR_decomposition" rel="noopener ugc nofollow" target="_blank"> QR </a>将它们分解成正交基。</li><li id="954b" class="oh oi iq kz b la oq ld or lg os lk ot lo ou ls om on oo op bi translated">使用第一个小批量在网络中迭代，并在每次迭代中调整权重，以使输出方差更接近1。重复直到输出方差为1或最大迭代次数已经发生。</li></ol><blockquote class="nt nu nv"><p id="0a3c" class="kx ky mk kz b la mf jr lc ld mg ju lf nw mh li lj nx mi lm ln ny mj lq lr ls ij bi translated">在论文中，作者提出缩放因子为<strong class="kz ir"> √Var(BL) </strong>，其中<br/>BL——其输出blob</p><p id="268c" class="kx ky mk kz b la mf jr lc ld mg ju lf nw mh li lj nx mi lm ln ny mj lq lr ls ij bi translated">作者还提出了一个最大迭代次数的值来防止无限循环，然而，在他们的实验中，他们发现单位方差是在1-5次迭代中达到的。</p></blockquote><p id="7222" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">LSUV Init可视为正交初始化和BatchNorm的组合，batch norm仅在第一个小批量上执行。作者在实验中表明，与完全批处理相比，这种方法具有很高的计算效率。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ov"><img src="../Images/766bada3486e84ebaa3f23e9d3074a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8FQ3PtzJ8I_eZBsAxeGYQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">LSUV算法。图像取自LSUV纸张。</p></figure><h1 id="145a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">迁移学习</h1><p id="e05f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae mm" href="https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks." rel="noopener ugc nofollow" target="_blank">迁移学习</a>是在我们的新模型中使用已经训练过的模型的权重的方法，该模型是为类似的任务而训练的。这些重量已经学习了许多有用的信息，我们可以简单地为我们的特定目标进行微调，然后<em class="mk">瞧</em>！我们有一个惊人的模型，没有初始化的麻烦。</p><p id="cffb" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">每次都使用另一个模型的预训练权重是最好的方法。我们唯一需要初始化我们自己的权重的时候是我们在一个从来没有人训练过的网络上工作。而在大多数实际场景中，几乎不会出现这种情况。</p></div></div>    
</body>
</html>