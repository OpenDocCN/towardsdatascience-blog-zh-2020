<html>
<head>
<title>Deriving the Backpropagation Equations from Scratch (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始推导反向传播方程(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deriving-the-backpropagation-equations-from-scratch-part-1-343b300c585a?source=collection_archive---------3-----------------------#2020-11-08">https://towardsdatascience.com/deriving-the-backpropagation-equations-from-scratch-part-1-343b300c585a?source=collection_archive---------3-----------------------#2020-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7412" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深入了解神经网络的训练方式</h2></div><p id="6852" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一系列简短的<a class="ae lb" rel="noopener" target="_blank" href="/deriving-the-backpropagation-equations-from-scratch-part-2-693d4162e779">两篇文章</a>中，我们将从头开始推导全连接(密集)层的三个著名反向传播方程:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/87c574672a66eaeefd66688da809bee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*QsTHnP4CZbcj2ExbfeZzbg.png"/></div></figure><p id="cb01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下所有解释都假设我们只向网络提供一个训练样本。如何将公式扩展到小批量将在这篇文章的最后解释。</p><h1 id="8142" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">正向传播</h1><p id="c4fe" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">我们首先简要回顾一下单层的正向传播(以矩阵形式):</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/8f12b8d3817ee629a33bc5e13cef36a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fn6sB8vIdqWwaTSZq--97g.png"/></div></div></figure><p id="e57b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图层𝑙的输入是矢量(通常称为特征矢量):</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/be0f8a6808d9779f299324b54f7c224e.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*vIcurbC6bridZbXTWZfYIA.png"/></div></figure><p id="e71a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">方括号中的值(上标中)表示网络层。我们用变量n来表示一层中的节点数。定义图层𝑙行为的系数(权重和偏差)为:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/dda6d2dd93fc886e94f4c26908258f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*qW-dRBHLJ7wBd_rjd2hqKw.png"/></div></figure><p id="e84f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">中介价值:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e66ad76a490a162ea80fdff6b7f478e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*WD_7F9bfZzUgPdBSFOnqfw.png"/></div></figure><p id="6c1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">被称为<strong class="kh ir"> <em class="mp">的加权输入</em> </strong> <em class="mp"> </em>是通过:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/92b97c2644ffcb6f7ef69c2a8581f3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*fHGJ4fGhkaAUNn89LrlP8g.png"/></div></figure><p id="7289" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加权输入以元素方式馈入激活函数(也称为非线性):</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4d2576ff69774100ad4b0182762adc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*rrhgFBmS4HJpLEs-D4Fb6g.png"/></div></figure><p id="c489" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要获得输出:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/9945566510649c98ba627c21ea53d94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*5v1Hzf8B1boU21NG6cjyYw.png"/></div></div></figure><p id="5091" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">常见的激活功能有ReLU、leaky ReLU、tanh、sigmoid、Swish等。如上标所示，理论上每层可以具有不同的激活函数。</p><h1 id="e683" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">获得直觉</h1><p id="87b9" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">在不知道反向传播是什么的情况下推导反向传播方程是没什么用的。为此，我们首先将上一节定义的单个层链接成一个𝐿层网络。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/683dadfadce3acb9c63cad5064c6ba7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*U3QZ_Yn4fcjbdUkIwHJ90w.gif"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">(图片由作者提供)</p></figure><p id="2e43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练神经网络的目标是提高其在给定任务上的性能，例如分类、回归。通过损失函数𝓛来评估性能，该损失函数在训练期间被添加为链的最后一个块。对于每个样本，损失函数将网络的输出与地面真实值𝒚进行比较，并输出单个实值(！idspnonenote)信号。)号。通常，较小的数字表示性能良好，而较大的数字表示性能不佳。</p><p id="e004" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们输入一个具体的样本到我们的网络中，例如</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi my"><img src="../Images/bdb72a4e55b3b6f022cf5f325bcd720c.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*drr1AJg0Y9h4ociQxP0-jg.png"/></div></figure><p id="c733" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并通过各层连续向前传播。我们在图层𝑙停下来，看看计算出的加权输入，比如说:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/53a12e1a1f4cb5cd806ee22f92bd8985.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*g1kdv5wjVda0uM9wntzy6Q.png"/></div></figure><p id="7852" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们继续传播以获得损失函数值，例如𝓛 = 0.3247。</p><p id="1ce2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们回到𝑙层，并增加其第一个节点的值:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/81e2e686c23a01392772bb3a60c2974f.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*_e2TMuAsml6b3wGMYmaY7A.png"/></div></figure><p id="84c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">微小的价值:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/1c8f6c57fcb42e6907ac9134ef5ccb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*Pmf9NXyRIi8ck2Vr7t_Hbw.png"/></div></figure><p id="7068" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时保持所有其他节点不变。从𝑙层开始，我们向前传播改变后的加权输入:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1a7a0734794216cb7289277b43c81de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*WUT9FwAUAHalGQw62IyeNg.png"/></div></figure><p id="5557" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过网络并获得损失函数𝓛 = 0.3242的微小变化值。</p><p id="0359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于网络的分级性质，微小的推动δ导致了后面层𝑙 + 1、𝑙 + 2等的所有激活。稍微更改它们的值，如下图所示:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nd"><img src="../Images/883bc1059bd1896d70f0cf354a755509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*btj3R0VPv01IdL90fijjxw.gif"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">绿色箭头表示增加，红色箭头表示减少(图片来自作者)</p></figure><p id="9618" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们设计的例子中，当我们增加𝑙.层中第一个节点的值时，损失函数的值减少了<br/>δ𝓛=-0.0005一般来说，对于一些节点，损失函数将减小，而对于其他节点，损失函数将增大。这完全取决于网络的权重和偏差。</p><p id="c2dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述两个三角形之比就是所谓的“斜率三角形”:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/80eb6c485f658560f170540f923c73f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/format:webp/1*bnpUu8EuXdGYPIT6XbT_jg.png"/></div></figure><p id="8055" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个斜率三角形当然可以对所有节点和所有层进行计算。它是下列偏导数的近似值:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/bd950408693c53f220e1dcd0efd4d59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*hGLl9gzE3SGqJI_zm6gvsg.png"/></div></figure><p id="4aac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在文献中，这种偏导数通常被称为<em class="mp">误差</em>，我们将随后使用这个术语。<em class="mp">误差</em>正是从最后一层𝐿开始，通过网络反向传播的量。</p><p id="593c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用第一反向传播方程，我们可以将层𝑙的<em class="mp">误差</em>向后传播一步到其前一层𝑙 -1:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ng"><img src="../Images/476ad3a4ccde20fa053901d6984ea0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*2C6_OsmCk5TrQC5TcBDHyw.png"/></div></div></figure><p id="a308" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重复应用该等式允许我们获得所有层的<em class="mp">误差</em>。</p><p id="64d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们简短地总结一下反向传播的机理:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nh"><img src="../Images/bb464eb0489e3b853e38dbc45cb98e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DnR_kCcu-snVfWeVtiWyOQ.png"/></div></div></figure><p id="a837" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练神经网络的过程包括通过调整网络的权重和偏差来最小化损失函数。使用梯度下降或其变体来完成自适应。事实证明，偏置的梯度与<em class="mp">误差</em>完全匹配:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d9b030a7f720b8febd35e6f60f06e859.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*SxtR0Wr7te1xYBie0mvhvw.png"/></div></figure><p id="0fe8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">权重的梯度也可以通过外积直接从<em class="mp">误差</em>中获得:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c62744f7259d6bc4a5965a3ae60a70d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*uzoiet3HU1wjo7ctW4w_LQ.png"/></div></figure><h1 id="b40a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">扩展到多个样本</h1><p id="7a42" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">要计算小批量样品的重量和偏差梯度，我们只需独立执行上述步骤(！)对于小批量的所有样品，随后平均梯度:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9f85db77ca8f7c8968c1f206aa279d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*kvwYi3-F7JNCo7dvAixsPQ.png"/></div></figure><p id="7e1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，𝑚表示小批量样品的数量，圆括号中的数值表示样品编号。</p><p id="d68b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，在计算效率极高的反向传播“矢量化版本”中，针对单个样本独立计算<em class="mp">误差</em>。然而，计算是并行进行的。</p><h1 id="ecd0" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">链式法则</h1><p id="e2f8" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">学校里教的基本链式法则允许我们计算嵌套函数的导数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/279e0bc8e039ed4332201269a93ee086.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*SDIljRCseI3sE3BHrkGyHA.png"/></div></figure><p id="e2f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">𝑔(.在哪里)和𝑓(.称之为“外层功能”)的“内在功能”。导数是:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/14701ce297e6429e1a89610e0eb9b2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*_q21_59EMFOHeY3INGZTXg.png"/></div></figure><p id="7e6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也可以写成这样:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/374d81bb9ed6a404371af648d1bc07e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*lY5eRz6HWOgmRd4n-Ps5sg.png"/></div></figure><p id="1f5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了推导反向传播方程，我们需要对基本的链式法则稍加扩展。首先，我们扩展𝑔和𝑓函数，以接受多个变量。我们选择外部函数𝑔取三个实变量并输出一个实数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d4141b7ebc5cc02893d980cc0153924e.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*YTjogeJ4nSXB6pK4KHJQ4A.png"/></div></figure><p id="d01c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为𝑔现在接受三个变量，我们也需要三个不同的内部函数𝑓s，用于每个变量。假设每个𝑓s接受两个实数变量并输出一个实数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e974fc961d59415ae825d1e09f33befc.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*zmtw_7lg92ppnLXPbrqq9A.png"/></div></figure><p id="1c61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将内部函数插入外部函数，得到:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2e536d3700c5cd9e8063d45af0abf7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*vwj3AywpE5h2fO9jtBwV0A.png"/></div></figure><p id="69a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，得到的函数只是两个(！idspnonenote)的函数。)实变量。现在，我们想获得下面的偏导数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7e62b4fac63b149ead13d7635753d8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*eXN3VoYar4skBqXgaIIhQA.png"/></div></figure><p id="0e05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与上述基本链规则一样，我们首先获取外部函数𝑔相对于其第一个变量的偏导数，并将其乘以第一个内部函数的偏导数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2b9d88d86a91acad32982a4ca353766b.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*9qLsLaDmRszTV5NUP3MWDw.png"/></div></figure><p id="3431" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们首先取外部函数𝑔的偏导数，就好像内部函数不存在一样。只是随后我们将内部函数插入到变量中，用垂直线表示。</p><p id="5316" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，我们还没有完成，因为外部函数不仅仅依赖于一个变量。幸运的是，扩展链规则具有非常清晰的结构，因此我们接下来对第二个和第三个变量执行完全相同的步骤。随后，将所有步骤加在一起:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nr"><img src="../Images/2bb95203ad1ab4a763791f552302dd1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*HGTnnPFjOvAX2UkfI2YCvA.png"/></div></div></figure><p id="9d68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更一般地说，对于任意外函数和𝑛任意内函数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d65be1566855c0b34ab565e9577911e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*39bLMK1nez8TzaqsapCEyg.png"/></div></figure><p id="ccd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">链式法则是:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6900827b35e003e77bd5c893a58035f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*5K-NOotqDuOjGgN-PrElPA.png"/></div></figure><p id="c3d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了清楚起见，我们去掉了在具体值</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b7c66ed03f82376f3743fefa27173376.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*xBOuoiSyoQBfXXoj7p09Sg.png"/></div></figure><p id="9e1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样的程序适用于其他变量的导数。</p><p id="feeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本系列的第二部分中，我们将使用扩展链规则来推导反向传播方程。</p></div></div>    
</body>
</html>