<html>
<head>
<title>Algorithms are not enough</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">算法是不够的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-are-not-enough-fdee1d65e536?source=collection_archive---------4-----------------------#2020-10-06">https://towardsdatascience.com/algorithms-are-not-enough-fdee1d65e536?source=collection_archive---------4-----------------------#2020-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="3516" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/opinion" rel="noopener" target="_blank">意见</a></h2><div class=""/><div class=""><h2 id="df7f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">人工智能的下一个突破需要重新思考我们的硬件</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5060fef0d44270b52b2498272ff9a806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ueMIA_NyIy7-Vu5E"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@umby?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">翁贝托</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e8a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在的AI有一个问题:贵。训练Resnet-152，一个现代计算机视觉模型，估计花费大约100亿次浮点运算，这与现代语言模型相比相形见绌。OpenAI最近的自然语言模型GPT-3的训练预计将花费3000亿万亿次浮点运算，这在商用GPU上至少需要500万美元。相比之下，人脑可以识别人脸，回答问题，驾驶汽车，只需一根香蕉和一杯咖啡。</p><h2 id="172f" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">我们是怎么到这里的？</h2><p id="6163" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们已经走了很长一段路。第一台计算机是专用机器。1822年，英国数学家查尔斯·巴贝奇创造了“差分机”，它的唯一目的是计算多项式函数。1958年，康奈尔大学教授弗兰克·罗森布拉特(Frank Rosenblatt)创造了“Mark I”，这是一种用于机器视觉任务的单层感知机的物理化身。在早期，硬件和算法是一回事。</p><p id="fcf7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着<em class="nb">冯诺依曼架构</em>的引入，硬件和算法的统一性发生了变化，这是一种由用于计算的处理单元和用于存储数据和程序指令的存储单元组成的芯片设计。这种范式转变使得制造通用机器成为可能，这些机器可以被编程来完成任何期望的任务。冯-诺依曼体系结构已经成为现代数字计算机的蓝图。</p><p id="dfb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，有一个条件。数据密集型程序需要内存和计算单元之间的大量通信，从而降低了计算速度。这种“冯-诺依曼瓶颈”是人工智能早期尝试失败的原因。标准CPU在大型矩阵乘法(深度神经网络中的核心计算操作)方面效率低下。由于现有硬件的瓶颈，早期的神经网络太浅，性能不佳。</p><p id="76bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个问题的解决方案不是来自学术界，而是来自游戏行业，这是历史的讽刺之一。GPU是20世纪70年代为加速视频游戏而开发的，它通过数千个计算核心并行处理数据密集型操作。这种并行性是解决冯-诺依曼瓶颈的有效方法。GPU能够训练更深层次的神经网络，并已成为现代人工智能的现状硬件。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/b9b8219d06a3c335f48cd8a0832df6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jJkDJFj_aB-FZ4ky"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Riho Kroll 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="f048" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">硬件彩票</h2><p id="f0e6" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">人工智能的研究有一点纯粹的运气成分。谷歌研究员萨拉·胡克称之为“<a class="ae lh" href="https://hardwarelottery.github.io/" rel="noopener ugc nofollow" target="_blank">硬件彩票</a>”:早期的人工智能研究人员只是运气不好，因为他们被缓慢的CPU卡住了。在GPU出现的时候，恰好在该领域的研究人员“赢得”了硬件彩票。他们可以通过训练由高效GPU加速提供动力的深度神经网络来取得快速进展。</p><p id="15c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">硬件彩票的问题是，一旦这个领域作为一个整体确定了赢家，就很难探索新的东西。硬件开发速度缓慢，而且需要芯片制造商在回报不确定的情况下进行大量前期投资。一个安全的赌注是简单地优化矩阵乘法，这已经成为现状。但从长远来看，这种对硬件和算法的特定组合的关注会限制我们的选择。</p><p id="6e68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们回到最初的问题。为什么当今的人工智能如此昂贵？答案可能是我们还没有合适的硬件。硬件彩票的存在，加上商业激励，使得经济上很难打破目前的现状。</p><p id="0ac7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为一个例子，考虑杰弗里辛顿的胶囊神经网络，一种计算机视觉的新方法。谷歌研究人员Paul Barham和Michael Isard <a class="ae lh" href="https://dl.acm.org/doi/pdf/10.1145/3317550.3321441" rel="noopener ugc nofollow" target="_blank">发现</a>这种方法在CPU上工作得相当好，但在GPU和TPU上表现不佳。原因？加速器已经针对最频繁的操作进行了优化，例如标准矩阵乘法，但是缺少针对胶囊卷积的优化。他们的结论(这也是他们论文的标题):<em class="nb"> ML系统停滞不前</em>。</p><p id="74df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">人工智能研究人员存在“过度适应”现有硬件的风险，从长远来看，这将抑制该领域的创新。</p><h2 id="5d19" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">前进的道路</h2><blockquote class="nd"><p id="080c" class="ne nf it bd ng nh ni nj nk nl nm md dk translated">“下一个突破可能需要一种根本不同的方式，用硬件、软件和算法的不同组合来建模世界。”莎拉·胡克，谷歌大脑</p></blockquote><p id="b516" class="pw-post-body-paragraph li lj it lk b ll nn kd ln lo no kg lq lr np lt lu lv nq lx ly lz nr mb mc md im bi translated">在人脑中，记忆和计算不是两个独立的组件，而是发生在同一个地方:神经元。记忆来自神经元通过突触连接在一起的方式，而计算来自神经元放电和传播来自感觉输入的信息的方式。硬件和算法是一回事，很像早期的计算机。这与我们今天做人工智能的方式完全不同。</p><p id="1154" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由GPU和TPU支持的深度神经网络，即使它们在今天的许多任务中表现得非常好，从长远来看可能不是前进的方向。也许在硬件和算法架构的可能组合的广阔前景中，它们只是局部最优。</p><p id="2aa5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">前进的道路始于认识到算法是不够的。对于下一代人工智能，我们需要在硬件和算法上进行创新。在GPU之前，AI研究被卡住了。硬件上没有新的突破，就有可能再次卡住。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><div class="ks kt ku kv gt nz"><a rel="noopener follow" target="_blank" href="/supervised-learning-is-not-enough-8254814dfcc5"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">监督学习是不够的</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">为了在人工智能方面取得进展，我们的模型需要学会应对混乱的现实世界</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">towardsdatascience.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on lb nz"/></div></div></a></div><div class="oo op gp gr oq nz"><a rel="noopener follow" target="_blank" href="/the-origin-of-intelligent-behavior-3d3f2f659dc2"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">智能行为的起源</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">为什么真正的人工智能需要的不仅仅是模式识别</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">towardsdatascience.com</p></div></div><div class="oi l"><div class="or l ok ol om oi on lb nz"/></div></div></a></div></div></div>    
</body>
</html>