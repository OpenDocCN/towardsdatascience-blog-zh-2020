<html>
<head>
<title>Avoiding cold starts on AWS Lambda for a long-running API request</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对于长时间运行的API请求，避免在AWS Lambda上冷启动</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/avoiding-cold-starts-on-aws-lambda-for-a-long-running-api-request-15b8194f2e01?source=collection_archive---------15-----------------------#2020-10-22">https://towardsdatascience.com/avoiding-cold-starts-on-aws-lambda-for-a-long-running-api-request-15b8194f2e01?source=collection_archive---------15-----------------------#2020-10-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6a3265b0b4298677843525200c3b2e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nqaz98QjbFA44baGdyn5LA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">布拉登·科拉姆在<a class="ae kc" href="https://unsplash.com/s/photos/sprinter?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="92bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你和我一样，你会认为无服务器很棒。可忽略不计的运行成本，无需占用您时间的服务器配置，默认自动扩展等。等等。当然，根据“没有免费的午餐”的说法，这种便利和成本节约是有代价的，虽然无服务器有许多实际成本(内存限制、包大小、运行时间限制、开发人员学习曲线)，但本文将假设您已经解决了这些问题(或者不关心它们)，而是专注于特定的性能成本—冷启动。</p><h2 id="32eb" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">冷启动为什么会伤害数据科学应用？</h2><p id="b62c" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">冷启动很慢。这就是问题所在，到底有多慢取决于各种因素，比如你的运行时(python实际上是启动更快的lambda容器之一)，以及你在Lambda的设置阶段做了什么(<a class="ae kc" href="https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/" rel="noopener ugc nofollow" target="_blank">源</a>)。</p><p id="58c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为数据科学家和开发人员，我们习惯了缓慢的节奏。开始下载200 GBs的文本数据(放上水壶)，加载模型(可怜的茶)，运行聚类算法(去散步)……听起来熟悉吗？虽然这种缓慢的速度对于实验(以及运行在持久服务器上的生产系统)来说是好的，但是对于无服务器模式来说却是致命的。AWS Lambda只允许在函数超时和容器死亡之前最多运行15分钟，将所有未持久化的工作(和运行的计算)限制在死亡容器的墓地中，永远不会重新启动。</p><p id="b173" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当您的lambda函数实际上是一个API端点，客户端可以临时调用它来触发一些实时数据科学/分析过程(在我们的示例中，NLP基于任意长度的单个短语)时，这个问题就变得复杂了，因为15分钟的Lambda运行时间窗口突然缩减为30秒的窗口，以向客户端返回HTTP响应。</p><p id="1cd3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文并不是试图让您停止在lambda中运行特定的NLP管道，相反，它希望帮助您了解这些约束以及克服它们的一些方法(或者，从理解的角度选择忽略，帮助您生活在边缘)。</p><h2 id="1f1e" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">什么是冷启动？</h2><p id="da6a" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">冷启动发生在容器关闭时，然后在调用lambda函数时必须重新启动，通常这发生在大约5分钟的不活动之后。</p><p id="30b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">已经有很多关于冷启动的文章，所以这篇文章不会提供详细的指导(我建议你查看这篇文章)。但是简单来说…</p><p id="1f94" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当容器从冷态启动时，该功能需要:</p><ol class=""><li id="c93c" class="lz ma iq kf b kg kh kk kl ko mb ks mc kw md la me mf mg mh bi translated">从外部永久存储器(例如S3)获取并加载包含lambda代码的包；</li><li id="fc16" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">旋转容器；</li><li id="3363" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">将包代码加载到内存中；</li><li id="047f" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">运行函数的处理程序方法/函数。</li></ol><p id="fcce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(<a class="ae kc" href="https://dashbird.io/blog/can-we-solve-serverless-cold-starts/" rel="noopener ugc nofollow" target="_blank">https://dash bird . io/blog/can-we-solve-server less-cold-starts/</a>)</p><p id="8aee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:每当你调用一个lambda函数(这是你的代码)时，第4步总是发生。)，但步骤1-3仅适用于冷启动。由于设置阶段完全发生在AWS中，它们不在我们的控制范围内。我们可以在步骤4中优化我们内心的内容，但是步骤1-3仍然会随意地给我们带来大约10秒以上的延迟。这显然是同步API的问题。</p><h2 id="9074" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated"><strong class="ak">我们的具体问题</strong></h2><p id="96e6" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在开始讨论我们的具体问题。我们有一个<em class="mn">同步</em> API，它:</p><ol class=""><li id="da6f" class="lz ma iq kf b kg kh kk kl ko mb ks mc kw md la me mf mg mh bi translated">从HTTP请求中获取任意文本输入</li><li id="b5bc" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">从S3下载了一个NLP模型(大约220mb)</li><li id="86ae" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">使用模型对输入执行NLP</li><li id="5c9e" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">将序列化的结果返回给调用者。</li></ol><p id="e7c1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的问题是第二步。每次调用时从s3下载模型可能需要15-20秒。这在大多数情况下对我们的用例来说是好的，因为尽管我们提供了一个<em class="mn">长时间运行的同步</em>端点，但我们并不期望它很快(我们讨论的是动态NLP，而不是简单的GET请求)。</p><p id="d801" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在冷启动期间，我们经常看到请求超时。这完全说得通，就好像lambda需要10秒启动，而下载模型需要20秒，我们没有太多时间来运行NLP并在30秒的HTTP窗口中返回结果！</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/3f63973209e6ed226f59ce58a0129aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUTRsKVVREI6un_mAdlEeg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图1(作者截图):3个月窗口中各种API调用的平均延迟。当线路在29k毫秒处中断时，将会发生超时，并引发504错误。</p></figure><h2 id="592b" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">可能的解决方案:</h2><p id="f16d" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">有各种方法可以解决这个问题，例如:</p><p id="a5a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">a.供应的并发</p><p id="2480" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">b.无服务器插件预热</p><p id="5bef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">c.EC2</p><p id="dfc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">d.定制的解决方案</p><p id="11cd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面将逐一讨论:</p><h2 id="abca" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">A.供应的并发</h2><p id="060e" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">如果您知道一个确切的窗口，知道您的lambda流量的预期时间，那么供应并发是一个很好的解决方案。它基本上预先分配了一定数量的容器来运行您的lambda，这些容器将在指定窗口的持续时间内保持运行。</p><p id="031c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种解决方案的主要优势之一是，它可以改善上述lambda流程的设置(1–3)和运行时(4)阶段的延迟。显而易见，由于容器已经在运行(即没有冷启动)，设置阶段在提供的时隙中被消除了，但这种方法如何加快运行时间可能不太明显。答案在于，通过保持容器温暖，避免了后续调用中不必要的初始化(例如，建立数据库连接、初始化对象、下载参考数据或加载沉重的框架等任务)(<a class="ae kc" href="https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/" rel="noopener ugc nofollow" target="_blank">源</a>)。</p><p id="d314" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的用例，供应并发的问题是，由于我们的API必须支持全球不同时区的集成器，供应窗口将变得非常大，这对我们来说将成为无效的解决方案。此外，我们不知道这些窗口的确切时间。</p><h2 id="540a" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">B.无服务器插件预热</h2><p id="f01f" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们使用(强烈推荐的)无服务器框架，采用该框架的最大好处之一是可用的开源插件的社区和范围。一个解决这个问题的插件是<a class="ae kc" href="https://github.com/FidelLimited/serverless-plugin-warmup" rel="noopener ugc nofollow" target="_blank">无服务器插件预热</a>。</p><p id="c377" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个插件采取了一种类似的方法来提供并发性，因为它试图防止lambda容器变冷，但它是通过在指定的时间窗口内用虚拟请求撞击它们来实现的。该插件有很好的文档记录，高度可配置，但最终我们认为它不是我们的最佳途径，原因与提供并发选项相同——我们不一定能准确预测何时需要并发，将其配置为宽窗口将是浪费/过度。</p><p id="348a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们简单关注最后一点——浪费。Lambdas像芯片一样便宜，但对我们来说，真正的成本是下载NLP模型。一个烟盒计算表明，如果我们试图在整个月的正常英国办公时间内保持10个集装箱运行，我们一个月就会下载1.89TB的数据。虽然这在S3还不是一笔大开销，但比我们目前的支出高出了几个数量级。在扩展您的lambda时，请记住真正的成本——通常不仅仅是lambda本身的运行时间！</p><h2 id="6923" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">C.EC2</h2><p id="78e2" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">每次都从S3下载模型听起来效率很低——难道我们就不能启动几个自动缩放的网络服务器，然后在上面加载模型，这样就可以开始了吗？问题解决了，不是吗？</p><p id="cd81" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，但是A)这需要大量投资来重构我们当前完全无服务器的API后端，B)另一个烟盒成本计算显示，这将比实现A或B的成本高几个数量级，而A或B本身比我们当前所做的要贵几个数量级。</p><h2 id="685d" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">D.定制的解决方案</h2><p id="8be1" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">排除上述解决方案后，我们决定采用定制解决方案。</p><p id="e122" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，这里的警告是，当选择推出自己的解决方案时，不要忘记开发的成本。我喜欢Yevgeniy Brikman的规则，即如果你是一家初创公司，除非你正在构建的是你的核心价值主张，否则不要构建它——使用开源库，否则使用专有解决方案。</p><p id="b93c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在这种情况下，我决定打破这条规则，因为替代方案的持续成本会使该特性的一次性开发成本相形见绌。</p><h2 id="49ba" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">那么解决方案是什么呢？</h2><p id="07ed" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们的解决方案基于这样一个认识:虽然我们无法准确预测何时需要预热服务，但我们知道，为了使用服务，客户端必须首先使用OAuth验证自己。如果发送了一个获取OAuth令牌的身份验证请求，很可能会接着发送一个对该令牌执行某些操作的请求。</p><p id="713c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，模式如下:</p><ol class=""><li id="8b3b" class="lz ma iq kf b kg kh kk kl ko mb ks mc kw md la me mf mg mh bi translated">当生成OAuth令牌时，也异步预热lambda容器。</li><li id="c851" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">当发出后续客户端请求时，会命中warmed lambda。</li></ol><p id="3eb4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个序列图:</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/a7dc573c2db3da66566601d6c6f0279e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ibtAODuoMjR240E3D1Xyxw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图2(图片由作者提供):在认证请求期间处理NLP模型lambda预热的序列图。</p></figure><p id="7511" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:这个解决方案有一个小问题，如果客户端在lambda完全预热之前发送后续请求，它会引入一个竞争条件。在我们的例子中，只要我们向我们的API集成者明确表示，在认证后发送第一个请求之前，给lambda留出预热时间，这就可以了。</p><h2 id="37c6" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">结论</h2><p id="abb8" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">无服务器很酷。在无服务器上做NLP很酷。API请求超时并不酷(讽刺的是，冷启动并不酷)。如果你想变得冷静，通过考虑上面强调的方法之一来解决如何避免冷启动的问题。</p><p id="5643" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据您的具体使用情况，像<em class="mn">提供的并发</em>这样的现成解决方案可能是您的正确选择，或者，如果这些解决方案都不理想，那么就像我们一样考虑简单的定制方法。始终考虑运行和扩展服务的全部成本(不仅仅是lambda调用运行时)。最后，在避免过早优化的同时，如果您的API开始遇到超时问题，那么您知道现在是优化的时候了！</p><h2 id="c71c" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">脚注</h2><p id="0f04" class="pw-post-body-paragraph kd ke iq kf b kg lu ki kj kk lv km kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">关于这一点:“每次都从S3下载模型听起来非常低效”。的确如此。这里的解决方案是利用AWS lambdas附带的“/tmp”存储，并且只有在模型还没有保存在那里的情况下才重新下载模型(即冷启动)。</p></div></div>    
</body>
</html>