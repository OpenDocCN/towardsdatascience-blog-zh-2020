<html>
<head>
<title>Hacking Super Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">黑客超级智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26?source=collection_archive---------11-----------------------#2020-10-17">https://towardsdatascience.com/hacking-super-intelligence-af5fe1fe6e26?source=collection_archive---------11-----------------------#2020-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/76e804c528ccf9a8f20d00bedb3f61a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfl0cA4OenIaBo8m4zUHzQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由Allan Beaufour拍摄，许可:<a class="ae jd" href="https://creativecommons.org/licenses/by-nc-nd/2.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-NC-ND 2.0 </a></p></figure><div class=""/><div class=""><h2 id="b476" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">人工智能/人工智能正在影响敏感的决策——为了保护我们的系统，我们需要一个统一的框架和一个新的学科:人工智能安全</h2></div><p id="2e01" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">世界上出现了一种新型的网络攻击。这些攻击与传统攻击不同，无法用传统方法应对。今天，这种攻击还很少，但在未来十年，我们可能会面临一场海啸。为了做好准备，我们需要从今天开始保护我们的人工智能系统。</p><p id="3599" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我想从人工智能安全性的前提开始，只是意识到我在冒一个关于人工智能如何具有破坏性的陈词滥调的风险。为了让它离开桌子，我将提到AI不仅是我们日常生活的一部分(搜索引擎建议，照片过滤器，数字语音助手)。它已经参与了关键的决策过程；国家机构使用AI来更好地分析数据。汽车使用它来做出拯救生命的决定(并自动驾驶)。金融机构每天都在使用它——我们可能正在经历人工智能应用的寒武纪大爆发，在未来十年，人工智能将改变我们生活中最敏感的领域:医疗诊断、金融机构的决策、关键基础设施的控制和军事系统的决策。</p><p id="2be8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于30多年的数字黑客和欺诈，我们可以假设犯罪分子已经在努力利用这项技术的弱点——其中一些弱点就像人工智能一样新[1]。尽管如此，在成千上万致力于开发这些人工智能系统的人中，可能<strong class="kx jh">只有一小部分人在关注</strong> <strong class="kx jh">人工智能系统</strong>的安全性。</p><p id="2a94" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇短文中，我打算简要回顾一下“人工智能固有的风险”，提供一些人工智能政策的背景，以强调这个主题被处理的强度，然后提出一个新的安全规程的框架:<em class="lr">人工智能安全</em>。</p><h2 id="4aab" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">风险</strong></h2><blockquote class="ml"><p id="6030" class="mm mn jg bd mo mp mq mr ms mt mu lq dk translated"><em class="mv"> " </em>人工智能是未来，不仅仅是俄罗斯，而是全人类。它带来了巨大的机遇，但也带来了难以预测的威胁。谁成为这个领域的领导者，谁就将成为世界的统治者。——弗拉基米尔·普京(<a class="ae jd" href="https://www.rt.com/news/401731-ai-rule-world-putin/" rel="noopener ugc nofollow" target="_blank"> RT </a>)</p></blockquote><p id="d969" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">在技术世界里，新的风险通常伴随着创新。这些风险开始很小，然后爆炸(见<a class="ae jd" href="https://medium.com/@GeorgeRaines/cyber-risks-in-new-industries-9481ea317daf?source=friends_link&amp;sk=4c1339fb37dfa892ddf70931e0f9450b" rel="noopener">我关于指数风险的帖子</a>，特别是物联网用例)。人工智能具有指数级的破坏力。想想它对数字数据的影响；想想所有的在线视频、录音电话、安全摄像头、网站……现在想象一种人工智能算法，它可以分析任何这样的数据，找到隐藏的联系，并从中提取新的意义。数据分析<strong class="kx jh">创建了一个新的数据体，几乎和原始数据体</strong>一样大，没有人能预料这些数据会是什么样子。今天，我们赋予这些系统反应和交互的能力，例如，让人工智能接听电话或驾驶汽车，从而成倍增加了破坏的途径。依靠不可预测的数据，以不可预测的方式做出决策。你觉得有多安全？</p><p id="e38c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">即使你信任人工智能做出正确决定的能力，并且你信任使用人工智能的企业/机构，也有另一个重要的警告:<strong class="kx jh">人工智能容易受到新型攻击</strong>。当今的网络安全主要集中在保护IT系统和网络物理系统(CPS)免受逻辑、代码或架构中漏洞的利用。当攻击人工智能系统时，攻击者拥有几乎无人能敌的新操作方法，例如:</p><ol class=""><li id="45ed" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">在学习阶段毒害训练数据以使模型产生偏差。</li><li id="a2b5" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">利用训练白点欺骗人工智能的对抗性攻击。</li><li id="0728" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">暴力迫使模型提取训练数据，违反隐私或数据保密。</li><li id="9311" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">物理模型提取，允许攻击者提取模型，从而侵犯所有者的知识产权。</li></ol><p id="9a64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还有许多其他的例子[2]。当人工智能在公司的业务流程和敏感决策中变得至关重要时，很容易看到上述攻击如何在SFOP范围内导致<strong class="kx jh">损害场景</strong>:安全、财务、运营和隐私。</p><h2 id="5bff" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">相关政策和研究</h2><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/9aab6428127d3ae681b2d6930764b853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ohJW8EJ8RB4s88gwGl6zRQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">AI战略文件时间表，自2020年4月起生效，<a class="ae jd" href="http://www.unicri.it/index.php/topics/ai_robotics" rel="noopener ugc nofollow" target="_blank">犯罪司法所</a>根据<a class="ae jd" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></figure><p id="fa08" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人工智能安全工程一词早在2010年就由Roman Yampolskiy创造，开始引起人们对我们面临的问题的关注。2017年1月，一群领先的行业和学术研究人员设计了<em class="lr"> Asilomar AI原则[3] </em>。由1600多名研究人员签名，并得到埃隆·马斯克和斯蒂芬·霍金等名字的认可，阿西洛马人工智能原则旨在为人工智能研究提供一个重要考虑因素的框架。大部分原则都是围绕AI伦理和价值观。<strong class="kx jh">原则6明确指出</strong> <strong class="kx jh"> AI安全必须是所有AI实现中的一个考虑因素</strong>。人们的意识肯定在提高，然而，只有一小部分人在积极研究人工智能的安全性。</p><p id="9928" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从好的方面来看，世界各地的监管者都很早就意识到了这种风险。这与过去的技术风险相反。<a class="ae jd" href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation" rel="noopener ugc nofollow" target="_blank">例如，GDPR </a>是在技术侵犯我们的隐私约30年后，以及物联网成为现实20年后加州物联网法案[4]出台的。就人工智能而言，政府很早就开始关注了[5]。例如，白宫在2018年发布了“美国人民的人工智能”概况介绍。欧盟委员会于2019年4月发布了“可信人工智能(AI)道德准则”[6]，中国多年来一直在该领域投资。人工智能政策是一个伟大的话题，我不会在这篇文章中详细阐述。为了更深入的了解，你可以研究上述战略文件的时间表，并阅读CLTC的总结论文[7]。</p><p id="6259" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但即使有所有这些政策，任何人都可以在几分钟内编写一个人工智能应用程序，而不被人注意。<strong class="kx jh">我们用来管理风险的工具实际上并不存在</strong>。</p><p id="09e3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解决人工智能安全有很多角度，作为一名技术人员，我想把重点放在技术选项上。</p><h2 id="a34b" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">解决方案景观</strong></h2><p id="607c" class="pw-post-body-paragraph kv kw jg kx b ky nu kh la lb nv kk ld le nw lg lh li nx lk ll lm ny lo lp lq ij bi translated">首先，让我们定义我们的术语。</p><p id="a27e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> AI安全</em>，在我的词汇里，只涵盖了我们需要信任AI的一部分。概括来说，我使用术语<em class="lr">人工智能信任:</em>当人工智能算法做出决策时，企业和用户睡得好的能力。为了指明保护人工智能系统免受攻击的原则，我使用了术语<em class="lr">人工智能安全</em>。</p><p id="0a6d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">人工智能信任</em>:人类信任人工智能考虑结果的能力:</p><ol class=""><li id="92f6" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">有限可解释性</li><li id="0e0d" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">易受针对IT系统的网络攻击</li><li id="7e1c" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">易受针对人工智能工作方式的攻击(例如敌对攻击、投毒等)</li><li id="a605" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">重大数据泄露的风险</li></ol><p id="14f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">人工智能安全</em>:保护人工智能系统免受以下危害的规则:</p><ol class=""><li id="0ea6" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">人工智能本土攻击(敌对，中毒等…)</li><li id="b664" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">盗窃人工智能模型(即知识产权)</li><li id="4b82" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">训练数据的推断</li><li id="16ca" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">侵犯隐私</li></ol><p id="8566" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然我一直在说只有一小部分人从事AI安全/安保，但业界和学术界并没有完全睡着。在加州大学的一篇技术论文[8]中，作者绘制了考虑到人工智能大规模采用的不同研究领域。其中三个领域属于人工智能安全领域:<em class="lr">稳健决策、安全领域、机密数据共享学习</em>和<em class="lr">对抗学习</em>。近年来，各大会议都选择将AI安全作为主题。例如，在Black Hat USA 2020中，围绕ML/AI主题进行了8次简报，其中两次直接关注模型提取[9]和防范敌对攻击[10]。ACM人工智能与安全研讨会(<a class="ae jd" href="https://aisec.cc/" rel="noopener ugc nofollow" target="_blank"> AISec </a>)涵盖了越来越多的人工智能安全主题会议。除了学术兴趣之外，我们还应该加上业界的关注点:例如，参见华为[11]的这篇综述文章和IBM [12]领导的开源项目，该项目为您提供了一个完整的工具包，让您的系统做好准备，以应对恶意攻击(您可以在此处下载<a class="ae jd" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" rel="noopener ugc nofollow" target="_blank"/>)。</p><h2 id="14e1" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak"> AI安全技术</strong></h2><p id="3396" class="pw-post-body-paragraph kv kw jg kx b ky nu kh la lb nv kk ld le nw lg lh li nx lk ll lm ny lo lp lq ij bi translated">研究兴趣导致了实际的解决方案。有远见的公司已经可以依靠现有的知识体系来对抗人工智能的攻击。从专业的角度来看，我认为我们应该首先报道那些无聊的东西:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/4377d155c37e980c3ff903f17bf40a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cn_6a1LL0WwOwWrnhvM_Tw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="cfbe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">治理:</p><ol class=""><li id="da2c" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">人工智能安全领域的清晰定义:我们要解决什么？</li><li id="e463" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">组织中AI安全的明确责任:是产品所有者、工程部门还是合规部门？</li></ol><p id="f343" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">R&amp;D进程:</p><ol class=""><li id="6746" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">在开发生命周期中考虑与AI相关的风险。这应涵盖产品开发和数据科学运营。</li><li id="bb40" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">每个项目应在开始时确定人工智能风险和相关的人工智能安全目标。</li><li id="da9b" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">安全测试和红队应该被训练使用人工智能攻击媒介。</li><li id="5f60" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">必须保护培训数据，并对威胁进行分析。</li></ol><p id="30a8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">技术:</p><ol class=""><li id="fbc7" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">R&amp;D团队应该使用标准的现成工具来强化系统。</li><li id="366c" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">必须对数据进行过滤，并做好应对敌对攻击的准备。</li><li id="dc65" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">人工智能模型应该被强化以降低敏感度，如果可能的话，还应该能够抵御攻击。</li><li id="ca5c" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">生产中的部署应该使用不同的控制措施来保护模型和培训数据的机密性。</li></ol><p id="da17" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">监控:</p><ol class=""><li id="b674" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">应该监控人工智能系统的偏见、中毒和其他攻击企图。</li><li id="8830" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">应在生产中分析推理和查询，以检测攻击企图(即恶意攻击或数据提取)。</li></ol><p id="7096" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就技术而言，现有工具至少可以应对基本威胁:</p><ol class=""><li id="4f56" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated"><strong class="kx jh">模型提取:</strong>分析输入以检测模型推理尝试。例如，在线集成PRADA检测模型[13]并采取措施应对攻击企图。</li><li id="efa1" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">投毒:</strong>验证训练数据并采样(疏删样本，有多疼就多:)。在生产中，尽量实现不同独立模型之间推理结果的比较，即集成分析。此外，寻找数据中的异常。</li><li id="250e" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">对抗性攻击:</strong>用对抗性训练数据训练你的模型。这可以在培训阶段使用ART工具箱来实现。在生产之前，尝试最小化你的模型的敏感性，例如通过使用知识转移[14]。最后，探索通过重建[15]和检测可疑输入来保护您的模型的选项。</li><li id="4615" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh"> IP盗窃:</strong>模型的部署，尤其是在边缘部署的情况下，必须在一个架构中保护模型的至少一部分免受本地攻击者的攻击。这可以通过在不使用模型时对其进行加密，并利用安全区域和三通来保护关键部分来实现。最后，考虑在你的模型中加入特定的水印，使用只由特定输入触发的神经路径，它可以帮助你发现别人是否偷了你的模型。</li></ol><p id="aa20" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如这篇2016年的论文[16]中提到的，我们真正需要的是一个统一的框架。像网络安全的其他领域一样，我们不希望每家公司都开发自己的安全风格——从社会的角度来看，技术领导者开发人工智能安全学科和最先进的技术来保护人工智能系统非常重要。</p><h2 id="7699" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak"> AI安全创新</strong></h2><p id="0705" class="pw-post-body-paragraph kv kw jg kx b ky nu kh la lb nv kk ld le nw lg lh li nx lk ll lm ny lo lp lq ij bi translated">在我看来，我们需要一个整体的解决方案，让公司在购买现成安全产品的同时专注于人工智能开发。该解决方案将涵盖从构思、开发到部署的人工智能安全。这种解决方案至少应包括以下要素:</p><ol class=""><li id="84d2" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated"><strong class="kx jh">风险和流程:</strong>基础培训、治理框架和专业安全测试应由有能力的公司和卓越中心作为专业服务提供。</li><li id="bcbb" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">数据卫生性和健壮性</strong>:过滤训练数据中的异常点，并向训练数据中注入敌对样本。</li><li id="1cbc" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">模型安全性</strong>:执行标准例程(提取、修剪)并集成高级控制，使模型更加健壮。将水印路径注入到模型中，以便能够在野外进行模型盗窃检测。</li><li id="7f8b" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">安全部署</strong>:通过在不可信和可信执行环境之间划分模型部署，保护模型免受物理盗窃[17]。在边缘运行时自动加密和解密模型。</li><li id="e22d" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated"><strong class="kx jh">威胁防范&amp;检测</strong>:在操作过程中(“现场”)过滤和预处理输入，以阻止攻击企图。监控使用情况以检测敌对攻击企图和/或模型提取企图。</li></ol><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/62b05555f45966634a36df3198fce18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBtjoEU7jTaj277t6WDmkA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="0112" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将上述控制集成到AI开发周期中对于任何客户来说都应该是容易和可访问的，从而将开发团队从开发内部能力和自制AI安全工具的需要中解放出来。它应该是每个公司都使用的标准解决方案，就像所有公司都在其系统中使用TLS并安装现成的防火墙一样。此外，还可以提供人工智能安全专业服务:例如，专注于人工智能的红队、风险评估和监控SOC。</p><p id="f0b2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们应该问自己的唯一问题是，是否有足够强大的商业案例来开发上述解决方案并将其作为产品进行营销。我认为这不是一个“<strong class="kx jh">如果</strong>会有商业案例”的问题，而是一个“<strong class="kx jh">什么时候</strong>的问题。至少有4种力量在发挥作用，使未来5年成为人工智能安全领域创新的有利可图的时间(没有特定的顺序):</p><ul class=""><li id="35fd" class="nb nc jg kx b ky kz lb lc le nd li ne lm nf lq ob nh ni nj bi translated">人工智能在商业决策过程中的流行[18]</li><li id="e7e1" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ob nh ni nj bi translated">规程</li><li id="8e24" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ob nh ni nj bi translated">信息物理系统开始利用人工智能[19]</li><li id="7c5f" class="nb nc jg kx b ky nk lb nl le nm li nn lm no lq ob nh ni nj bi translated">人工智能操作成为一件事(简而言之:在人工智能世界里有一个类似DevOps的运动)[20]</li></ul><p id="179d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是对于投机者来说，未来5年内什么时候是个问题。既然我们在谈论安全性，那么有理由假设大玩家(即方)会照顾好自己，少数早期采用者会从外部寻找解决方案，而市场的大多数人只会等待大黑客的消息。让我们看看结果如何。</p><h2 id="c47d" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><strong class="ak">总结</strong></h2><p id="bc80" class="pw-post-body-paragraph kv kw jg kx b ky nu kh la lb nv kk ld le nw lg lh li nx lk ll lm ny lo lp lq ij bi translated">作为一名在保护CPS方面拥有专业知识的网络安全专家，我亲眼目睹了安全创新如何通常<strong class="kx jh">跟随风险而不是先于风险</strong>。对于一家开发人工智能的公司来说，正常的轨迹是首先投资让它工作，然后优化它，然后才保护它。然而，由于我们处于2020年，考虑到这项技术的颠覆性，我们不能等待人工智能安全的投资。监管机构非常(也是可以理解的)关注<em class="lr">可解释性— </em>但是人工智能信任，即我们信任人工智能做出决策的能力，需要同样多的投资来应对这项技术带来的新攻击。</p><p id="3af0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">今天有很多事情可以做，甚至使用开源工具来保护你的人工智能系统。下一步自然是行业开发人工智能安全产品。虽然我不能100%肯定会有一家领先的人工智能安全产品公司，但我非常确信我们会在该领域看到专家顾问和专业服务的利基。如果你觉得这个话题有趣，现在可以是发展你的人工智能安全技能的好时机。</p><p id="0124" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望这篇短文提供一些思考的食粮，也许是管理你自己的AI安全风险的起点。</p><p id="c176" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我<strong class="kx jh">如果你想参与讨论，请评论，分享，随时联系我:</strong>【harpakguy@gmail.com】T4</p><p id="d14d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">谢谢:)</p><p id="0956" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">免责声明</strong>:我的观点仅代表我个人，与我工作的任何公司或个人无关。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="d5fa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">参考资料:</p><p id="c536" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1]- Roman V. Yampolskiy，人工智能安全和网络安全:人工智能失败的时间线(2016年)，<a class="ae jd" href="https://arxiv.org/pdf/1610.07997.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="b6e8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2]- <a class="ae jd" href="https://towardsdatascience.com/@alexanderpolyakov?source=post_page-----6af8026675ff--------------------------------" rel="noopener" target="_blank">亚历山大·波利亚科夫</a>，AI安全与对抗机器学习101 (2019)，<a class="ae jd" rel="noopener" target="_blank" href="/ai-and-ml-security-101-6af8026675ff">中</a></p><p id="58e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3]-阿西洛马尔原则(2017)，<a class="ae jd" href="https://futureoflife.org/ai-principles/?cn-reloaded=1" rel="noopener ugc nofollow" target="_blank">https://futureoflife.org/ai-principles/?cn-reloaded=1</a></p><p id="1e22" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]- <a class="ae jd" href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB327" rel="noopener ugc nofollow" target="_blank">参议院第327号法案</a></p><p id="ac35" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5]-维基百科:<a class="ae jd" href="https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence" rel="noopener ugc nofollow" target="_blank">人工智能条例</a></p><p id="3f8f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6]- <a class="ae jd" href="https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai" rel="noopener ugc nofollow" target="_blank">可信人工智能(AI)的伦理准则</a></p><p id="e5c7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[7]-加州大学伯克利分校CLTC分校，走向人工智能安全:全球对更具弹性的未来的渴望，<a class="ae jd" href="https://cltc.berkeley.edu/towardaisecurity/" rel="noopener ugc nofollow" target="_blank"> CLTC </a></p><p id="8a96" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[8]--斯托伊察、扬和宋、道恩和波帕、拉卢卡·阿达和帕特森、大卫·A .和马奥尼、迈克尔·w .和卡茨、兰迪·h .和约瑟夫、安东尼·d .和乔丹、迈克尔和赫勒斯坦、约瑟夫·m .和冈萨雷斯、约瑟夫和戈德堡、肯和古德西、阿里和卡勒、大卫·e .和阿贝耳、彼得、伯克利的人工智能系统挑战观点(2017)，<a class="ae jd" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-159.html" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="09d9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[9]-于，杨，张，蔡永元，何永元，金永元，“云漏洞:通过对抗实例窃取大规模深度学习模型”，《网络与分布式系统安全研讨会论文集》，(NDSS)，2020</p><p id="cc87" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[10]- Ariel Herbert-Voss，对抗对抗性机器学习的实际防御(2020)，<a class="ae jd" href="https://www.blackhat.com/us-20/briefings/schedule/#practical-defenses-against-adversarial-machine-learning-20476" rel="noopener ugc nofollow" target="_blank"> Blackhat 2020 </a></p><p id="bfd2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[11]-华为，AI安全白皮书(2018年10月01日)，<a class="ae jd" href="https://www.huawei.com/en/trust-center/resources/ai-security-white-paper" rel="noopener ugc nofollow" target="_blank">华为</a></p><p id="6b6d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[12]-玛丽亚-伊琳娜·尼古拉、马蒂厄·辛恩、明·恩哥克·特兰、比特·布瑟尔、安布里什·拉瓦特、马丁·威斯特巴、瓦伦蒂娜·赞泰德斯基、娜塔莉·巴拉卡尔多、布赖恩特·陈、海科·路德维希、伊恩·m·莫洛伊、本·爱德华兹，对抗性鲁棒性工具箱v1.0.0 (2018)，<a class="ae jd" href="https://arxiv.org/pdf/1807.01069.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="02ec" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[13]-米卡·尤蒂，塞巴斯蒂安·西勒，萨缪尔·马沙尔，n .阿索坎，普拉达:防范DNN模特窃取攻击(2019)，<a class="ae jd" href="https://arxiv.org/pdf/1805.02628.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="acf1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[14]- Nicolas Papernot，Patrick McDaniel，Wu，Somesh Jha，Ananthram Swami，蒸馏作为对抗深度神经网络的对抗性扰动的防御(2016)，<a class="ae jd" href="https://arxiv.org/abs/1511.04508" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="eeef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[15]——石祥·古，卢卡·里加齐奥，面向对抗示例的深度神经网络架构(2015)，<a class="ae jd" href="https://arxiv.org/abs/1412.5068" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="e2e6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[16]- Nicolas Papernot，Patrick McDaniel，Arunesh Sinha，Michael Wellman，SoK:走向机器学习中的安全和隐私科学(2016)，<a class="ae jd" href="https://arxiv.org/pdf/1611.03814.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="e5ee" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[17]- GlobalPlatform，TEE系统架构v1.2，<a class="ae jd" href="https://globalplatform.org/specs-library/tee-system-architecture-v1-2/" rel="noopener ugc nofollow" target="_blank">https://global platform . org/specs-library/TEE-System-Architecture-v1-2/</a></p><p id="52a3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[18]-约瑟芬·沃尔夫(Josephine Wolff)，如何为人工智能提高网络安全(2020)，<a class="ae jd" href="https://www.brookings.edu/research/how-to-improve-cybersecurity-for-artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">布鲁金斯学会</a></p><p id="f5de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[19]-<a class="ae jd" href="https://www.theguardian.com/science/2019/mar/29/uk-us-russia-opposing-killer-robot-ban-un-ai" rel="noopener ugc nofollow" target="_blank">https://www . the guardian . com/science/2019/mar/29/uk-us-Russia-opposing-killer-robot-ban-un-ai</a></p><p id="9452" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[20]-比如:【https://research.aimultiple.com/ai-platform/】T4</p></div></div>    
</body>
</html>