<html>
<head>
<title>AutoDiff and Dynamic Subclassed Models with PyTorch Vs TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch与TensorFlow的自动挖掘和动态子类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autodiff-and-dynamic-subclassed-models-with-pytorch-vs-tensorflow-c5224a0a375c?source=collection_archive---------59-----------------------#2020-10-13">https://towardsdatascience.com/autodiff-and-dynamic-subclassed-models-with-pytorch-vs-tensorflow-c5224a0a375c?source=collection_archive---------59-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9f87" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">比较PyTorch 1.x和TensorFlow 2.x之间的自动Diff和动态模型子分类方法，使用自定义动态模型类和手动训练循环/损失函数从零开始训练线性回归</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2548f2cc9b4ac87466c7034c93d6e0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTAOGnrw0ROcSgh8nqzeWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者</p></figure><p id="e9b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇短文重点介绍如何在PyTorch 1.x和TensorFlow 2.x中分别使用带有模块/模型API的动态子类模型，以及这些框架如何在训练循环中使用AutoDiff来获得损失的梯度，并从头开始实现一个非常简单的梯度后代实现。</p><h1 id="9e55" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">生成一些带有一点噪声的线性数据</h1><p id="b33f" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">为了专注于自动差异/自动梯度功能的核心，我们将使用最简单的模型，即线性回归模型，我们将首先使用numpy生成一些线性数据，添加随机水平的噪声。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/1709078d344c8cfed89fa951a8240cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kj-2K8Fdh9J1SOHXLpYm2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者</p></figure><h1 id="1bb8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">模型</h1><p id="e515" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">然后，我们将在TF和PyTorch中从头开始实现一个线性回归模型，不使用任何层或激活器，而是简单地定义两个张量w <em class="mr"> </em>和<em class="mr"> b </em>，分别表示我们的线性模型的<em class="mr">权重</em>和<em class="mr">偏差</em>，并简单地实现线性函数:<em class="mr"> y = wx + b </em></p><p id="a5b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如你在下面看到的，我们的模型的TF和PyTorch类定义基本上是完全相同的，只有在一些api名称上有很小的不同。</p><p id="b7d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里唯一值得注意的区别是PyTorch显式地使用了一个参数对象来定义图表要“捕获”的<em class="mr">权重</em>和<em class="mr">偏差</em>张量，而TF在这里似乎更“神奇”，自动捕获图表要使用的参数。</p><p id="595c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事实上，在PyTorch中，参数是张量子类，当与模块api一起使用时，具有非常特殊的属性，可以自动将自身添加到模块参数列表中，并且会出现在例如Parameters()迭代器中。</p><p id="378d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">无论如何，这两个框架都能够从这个类定义和执行方法(<em class="mr"> __call__ </em>或<em class="mr"> forward </em>)、参数和图形定义中提取，以便向前运行图形执行，并且正如我们稍后将看到的，通过自动微分功能获得梯度，以便还能够执行反向传播。</p><h2 id="08d3" class="ms ls iq bd lt mt mu dn lx mv mw dp mb le mx my md li mz na mf lm nb nc mh nd bi translated">张量流动态模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h2 id="fd88" class="ms ls iq bd lt mt mu dn lx mv mw dp mb le mx my md li mz na mf lm nb nc mh nd bi translated">PyTorch动态模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h1 id="def4" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">训练循环、反向传播和优化器</h1><p id="340e" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在，我们已经实现了简单的TensorFlow和PyTorch模型，我们可以使用TF和PyTorch api定义实现均方误差的损失函数，最后实例化我们的模型类，并运行一系列时期的训练循环。</p><p id="7802" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，为了专注于自动区分/自动分级功能的核心，我们将使用TF和PyTorch特定的自动区分实现来实现一个定制的训练循环，以便为我们的简单线性函数提供梯度，并使用一个特别的简单梯度下降优化器来手动优化<em class="mr">权重</em>和<em class="mr">偏差参数</em>。</p><p id="c170" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在TensorFlow训练循环中，我们将特别明确地使用GradientTape API来记录我们模型的正向执行和损失计算，然后我们将从该GradientTape获得梯度，以用于优化我们的<em class="mr">权重</em>和<em class="mr">偏差参数</em>。</p><p id="814c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PyTorch在这种情况下提供了一种更“神奇”的自动梯度方法，隐式捕获参数张量上的任何操作，并为我们提供相同的梯度来优化我们的<em class="mr">权重</em>和<em class="mr">偏差参数</em>，而不使用任何特定的api。</p><p id="7b5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦我们有了<em class="mr">权重</em>和<em class="mr">偏差梯度</em>，那么在PyTorch和TensorFlow上实现我们的自定义梯度下降方法就像减去<em class="mr">权重</em>和<em class="mr">偏差</em>参数一样简单，这些梯度乘以一个恒定的学习速率。</p><p id="7fa2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里唯一的微小差别是，当我们在反向传播中更新<em class="mr">权重</em>和<em class="mr">偏差参数</em>时，PyTorch以一种更隐式和“神奇”的方式实现自动区分/自动嫁接，我们需要确保不要继续让PyTorch从这最后一次更新操作中提取grad，这次显式调用no_grad api，并最终将我们的<em class="mr">权重</em>和<em class="mr">偏差参数</em>的梯度归零。</p><h2 id="1560" class="ms ls iq bd lt mt mu dn lx mv mw dp mb le mx my md li mz na mf lm nb nc mh nd bi translated">张量流训练回路</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h2 id="7711" class="ms ls iq bd lt mt mu dn lx mv mw dp mb le mx my md li mz na mf lm nb nc mh nd bi translated">PyTorch训练循环</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h1 id="b753" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结论</h1><p id="fd97" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">正如我们所看到的，TensorFlow和PyTorch自动微分和动态子类API非常相似，当然这两个模型的训练给了我们非常相似的结果。</p><p id="c2c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的代码片段中，我们将分别使用Tensorflow和py torch<em class="mr">trainible _ variables</em>和<em class="mr"> parameters </em>方法来访问模型参数，并绘制我们学习的线性函数的图形。</p><h2 id="98a8" class="ms ls iq bd lt mt mu dn lx mv mw dp mb le mx my md li mz na mf lm nb nc mh nd bi translated">标绘结果</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/83794ab7905048080fd484c0a1b3b502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGmT_dS8XICStXmEsa7U1A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者</p></figure><h1 id="8b19" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">源代码</h1><div class="ne nf gp gr ng nh"><a href="https://github.com/JacopoMangiavacchi/TF-VS-PyTorch" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">雅格布曼吉亚瓦奇/TF VS . py torch</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">比较PyTorch和TensorFlow 2.x之间的自动Diff动态模型方法从零开始训练线性…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv kp nh"/></div></div></a></div></div></div>    
</body>
</html>