<html>
<head>
<title>Implementing Transformer for Language Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为语言建模实现转换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-transformer-for-language-modeling-ba5dd60389a2?source=collection_archive---------24-----------------------#2020-09-21">https://towardsdatascience.com/implementing-transformer-for-language-modeling-ba5dd60389a2?source=collection_archive---------24-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2dee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Fairseq训练变压器模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6417e25e089f2ac6d904f287af07a5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYhX4_OOTKjLmKUS3ETHjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供(Fairseq logo: <a class="ae ky" href="https://github.com/pytorch/fairseq/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="5ea1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="6e10" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">自然语言处理</strong>的最新趋势是建立在该领域历史上最大的突破之一之上的:<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">转换器</strong> </a>。Transformer是主要由<a class="ae ky" href="https://research.google/teams/brain/" rel="noopener ugc nofollow" target="_blank"> Google Brain </a>和<a class="ae ky" href="https://research.google/" rel="noopener ugc nofollow" target="_blank"> Google Research </a>研究的模型架构。它最初被证明在翻译任务中达到了最先进的水平，但后来当它被大规模采用时，被证明在几乎任何NLP任务中都是有效的。transformer架构由一系列编码器和解码器组成，具有自我关注层，有助于模型关注各自的输入。你可以在原文<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多关于变形金刚的知识。</p><p id="201c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我们将向您展示如何实现语言建模任务的转换器。<strong class="lt iu">语言建模</strong>是给语言中的句子分配概率的任务。语言建模的目标是让模型将高概率分配给我们数据集中的真实句子，以便它能够通过解码器方案生成接近人类水平的流畅句子。我们将使用<a class="ae ky" href="https://github.com/pytorch/fairseq/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Fairseq </strong> </a>库来实现转换器。</p><h1 id="ca22" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第一步:准备数据集(来自我之前的博文)</h1><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">使用Pytorch微调用于文本生成的GPT2</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">使用Pytorch和Huggingface微调用于文本生成的GPT2。我们在CMU图书摘要数据集上进行训练，以生成…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><p id="d2b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我们将再次使用<a class="ae ky" href="http://www.cs.cmu.edu/~dbamman/booksummaries.html#:~:text=This%20dataset%20contains%20plot%20summaries,author%2C%20title%2C%20and%20genre." rel="noopener ugc nofollow" target="_blank"> CMU图书摘要数据集</a>来训练Transformer模型。可以参考博文的步骤1来获取和准备数据集。准备好数据集后，您应该准备好与数据集的三个分区相对应的<em class="nk"> train.txt </em>、<em class="nk"> valid.txt和test.txt </em>文件。</p><h1 id="bf46" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">步骤2:下载并安装Fairseq</h1><p id="e2f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你没有听说过<a class="ae ky" href="https://github.com/pytorch/fairseq" rel="noopener ugc nofollow" target="_blank"> Fairseq </a>，它是一个流行的NLP库，由<a class="ae ky" href="https://ai.facebook.com/" rel="noopener ugc nofollow" target="_blank">脸书AI </a>开发，用于实现翻译、摘要、语言建模和其他生成任务的定制模型。你可以在这里查看我对Fairseq <a class="ae ky" rel="noopener" target="_blank" href="/top-nlp-libraries-to-use-2020-4f700cdb841f">的评论。</a></p><p id="ce81" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，为了下载并安装Fairseq，运行以下命令:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="9268" class="nq la it nm b gy nr ns l nt nu">git clone https://github.com/pytorch/fairseq<br/>cd fairseq<br/>pip install --editable ./</span></pre><p id="f229" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您的GPU允许，您还可以选择安装NVIDIA的apex库来加快训练速度:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="f89b" class="nq la it nm b gy nr ns l nt nu">git clone https://github.com/NVIDIA/apex<br/>cd apex<br/>pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" \<br/>  --global-option="--deprecated_fused_adam" --global-option="--xentropy" \<br/>  --global-option="--fast_multihead_attn" ./</span></pre><p id="b366" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，您已经成功安装了Fairseq，我们终于可以开始了！</p><h1 id="80c9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">步骤3:预处理数据集</h1><p id="f2b3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了预处理数据集，我们可以使用<a class="ae ky" href="https://fairseq.readthedocs.io/en/latest/command_line_tools.html#" rel="noopener ugc nofollow" target="_blank"> fairseq命令行工具</a>，这使得开发人员和研究人员可以很容易地从终端直接运行操作。为了预处理我们的数据，我们可以使用<code class="fe nv nw nx nm b">fairseq-preprocess</code>来构建我们的词汇表，并对训练数据进行二进制化。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="566b" class="nq la it nm b gy nr ns l nt nu">cd fairseq/<br/>DATASET=/path/to/dataset<br/>fairseq-preprocess \<br/>--only-source \<br/>--trainpref $DATASET/train.txt \<br/>--validpref $DATASET/valid.txt \<br/>--testpref $DATASET/test.txt \<br/>--destdir data-bin/summary \<br/>--workers 20</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/0ae52224fd4cc8de217bb4ec6b8d2e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNYuHWWsMk2G5Bqq7V4cyg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理的命令输出</p></figure><p id="3ae6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">执行上述命令后，预处理后的数据将保存在<code class="fe nv nw nx nm b">--destdir</code>指定的目录下。</p><h1 id="dd5c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">步骤4:训练变压器</h1><p id="d50c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">终于可以开始训练变形金刚了！要训练一个模型，我们可以使用<code class="fe nv nw nx nm b">fairseq-train</code>命令:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="2d6c" class="nq la it nm b gy nr ns l nt nu">CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling \<br/>data-bin/summary \<br/>--save-dir checkpoints/transformer_summary \<br/>--arch transformer_lm --share-decoder-input-output-embed \<br/>--dropout 0.1 \<br/>--optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \<br/>--lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \<br/>--tokens-per-sample 512 --sample-break-mode none \<br/>--max-tokens 2048 --update-freq 16 \<br/>--fp16 \<br/>--max-update 50000 \<br/>--max-epoch 12</span></pre><p id="be2e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们的例子中，我们将GPU指定为第0个(<code class="fe nv nw nx nm b">CUDA_VISIBLE_DEVICES</code>)，将任务指定为语言建模(<code class="fe nv nw nx nm b">--task</code>)，将数据指定为<code class="fe nv nw nx nm b">data-bin/summary</code>，将架构指定为转换器语言模型(<code class="fe nv nw nx nm b">--arch</code>)，将历元数指定为12 ( <code class="fe nv nw nx nm b">--max-epoch</code>，以及其他超参数。训练完成后，模型的最佳检查点将保存在<code class="fe nv nw nx nm b">--save-dir</code>指定的目录下。</p><p id="0e1a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">12个纪元需要一段时间，所以当你的模型训练时请坐好！当然，你也可以根据自己的需要减少历元数来训练。训练完成后，将显示以下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/019c0e01d1fcb4a6e1fab5c870be9971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOwhNGVnzXugfD0pDzfuKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练结束时的命令输出</p></figure><p id="b030" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请注意，在每个历元中，会显示相关的数字，例如丢失和困惑。这些可能有助于在训练过程中评估模型。</p><h1 id="c80a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">步骤5:评估语言模型</h1><p id="345b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在您的模型完成训练之后，您可以使用<code class="fe nv nw nx nm b">fairseq-eval-lm</code>评估生成的语言模型:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="5a12" class="nq la it nm b gy nr ns l nt nu">fairseq-eval-lm data-bin/summary \<br/>--path checkpoints/transformer_summary/checkpoint_best.pt \<br/>--max-sentences 2 \<br/>--tokens-per-sample 512 \<br/>--context-window 400</span></pre><p id="1ad2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这里，将评估测试数据以对语言模型进行评分(在训练阶段使用训练和验证数据来寻找模型的优化超参数)。下面显示了评估后的命令输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/27eba1e1fee79f8e85e32d33ef55b825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HbVFTfhkyieajGsauGusw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于评估的命令输出</p></figure><p id="3ed1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如你所见，我们模型的损失是9.8415，困惑度是917.48(以2为基数)。</p><h1 id="6379" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第六步:终于！让我们生成一些文本:D</h1><p id="339f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在训练模型之后，我们可以尝试使用我们的语言模型生成一些样本。要生成，我们可以使用<code class="fe nv nw nx nm b">fairseq-interactive</code>命令创建一个交互会话来生成:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="80a3" class="nq la it nm b gy nr ns l nt nu">fairseq-interactive data-bin/summary \<br/>--task language_modeling \<br/>--path checkpoints/transformer_summary/checkpoint_best.pt \<br/>--beam 5</span></pre><p id="54fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在交互式会话期间，程序将提示您输入文本。输入文本后，模型将在输入后生成标记。一个代样给定<em class="nk">书发生地</em>作为输入<em class="nk">T7是这样的:</em></p><blockquote class="ob oc od"><p id="6645" class="lr ls nk lt b lu mn ju lw lx mo jx lz oe mp mc md of mq mg mh og mr mk ml mm im bi translated">这本书发生在故事的故事，故事的故事，故事的故事，故事的人物…</p></blockquote><p id="c8bb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">生成是重复的，这意味着模型需要用更好的参数来训练。上述命令使用光束尺寸为5的<a class="ae ky" href="https://arxiv.org/abs/1701.03185" rel="noopener ugc nofollow" target="_blank">光束搜索</a>。我们还可以使用采样技术，如<a class="ae ky" href="https://arxiv.org/abs/1805.04833" rel="noopener ugc nofollow" target="_blank"> top-k采样</a>:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="07fe" class="nq la it nm b gy nr ns l nt nu">fairseq-interactive data-bin/summary \<br/>--task language_modeling \<br/>--path checkpoints/transformer_summary/checkpoint_best.pt \<br/>--sampling --beam 1 --sampling-topk 10</span></pre><p id="1155" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以及<a class="ae ky" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank"> top-p采样</a>:</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="dff2" class="nq la it nm b gy nr ns l nt nu">fairseq-interactive data-bin/summary \<br/>--task language_modeling \<br/>--path checkpoints/transformer_summary/checkpoint_best.pt \<br/>--sampling --beam 1 --sampling-topp 0.8</span></pre><p id="9db3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">注意，当使用top-k或top-sampling时，我们必须添加<code class="fe nv nw nx nm b">beam=1</code>来抑制当<code class="fe nv nw nx nm b">--beam</code>不等于<code class="fe nv nw nx nm b">--nbest</code>时产生的误差。这好像是个bug。</p><h1 id="4e22" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="f7e7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇博文中，我们使用流行的Fairseq库在书籍摘要上训练了一个经典的transformer模型！虽然生成示例是重复的，但是本文可以作为一个指南，带您在语言建模上运行一个转换器。如果:D感兴趣，看看我的其他帖子</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/top-nlp-libraries-to-use-2020-4f700cdb841f"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">2020年将使用的顶级NLP库</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">AllenNLP，Fast.ai，Spacy，NLTK，TorchText，Huggingface，Gensim，OpenNMT，ParlAI，DeepPavlov</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="oh l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">使用Pytorch微调用于文本生成的GPT2</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">使用Pytorch和Huggingface微调用于文本生成的GPT2。我们在CMU图书摘要数据集上进行训练，以生成…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/controlling-text-generation-from-language-models-6334935e80cf"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">控制语言模型的文本生成</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">控制机器生成文本的样式和内容的实际操作方法</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="oi l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/bert-text-classification-using-pytorch-723dfb8b6b5b"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">使用Pytorch的BERT文本分类</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">文本分类是自然语言处理中的一项常见任务。我们应用BERT，一个流行的变压器模型，对假新闻检测使用…</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="oj l ng nh ni ne nj ks mv"/></div></div></a></div><h1 id="a3e1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="87cb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] A .瓦斯瓦尼，n .沙泽尔，n .帕尔马等。，<a class="ae ky" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你需要的全部</a> (2017)，第31届神经信息处理系统会议</p><p id="706c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] L .邵、s .古乌斯、d .布里兹等。，<a class="ae ky" href="https://arxiv.org/abs/1701.03185" rel="noopener ugc nofollow" target="_blank">用序列对序列模型生成高质量、信息量大的会话响应</a> (2017)，自然语言处理中的经验方法</p><p id="9b61" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3] A. Fan，M. Lewis，Y. Dauphin，【分层神经故事生成】 (2018)，计算语言学协会</p><p id="a8f7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4] A .霍尔茨曼、j .买斯、l .杜等。，<a class="ae ky" href="https://arxiv.org/abs/1904.09751" rel="noopener ugc nofollow" target="_blank">神经文本退化的好奇案例</a> (2019)，国际学习表征会议</p><p id="82b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[5] <a class="ae ky" href="https://github.com/pytorch/fairseq/" rel="noopener ugc nofollow" target="_blank"> Fairseq Github </a>，脸书艾研究</p><p id="db52" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[6] <a class="ae ky" href="https://fairseq.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> Fairseq文档</a>，脸书艾研究</p></div></div>    
</body>
</html>