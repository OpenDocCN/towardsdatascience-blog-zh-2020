<html>
<head>
<title>Deformable Convolutions Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">去神秘化的可变形卷积</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deformable-convolutions-demystified-2a77498699e8?source=collection_archive---------5-----------------------#2020-09-30">https://towardsdatascience.com/deformable-convolutions-demystified-2a77498699e8?source=collection_archive---------5-----------------------#2020-09-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="de58" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可变形卷积越来越受欢迎，并被应用于复杂的计算机视觉任务，如物体检测。在这篇文章中，我将尝试详细解释它们，并阐明它们在未来计算机视觉应用中的重要性。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/93901fbb78fe95135d58c129c2707540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qrbTQDi6fJtfeBWH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://unsplash.com/photos/NE0XGVKTmcA" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="a2c0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">先决条件:</h1><p id="d0ce" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">帖子的读者必须对卷积神经网络有一个基本的了解。如果你不熟悉这个话题，你可以参考这个<strong class="lt iu"> </strong> <a class="ae ky" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">链接</strong> </a>如果你想了解更多关于卷积运算的知识，它实际上是从基本的图像处理中派生出来的，你也可以阅读这个<a class="ae ky" rel="noopener" target="_blank" href="/convolution-vs-correlation-af868b6b4fb5"> <strong class="lt iu">博客</strong> </a>。</p><h1 id="6585" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="6365" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">简而言之，卷积神经网络或CNN是人工智能研究在一个非常艾龙的冬天之后复兴的主要原因之一。基于它们的应用首次展示了人工智能或深度学习的力量，并恢复了该领域的信心。在马文·明斯基指出感知器只能处理线性可分数据，而不能处理最简单的非线性函数(如XOR)后，这种信心已经丧失。<br/>卷积神经网络在计算机视觉领域非常流行，几乎所有最新的应用程序，如谷歌图像、无人驾驶汽车等，都基于它。在非常高的水平上，它们是一种神经网络，其关注局部空间信息，并使用权重共享以分层方式提取特征，这些特征最终以某种特定于任务的方式聚集，以给出特定于任务的输出。</p><p id="8516" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然CNN在视觉识别任务方面表现出色，但在对物体比例、姿态、视点和部分变形的几何变化或几何变换建模时却非常有限。<br/>几何变换是将图像的位置和方向变换为另一个位置和方向的基本变换。<br/>一些基本的几何变换包括缩放、旋转、平移等。<br/>卷积神经网络缺乏模拟几何变化的内部机制，只能使用固定且受用户知识限制的数据扩充来模拟几何变化，因此CNN无法学习用户未知的几何变换。<br/>为了克服这个问题并增加CNN的能力，微软亚洲研究院推出了<a class="ae ky" href="https://arxiv.org/abs/1703.06211" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a><strong class="lt iu"/>。在他们的工作中，他们引入了一种<strong class="lt iu">简单的</strong>、<strong class="lt iu">高效的</strong>和<strong class="lt iu">端到端的</strong>机制，使得CNN能够根据给定的数据学习各种几何变换。</p><h2 id="dc54" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated"><strong class="ak">为什么卷积神经网络无法模拟几何变换？</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/2697607adf126b29e01f54b502a77a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/0*U9bPmjN-iAvhtuNS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图一。</strong>简单卷积运算<strong class="bd nf"> ( </strong> <a class="ae ky" href="https://www.freecodecamp.org/news/content/images/2019/07/convSobel.gif" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nf">来源</strong> </a> <strong class="bd nf"> ) </strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0668d103bd0cedec5d2cb432236d0f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*9P8t7bdbHyDCG4j7.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。2x2矩形内核的最大池操作(<a class="ae ky" href="https://miro.medium.com/max/1204/0*uYmuSGuwM7N-88QT.gif" rel="noopener">来源</a>)</p></figure><p id="c7d7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">CNN对模型几何变换的限制源于用于从特征图采样的核的固定结构。CNN内核使用固定的矩形窗口(<strong class="lt iu">图1 </strong>)在固定位置从输入特征地图中进行采样，汇集层使用相同的矩形内核(<strong class="lt iu">图2 </strong>)以固定比率降低空间分辨率。这引入了各种问题，例如给定CNN层中的所有激活单元具有相同的感受野，即使在不同的空间位置可能存在不同尺度的对象。对于需要精细定位的视觉识别任务，例如物体检测、分割等，需要适应物体的比例并对不同的物体具有不同的感受野大小。</p><h1 id="f9ef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">可变形卷积</h1><p id="d4d1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">高级解释</strong>:</p><p id="d566" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在可变形卷积中，为了将不同对象的尺度考虑在内并根据对象的尺度具有不同的感受野，在标准卷积运算中，将2D偏移添加到规则网格采样位置，从而使前面激活单元的恒定感受野变形。使用额外的卷积层，添加的偏移可以从前面的特征图中获知。因此，所应用的变形以局部、密集和自适应的方式取决于输入特征。添加的可变形卷积层向现有模型添加了非常小的参数和计算，并且可以使用正常的反向传播进行端到端训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/85cedf493d8d635642d297684147ac17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*NQusHGscVYW_UrNYUSoZ3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图3。</strong>正常卷积运算(a)与可变形卷积运算(b，c，d)的采样位置。</p></figure><p id="9c60" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">详解</strong>:</p><p id="38a9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了详细解释可变形卷积，我将首先讨论正常的卷积运算，然后解释将它们转换为可变形卷积的简单思想。</p><p id="f474" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正常的卷积运算包括两个基本步骤:</p><ol class=""><li id="1f3e" class="ni nj it lt b lu mn lx mo ma nk me nl mi nm mm nn no np nq bi translated">使用矩形核对输入图像或特征图的小区域进行采样。</li><li id="ed5c" class="ni nj it lt b lu nr lx ns ma nt me nu mi nv mm nn no np nq bi translated">将采样值乘以矩形核的权重，然后在整个核上对它们求和，以给出单个标量值。</li></ol><p id="185b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我会用方程式和视觉的形式来解释以上两个概念。<br/>让我们先试着用数学方程式来理解。</p><p id="3ba2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">设R是一个3×3的核，用于对输入特征图的一个小区域进行采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/1711311476d27b61a2fcfea607798f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*akFx-VIq1vMJSJAKZatNkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">方程式1。</strong>采样内核</p></figure><p id="ce2c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">那么正常的二维卷积运算的等式将如下图所示，其中<strong class="lt iu"> w </strong>是核的权重，<strong class="lt iu"> x </strong>是输入特征图，<strong class="lt iu"> y </strong>是卷积运算的输出，<strong class="lt iu"> p₀ </strong>是每个核的起始位置，<strong class="lt iu"> pₙ </strong>是r中所有位置的枚举</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b4757d14327eccd6ac5c63f997c19247.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*EoFakKs6Lh_c9qIesfSLLg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">方程式2。</strong>普通卷积运算</p></figure><p id="90db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该等式表示卷积运算，其中采样网格上的每个位置首先乘以权重矩阵的相应值，然后求和以给出标量输出，并且在整个图像上重复相同的运算给出了新的特征图。</p><p id="a34c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面直观地描述了上面解释的操作，其中绿色内核滑过由蓝色矩阵描述的图像，并且相应的权重值与来自图像的采样值相乘，然后求和以给出输出特征图中给定位置的最终输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8d68fb42fc5eca541459db225a4e9e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/0*gcnca70tXo-IW3SH"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图4 </strong>。卷积运算的直观演示</p></figure><p id="3c3d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可变形卷积不是使用简单的固定采样网格，而是将2D偏移引入到上述正常卷积运算中。</p><p id="55b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果R是正常网格，则可变形卷积运算将学习到的偏移增加到网格，从而使网格的采样位置变形。</p><p id="5f6d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可变形卷积运算由下面的等式描述，其中<strong class="lt iu">δpₙ</strong>表示添加到正常卷积运算的偏移。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/52483db79a0110a111a206164d47234c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*RPl4m8Ugix9x_uHMS9VU3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">方程式3 </strong>。变形卷积运算</p></figure><p id="f329" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，由于采样是在不规则和偏移位置上进行的，并且<strong class="lt iu">δpₙ</strong>通常是分数，我们使用双线性插值来实现上述等式。<br/>使用双线性插值是因为当我们向现有采样位置添加偏移时，我们获得的分数点不是网格上定义的位置，为了估计它们的像素值，我们使用双线性插值，使用相邻像素值的2x2网格来估计新变形位置的像素值。</p><p id="c1a2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面给出了用于执行双线性插值和估计分数位置处的像素值的等式，其中p(<strong class="lt iu">p₀+pₙ+</strong><strong class="lt iu">δpₙ)</strong>是变形位置，<strong class="lt iu"> q </strong>列举了输入特征图上的所有有效位置，而<strong class="lt iu"> G(..)</strong>是双线性插值核。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8e10d0f44eff5cb8b7f6618d64efebc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*5sCE1rQdIP--AYPkyNmV9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">方程式4。</strong>双线性插值运算</p></figure><p id="00e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">注:</strong> G(..)是二维的，并且可以根据轴分解成两个一维内核，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/36cfb4df6d05638ec0ccc8fa566b2876.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*rnTTUs3Chz9Nu4le2_ZS9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">方程式5。</strong>轴向双线性插值内核</p></figure><p id="c2bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从视觉上看，可变形卷积的实现如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/5f5a36e367f54f19127097d4d5706a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhYDDt3WFhC0IqSLzUMXpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图五。</strong>可变形卷积运算的可视化表示</p></figure><p id="f4f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如图<strong class="lt iu">图5 </strong>所示，通过在输入特征地图上应用卷积层获得偏移。所使用的卷积核具有与当前卷积层相同的空间分辨率和膨胀。输出偏移字段的分辨率与输入要素地图的分辨率相同，具有2N个通道，其中2N对应于N个2d偏移。</p><h1 id="4e67" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">网络修改详细信息</h1><p id="f0ef" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">可变形卷积层主要应用于卷积网络的最后几层，因为与提取更多基本特征如形状、边缘等的早期层相比，它们更可能包含对象级语义信息。实验结果表明，将可变形卷积应用于最后3个卷积层在诸如对象检测、分割等任务中提供了最佳性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b05c9b15f0d6d6e6bba4ef7309b0260b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*0oJRenb4txHd_Z-mnxNDxA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。标准卷积中固定感受野和变形卷积运算中自适应感受野的图示。</p></figure><h1 id="3383" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用可变形卷积的优点</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ce8eaa6663e85e923f0e235f0c063c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUZxZH1N-tl5P-ciU5NA3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图7。</strong>描绘每个对象的适应性感受野的三联图像。</p></figure><p id="cb46" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用可变形卷积运算的优势在<strong class="lt iu">图7 </strong>中有清晰的描述。如您所见，有4个图像三元组，其中特定三元组中的每个图像描绘了关于特定对象的感受野。如果这是一个正常的卷积运算，那么给定图像中所有物体的感受野应该是相同的。但是正如你所注意到的，在可变形卷积的情况下，感受野根据物体的大小是自适应的。与大尺寸物体相比，小尺寸物体(例如第一组中的汽车)具有较小的感受野。你可以注意到，背景物体的感受野是最大的，与前景物体相比，需要大的感受野来检测背景物体。</p><h1 id="7e88" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论:</h1><p id="1898" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我试图解释可变形卷积，这种卷积在当前新颖的对象检测和分割模型中很容易应用。它们获得动力的主要原因是它们提供了内部机制，使卷积神经网络能够模拟各种空间变换。它们提供了自适应感受野的优势，该感受野从数据中学习并根据对象的规模而变化。</p><p id="b250" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">希望你喜欢这篇文章，如果你有任何疑问或建议，请使用<a class="ae ky" href="https://twitter.com/Perceptron97" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae ky" href="https://www.linkedin.com/in/divyanshu-mishra-ai/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p><h1 id="e5a2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">参考文献:</strong></h1><ol class=""><li id="29d3" class="ni nj it lt b lu lv lx ly ma of me og mi oh mm nn no np nq bi translated">可变形卷积网络。戴，齐，熊，李，张，胡，魏</li></ol></div></div>    
</body>
</html>