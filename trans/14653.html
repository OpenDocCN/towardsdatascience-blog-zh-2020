<html>
<head>
<title>A Beginners Guide to Artificial Neural Network using Tensor Flow &amp; Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用张量流和Keras的人工神经网络初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-artificial-neural-network-using-tensor-flow-keras-41ccd575a876?source=collection_archive---------28-----------------------#2020-10-09">https://towardsdatascience.com/a-beginners-guide-to-artificial-neural-network-using-tensor-flow-keras-41ccd575a876?source=collection_archive---------28-----------------------#2020-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0623" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用人工神经网络构建欺诈检测模型&amp;使用RandomizedSearchCV微调超参数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2e9f56904dd7f539a2e72f01b00f6683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TFZg3usRA-U0xyZQ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="3299" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8227" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">ann(人工神经网络)是深度学习的核心，是机器学习技术的高级版本。人工神经网络涉及以下概念。输入和输出层、隐藏层、隐藏层下的神经元、前向传播和反向传播。简而言之，输入层是独立变量的集合，输出层代表最终输出(因变量)，隐藏层由神经元组成，在其中开发方程并应用激活函数。前向传播讨论如何开发方程以实现最终输出，而后向传播计算梯度下降以相应地更新学习率。关于操作过程的更多信息可以在下面的文章中找到。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/introduction-to-artificial-neural-networks-for-beginners-2d92a2fb9984"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">面向初学者的人工神经网络介绍</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">理解神经网络的概念</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><h1 id="3912" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">深度神经网络</h1><p id="64d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当一个ANN包含一个很深的隐藏层堆栈时，它被称为一个<em class="nf">深度神经网络</em> (DNN)。DNN使用多个权重和偏差项，每个权重和偏差项都需要训练。只要两次通过网络，该算法就可以自动计算梯度下降。换句话说，它可以确定如何调整所有神经元的每个权重和每个偏置项以减少误差。该过程重复进行，除非网络收敛到最小误差。</p><p id="fa98" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">让我们一步一步地运行这个算法:</p><ul class=""><li id="338b" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm nq nr ns nt bi translated">开发训练和测试数据以训练和验证模型输出。因为它遵循参数结构，其中它优化了权重和偏置参数项。所有涉及相关性、异常值处理的统计假设仍然有效，必须进行处理</li><li id="756b" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">输入层由独立变量及其各自的值组成。一个小批量的数据(取决于批量大小)多次通过完整的训练集。每一遍被称为一个时期。纪元越高，训练时间就越长</li><li id="462e" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">每个小批量被传递到输入层，输入层将其发送到第一个隐藏层。计算该层中所有神经元的输出(对于每个小批量)。结果传递到下一层，重复这个过程，直到我们得到最后一层的输出，即输出层。这是向前传递:它类似于进行预测，只是所有中间结果都被保留，因为向后传递需要它们</li><li id="33ad" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">然后使用损失函数测量网络的输出误差，该损失函数将期望输出与网络的实际输出进行比较</li><li id="2808" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">计算每个中子对误差项的科学贡献</li><li id="6262" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">该算法执行梯度下降以基于学习速率(反向传播)调整权重和参数，并且该过程重复进行</li></ul><blockquote class="nz oa ob"><p id="b3c9" class="lr ls nf lt b lu ng ju lw lx nh jx lz oc ni mc md od nj mg mh oe nk mk ml mm im bi translated">随机初始化所有隐藏层的连接权重是很重要的，否则训练将会失败。例如，如果将所有权重和偏差初始化为零，则给定层中的所有神经元将完全相同，因此反向传播将以完全相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，但你的模型将表现得好像每层只有一个神经元:它不会太聪明。相反，如果你随机初始化权重，你会破坏对称性，并允许反向传播来训练一个多样化的神经元团队(Auré lien Gé ron，2017年，第290-291页)</p></blockquote><h1 id="62a4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">激活功能</h1><p id="dd16" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">激活函数是梯度下降的关键。梯度下降不能在平面上移动，因此有一个明确定义的非零导数以允许梯度下降在每一步都取得进展是很重要的。Sigmoid通常用于逻辑回归问题，然而，也有其他流行的选择。</p><h2 id="3b92" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">双曲正切函数</h2><p id="a14d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个函数是S形的，连续的，除了输出范围从-1到+1之外可以微分。在训练开始时，每层的输出或多或少以0为中心，因此有助于加快收敛。</p><h2 id="98b3" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">整流线性单元</h2><p id="b0cc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在Z=0处不可微的连续函数，当Z&lt;0时，它的导数为0。它产生良好的输出，更重要的是有更快的计算。该函数没有最大输出，因此在梯度下降过程中可能出现的一些问题得到了很好的处理。</p><h2 id="34d3" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">为什么我们需要一个激活函数？</h2><p id="190c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">假设f(x) = 2x + 5，g(x) = 3x -1。两个不同神经元的方程，其中x是输入变量，2和3是权重，5和-1是偏差项。在链接这些函数时，我们得到f(g(x)) = 2(3x -1) + 5 = 6x + 3，这也是一个线性方程。没有非线性类似于在深度神经网络中有一个方程。这种场景下的复杂问题空间是处理不了的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ee5411458158be2a9a7718c1d455bd10.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*WzoKsPrC5EAUf4sKUWpw5Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。说明了人工神经网络体系结构中常用的激活函数。作者使用Excel开发的图像。</p></figure><h1 id="302e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">损失函数</h1><p id="c021" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在处理回归问题时，我们不需要对输出层使用任何激活函数。训练回归问题时使用的损失函数是均方误差。然而，训练集中的异常值可以使用平均绝对误差来处理。对于基于回归的任务，Huber损失也是广泛使用的误差函数。</p><p id="d233" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">当误差小于阈值t(通常为1)时，Huber损失是二次的，但是当误差大于t时，Huber损失是线性的。当与均方误差相比时，线性部分允许其对异常值不太敏感，并且二次部分允许比平均绝对误差更快的收敛和更精确的数字。</p><p id="8128" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">分类问题通常使用二元交叉熵或分类交叉熵或稀疏分类交叉熵。二元交叉熵用于二元分类，而分类或稀疏分类交叉熵用于多类分类问题。你可以在下面的链接中找到更多关于损失函数的细节。</p><p id="b313" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">注意:</strong> <em class="nf">分类交叉熵用于因变量的一键表示，当标签以整数形式提供时，使用稀疏分类交叉熵。</em></p><div class="mn mo gp gr mp mq"><a href="https://keras.io/api/losses/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">Keras文件:损失</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">损失函数的目的是计算模型在训练过程中应该寻求最小化的数量。注意…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">keras.io</p></div></div><div class="mz l"><div class="os l nb nc nd mz ne ks mq"/></div></div></a></div><h1 id="00e2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用Python开发人工神经网络</h1><p id="fa40" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用来自<a class="ae ky" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的信用数据来开发一个使用Jupyter Notebook的欺诈检测模型。同样的事情也可以在Google Colab中完成。数据集包含欧洲持卡人在2013年9月的信用卡交易。该数据集显示了两天内发生的交易，其中284，807笔交易中有492笔欺诈。数据集高度不平衡，正类(欺诈)占所有交易的0.172%。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="4eba" class="of la it ou b gy oy oz l pa pb">import tensorflow as tf<br/>print(tf.__version__)</span><span id="1c2b" class="of la it ou b gy pc oz l pa pb">import pandas as pd<br/>import numpy as np</span><span id="d732" class="of la it ou b gy pc oz l pa pb">from sklearn.model_selection import train_test_split<br/>import tensorflow as tf</span><span id="cac3" class="of la it ou b gy pc oz l pa pb">from sklearn import preprocessing</span><span id="f101" class="of la it ou b gy pc oz l pa pb">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Dropout, BatchNormalization</span><span id="2559" class="of la it ou b gy pc oz l pa pb">from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc</span><span id="3be1" class="of la it ou b gy pc oz l pa pb">import matplotlib.pyplot as plt<br/>from tensorflow.keras import optimizers</span><span id="d884" class="of la it ou b gy pc oz l pa pb">import seaborn as sns</span><span id="fa75" class="of la it ou b gy pc oz l pa pb">from tensorflow import keras</span><span id="c0c1" class="of la it ou b gy pc oz l pa pb">import random as rn</span><span id="26d3" class="of la it ou b gy pc oz l pa pb">import os<br/>os.environ["CUDA_VISIBLE_DEVICES"] = "3"<br/>PYTHONHASHSEED=0</span><span id="1916" class="of la it ou b gy pc oz l pa pb">tf.random.set_seed(1234)<br/>np.random.seed(1234)<br/>rn.seed(1254)</span></pre><p id="d9f7" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">数据集由以下属性组成。时间、主要成分、数量和类别。更多信息请参考Kaggle网站。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="3278" class="of la it ou b gy oy oz l pa pb">file = tf.keras.utils<br/>raw_df = pd.read_csv(‘<a class="ae ky" href="https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv'" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv'</a>)<br/>raw_df.head()</span></pre><p id="6d26" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">因为大多数属性是主成分，所以相关性将总是为0(主成分中正交向量的属性)。唯一可能存在异常值的列是金额。对其的快速描述提供了下面概述的统计数据。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="9d3a" class="of la it ou b gy oy oz l pa pb">count    284807.00<br/>mean         88.35<br/>std         250.12<br/>min           0.00<br/>25%           5.60<br/>50%          22.00<br/>75%          77.16<br/>max       25691.16<br/>Name: Amount, dtype: float64</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/07a54ba11680eceda58e963b1bad0d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3tCCFZzO-8Vr422lpCYvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。说明了数据中所有属性的相关矩阵。作者使用Jupyter笔记本开发的图像。</p></figure><p id="e610" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">异常值对于检测欺诈至关重要，因为潜在的假设是，较高的交易可能是欺诈活动的迹象。然而，箱线图没有揭示任何特定的趋势来验证上述假设。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/a81f7cf3627c8f9678aa9c6cdb89e4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4N9yRalmp_6Ga6GIyegVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。说明了欺诈性和非欺诈性活动的金额的箱线图表示。作者使用Jupyter笔记本开发的图像。</p></figure><h2 id="c549" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">准备输入输出和训练测试数据</h2><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="cff3" class="of la it ou b gy oy oz l pa pb">X_data = credit_data.iloc[:, :-1]</span><span id="b533" class="of la it ou b gy pc oz l pa pb">y_data = credit_data.iloc[:, -1]</span><span id="4a20" class="of la it ou b gy pc oz l pa pb">X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 7)</span><span id="038d" class="of la it ou b gy pc oz l pa pb">X_train = preprocessing.normalize(X_train)</span></pre><p id="d1b4" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">数量和主成分分析变量使用不同的尺度，因此数据集被归一化。归一化在梯度下降中起着重要的作用。标准化数据的收敛速度要快得多。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="5d1f" class="of la it ou b gy oy oz l pa pb">print(X_train.shape)<br/>print(X_test.shape)<br/>print(y_train.shape)<br/>print(y_test.shape)</span></pre><p id="de4d" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">输出:</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="cf95" class="of la it ou b gy oy oz l pa pb">(227845, 29) #--Number of records x Number of columns<br/>(56962, 29)<br/>(227845,)<br/>(56962,)</span></pre><h2 id="d244" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">开发人工神经网络层</h2><p id="a515" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">上面的输出表明我们有29个独立变量要处理，因此输入层的形状是29。任何人工神经网络结构的一般结构概述如下。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="822c" class="of la it ou b gy oy oz l pa pb">+----------------------------+----------------------------+<br/> |      Hyper Parameter       |   Binary Classification    |<br/> +----------------------------+----------------------------+<br/> | # input neurons            | One per input feature      |<br/> | # hidden layers            | Typically 1 to 5           |<br/> | # neurons per hidden layer | Typically 10 to 100        |<br/> | # output neurons           | 1 per prediction dimension |<br/> | Hidden activation          | ReLU, Tanh, sigmoid        |<br/> | Output layer activation    | Sigmoid                    |<br/> | Loss function              | Binary Cross Entropy       |<br/> +----------------------------+----------------------------+</span><span id="bb8d" class="of la it ou b gy pc oz l pa pb">+-----------------------------------+----------------------------+<br/> |          Hyper Parameter          | Multiclass Classification  |<br/> +-----------------------------------+----------------------------+<br/> | # input neurons                   | One per input feature      |<br/> | # hidden layers                   | Typically 1 to 5           |<br/> | # neurons per hidden layer        | Typically 10 to 100        |<br/> | # output neurons                  | 1 per prediction dimension |<br/> | Hidden activation                 | ReLU, Tanh, sigmoid        |<br/> | Output layer activation           | Softmax                    |<br/> | Loss function                     | "Categorical Cross Entropy |<br/> | Sparse Categorical Cross Entropy" |                            |<br/> +-----------------------------------+----------------------------+</span></pre><p id="ce8a" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">密集函数的输入</strong></p><ol class=""><li id="a2c0" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm pf nr ns nt bi translated">单位-输出的尺寸</li><li id="c600" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">激活—激活功能，如果未指定，则不使用任何内容</li><li id="b3c6" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">use _ bias表示图层是否使用偏置矢量的布尔值</li><li id="052c" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">内核初始化器——内核权重的初始化器</li><li id="a3d4" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">bias_initializer —偏差向量的初始值设定项。</li></ol><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="df88" class="of la it ou b gy oy oz l pa pb">model = Sequential(layers=None, name=None)<br/>model.add(Dense(10, input_shape = (29,), activation = 'tanh'))<br/>model.add(Dense(5, activation = 'tanh'))<br/>model.add(Dense(1, activation = 'sigmoid'))</span><span id="acb5" class="of la it ou b gy pc oz l pa pb">sgd = optimizers.Adam(lr = 0.001)</span><span id="6f01" class="of la it ou b gy pc oz l pa pb">model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics=['accuracy'])</span></pre><p id="556b" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">架构概要</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="a070" class="of la it ou b gy oy oz l pa pb">model.summary()</span><span id="4658" class="of la it ou b gy pc oz l pa pb">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense (Dense)                (None, 10)                300       <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 5)                 55        <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 1)                 6         <br/>=================================================================<br/>Total params: 361<br/>Trainable params: 361<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><h2 id="8172" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">让我们试着理解上面的输出(使用两个隐藏层提供输出解释):</h2><ol class=""><li id="35d8" class="nl nm it lt b lu lv lx ly ma pg me ph mi pi mm pf nr ns nt bi translated">我们已经创建了一个具有一个输入层、两个隐藏层和一个输出层的神经网络</li><li id="8f76" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">输入层有29个变量和10个神经元。因此，权重矩阵的形状为10×29，偏差矩阵的形状为10×1</li><li id="5a6a" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">第1层中的参数总数= 10 x 29 + 10 x 1 = 300</li><li id="d557" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">第一层使用tanh作为激活函数，有10个输出值。第二层有5个神经元，使用10个输入，因此权重矩阵是5×10，偏差矩阵是5×1</li><li id="bff6" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">第2层中的总参数= 5 x 10 + 5 x 1 = 55</li><li id="6bef" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pf nr ns nt bi translated">最后，输出层具有一个神经元，但是它具有来自隐藏层2的5个不同输入，并且具有偏置项，因此神经元的数量= 5+1=6</li></ol><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="92b5" class="of la it ou b gy oy oz l pa pb">model.fit(X_train, y_train.values, batch_size = 2000, epochs = 20, verbose = 1)</span><span id="68b1" class="of la it ou b gy pc oz l pa pb">Epoch 1/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.9847<br/>Epoch 2/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.1029 - accuracy: 0.9981<br/>Epoch 3/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9983<br/>Epoch 4/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9986<br/>Epoch 5/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9987<br/>Epoch 6/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9988<br/>Epoch 7/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 0.9988<br/>Epoch 8/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0152 - accuracy: 0.9988<br/>Epoch 9/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0137 - accuracy: 0.9989<br/>Epoch 10/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0125 - accuracy: 0.9989<br/>Epoch 11/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9989<br/>Epoch 12/20<br/>114/114 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9989<br/>Epoch 13/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0104 - accuracy: 0.9989<br/>Epoch 14/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 0.9989<br/>Epoch 15/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0095 - accuracy: 0.9989<br/>Epoch 16/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 0.9989<br/>Epoch 17/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 0.9989<br/>Epoch 18/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 0.9989<br/>Epoch 19/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0084 - accuracy: 0.9989<br/>Epoch 20/20<br/>114/114 [==============================] - 0s 1ms/step - loss: 0.0082 - accuracy: 0.9989</span></pre><p id="5dfd" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">评估输出</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="f902" class="of la it ou b gy oy oz l pa pb">X_test = preprocessing.normalize(X_test)</span><span id="8bb2" class="of la it ou b gy pc oz l pa pb">results = model.evaluate(X_test, y_test.values)</span><span id="6423" class="of la it ou b gy pc oz l pa pb">1781/1781 [==============================] - 1s 614us/step - loss: 0.0086 - accuracy: 0.9989</span></pre><h2 id="8202" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">使用张量板分析学习曲线</h2><p id="098b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TensorBoard是一个非常棒的交互式可视化工具，可用于查看训练过程中的学习曲线，比较多次跑步的学习曲线，分析训练指标等等。该工具随TensorFlow自动安装。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="80b2" class="of la it ou b gy oy oz l pa pb">import os<br/>root_logdir = os.path.join(os.curdir, “my_logs”)</span><span id="6a25" class="of la it ou b gy pc oz l pa pb">def get_run_logdir():<br/> import time<br/> run_id = time.strftime(“run_%Y_%m_%d-%H_%M_%S”)<br/> return os.path.join(root_logdir, run_id)</span><span id="9727" class="of la it ou b gy pc oz l pa pb">run_logdir = get_run_logdir()</span><span id="d8de" class="of la it ou b gy pc oz l pa pb">tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)</span><span id="6cc0" class="of la it ou b gy pc oz l pa pb">model.fit(X_train, y_train.values, batch_size = 2000, epochs = 20, verbose = 1, callbacks=[tensorboard_cb])</span><span id="52b3" class="of la it ou b gy pc oz l pa pb">%load_ext tensorboard<br/>%tensorboard --logdir=./my_logs --port=6006</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/22deedee9fe9fe68b0ee16b9bfeb8c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIpNUAMMup0u0oD16RLlNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。说明了人工神经网络运行的张量板输出。作者使用Jupyter笔记本开发的图像。</p></figure><h2 id="1937" class="of la it bd lb og oh dn lf oi oj dp lj ma ok ol ll me om on ln mi oo op lp oq bi translated">超调模型参数</h2><p id="244b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如前所述，对于有多少隐藏层或多少神经元最适合一个问题空间，没有预定义的规则。我们可以使用RandomizedSearchCV或GridSearchCV来优化一些参数。可以微调的参数如下:</p><ul class=""><li id="529c" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm nq nr ns nt bi translated">隐藏层数</li><li id="d401" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">隐藏层中的神经元</li><li id="9df3" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">【计算机】优化程序</li><li id="38fc" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">学习率</li><li id="3879" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">世</li></ul><p id="2c48" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">声明开发模型的函数</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="6297" class="of la it ou b gy oy oz l pa pb">def build_model(n_hidden_layer=1, n_neurons=10, input_shape=29):<br/>    <br/>    # create model<br/>    model = Sequential()<br/>    model.add(Dense(10, input_shape = (29,), activation = 'tanh'))</span><span id="b76e" class="of la it ou b gy pc oz l pa pb">for layer in range(n_hidden_layer):<br/>        model.add(Dense(n_neurons, activation="tanh"))</span><span id="42f1" class="of la it ou b gy pc oz l pa pb">model.add(Dense(1, activation = 'sigmoid'))<br/>    <br/>    # Compile model</span><span id="46dd" class="of la it ou b gy pc oz l pa pb">model.compile(optimizer ='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])<br/>    <br/>    return model</span></pre><p id="65d2" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">使用包装类克隆模型</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="c6bf" class="of la it ou b gy oy oz l pa pb">from sklearn.base import clone<br/> <br/>keras_class = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_model,nb_epoch = 100,<br/> batch_size=10)<br/>clone(keras_class)</span><span id="39dc" class="of la it ou b gy pc oz l pa pb">keras_class.fit(X_train, y_train.values)</span></pre><p id="b7ab" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">创建随机搜索网格</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="5f8f" class="of la it ou b gy oy oz l pa pb">from scipy.stats import reciprocal<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="604b" class="of la it ou b gy pc oz l pa pb">param_distribs = {<br/> “n_hidden_layer”: [1, 2, 3],<br/> “n_neurons”: [20, 30],<br/># “learning_rate”: reciprocal(3e-4, 3e-2),<br/># “opt”:[‘Adam’]<br/>}</span><span id="393b" class="of la it ou b gy pc oz l pa pb">rnd_search_cv = RandomizedSearchCV(keras_class, param_distribs, n_iter=10, cv=3)</span><span id="0c76" class="of la it ou b gy pc oz l pa pb">rnd_search_cv.fit(X_train, y_train.values, epochs=5)</span></pre><p id="602f" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu">检查最佳参数</strong></p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="62ba" class="of la it ou b gy oy oz l pa pb">rnd_search_cv.best_params_</span><span id="2c59" class="of la it ou b gy pc oz l pa pb">{'n_neurons': 30, 'n_hidden_layer': 3}</span><span id="8c0c" class="of la it ou b gy pc oz l pa pb">rnd_search_cv.best_score_</span><span id="0e11" class="of la it ou b gy pc oz l pa pb">model<!-- --> <!-- -->=<!-- --> <!-- -->rnd_search_cv.best_estimator_.model</span></pre><p id="4a53" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">优化器也应该微调，因为它们影响梯度下降、收敛和学习率的自动调整。</p><ul class=""><li id="6c9f" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm nq nr ns nt bi translated"><strong class="lt iu"><em class="nf">Adadelta—</em></strong>Adadelta是Adagrad的更健壮的扩展，它基于梯度更新的移动窗口来调整学习速率，而不是累积所有过去的梯度</li><li id="1e9e" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nf">随机梯度下降— </em> </strong>常用。需要使用搜索网格来微调学习率</li><li id="4a69" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nf"> Adagrad — </em> </strong>对于其他优化器，学习率对于所有参数和每个周期都是恒定的。然而，Adagrad在处理误差函数的导数时，改变每个参数和每个时间步长“t”的学习速率“η”</li><li id="f217" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nf"> ADAM — </em> </strong> Adam(自适应矩估计)与一阶和二阶动量一起工作，以防止跳过局部最小值。亚当保存了过去梯度的指数衰减平均值</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/aa94901db81d0d8fc5473e0733d16a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*UfQmy5bKpXN3yFhj2uTjHw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。展示了不同优化器之间的收敛性。图片来自GIPHY。</p></figure><p id="96e8" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">一般来说，通过增加层数而不是每层神经元的数量可以获得更好的输出。</p><h1 id="b9b9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="140a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">奥雷连戈罗恩(2017)。<em class="nf">用Scikit-Learn和TensorFlow进行动手机器学习:构建智能系统的概念、工具和技术</em>。塞瓦斯托波尔，加利福尼亚州:奥赖利媒体</p></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><p id="0820" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="nf">关于作者:高级分析专家和管理顾问，帮助公司通过对组织数据的商业、技术和数学的组合找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；可以在</em><a class="ae ky" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"><em class="nf">LinkedIn</em></a><em class="nf">和</em><a class="ae ky" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"><em class="nf">Twitter</em></a><em class="nf">上联系我。</em></p></div></div>    
</body>
</html>