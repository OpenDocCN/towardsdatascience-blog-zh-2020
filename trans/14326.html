<html>
<head>
<title>Deep Learning to Jump</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习跳跃</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-to-jump-e507103ab8d3?source=collection_archive---------50-----------------------#2020-10-02">https://towardsdatascience.com/deep-learning-to-jump-e507103ab8d3?source=collection_archive---------50-----------------------#2020-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8325" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Maxime Bergeron &amp; Ivan Sergienko，Riskfuel</h2></div><blockquote class="kf kg kh"><p id="0886" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">在这篇简短的笔记中，我们描述了一个<strong class="kl ir">跳转单元</strong>，它可以用来用一个简单的神经网络拟合阶跃函数。我们的动机来自于经常出现不连续性的定量金融问题。</p></blockquote><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/e2baccbde0bbe11a6ee548752de0eba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*0BS1rngiu3hj05W_5vxk7g.jpeg"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="4391" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated"><strong class="kl ir"> <em class="kk">来自《走向数据科学》编辑的提示:</em> </strong> <em class="kk">虽然我们允许独立作者根据我们的</em> <a class="ae lu" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="kk">规则和指导方针</em> </a> <em class="kk">发表文章，但我们不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae lu" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="kk">读者术语</em> </a> <em class="kk">。</em></p><h1 id="2348" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">为什么要跳？</h1><p id="5b3e" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr lr mp ku kv ls mq ky kz lt mr lc ld le ij bi translated">不连续函数在金融工具中是常见的。例如，下图显示了典型的五年期固定利率债券的价格，票面利率为半年。我们设定的票面利率高于贴现率，所以债券的价值保持在票面价值100美元以上。如果你不熟悉债券定价，这里有一本很好的入门书<a class="ae lu" href="https://thismatter.com/money/bonds/bond-pricing.htm" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ac39b42f2daa7fd1fa9804533ea66821.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*io9I1He04qKy0xW5KjJhmg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="3ac4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">为了我们的目的，需要注意的重要事情是在每个息票支付日发生的跳跃。这仅仅是因为钱不可能凭空产生。证券所有者的“财富”在息票前后保持不变。因此，我们有:</p><blockquote class="kf kg kh"><p id="3738" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">息票前价值=息票后价值+息票现金。</p></blockquote><p id="1a7c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">在行权日，更复杂的路径依赖型金融衍生品的价值也会出现类似的跃升。这里的一个经典例子是百慕大互换期权，这是一种用于管理抵押贷款提前还款风险的流行工具。百慕大式期权可以在价值可能跃升的预定日期行使。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/40b3ebfd21afb113251a6a22b1c91fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*l0uDGVAP_sZZhdMP93hVhA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="788a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">为了使事情具体化，我们将把注意力集中在学习具有单一向下跳跃的分段常数函数的子问题上。我们生成的训练数据如上图所示。请读者跟随我们在<a class="ae lu" href="https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb" rel="noopener ugc nofollow" target="_blank">这个Jupyter笔记本</a>中的代码。</p><h1 id="ebd3" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">Sigmoid函数</h1><p id="6e86" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr lr mp ku kv ls mq ky kz lt mr lc ld le ij bi translated">那么我们机器如何学习跳跃呢？这里一个自然的方法是<em class="kk"> </em>使用一个s形<em class="kk">。</em>然而，在我们的问题中，步骤是尖锐的。息票支付日期左边的所有点都严格位于右边的点之上，这导致了真正的不连续。拟合简单的一维神经网络</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/aff01a1cbc9d3abb42d589db29f4c6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*di7gqKMLdz3nTRBMzY6gXQ.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="1371" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">包括夹在两个线性层之间的s形激活函数需要初始线性层中的无界系数(权重)。数量无限增长对于数值方法来说总是坏消息，但对于神经网络来说情况更糟。事实上，这导致了臭名昭著的爆炸梯度问题，使最佳参数值几乎不可能学习。我们将不得不通过保持大而有限的权重来进行妥协，从而导致输入的一个区域(就在跳跃附近)难以消除显著的误差。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5560a16ed3a389bb0a48f57e45aee0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*0HOsJopEe2THEENTnhlvtw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="7f23" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated"><em class="kk">我们能做得更好吗？</em></p><h1 id="d8d9" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">亥维赛函数</h1><p id="cf82" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr lr mp ku kv ls mq ky kz lt mr lc ld le ij bi translated">为什么不简单地用…一个跳跃来学习一个跳跃呢？用不连续的激活来代替上面一维网络中的sigmoid函数是很有诱惑力的。这里最简单的候选函数是Heaviside阶跃函数<em class="kk"> H(x) </em>，对于<em class="kk"> x &lt; 0 </em>它等于<em class="kk"> 0 </em>，否则等于<em class="kk"> 1 </em>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/3b19dfc22c4ef3fafd720c5192877c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vt_l-IL-5J3P2UfknCJz6w.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="6b35" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">这是一个显而易见的想法，它会立即导致如下图所示的失败:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/79426f10edf4a33a9ac2e9d26ad37eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*4hZkVvPymKVSU9yml0JPrg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="7a1d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">为了理解这里的问题，让我们看看数学。我们的小神经网络对应的损失函数是:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mz"><img src="../Images/4508aca0bcfe94ac739a15e31f9e5bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FIu7MkHDXolhmxwEppE0VQ.png"/></div></div></figure><p id="b5ee" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">这里，<em class="kk"> yᵢ </em>是对应于时间<em class="kk"> tᵢ的训练数据值，</em>子脚本<em class="kk"> w </em>项是最后一个线性层的权重，子脚本<em class="kk"> b </em>项是来自两个线性层的偏差。敏锐的读者会注意到，在不失一般性的情况下，我们已经将第一个线性层的权重设置为<em class="kk"> 1 </em>，剩余的权重和偏差决定了台阶的大小和位置。深度学习所依赖的用于最小化误差函数的梯度下降方法需要使用一阶导数。在这方面，其中一个问题很大:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi na"><img src="../Images/f88944cc12326ce6c3ffeb08da62e39c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODvFjlXxRS86AbO4-k7Gww.png"/></div></div></figure><p id="46e4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">这里的问题是第二个因素，当<em class="kk"> b </em>和<em class="kk"> t </em>项相互抵消时，这个因素就会爆发。下面的图清楚地说明了这个问题:函数是分段常数！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f559c3b520c49fea6deda4214b945257.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*c2_Jw6oJYa0a-f-MeJFTkg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="7feb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">虽然实际的最小值是我们期望的，但基于梯度的方法永远达不到它。相反，这一过程陷入了一个小平台。</p><p id="f55d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">事实上，这个问题是sigmoid函数成为机器学习的主要内容的原因之一。用sigmoidal函数代替Heaviside函数允许通过梯度下降成功地训练复杂的神经网络，例如多层感知器。</p><p id="54ce" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">在这一点上，可能会开始觉得我们在兜圈子。s形函数在拟合急剧跳跃方面用处有限，但它们的引入是为了解决Heaviside函数的明显问题。</p><h1 id="8571" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">跳跃单元</h1><p id="ee94" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr lr mp ku kv ls mq ky kz lt mr lc ld le ij bi translated">下表总结了前面两个部分，并显示了当面临学习锐阶跃函数的任务时，我们对这两个激活函数的喜欢和不喜欢之处:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/ccf2e5ddea1607ec598d528239ac2a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgYH2GsidrRb8fKK6Ht0lA.png"/></div></div></figure><p id="c725" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">一个自然的问题是，我们是否可以将这两种功能结合起来，保留我们喜欢的功能，丢弃我们不喜欢的功能。进入<em class="kk">跳转单元</em>，如下图所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nc"><img src="../Images/c3a68176ab4f0a89520430e2bf6b9ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fFwYVjUcCBUYdFZB.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="e473" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">它由三个线性节点以及平行排列的sigmoid和Heaviside激活函数组成。为了理解这个单元如何工作，考虑它编码的等式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nd"><img src="../Images/b2d1b83d712b84443dc5b3140fd2cbc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aEJrI2cW3cNnIjgwTu_qBg.png"/></div></div></figure><p id="13a9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">为了简化方程，我们省略了sigmoid激活之后的线性层中的偏置项，因为它是由Heaviside激活之后的线性层的偏置项来解决的。现在让我们看看之前有问题的导数:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi ne"><img src="../Images/620b189a8fe98f39a5bd2457d3a1d0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_WTe7Vp1bKBAv6ccojf-g.png"/></div></div></figure><p id="9300" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">由于麻烦的偏差项现在出现在<em class="kk"> S(-) </em>和<em class="kk">H(-)</em>的论证中，其梯度在大多数点不再消失，网络能够<em class="kk">学习</em>。请注意，在这个过程的最后，我们还希望sigmoid激活后的线性层的重量消失，这样只有Heaviside贡献保留下来。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nf"><img src="../Images/a34eb9dab8bc3303a5fe818e56feea30.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*Vv5nZqyh_goYCgh3ke1kuA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="60d5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">上面的图显示了我们的跳转单元的MSE误差作为它的两个关键参数的函数。我们看到，沿着偏置轴的阶跃之间的平台现在是倾斜的，允许基于梯度的算法学习全局最小值。</p><p id="d243" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">最后，我们画出结果函数，瞧！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c663315566f480135df86b3da579355d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*lE-7LstL8IMJ2D9_d3C53A.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="5afd" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">结论</h1><p id="b50e" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr lr mp ku kv ls mq ky kz lt mr lc ld le ij bi translated">总之，我们已经展示了如何结合sigmoidal和Heaviside激活函数的优点和缺点来产生能够通过梯度下降学习不连续阶跃函数的跳跃单元。</p><p id="516f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lr kt ku kv ls kx ky kz lt lb lc ld le ij bi translated">我们鼓励读者在<a class="ae lu" href="https://github.com/Riskfuel-Collaborations/jump-unit/blob/main/notebook/JumpUnit.ipynb" rel="noopener ugc nofollow" target="_blank">这款Jupiter笔记本中亲自尝试一下！</a></p></div></div>    
</body>
</html>