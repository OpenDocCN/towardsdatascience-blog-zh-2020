<html>
<head>
<title>Credit Risk Analysis with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的信用风险分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/credit-risk-analysis-with-machine-learning-736e87e95996?source=collection_archive---------10-----------------------#2020-10-13">https://towardsdatascience.com/credit-risk-analysis-with-machine-learning-736e87e95996?source=collection_archive---------10-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/396a3f54f448f11d8a1ed4ae70bf6192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WZkQz9TsfwsCdE3ATs0ow.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迈克尔·朗米尔在Unsplash<a class="ae jd" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">上的照片</a></p></figure><div class=""/><div class=""><h2 id="79c1" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用XGBoost、LightGBM和CatBoost预测客户违约风险</h2></div><p id="d585" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="lr">来自《走向数据科学》编辑的提示:</em> </strong> <em class="lr">虽然我们允许独立作者根据我们的</em> <a class="ae jd" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="lr">规则和指南</em> </a> <em class="lr">发表文章，但我们并不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae jd" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="lr">读者术语</em> </a> <em class="lr">。</em></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="98f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">信用风险</strong>与客户无法履行合同义务的可能性有关，如抵押贷款、信用卡债务和其他类型的贷款。</p><p id="2d16" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最大限度地降低违约风险是金融机构主要关心的问题。出于这个原因，商业和投资银行、风险投资基金、资产管理公司和保险公司等越来越依赖技术来预测哪些客户更有可能停止偿还债务。</p><p id="407a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">机器学习模型一直在帮助这些公司提高信用风险分析的准确性，提供了一种提前识别潜在债务人的科学方法。</p><p id="9e25" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将建立一个模型来预测<a class="ae jd" href="https://nubank.com.br/en/about-us/" rel="noopener ugc nofollow" target="_blank"> Nubank </a>的客户违约风险，Nubank是一家著名的巴西金融科技公司。</p><h1 id="eb64" class="lz ma jg bd mb mc md me mf mg mh mi mj km mk kn ml kp mm kq mn ks mo kt mp mq bi translated">关于数据</h1><p id="6511" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">Nubank 是一家巴西数字银行，也是拉丁美洲最大的金融科技公司之一。众所周知，它是一家数据驱动的公司，利用技术来制定决策和改善服务。</p><p id="c9cf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据集可以在<a class="ae jd" href="http://dl.dropboxusercontent.com/s/xn2a4kzf0zer0xu/acquisition_train.csv?dl=0" rel="noopener ugc nofollow" target="_blank">这里</a>下载。一些私人信息被散列以保持数据匿名。</p><h1 id="d9d2" class="lz ma jg bd mb mc md me mf mg mh mi mj km mk kn ml kp mm kq mn ks mo kt mp mq bi translated">数据分析</h1><p id="d7f6" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我们正在为45，000名客户处理包含43个特征的数据集。<code class="fe mw mx my mz b">target_default</code>是一个真/假特征，是我们试图预测的目标变量。在探索数据集之后，我们发现一些特征具有异常值和缺失值。其他不会给模型增加价值的变量都被删除了(你可以在这里查看代码和完整的解释<a class="ae jd" href="https://github.com/rmpbastos/data_science/blob/master/Credit_Risk_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="ab03" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的直方图有助于我们可视化数字特征的分布:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/03baa900dd0d93fdb8ceb67c7ad15a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTuUlgg58gzQJGevTVyW5Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图一。数字特征的直方图</p></figure><p id="9bec" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述要素具有需要处理的缺失值。如我们所见，它们呈偏态分布，这表明我们应该用每个要素的中值来填充缺失值。</p><p id="3a76" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">是时候处理剩余列中缺失的值了。我们根据每个特性的特殊性填充这些值，如下所示:</p><ul class=""><li id="d724" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">分类变量将用最常出现的值填充。</li><li id="a038" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">数值变量将用它们的中值填充。</li><li id="47fc" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">在某些特定的情况下，我们将用零填充缺失的值，因为有理由相信不是每个客户端都有分配给这些变量的值。</li></ul><p id="4df3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">清理数据集并处理缺失值后，我们现在使用以下功能:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2ec525cef45bd181a2e9bc0851330e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*d6a1XHDxjoE-CouGfI_udg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图二。功能列表</p></figure><p id="68b6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在设置机器学习算法之前，我们需要执行一些预处理。考虑到大多数机器学习算法在处理数字输入时效果更好，我们将使用Scikit Learn的<code class="fe mw mx my mz b">LabelEncoder</code>对二进制变量和pandas的<code class="fe mw mx my mz b">get_dummies</code>对其他分类变量进行预处理。</p><h1 id="fc68" class="lz ma jg bd mb mc md me mf mg mh mi mj km mk kn ml kp mm kq mn ks mo kt mp mq bi translated">机器学习模型</h1><p id="f920" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">我们正在试验以下3种梯度增强算法，以确定哪一种能产生更好的结果:</p><ul class=""><li id="e1ba" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">XGBoost</li><li id="e656" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">LightGBM</li><li id="5d00" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">CatBoost</li></ul><p id="5fff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在建立模型之前，我们需要将数据分成训练集和测试集。之后，由于我们正在处理一个不平衡的数据集，我们将分别使用<code class="fe mw mx my mz b">StandardScaler</code>和<code class="fe mw mx my mz b">RandomUnderSampler</code>对训练集进行标准化和重新采样。</p><p id="b7c0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">评估指标</strong></p><p id="a974" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于模型的评估，值得一提的是，我们应该考虑将<code class="fe mw mx my mz b">Precision</code>、<code class="fe mw mx my mz b">Recall</code>和<code class="fe mw mx my mz b">F1 Score</code>作为评估指标，原因如下:</p><ul class=""><li id="d3a6" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated">精度将会给我们正确的肯定识别的比例。它可以定义为:</li></ul><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/3386a1111acafcee50ab87b055a821ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*XenOYu3cwthhKrElQTGEnw.png"/></div></div></figure><ul class=""><li id="cfa7" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><strong class="kx jh">回忆</strong>将确定被正确识别的真实阳性的比例，它可以被定义为:</li></ul><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c36e748958b496eef6d642a858e2a7f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*Iyn3gN9ymcMiySLL03SIaw.png"/></div></figure><ul class=""><li id="9e82" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><strong class="kx jh"> F1分数</strong>是一个有用的指标，当我们需要在精确度和召回率之间寻求平衡时。该公式定义为:</li></ul><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e6df00c27f38ad35731b8e96118a76ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*sSfcPijuYNndAJ5baX6slw.png"/></div></figure><p id="4db2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于我们的目标是最小化公司损失，预测客户违约的风险，良好的召回率是可取的，因为我们想要确定确实倾向于停止支付债务的客户的最大数量，因此，我们正在追求少量的<em class="lr">假阴性</em>。</p><p id="cd54" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，我们还寻求尽量减少误报的数量，因为我们不希望客户被错误地认定为违约者。因此，良好的精确率也是期望的。</p><p id="7417" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，在精确度和召回率之间总是有一个折衷。对于本文，我们选择更加强调回忆，将其作为我们的评估标准。</p><p id="7d88" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们还使用交叉验证来获得更好的结果。<code class="fe mw mx my mz b">cross_validate</code>方法不是简单地将数据分成训练和测试集，而是将我们的训练数据分成k个折叠，从而更好地利用数据。在我们的例子中，我们将执行5重交叉验证，因为我们让默认的k值。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/00f5c21f82c5fff26330f13f5d8a688f.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*I3xTkb_C_yHafNN2ZI5kqg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图三。回忆价值观</p></figure><p id="68e1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，所有三个模型都产生了相似的结果。我们现在将调整模型上的一些超参数，看看我们是否可以实现更高的分值。这里使用的方法是<code class="fe mw mx my mz b">GridSearchCV</code>，它将搜索每个估计器的指定参数值。</p><p id="b4f4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个模型的超参数调整如下:</p><h2 id="a9dd" class="ny ma jg bd mb nz oa dn mf ob oc dp mj le od oe ml li of og mn lm oh oi mp oj bi translated">XGBoost</h2><p id="9902" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">对于XGBoost模型，我们将根据<a class="ae jd" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>调整以下超参数:</p><ul class=""><li id="43d6" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">n_estimators</code> -模型中的树木数量</li><li id="068b" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">max_depth</code> -一棵树的最大深度</li><li id="17cd" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">min_child_weight</code> -一个孩子所需实例重量的最小总和</li><li id="3f4e" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">gamma</code> -在树的叶节点上进行进一步划分所需的最小损失减少</li><li id="550b" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">learning_rate</code> -更新中使用步长收缩来防止过度拟合</li></ul><pre class="nb nc nd ne gt ok mz ol om aw on bi"><span id="9f7d" class="ny ma jg mz b gy oo op l oq or">param_grid = {'n_estimators': range(0,1000,50),<br/>              'max_depth': [1, 3, 5],<br/>              'min_child_weight': [1, 3, 6],<br/>              'gamma': [0, 1, 5],<br/>              'learning_rate': [0.0001, 0.001, 0.01, 0.1]}</span></pre><p id="efe8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">最佳召回率:{'n_estimators': 50，'</em> max_depth': 3，' min_child_weight': 6，' gamma': 1，' learning _ rate ':0.0001<em class="lr">}</em></p><h2 id="4754" class="ny ma jg bd mb nz oa dn mf ob oc dp mj le od oe ml li of og mn lm oh oi mp oj bi translated">LightGBM</h2><p id="17a5" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">现在，转到LightGBM模型，另一个基于树的学习算法，我们将参考<a class="ae jd" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank">文档</a>调整以下超参数:</p><ul class=""><li id="63ba" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">max_depth</code> -一棵树的最大深度</li><li id="dc07" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">learning_rate</code> -收缩率</li><li id="8786" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">num_leaves</code> -一棵树的最大叶子数量</li><li id="14b2" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">min_data_in_leaf</code> -一片树叶中的最小数据量</li></ul><pre class="nb nc nd ne gt ok mz ol om aw on bi"><span id="b2c7" class="ny ma jg mz b gy oo op l oq or">param_grid = {'max_depth': np.arange(5, 75, 10),<br/>              'learning_rate' : [0.001, 0.01, 0.1],<br/>              'num_leaves': np.arange(20, 220, 50),<br/>              'min_data_in_leaf': np.arange(100, 1000, 100)}</span></pre><p id="1ede" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">最佳召回率:{</em>' learning _ rate ':0.69，' max _ depth ':0.01，' num_leaves': 70，' min _ data _ in _ leaf ':400<em class="lr">}</em></p><h2 id="c1b8" class="ny ma jg bd mb nz oa dn mf ob oc dp mj le od oe ml li of og mn lm oh oi mp oj bi translated">CatBoost</h2><p id="3f2d" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">最后，我们将搜索CatBoost的超参数值，这是我们的第三个梯度增强算法。根据<a class="ae jd" href="https://catboost.ai/docs/concepts/parameter-tuning.html" rel="noopener ugc nofollow" target="_blank">文档</a>，将调整以下超参数:</p><ul class=""><li id="1eae" class="nf ng jg kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">depth</code> -树的深度</li><li id="6e65" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">learning_rate</code>——我们已经知道，学习率</li><li id="7653" class="nf ng jg kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><code class="fe mw mx my mz b">l2_leaf_reg</code> -成本函数的L2正则化项的系数</li></ul><pre class="nb nc nd ne gt ok mz ol om aw on bi"><span id="22fd" class="ny ma jg mz b gy oo op l oq or">param_grid = {'depth': [6, 8, 10],<br/>              'learning_rate': [0.03, 0.1],<br/>              'l2_leaf_reg': [1, 5, 10]}</span></pre><p id="c4a3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">最佳召回率:{</em>' depth ':0.65，' l2_leaf_reg': 5，' learning_rate': 0.03 <em class="lr"> } </em></p><h2 id="ad71" class="ny ma jg bd mb nz oa dn mf ob oc dp mj le od oe ml li of og mn lm oh oi mp oj bi translated">在测试集上评估模型</h2><p id="9ad6" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">调整超参数后，所有三个模型都显示出更好的结果。值得一提的是，XGBoost的得分有了很大的提高，而LightGBM和CatBoost的得分则略有提高。</p><p id="53c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可以检查这些模型在<strong class="kx jh">测试集</strong>上的表现。为了帮助我们将结果可视化，我们为每个人绘制了一个<strong class="kx jh">混淆矩阵</strong>。</p><p id="0358" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> XGBoost </strong></p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/3caae39bf5c7a38887e084f3a2b7e108.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*ijSKxj3vUm1CHoUpq_gdNg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4。XGBoost混淆矩阵</p></figure><p id="14f7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">灯GBM </strong></p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/24af2e6520b88abccb175c48f37063b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*wNNFOGBe3ozdX7HRwVSGCw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图五。LightGBM混淆矩阵</p></figure><p id="7cac" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> CatBoost </strong></p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c5e70a8433266093925b2085a24c10b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*Pv6Mdegvq9b3lWef5Yin7w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图六。CatBoost混淆矩阵</p></figure><h1 id="4479" class="lz ma jg bd mb mc md me mf mg mh mi mj km mk kn ml kp mm kq mn ks mo kt mp mq bi translated">结论</h1><p id="0cbf" class="pw-post-body-paragraph kv kw jg kx b ky mr kh la lb ms kk ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">这篇文章的主要目标是建立机器学习算法，能够识别潜在的违约者，从而减少公司损失。可能的最佳模式是能够最大限度地减少假阴性，识别客户群中的所有违约者，同时最大限度地减少假阳性，防止客户被错误地归类为违约者。</p><p id="dcd2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">满足这些要求可能非常棘手，因为在精确度和召回率之间存在权衡，这意味着增加其中一个指标的值通常会降低另一个指标的值。考虑到最小化公司损失的重要性，我们决定更加重视减少假阳性，搜索可以提高召回率的最佳超参数。</p><p id="b1c0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在测试的三个<strong class="kx jh">梯度增强算法</strong>中，<strong class="kx jh"> XGBoost </strong>产生了最好的结果，召回率为81%，尽管它产生了不希望的56%的假阳性。另一方面，<strong class="kx jh"> LightGBM </strong>和<strong class="kx jh"> CatBoost </strong>提供了更好的假阳性计数，分别为38%和33%，但它们的假阴性大大高于XGBoost，导致召回率较低。</p><p id="5f62" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文呈现了一个经典的评估指标困境。在这种情况下，将由公司的决策者在机器学习算法的帮助下分析大局，并决定最佳的后续计划。当然，在未来的文章中，我们可以测试不同的方法来实现更理想的结果，例如利用深度学习模型。</p><p id="57ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完整代码请参考<a class="ae jd" href="https://github.com/rmpbastos/data_science/blob/master/Credit_Risk_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p></div></div>    
</body>
</html>