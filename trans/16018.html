<html>
<head>
<title>Did you know this in Spark SQL?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你在Spark SQL中知道这个吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/did-you-know-this-in-spark-sql-a7398bfcc41e?source=collection_archive---------16-----------------------#2020-11-04">https://towardsdatascience.com/did-you-know-this-in-spark-sql-a7398bfcc41e?source=collection_archive---------16-----------------------#2020-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="37b7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="3506" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Spark SQL中值得了解的8个不明显的特性。</h2></div><p id="ec27" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Spark SQL的DataFrame API是用户友好的，因为它允许用高级术语表达甚至非常复杂的转换。尤其是现在的Spark 3.0，已经相当丰富和成熟了。然而，在某些情况下，您可能会发现它的行为出乎意料，或者至少不是很直观。这可能会令人沮丧，尤其是当你发现你的生产流水线产生了你没有预料到的结果。</p><p id="7df3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将介绍Spark的一些乍一看并不明显的特性，了解这些特性可以避免愚蠢的错误。在一些例子中，我们还会看到一些很好的优化技巧，根据您的转换，这些技巧会变得很方便。对于代码示例，我们将在Spark 3.0中使用Python API。</p><h2 id="a472" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">1.array_sort和sort_array有什么区别？</h2><p id="3617" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">这两个函数都可以用来对数组进行排序，但是在用法和空值处理上有所不同。虽然<em class="mk"> array_sort </em>只能按升序对数据进行排序，但是<em class="mk"> sort_array </em>采用了第二个参数，在这个参数中，您可以指定数据应该按降序还是升序排序。<em class="mk"> array_sort </em>会将空元素放在数组的末尾，当按降序排序时<em class="mk"> sort_array </em>也会这样做。但是当以升序(默认)使用<em class="mk"> sort_array </em>时，空元素将被放在开头。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="146d" class="ln lo it mq b gy mu mv l mw mx">l = [(1, [2, None, 3, 1])]</span><span id="0661" class="ln lo it mq b gy my mv l mw mx">df = spark.createDataFrame(l, ['id', 'my_arr'])</span><span id="489f" class="ln lo it mq b gy my mv l mw mx">(<br/>    df<br/>    .withColumn('my_arr_v2', array_sort('my_arr'))<br/>    .withColumn('my_arr_v3', sort_array('my_arr'))<br/>    .withColumn('my_arr_v4', sort_array('my_arr', asc=False))<br/>    .withColumn('my_arr_v5', reverse(array_sort('my_arr')))<br/>).show()</span><span id="cdf7" class="ln lo it mq b gy my mv l mw mx">+---+----------+----------+-----------+----------+-----------+<br/>| id|    my_arr| my_arr_v2|  my_arr_v3| my_arr_v4|  my_arr_v5|<br/>+---+----------+----------+-----------+----------+-----------+<br/>|  1|[2,, 3, 1]|[1, 2, 3,]|[, 1, 2, 3]|[3, 2, 1,]|[, 3, 2, 1]|<br/>+---+----------+----------+-----------+----------+-----------+</span></pre><p id="fc0f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">关于如何使用函数<em class="mk"> array_sort </em>还有一个选项，即直接在<a class="ae mz" href="https://spark.apache.org/docs/latest/api/sql/index.html#array_sort" rel="noopener ugc nofollow" target="_blank"> SQL </a>中使用(或者作为SQL表达式作为<em class="mk"> expr() </em>函数的参数),其中它接受第二个参数，该参数是一个比较器函数(从Spark 3.0开始支持)。使用此函数，您可以定义如何比较元素来创建订单。这实际上带来了非常强大的灵活性，例如，您可以对结构数组进行排序，并定义应该按照哪个结构字段进行排序。让我们看看这个例子，在这个例子中，我们通过第二个struct字段进行显式排序:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="98d6" class="ln lo it mq b gy mu mv l mw mx">schema = StructType([<br/>    StructField('arr', ArrayType(StructType([<br/>        StructField('f1', LongType()), <br/>        StructField('f2', StringType())<br/>    ])))<br/>])<br/>l = [(1, [(4, 'b'), (1, 'c'), (2, 'a')])]<br/>df = spark.createDataFrame(l, schema=schema)</span><span id="eae4" class="ln lo it mq b gy my mv l mw mx">(<br/>    df<br/>    .withColumn('arr_v1', array_sort('arr'))<br/>    .withColumn('arr_v2', expr(<br/>        "array_sort(arr, (left, right) -&gt; case when left.f2 &lt; right.f2 then -1 when left.f2 &gt; right.f2 then 1 else 0 end)"))<br/>).show(truncate=False)</span></pre><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi na"><img src="../Images/ddb779aeee055b7346b5bc667fc65fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g96PKiUJvoNkX_A_vM7Ziw.png"/></div></div></figure><p id="149e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这里，您可以看到SQL中表示的比较函数采用两个参数<em class="mk">左</em>和<em class="mk">右</em>，它们是数组的元素，它定义了应该如何比较它们(即根据第二个字段<em class="mk"> f2 </em>)。</p><h2 id="0e4a" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">2.<em class="ni"> concat </em>函数不允许空值</h2><p id="bc85" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated"><a class="ae mz" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat" rel="noopener ugc nofollow" target="_blank"> <em class="mk"> concat </em> </a>函数可以用于连接字符串，也可以用于连接数组。不太明显的一点是，该函数不允许空值，这意味着如果任何参数为空，那么输出也将为空。例如，当连接两个数组时，如果一个数组为空，我们很容易丢失另一个数组的数据，除非我们显式地处理它，例如，使用<em class="mk"> coalesce </em>:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c48a" class="ln lo it mq b gy mu mv l mw mx">from pyspark.sql.types import *<br/>from pyspark.sql.functions import concat, coalesce, array</span><span id="12c1" class="ln lo it mq b gy my mv l mw mx">schema = StructType([<br/>    StructField('id', LongType()),<br/>    StructField('arr_1', ArrayType(StringType())),<br/>    StructField('arr_2', ArrayType(StringType()))<br/>])</span><span id="124d" class="ln lo it mq b gy my mv l mw mx">l = [(1, ['a', 'b', 'c'], None)]<br/>df = spark.createDataFrame(l, schema=schema)</span><span id="a4de" class="ln lo it mq b gy my mv l mw mx">(<br/>    df<br/>    .withColumn('combined_v1', concat('arr_1', 'arr_2'))<br/>    .withColumn('combined_v2', concat(coalesce('arr_1'), array(), coalesce('arr_2', array())))<br/>).show()</span><span id="867e" class="ln lo it mq b gy my mv l mw mx">+---+---------+-----+-----------+-----------+<br/>| id|    arr_1|arr_2|combined_v1|combined_v2|<br/>+---+---------+-----+-----------+-----------+<br/>|  1|[a, b, c]| null|       null|  [a, b, c]|<br/>+---+---------+-----+-----------+-----------+</span></pre><h2 id="84d0" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">3.collect_list不是一个确定性函数</h2><p id="a5bf" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">聚合函数<em class="mk"> collect_list </em>可用于在按某个键分组后创建元素数组，但它不是确定性的，因为结果数组中元素的顺序取决于行的顺序，而行的顺序在洗牌后可能不是确定性的。</p><p id="7366" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">了解优化器对非确定性函数的处理非常小心也是有好处的，例如，优化器不会对其进行筛选，如以下查询所示:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="12b7" class="ln lo it mq b gy mu mv l mw mx">(<br/>  df.groupBy('user_id')<br/>  .agg(collect_list('question_id'))<br/>  .filter(col('user_id').isNotNull())<br/>).explain()</span></pre><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/39d9a4b3cb9c1e5cbd4d1efa0cd1d23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBuZb3jI12ULVSHUxCsteg.png"/></div></div></figure><p id="5377" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从计划中可以看到，过滤器是最后一个转换，因此Spark将首先计算聚合，然后过滤掉一些组(这里我们过滤掉group，其中<em class="mk"> user_id </em>为null)。然而，如果数据首先被过滤器减少，然后被聚集，这将更有效，这确实会发生在确定性函数中，例如<em class="mk">计数</em>:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="d041" class="ln lo it mq b gy mu mv l mw mx">(<br/>  df.groupBy('user_id')<br/>  .agg(count('*'))<br/>  .filter(col('user_id').isNotNull())<br/>).explain()</span></pre><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nk"><img src="../Images/4faa83863d9dd8ef075042d350ec029f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ADIQ8_tOgfNBLY0p270gA.png"/></div></div></figure><p id="b5b5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里的<em class="mk">过滤器</em>被推得更靠近源，因为聚合函数<em class="mk">计数</em>是确定性的。</p><p id="10fd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">除了<em class="mk"> collect_list </em>之外，还有其他非确定性函数，例如<em class="mk"> collect_set </em>、<em class="mk"> first </em>、<em class="mk"> last </em>、<em class="mk"> input_file_name </em>、<em class="mk"> spark_partition_id </em>或<em class="mk"> rand </em>等等。</p><h2 id="87f2" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">4.对窗口进行排序会改变框架</h2><p id="df70" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">有多种聚合和分析函数可以在所谓的窗口中调用，定义如下:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="7168" class="ln lo it mq b gy mu mv l mw mx">w = Window().partitionBy(key)</span></pre><p id="3c91" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该窗口也可以通过调用<em class="mk"> orderBy(key) </em>进行排序，并且可以通过<em class="mk"> rowsBetween </em>或<em class="mk"> rangeBetween </em>指定一个帧。这个框架决定了在窗口中调用哪个行的函数。一些功能也需要对窗口进行排序(例如<em class="mk"> row_count </em>)，但是对于一些功能，排序是可选的。关键是排序可以改变可能不直观的框架。考虑带有<em class="mk"> sum </em>函数的示例:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="053e" class="ln lo it mq b gy mu mv l mw mx">from pyspark.sql import Window<br/>from pyspark.sql.functions import sum</span><span id="59cc" class="ln lo it mq b gy my mv l mw mx">l = [<br/>  (1, 10, '2020-11-01'), <br/>  (1, 30, '2020-11-02'), <br/>  (1, 50, '2020-11-03')<br/>]</span><span id="41f2" class="ln lo it mq b gy my mv l mw mx">df = spark.createDataFrame(l,['user_id', 'price', 'purchase_date'])</span><span id="817e" class="ln lo it mq b gy my mv l mw mx">w1 = Window().partitionBy('user_id')<br/>w2 = Window().partitionBy('user_id').orderBy('purchase_date')</span><span id="3283" class="ln lo it mq b gy my mv l mw mx">(<br/>  df<br/>  .withColumn('total_expenses', sum('price').over(w1))<br/>  .withColumn('cumulative_expenses', sum('price').over(w2))<br/>).show()</span><span id="ee72" class="ln lo it mq b gy my mv l mw mx">+-------+-----+-------------+--------------+-------------------+<br/>|user_id|price|purchase_date|total_expenses|cumulative_expenses|<br/>+-------+-----+-------------+--------------+-------------------+<br/>|      1|   10|   2020-11-01|            90|                 10|<br/>|      1|   30|   2020-11-02|            90|                 40|<br/>|      1|   50|   2020-11-03|            90|                 90|<br/>+-------+-----+-------------+--------------+-------------------+</span></pre><p id="8b03" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如你所看到的，排序窗口将改变帧从开始到当前行，所以求和将产生一个累积和而不是总和。然而，如果我们不使用排序，默认框架将是整个窗口，求和将产生总和。</p><h2 id="121b" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">5.写入表会使缓存失效</h2><p id="e4b2" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">不完全是这样，但是如果您的缓存数据是基于某人刚刚向其追加了数据(或者覆盖了数据)的这个表，那么一旦您调用另一个操作，数据将被扫描并再次缓存。让我们看看这个例子:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="98e7" class="ln lo it mq b gy mu mv l mw mx">df = spark.table(tableA)<br/>df.cache()<br/>df.count()  # now the data is placed in cache</span><span id="df31" class="ln lo it mq b gy my mv l mw mx"># someone writes to tableA now:<br/>dx.write.mode('append').option('path', path).saveAsTable(tableA)</span><span id="702b" class="ln lo it mq b gy my mv l mw mx"># now df is no longer cached, but it will be again after calling some action on it</span><span id="03b0" class="ln lo it mq b gy my mv l mw mx">df.count()  # the data is now placed to memory again but its content was changed</span></pre><p id="7407" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，这里意想不到的事情是，如果有人同时追加表，对缓存的数据帧调用相同的计算可能会导致不同的结果。</p><h2 id="4694" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">6.为什么调用show()会运行多个作业？</h2><p id="674e" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">在Spark中，有两种类型的操作、转换和动作，前者是懒惰的，而后者将物化查询并运行作业。<em class="mk"> show() </em>函数是一个动作，所以它运行一个作业，然而令人困惑的是有时它运行更多的作业。为什么会这样？一个典型的例子是这个查询:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="6d3f" class="ln lo it mq b gy mu mv l mw mx">(<br/>  spark.table(table_name).filter(col('user_id') == xxx)<br/>).show()</span></pre><p id="29c8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，根据数据的属性，情况可能如下图所示，这是Spark UI的一个屏幕截图，我们可以看到Spark在返回结果之前运行了五个作业:</p><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nl"><img src="../Images/cc000233db9fce6b7954ea60aa20b58c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zyijJrdvGa2kVAonqsSZLg.png"/></div></div></figure><p id="0b77" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">还要注意，这些作业中的任务数量是不同的。第一个作业(作业id = 10)只有一个任务！下一个作业运行了4个任务，然后是20、100个任务，最后是一个有75个任务的作业。顺便说一下，执行这个任务的集群有32个可用内核，因此32个任务可以并行运行。</p><p id="b5e0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在多个作业中执行查询的想法是为了避免处理所有输入数据。默认情况下，<em class="mk"> show </em>函数只返回20行(这可以通过传递<em class="mk"> n </em>参数来改变)，所以也许我们可以只处理数据的一个分区来返回20行。这就是为什么Spark首先运行一个只有一个任务的作业，只处理数据的一个分区，希望找到输出所需的20行。如果Spark没有找到这20行，它将启动另一个作业来处理另外四个分区(这就是为什么第二个作业有四个任务),这样情况会重复，Spark在每个进一步的作业中都会增加要处理的分区数量，直到它找到所需的20行或所有分区都被处理。</p><p id="1caa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是一个有趣的优化，特别是当您的数据集非常大(包含许多分区)并且您的查询不是非常有选择性时，所以Spark实际上只能处理前几个分区来找到20行。另一方面，如果您的查询具有很强的选择性，例如您要在一个非常大的数据集中查找一行(甚至可能不存在)，那么使用<em class="mk"> collect </em>函数可能会更有用，该函数将从一开始就充分利用集群的潜力，并在一个作业中处理所有数据，因为最终无论如何都必须处理所有分区(如果记录不存在)。</p><h2 id="4985" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">7.如何确保用户定义的函数只执行一次？</h2><p id="3f6d" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">众所周知，如果用户定义函数(UDF)不是必需的，最好避免使用，因为它们会带来一些性能损失(损失有多大，取决于UDF是用scala/java还是python实现的)。但不太明显的是，如果使用UDF，它可能会比预期执行更多次，因此开销会变得更大。然而，这是可以避免的，因此总的惩罚将会减轻。让我们看一个简单的例子:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="ff10" class="ln lo it mq b gy mu mv l mw mx">@udf('long')<br/>def add_one(x):<br/>    return x + 1</span><span id="db19" class="ln lo it mq b gy my mv l mw mx">(<br/>    spark.range(10)<br/>    .withColumn('increased', add_one(col('id')))<br/>    .filter(col('increased') &gt; 5)<br/>).explain()</span></pre><p id="95b7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本例中，我们创建了一个简单的UDF，用于向DataFrame添加一个新列，然后基于该列进行筛选。如果我们通过调用explain来检查查询计划，我们将看到:</p><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nm"><img src="../Images/c4db1b66c96d51e01d173280bb04ad74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LewJuirCf1QsFDLHfCCQ8w.png"/></div></div></figure><p id="7c1a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如您所见，BatchEvalPython 操作符在计划中出现了两次，这意味着Spark将执行UDF两次。显然，这不是最佳的执行计划，尤其是当UDF成为瓶颈时，这是常有的事。幸运的是，有一个很好的技巧可以让Spark只调用UDF一次，那就是让函数变得不确定(参见<a class="ae mz" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf" rel="noopener ugc nofollow" target="_blank">文档</a>):</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="1cab" class="ln lo it mq b gy mu mv l mw mx">add_one = add_one.asNondeterministic()</span><span id="76e7" class="ln lo it mq b gy my mv l mw mx">(<br/>    spark.range(10)<br/>    .withColumn('increased', add_one(col('id')))<br/>    .filter(col('increased') &gt; 5)<br/>).explain()</span></pre><p id="2274" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，检查查询计划发现UDF只被调用了一次:</p><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/873121c5b5bec0abae5dcdcc5a243542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9CY0jWZSm2Do2Pn6YBsqw.png"/></div></div></figure><p id="355c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是因为Spark现在认为函数是不确定的，所以调用它两次是不安全的，因为每次调用它都会返回不同的结果。同样值得理解的是，通过这样做，我们对优化器施加了一些约束，优化器现在将以与其他非确定性表达式类似的方式处理它，例如，过滤器将不会像我们在上面的<em class="mk"> collect_list </em>函数中看到的那样被推送。</p><h2 id="da05" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">8.UDF可以破坏你的数据分发</h2><p id="4a22" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">不是字面意思。但让我解释一下我这么说的意思。假设有这样一种情况，您希望连接两个分桶的表，并且还需要在其中一列上调用UDF。分桶将允许您在不混乱的情况下进行连接，但是您需要以正确的顺序调用转换。考虑以下查询:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="70b7" class="ln lo it mq b gy mu mv l mw mx">(<br/>  dfA.join(dfB, 'user_id')<br/>  .withColumn('increased', add_one('comments'))<br/>).explain()</span></pre><p id="b63b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里，我们将两个在<em class="mk"> user_id </em>列上分桶的表连接到相同数量的桶，并在dfA的其中一列上应用UDF ( <em class="mk"> add_one </em>)。该计划如下所示:</p><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi no"><img src="../Images/7f4771827bfa721722e9bb11c0982a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sM-CLWJDZR9aawRry-bM0g.png"/></div></div></figure><p id="118e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这里你可以看到一切都很好，因为计划没有<em class="mk">交换</em>操作符，执行将是无洗牌的，这正是我们所需要的，这是因为Spark知道数据的分布，可以用它来连接。</p><p id="f1e9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另一方面，让我们看看如果先应用UDF，然后再执行连接会发生什么:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="300b" class="ln lo it mq b gy mu mv l mw mx">(<br/>  dfA<br/>  .withColumn('increased', add_one('comments'))<br/>  .join(dfB, 'user_id')<br/>).explain()</span></pre><figure class="ml mm mn mo gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi np"><img src="../Images/efa643292b4e6c882a2ee8941ea8086c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vsr3a4LoMTWViQkgaaYpfw.png"/></div></div></figure><p id="4da6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在情况发生了变化，我们在计划中有两个<em class="mk">交换</em>操作符，这意味着Spark现在将在连接之前洗牌。这是因为调用UDF删除了关于数据分布的信息，Spark现在不知道数据实际上分布得很好，它将不得不洗牌以确保分区是正确的。因此，调用UDF并没有真正破坏分布，但它删除了关于它的信息，所以Spark将不得不假设数据是随机分布的。</p><h2 id="8856" class="ln lo it bd lp lq lr dn ls lt lu dp lv la lw lx ly le lz ma mb li mc md me iz bi translated">结论</h2><p id="572b" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">在本文中，我们讨论了一些Spark特性的例子，这些特性可能不太明显，或者在编写查询时很容易忘记。其中一些使用不当会导致代码中的错误，例如，如果您忘记了对窗口排序将改变您的框架，或者如果一些输入参数为空，一些函数将产生空值(如<em class="mk"> concat </em>函数)。在一些例子中，我们还看到了一些简单的优化技巧，比如使UDF不确定可以避免执行两次，或者如果表是分桶的(或者根据一些特定的分区进行分布)，在连接后调用UDF可以避免混乱。</p><p id="ad72" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们还看到，使用SQL函数有时比DSL函数更灵活，一个特别的例子是<em class="mk"> array_sort </em>，它在SQL中允许您指定比较器函数来实现自定义排序逻辑。</p></div></div>    
</body>
</html>