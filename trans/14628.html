<html>
<head>
<title>YOLO V3 Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO V3解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yolo-v3-explained-ff5b850390f?source=collection_archive---------3-----------------------#2020-10-09">https://towardsdatascience.com/yolo-v3-explained-ff5b850390f?source=collection_archive---------3-----------------------#2020-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/2ebdb454edca4228332a232f3e06e3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*g6drkmdLwLf69zv9FQLIYQ.png"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">Yolo-V3检测。图片来源:<a class="ae jy" href="https://www.instagram.com/urialmog/" rel="noopener ugc nofollow" target="_blank"> Uri Almog Instagram </a></p></figure><p id="18be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本帖中，我们将讨论YOLO探测网络及其版本1、2，尤其是版本3。</p><p id="114d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">2016年<strong class="kb ir"> Redmon，Divvala，Girschick和法尔哈迪</strong>以一篇题为:<a class="ae jy" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> <strong class="kb ir"> <em class="kx">你只看一次:统一的、实时的物体检测</em> </strong> </a>的论文对物体检测进行了革命。在论文中，他们介绍了一种新的对象检测方法——将特征提取和对象定位统一到单个整体块中。此外，本地化和分类负责人也进行了整合。他们的单级架构，命名为<strong class="kb ir"> <em class="kx"> YOLO </em> </strong>(你只看一次)导致了非常快的推理时间。在Titan X GPU上，448x448像素图像的帧速率为45 fps(每幅图像0.022秒)，同时实现了最先进的<strong class="kb ir"> <em class="kx"> mAP </em> </strong>(平均精度)。更小且稍不精确的网络版本达到了150 fps。这种新方法，加上基于轻量级谷歌<strong class="kb ir"> <em class="kx"> MobileNet </em> </strong>主干的其他检测器，使边缘设备上的检测网络和其他CV任务的愿景(双关语)更加接近现实。原来的YOLO项目可以在这里找到<a class="ae jy" href="https://github.com/pjreddie/darknet" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="5159" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">顺便说一下，Redmon似乎是一个非常丰富多彩的家伙。他的YOLO项目网站可以在这里找到。</p><p id="03ad" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">检查我在上一篇文章中展示的GluonCV模型动物园图，可以看到经过GluonCV训练的不同版本的<strong class="kb ir"> YOLO-V3 </strong>(红点)达到了极好的精确度，仅次于慢得多的fast-RCNN。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ky"><img src="../Images/207085346887dfa5b354a5503eeb385e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ks7xBtYDWjLBIZV0Z99uqg.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">检测器性能图表。代表性架构。图表数据取自<a class="ae jy" href="https://gluon-cv.mxnet.io/model_zoo/detection.html" rel="noopener ugc nofollow" target="_blank"> GluonCV </a>模型动物园。图表创建者:Uri Almog</p></figure><p id="724c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">YOLO背后的想法是这样的:没有需要彼此同步的分类/检测模块，也没有像以前的2阶段检测器那样的重复区域提议循环(见我在<a class="ae jy" href="https://medium.com/@urialmog/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener"> <strong class="kb ir">上的帖子，早期的物体检测器</strong> </a>，如RCNN)。它基本上是一路向下的卷积(偶尔有maxpool层)。一个单一的整体网络需要处理特征提取、盒子回归和分类，而不是裁剪出一个物体的高概率区域，并将它们提供给一个找到盒子的网络。以前的模型有两个输出图层-一个用于类概率分布，一个用于盒预测，而这里的单个输出图层包含不同要素的所有内容。</p><h1 id="e5fd" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">约洛-V1建筑</h1><p id="1376" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">约罗-V1是第一次出现一级探测器的概念。该架构采用了<strong class="kb ir"><em class="kx"/></strong>(<strong class="kb ir"><em class="kx">BN</em></strong>)和<strong class="kb ir"><em class="kx">leaky ReLU activations</em></strong>，这在当时是比较新的。我不打算详细介绍V1，因为它已经相当过时，并且缺少一些后来引入的强大功能。</p><h1 id="c626" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">约洛-V2建筑</h1><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mk"><img src="../Images/6a087b7c39370a4133837559bcd52504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0ETBkjQYsg5CF-ei7asrw.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">约罗-V2包含22个卷积和5个最大池运算。要素地图高度代表空间分辨率。125个特征的输出用于具有20个类和5个锚点的VOC PASCAL数据集。来源:尤里·阿尔莫格</p></figure><p id="7194" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在约洛-V2版本中，作者们除了其他改动之外，还删除了末尾的全连接层。这使得该架构真正独立于分辨率(即网络参数可以适合任何输入分辨率)。这并不一定意味着网络在任何分辨率下都能表现良好。在训练期间采用了分辨率增强程序。雷德蒙创造了多种版本的约罗-V2，包括更小、更快(和更不精确)的版本，像<strong class="kb ir"> <em class="kx">蒂尼-约罗-V2 </em> </strong>等。</p><p id="5024" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Tiny-Yolo-V2的架构非常简单，因为它没有像它的兄弟姐妹那样奇怪的旁路和重新排列操作。微型版本只是一个漂亮的、长长的卷积和最大池链。</p><p id="fb82" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">描述这些架构的配置文件可以在darknet github 的<a class="ae jy" href="https://github.com/pjreddie/darknet/blob/master/cfg/" rel="noopener ugc nofollow" target="_blank"> cfg部分找到。</a></p><h1 id="2ff4" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">YOLO-V3架构</h1><p id="4f0e" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">受<strong class="kb ir"> <em class="kx"> ResNet </em> </strong>和<strong class="kb ir"> <em class="kx"> FPN </em> </strong>(特征金字塔网络)架构的启发，<strong class="kb ir"> <em class="kx"> YOLO-V3特征提取器</em> </strong>，称为<strong class="kb ir"> <em class="kx"> Darknet-53 </em> </strong>(它有52个卷积)包含跳过连接(像ResNet)和3个预测头(像FPN)——每个都以不同的空间压缩处理图像。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ml"><img src="../Images/8f5bf5fe92dfa805ab07ad8f027270c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDhtsluIu9mnmK9I_x0eHA.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">YOLO-V3架构。来源:尤里·阿尔莫格</p></figure><p id="8d2b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">像它的前身一样，Yolo-V3在各种输入分辨率下都有良好的性能。在<strong class="kb ir"> <em class="kx"> GluonCV的model zoo </em> </strong>中，你可以找到几个检查点:每一个都有不同的输入分辨率，但事实上存储在这些检查点中的网络参数是相同的。在COCO-2017验证集上使用输入分辨率608x608进行测试，Yolo-V3的mAP得分为37(平均平均精度)。这个分数与GluonCV的训练版<strong class="kb ir"><em class="kx">Faster-RCNN-</em>ResNet 50</strong>(一种以ResNet-50为主干的Faster-RCNN架构)相同，但快了17倍。在模型动物园中，速度足以与Yolo-V3 (Mobilenet-SSD架构)竞争唯一检测器的mAP得分为30及以下。</p><p id="8657" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">特色金字塔网(FPN):在两个婚礼上跳舞</strong></p><p id="9ad0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">特征金字塔(Feature-Pyramid)是由<strong class="kb ir"> <em class="kx"> FAIR </em> </strong>(脸书人工智能研究所)在2017年开发的一种拓扑结构，其中特征地图在空间维度上逐渐减小(这是通常的情况)，但后来特征地图再次扩展，并与之前具有相应大小的特征地图连接在一起。重复该过程，并且将每个连接的特征图馈送到单独的检测头。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mm"><img src="../Images/e57acca8c2f8e9380b438d1327b68ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPjH8SQbjp9jkQzIaTU_lQ.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">特征金字塔网络。来源:尤里·阿尔莫格</p></figure><p id="55dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">参考上面的YOLO-V3图示，FPN拓扑允许YOLO-V3学习不同大小的对象:与其他检测块相比，19x19检测块具有更宽的上下文和更差的分辨率，因此它专门用于检测大对象，而76x76检测块专门用于检测小对象。每个探测头都有一套独立的锚定标尺。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/06b703f04d2098d779dbaf10bcfac00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*gLDst38rUrpREBpVFaZESw.png"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">Yolo-V3探测不同大小的物体。来源:<a class="ae jy" href="https://www.facebook.com/uri.almog.photography/" rel="noopener ugc nofollow" target="_blank">尤里·阿尔莫格摄影</a></p></figure><p id="98ac" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">与<strong class="kb ir"> <em class="kx"> SSD </em> </strong>(单次检测器)架构不同，在该架构中，38×38和76×76块将仅接收来自特征提取器中间的高分辨率、部分处理的激活(图中顶部的两个箭头)，在FPN架构中，这些特征在特征提取器的末端与低分辨率、完全处理的特征连接在一起。</p><p id="49b9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这使得网络能够在两个婚礼上跳舞，正如他们在意第绪语中所说的那样，并利用高度处理但窄上下文特征和部分处理但宽上下文特征来进行预测。</p><p id="fe81" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">YOLO-V3的输出方案与V2相同，并且不同于旧的V1。</p><p id="3373" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> YOLO-V2/V3输出方案——单层击穿:</strong></p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi mo"><img src="../Images/20421035cf8dbf7e25ba50db40c8a49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6oWjSF0WX3JjTH4LpSlsWw.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">YOLO V2和YOLO V3输出层。Wout和Hout是输出要素地图的空间维度。对于每个锚点，特征按描述的顺序排列。来源:尤里·阿尔莫格</p></figure><p id="b536" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">输出图层的要素地图中的每个像元都预测了Yolo-V3的3个盒子和YOLO-V2的5个盒子-每个锚点一个盒子。每个箱式预测包括:</p><ol class=""><li id="c372" class="mp mq iq kb b kc kd kg kh kk mr ko ms ks mt kw mu mv mw mx bi translated"><strong class="kb ir">框中心偏移的2个值(在x和y方向，相对于单元格中心)，</strong></li><li id="3505" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated"><strong class="kb ir"> 2值框尺寸刻度(在x和y方向，相对于锚点尺寸)，</strong></li><li id="55e5" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated"><strong class="kb ir"> 1个客观分数值(0到1之间)，</strong></li><li id="4c3a" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated"><strong class="kb ir">类分数的类数值(介于0和1之间)。</strong></li></ol><p id="a024" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">(准确的说，盒子大小值是‘残差值’。在后处理中，它们用于通过以下方式计算盒子宽度</p><p id="688e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">box_width = anchor_width * exp(剩余值_of_box_width))</p><p id="aeaf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面的YOLO-V2图解是为20类VOC PASCAL数据集设计的，有5个锚点。125个要素的输出排列如下:每个空间像元有125个版本。要素0是对象性分数，要素1–2是盒子的x和y比例，要素3–4是盒子中心的x和y偏移(相对于像元坐标本身)，而要素5–24是20类分数。所有这些——为了第一个主播。特征25-49重复相同的特征分配-这一次使用第二个锚，以此类推。注意，锚尺寸不直接在特征中表示，但是那些特征中的比例值传递到后处理，并且被分配有用于盒解码的相应锚比例。</p><p id="c972" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx">对象性</em> </strong>是一个新概念，值得在几个段落中进行讨论。现在让我们把它想象成网络的<strong class="kb ir">置信度</strong>，即<strong class="kb ir">某个</strong>对象存在于给定的盒子中，并且假设盒子中存在一个对象，则类得分是<strong class="kb ir"> <em class="kx">条件概率</em> </strong>(即假设x类的概率在这个盒子中存在一个对象)。因此，每个类别的总置信度得分是<strong class="kb ir">客观性</strong>和<strong class="kb ir">类别得分</strong>的乘积。</p><p id="a6f3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">然后，网络的输出经过NMS和一个置信度阈值，给出最终的预测，正如我在关于<a class="ae jy" href="https://medium.com/@urialmog/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener"> <strong class="kb ir">探测器基础</strong> </a>的帖子中所解释的。</p><p id="5bd4" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">为什么YOLO比以前的架构表现更好？</strong></p><p id="b03e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果说我在研究YOLO和它的小版本时学到了什么的话，那就是外表可能会骗人:YOLO的拓扑结构如此小而简单，它能有多复杂呢？好吧。使它能够具有如此简单的结构，或者更准确地说是紧凑的损失函数是非常复杂的。正是失去赋予了这些特征意义。因此，一个精心制作的损失函数可以将大量信息打包到一个小的要素地图中。</p><p id="ca14" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在下一节，我们将讨论YOLO损失函数的最重要的特征。</p><h1 id="2578" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">YOLO培训和流失机制</h1><p id="afd5" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">这一部分是基于我对<strong class="kb ir"> <em class="kx"> Darknet </em> </strong>框架(Redmon开发的框架)的训练流所做的研究，当时我正在进行该框架的独立TensorFlow实现。</p><h1 id="fb47" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">输入分辨率增强</h1><p id="abf1" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">作为一个<strong class="kb ir"> <em class="kx">全卷积网络</em></strong>——不像以前的检测器那样包含用于分类任务的全连接层——它可以处理任何大小的输入图像。但是，由于不同的输入分辨率会产生不同的网络参数，因此使用<strong class="kb ir"> <em class="kx">分辨率增强来训练网络:</em> </strong>作者使用了384x384和672x672像素之间的10个输入分辨率步长，每隔几个训练批次随机交替，使网络能够推广其对不同分辨率的预测。</p><h1 id="9584" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">损失系数—分而治之</h1><p id="2993" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">损失函数对不同的盒子进行不同的处理。</p><p id="bc1d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如我在<a class="ae jy" href="https://medium.com/@urialmog/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener">上一篇文章</a>中解释的那样，网络输出层<strong class="kb ir">中的每个空间单元预测多个盒子</strong>(在YOLO-V3中有3个，在以前的版本中有5个)——所有的盒子都以那个单元为中心，通过一种叫做<strong class="kb ir"> <em class="kx">的机制锚定</em> </strong>。</p><p id="ece0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">每个盒预测的YOLO损失由以下项组成:</p><ol class=""><li id="a69b" class="mp mq iq kb b kc kd kg kh kk mr ko ms ks mt kw mu mv mw mx bi translated"><strong class="kb ir">坐标丢失</strong> —由于盒子预测没有完全覆盖对象，</li><li id="0b56" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated"><strong class="kb ir">对象丢失</strong> —由于错误的盒对象IoU预测，</li><li id="dff6" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated"><strong class="kb ir">分类损失</strong> —由于对正确类别的预测为‘1’而对该框中的对象的所有其他类别的预测为‘0’的偏差。</li><li id="16d2" class="mp mq iq kb b kc my kg mz kk na ko nb ks nc kw mu mv mw mx bi translated">这是一个特殊的损失，我们将在下面两节详细阐述。</li></ol><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/129dafeb1215a7d6ec4335b01e60f423.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*zznNlmn5S_sM5-VE2qIU2w.png"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">YOLO-V1损失函数。λ是损耗系数。前3行是“最佳盒子”(在每个空间单元中最佳捕获GT对象的盒子)造成的损失，而第4行是由于没有捕获对象的盒子造成的损失。在YOLO V2和V3中，直接的宽度和高度预测以及平方根被替换为剩余比例预测，以使损失参数与相对比例误差而不是绝对比例误差成比例。</p></figure><p id="a5c3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">一个盒子预测的质量是通过它的<strong class="kb ir"> <em class="kx"> IoU </em> </strong>(交集/并集)与它试图预测的对象(更准确地说，是与它的基础真值盒子)来衡量的。IoU值的范围从0(框完全错过对象)到1.0(完美匹配)。</p><p id="de12" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于每个空间像元，对于位于该像元中心的每个框预测，损失函数会找到对象位于该像元中心的具有最佳IoU的框。这种区分<strong class="kb ir"> <em class="kx">最佳盒子</em> </strong>和所有其他盒子的机制是YOLO损失的核心。</p><p id="850d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx">最佳框</em> </strong>和它们本身导致<strong class="kb ir">坐标损失</strong>(由于与对象不完全匹配)和<strong class="kb ir">分类损失</strong>(由于分类错误)。这推动了与那些盒子相关联的网络参数，以改进盒子的规模和位置，以及分类。这些盒子还会导致信心丧失——我们将立即对此进行解释。所有其他盒子只招致<strong class="kb ir">信任损失</strong>。</p><h1 id="f65c" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">失去目标——知道自己的价值</h1><p id="095d" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">每一个框预测都与一个称为“<strong class="kb ir">客观</strong>的预测相关联。它出现在像RCNN这样的以前的检测器中的地方，即区域提议包含对象的置信度，因为它乘以lass分数以给出绝对的类置信度。然而，与预期相反——该预测实际上是一个<strong class="kb ir"> IoU预测</strong>——即网络<strong class="kb ir"> <em class="kx">认为</em> </strong>盒子覆盖物体的程度。<strong class="kb ir">目标损失</strong>项教导网络预测<strong class="kb ir">正确的IoU </strong>，而<strong class="kb ir">坐标损失</strong>教导网络预测<strong class="kb ir">更好的盒子</strong>(最终将IoU推向1.0)。所有的盒预测都会造成对象损失，但只有每个空间像元中的最佳拟合盒也会造成坐标和分类损失。</p><p id="8c44" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为什么我们需要客观损失？</p><p id="570f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在推理中，我们通常有多个盒子，每个盒子有不同的覆盖范围。我们希望后处理算法选择以最精确的方式覆盖对象的盒子。我们还想选择对对象给出正确类别预测的盒子。算法怎么知道选哪个盒子？</p><p id="1984" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，对象性告诉我们覆盖率有多好——所以对象性非常小的盒子(&lt;0.005) are discarded and don’t even make it to the NMS block (see explanation in <a class="ae jy" href="https://medium.com/@urialmog/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener">我之前的帖子</a>)。这有助于消除大约90%的盒子，它们只是架构的工件，而不是真正的检测(当然，这取决于任务和数据。如果您的任务是检测装满瓶盖的盒子中的瓶盖，那么您可以预期有大量的实际检测)。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ne"><img src="../Images/c12c096f577c54056cc5ae4424bf3ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUmqfmGLYZd6TK5ONipJUg.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">抑制NMS时的多框检测结果。客观性分数告诉NMS哪些盒子要保留，哪些要丢弃。来源:<a class="ae jy" href="https://www.facebook.com/uri.almog.photography/" rel="noopener ugc nofollow" target="_blank">乌里阿尔莫摄影</a>。</p></figure><p id="54c2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">第二，NMS是为每个类分别做的，所以类的分数是由盒子的对象性来衡量的，以便进行有意义的比较。如果我们有两个高度重叠的框，第一个框的对象性为0.9，人员概率为0.8(加权分数为0.72)，第二个框的对象性为0.5，人员概率为0.3(加权分数为0.15)，则第一个框将持续存在，第二个框将在NMS中消失，因为第一个框的对象性使其更值得信任。</p><p id="7a42" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">为什么“最佳包厢”在培训期间会受到不同的对待？</strong></p><p id="ea73" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我没有看到Redmon对这个问题的任何解释，但我的直觉是这样的:想想一位教授，她有以下策略:在第一次作业中，她寻找表现好的学生，并努力检查和评分他们的作业，以便他们能在该科目中表现出色。为了集中注意力，她懒得批改成绩不太好的学生的作业。相反，她给他们一个机会，让他们在下一次作业中在另一个科目上表现出色。</p><p id="34bc" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">只推最好的盒子来提高覆盖率和档次的原因是专注。我们希望培训能够很好地融合。网络有丰富的参数，所有参数都有大量的工作要做，所以不要急于一次优化所有参数。试图推动所有盒子的参数来捕捉相同的对象——奖励所有盒子近似捕捉相同的对象——可能会导致损失景观中非常长且嘈杂的轨迹，或者更糟的是——陷入次优最小值(因为它们可能无法很好地学习检测具有不同特征的对象，但会陷入局部最小值，而无法学习不同的行为)。最好是<strong class="kb ir">利用</strong>一些盒子的相对成功，只推动它们在这种类型的对象中成功，同时让与不太成功的对象相对应的参数<strong class="kb ir">探索</strong>其他选项(以一种将很快解释的方式)。</p><p id="0553" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">另一方面，我们希望所有的盒子都经历对象性损失——为什么？我们希望所有的盒子——包括坏盒子——能够学会辨别它们是好是坏，即使他们在一生中(或至少在训练中)没有学到任何东西——因为NMS依赖于它(即使如此——YOLO给‘最好的盒子’比其他盒子更高的物体损失系数)。</p><h1 id="e7cc" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">收缩损失——利用与探索</h1><p id="2d21" class="pw-post-body-paragraph jz ka iq kb b kc mf ke kf kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw ij bi translated">我称之为<strong class="kb ir"> <em class="kx">锚收缩</em> </strong>的一个有趣机制在论文中没有明确提到，但我在代码中发现了它。每一个盒预测都会导致与其原始锚形状和位置的偏差成比例的小损失。在第一个训练时期，这种作用微弱但稳定地持续，之后损失系数预定消失。</p><p id="5e40" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">虽然这个损失项是非零的，但它产生一个弱力，使每个盒预测收缩回其锚形式。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi nf"><img src="../Images/34021f714a85f3d93273df16aa7cd9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODpOuFHiM7k7HKyuR7n_KQ.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">收缩损失:在训练期间，将两个盒子与以它们的单元为中心的物体(鸟)进行比较。红盒子具有最好的IoU，并且将贡献坐标损失和分类损失，这将推动它更好地覆盖鸟并预测其类别。蓝色盒子将被推回到它的锚形，这可能使它处于更好的位置来捕捉毛虫。来源:<a class="ae jy" href="https://www.facebook.com/uri.almog.photography" rel="noopener ugc nofollow" target="_blank">乌里阿尔莫格摄影</a>。</p></figure><p id="7039" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这个巧妙的机制具有以下效果:没有成功捕获对象的盒子(那些不包括在上面提到的“最佳盒子”组中的盒子)被推回到它们的原始锚形状。由于锚被设计为捕捉数据集中的对象的最佳先验，所以该动作增加了与那些盒子相关联的权重在未来尝试中生成更多成功盒子的机会。同时，成功的盒子(“最佳盒子”组)也经历了这种损失！但是坐标和分类损失要大得多(它们的系数更大),并且它们支配着与那些框相关联的参数的移动方向</p><p id="1e8d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在几个时期之后，假设网络已经学会相当好地预测盒子，并且锚收缩停止允许网络参数在实际基础上微调。</p><p id="5546" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我以前的帖子:</p><p id="360b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://medium.com/@urialmog/what-is-a-neural-network-dac400d5307d?source=friends_link&amp;sk=764555affffd1bbb5f73e6f87d36ed58" rel="noopener"> <strong class="kb ir"> <em class="kx">什么是神经网络？</em> </strong> </a></p><p id="42f2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"> <strong class="kb ir"> <em class="kx">【数学与人工智能】实际问题</em> </strong> </a></p><p id="a1d5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"> <strong class="kb ir"> <em class="kx">感知器</em> </strong> </a></p><p id="5db6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"> <strong class="kb ir"> <em class="kx">训练神经网络简单解释</em> </strong> </a></p><p id="9e07" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://medium.com/swlh/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener"> <strong class="kb ir"> <em class="kx">深度学习的物体检测— RCNN、锚点、非最大抑制</em> </strong> </a></p></div></div>    
</body>
</html>