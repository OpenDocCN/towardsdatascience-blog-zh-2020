<html>
<head>
<title>Gradient Descent — One Step at a Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降—一次一步</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-one-step-at-a-time-3c39a3642333?source=collection_archive---------12-----------------------#2020-11-05">https://towardsdatascience.com/gradient-descent-one-step-at-a-time-3c39a3642333?source=collection_archive---------12-----------------------#2020-11-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="52ba" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="d0e0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个直观的，初学者的梯度下降，是用来尽量减少机器学习中的各种损失函数的基本机制指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e612745d3b9d99647e5cfd3e9db06436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bU-YYX0thsFJ3d5S"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@tricell1991?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Waranont (Joe) </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dcec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">梯度下降(GD)是一种优化算法，用于迭代地最小化任何给定的成本函数。但是什么是成本函数呢？它是一个衡量经过训练的机器学习模型在进行预测时的准确性的函数。常见的例子包括均方误差(MSE)和交叉熵(或对数损失)。</p><p id="626f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GD在深度机器学习模型中被大量使用，作为更传统的方法的替代，以最小化成本函数。它更有效地处理深度学习中常见的海量数据。</p><p id="bf41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将详细地浏览GD来模拟一个过于简化的线性回归问题，以便对其机制有一个直观而深刻的理解。我假设你在解决这个问题的时候对最小二乘法、微积分和线性回归有所了解。</p><h1 id="50bc" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">问题设置</h1><p id="1c57" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这是我们的玩具数据，由3个数据点组成:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/90e22c86accdeef2e72ae9a44e33267a.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*mZ6bcRczMhXRuY2CdALgnA.png"/></div></figure><p id="27f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的目标是使用GD找到最佳拟合线，然后可以使用它根据一些新的<code class="fe nc nd ne nf b">X</code>值预测<code class="fe nc nd ne nf b">Y</code>值(也称为线性回归)。回想一下，直线可以完全用两个参数来描述:截距和斜率。因此，如果我们可以通过优化算法找到理想的截距和斜率参数，我们将获得最佳拟合线。</p><p id="cec5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的最佳拟合线可以表示如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/704ee3307c3e97fa2fd16956086cf47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*sM3KCtGuhx_ET0Hh-oKuxQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">情商。一</p></figure><p id="87fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中:</p><ul class=""><li id="d501" class="nh ni it lk b ll lm lo lp lr nj lv nk lz nl md nm nn no np bi translated"><code class="fe nc nd ne nf b">y</code> <em class="nq"> hat </em>是被预测的因变量</li><li id="291f" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md nm nn no np bi translated"><code class="fe nc nd ne nf b">x₁</code>是我们要预测<code class="fe nc nd ne nf b">y</code> <em class="nq">帽子</em>的<code class="fe nc nd ne nf b">X</code>值</li><li id="f41a" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md nm nn no np bi translated"><code class="fe nc nd ne nf b">θ₀</code>是截距项，需要优化</li><li id="1605" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md nm nn no np bi translated"><code class="fe nc nd ne nf b">θ₁</code>是要优化的斜率</li></ul><h1 id="3703" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">寻找最佳截距</h1><p id="1788" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了简单起见，我们首先只找出<code class="fe nc nd ne nf b">θ₀</code>的最佳值，同时假设<code class="fe nc nd ne nf b">θ₁</code>等于0.64(通过最小二乘法确定)。然后，一旦我们理解了GD是如何工作的，我们将使用它来求解最佳截距和斜率参数。所以我们修正后的直线方程变成了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/433ba74d65539b663df8579adf32d022.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*_c156xUOnUWmeiqHwQeIMQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">情商。2</p></figure><p id="83d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们首先为截距<code class="fe nc nd ne nf b">θ₀</code>选择一个随机值，称为<strong class="lk jd">随机初始化</strong>。随机初始化是我们最初的猜测，实际上可以是GD用来改进的任何实数。</p><p id="1d67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">先说0代表<code class="fe nc nd ne nf b">θ₀</code>(不是巧合，也是我最喜欢的数字！)，这给了我们下面一行:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/52876413f4624267b182ccebb909eb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*XKMokxYNdlYChKScFnoE0g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">y = 0 + 0.64x</p></figure><p id="416b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们将使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank">残差平方和</a> (RSS)损失函数来评估这条线与我们的数据的拟合程度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c20f62639427fb6c86bc195332975f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*IAHegoPivbGS9D7_S3Xw5w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS和截距= 0的最佳拟合线</p></figure><p id="5764" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用Eq。2利用<code class="fe nc nd ne nf b">θ₀ = 0</code>，我们得到<code class="fe nc nd ne nf b">y</code>，残差，残差平方，残差平方和的以下预测:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/553907f3c05f5068840e5aa1809303f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*bMpRsxO4LIOkqB1jb6g36g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">简易资讯聚合</p></figure><p id="7e7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着截距<code class="fe nc nd ne nf b">θ₀</code>值的增加，重复上述RSS计算，我们得到以下<code class="fe nc nd ne nf b">θ₀</code>对RSS的散点图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/66cf744d160222264af93f89c5c4ff2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*WlhCDsE_58e5Q2naGBICpw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">针对RSS的各种拦截的散点图</p></figure><p id="1f7f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从图中可以明显看出，截距值为0.95时RSS最小。但是我们怎么知道它实际上是0.95，而不是0.75到1.15之间的某个值呢？我们可以代入一堆不同的截距值，通过试错找到最优值。但是当我们拥有GD的时候为什么要这样做呢？</p><h2 id="121a" class="ob mf it bd mg oc od dn mk oe of dp mo lr og oh mq lv oi oj ms lz ok ol mu iz bi translated">梯度下降法</h2><p id="9faa" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">当GD在随机初始化点计算RSS时，在RSS值较高的情况下，需要相对较大的步长来确定要评估的下一个截距。随着RSS在后续迭代中接近零，截距的变化(称为步长)变得更小。换句话说，GD通过在远离最小RSS时采取大步，在接近最小RSS时采取小步来确定最佳参数值。</p><p id="bf17" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过使用RSS和我们的等式，我们可以得到成本函数的等式，该等式绘出了针对RSS的所有可能的截距。2如下(使用我们的三个原始数据点):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/edb35de5644d9b83ac984dbb1f49c972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qSfR-hv8cGFXNSB90dgkGw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数</p></figure><p id="1859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该函数的曲线图看起来如下，基本上与上面的散点图相同，只是添加了一条多项式趋势线:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/55049ffac449df7f2cb25f1e110d69ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*twpTsnvCdiTpiuVXz3c-zg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数图</p></figure><p id="5c41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以对该函数相对于<code class="fe nc nd ne nf b">θ₀</code>求导，并确定截距<code class="fe nc nd ne nf b">θ₀</code>的任意值处的斜率:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/fcc5e0410820b97398bf51cdf384adfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iHv7Tmmi1KjBYNSMYZbLw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数的导数—等式。3</p></figure><p id="7b9f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有了导数，GD将使用它来找到RSS的最低值。像之前一样，假设我们用初始值<code class="fe nc nd ne nf b">θ₀ = 0</code>初始化GD。</p><p id="1f59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的导数中插入<code class="fe nc nd ne nf b">θ₀ = 0</code>给我们<code class="fe nc nd ne nf b">-5.6</code>，这是RSS成本函数在<code class="fe nc nd ne nf b">θ₀ = 0</code>的梯度或斜率。注意，<code class="fe nc nd ne nf b">θ₀</code>的最佳值是在曲线上梯度为0的点处获得的，即在曲线的底部。还记得，当曲线的斜率远离0时，GD采取相对较大的步骤，当曲线的斜率接近0时，则采取小步骤。因此，步长的大小应该与曲线的斜率相关，因为它告诉GD它应该采取小步还是大步。但是，步长不能太大，以免跳过曲线上的最小点，到达另一边。</p><p id="c040" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">步长由称为<strong class="lk jd">学习率</strong>的参数调节。步长决定了GD计算RSS时使用的新截距:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="2373" class="ob mf it nf b gy ot ou l ov ow">step size = slope * learning rate<br/>new intercept = old intercept - step size</span></pre><p id="e88e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">学习率被设置为一个很小的数字，通常在实践中为0.2、0.1或0.01。</p><p id="bc4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">继续我们的例子，当<code class="fe nc nd ne nf b">θ₀ = 0</code>和学习速率<code class="fe nc nd ne nf b">0.1</code>时，步长为<code class="fe nc nd ne nf b">-0.56</code>:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="0af4" class="ob mf it nf b gy ot ou l ov ow">step size (at <!-- -->θ₀ = 0): -5.6 * 0.1<br/>step size (at <!-- -->θ₀ = 0): -0.56</span><span id="01a2" class="ob mf it nf b gy ox ou l ov ow">new intercept = 0 - (-0.56)<br/>new intercept = 0.56</span></pre><p id="9b21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">到目前为止，评估了两个截距的成本函数图如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/982cc56c33f99d9ce85fe1b9e4aa1c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*LvwdYRjubz0QpRv40FJung.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第一步后的RSS成本函数</p></figure><p id="19b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的图清楚地表明，随着相对较大的第一步，我们已经非常接近RSS的最小值了！回到我们的最佳拟合线，我们可以看到<code class="fe nc nd ne nf b">0.56</code>的新截距大大缩小了残差:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b689fb85d32b818a916e2e88c63607a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*G1JnZLekUyzGmtCX40VNnQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS和截距= 0.56的最佳拟合线</p></figure><p id="9315" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们进一步接近截距的最佳值。我们回到方程式中的导数。3，并插入新的截距值<code class="fe nc nd ne nf b">0.56</code>，这给出了曲线在该截距值处的斜率为<code class="fe nc nd ne nf b">-2.2</code>:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="6c7c" class="ob mf it nf b gy ot ou l ov ow">RSS slope (at <!-- -->θ₀ = 0.56): -2(1.08-0.56) - 2(0.4-0.56) - 2(1.3-0.56)<br/>RSS slope (at <!-- -->θ₀ = 0.56): -2.2</span><span id="32d0" class="ob mf it nf b gy ox ou l ov ow">step size (at <!-- -->θ₀ = 0.56): -2.2 * 0.1<br/>step size (at <!-- -->θ₀ = 0.56): -0.22</span><span id="5291" class="ob mf it nf b gy ox ou l ov ow">new intercept = 0.56 - (-0.22)<br/>new intercept = 0.78</span></pre><p id="3251" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的两个图如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/3bae2355975b572eb708c76d8eedb4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QiSaU_B5R4QLtNr53f7ymw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数和第二步后的最佳拟合线</p></figure><p id="71c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">总的来说，RSS变小了。另外，请注意，在RSS成本函数图中，第一步与第二步相比相对较大。</p><p id="88ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们也做第三步:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="1a71" class="ob mf it nf b gy ot ou l ov ow">RSS slope (at <!-- -->θ₀ = 0.78): -2(1.08-0.78) - 2(0.4-0.78) - 2(1.3-0.78)<br/>RSS slope (at <!-- -->θ₀ = 0.78): -0.88</span><span id="4dd4" class="ob mf it nf b gy ox ou l ov ow">step size (at <!-- -->θ₀ = 0.78): -0.88 * 0.1<br/>step size (at <!-- -->θ₀ = 0.78): -0.09</span><span id="d0b6" class="ob mf it nf b gy ox ou l ov ow">new intercept = 0.78 - (-0.09)<br/>new intercept = 0.87</span></pre><p id="2f09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的两个图如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/13929c21156b2a20bd3c829c670eb79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtySp5G6m_qA8eN8Y3HoMQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数和第三步后的最佳拟合线</p></figure><p id="9e95" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">再继续三步，GD对截距的估计结果是<code class="fe nc nd ne nf b">0.95</code>，这与用最小二乘法计算的结果完全相同(你可以自己检查最小二乘法的计算结果！).</p><h2 id="9626" class="ob mf it bd mg oc od dn mk oe of dp mo lr og oh mq lv oi oj ms lz ok ol mu iz bi translated">什么时候停止？</h2><p id="43ef" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">但是GD怎么知道什么时候停止做额外的计算呢？换句话说，它如何知道自己已经达到了最佳估计值？当步长变得非常接近0时，它会这样做，这将在梯度非常接近0时发生。实践者通常通过最小步长参数来控制这一点，该参数通常设置为0.001或更小。</p><p id="6b48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">也就是说，GD还包括一个放弃前的步数限制，通常参数化为1000步或更多。因此，即使步长很大，如果GD已经执行了最大数量的步骤，它也会停止。</p><h1 id="22c1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">一起寻找最佳截距和斜率</h1><p id="2808" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">既然我们已经理解了GD的基本原理，让我们一起来计算最佳截距和斜率。</p><p id="ae02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这意味着我们将不会使用0.64的斜率，如等式。2.相反，我们将有两个独立的参数，<code class="fe nc nd ne nf b">θ₀</code>和<code class="fe nc nd ne nf b">θ₁</code>，我们需要一起优化它们。这需要多变量微积分，并对截距<code class="fe nc nd ne nf b">θ₀</code>和斜率<code class="fe nc nd ne nf b">θ₁</code>取偏导数。成本函数现在看起来像这样:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/eaaa719b23285ee499f3a3cc4479525d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9SZNRIEzTVoDk44p1-xWw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RSS成本函数—等式。四</p></figure><p id="c0ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于截距<code class="fe nc nd ne nf b">θ₀</code>的偏导数如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/ca4c26f332bc0e511244e48132fb533e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-keEuqbf3XjvBRc0aJZJ1w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">截距的偏导数——等式。5</p></figure><p id="0c3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于截距<code class="fe nc nd ne nf b">θ₁</code>的偏导数如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/f6cb1df10c630cac65fa59ec5ba9a499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X3INCJ_NXV3xM1I4QoEjg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">斜率的偏导数——等式。6</p></figure><p id="d673" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将使用这两个偏导数来寻找RSS成本函数(等式)中的最低点。4).就像之前一样，我们将开始为截距<code class="fe nc nd ne nf b">θ₀</code>和斜率<code class="fe nc nd ne nf b">θ₁</code>选择随机数。先说<code class="fe nc nd ne nf b">θ₀ = 0</code>和<code class="fe nc nd ne nf b">θ₁ = 1</code>。因此，GD将从这条线开始:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/27b625a5f52050ca4d976ef87fe5d6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Xeta9GXUYoTdiv_Yg10Iiw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GD随机初始化</p></figure><p id="c8c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们将<code class="fe nc nd ne nf b">θ₀ = 0</code>和<code class="fe nc nd ne nf b">θ₁ = 1</code>代入偏导数方程:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="56a3" class="ob mf it nf b gy ot ou l ov ow">partial derivative wrt <!-- -->θ₀ (Eq. 5) = (6 * 0) + (11.4 * 1) - 13<br/>partial derivative wrt <!-- -->θ₀ (Eq. 5) = -1.6</span><span id="e159" class="ob mf it nf b gy ox ou l ov ow">partial derivative wrt <!-- -->θ₁ (Eq. 6) = (11.4 * 0) + (27.9 * 1) - 28.7<br/>partial derivative wrt <!-- -->θ₁ (Eq. 6) = -0.8</span></pre><p id="af45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">计算学习率为0.01的步长以及截距和斜率的新值:</p><pre class="ks kt ku kv gt op nf oq or aw os bi"><span id="77e9" class="ob mf it nf b gy ot ou l ov ow">intercept step size = -1.6 * 0.01<br/>intercept step size = -0.016<br/>new intercept = 0 - (-0.016)<br/>new intercept = 0.016</span><span id="d13e" class="ob mf it nf b gy ox ou l ov ow">slope step size = -0.8 * 0.01<br/>slope step size = -0.008<br/>new slope = 1 - (-0.008)<br/>new slope = 1.008</span></pre><p id="409b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们更新后的最佳产品系列大致如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ed205a5c8d1d011f28101ddb39bf2c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*DydqUeHw38xMxLcO1FT-Vw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第一步后的最佳拟合线</p></figure><p id="7cd4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种差别似乎不是实质性的。然而，在计算步长和更新的参数的更多迭代之后(直到步长小于阈值或者达到最大迭代次数)，我们得到以下最佳拟合线:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/56ea4b4323c077cf0de163cb37318806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*4lMfbjtjQ2PBhD0s5rYo8A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">与GD最匹配的最终线</p></figure><p id="c1ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GD找到的最佳参数将与最小二乘法找到的参数相同或非常接近，即<code class="fe nc nd ne nf b">θ₀ = 0.95</code>和<code class="fe nc nd ne nf b">θ₁ = 0.64</code>。</p><h1 id="b8bc" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">多参数GD和其他损失函数</h1><p id="83ba" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们刚刚完成了两个参数的GD计算。如果我们的数据有更多的特征(实际数据集通常如此)，我们只需要取更多的偏导数，其他一切都将保持不变。</p><p id="0ac1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里要注意的一个关键点是，所有参数应该在每一步同时更新(而不是一个接一个地更新)。</p><p id="4889" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个例子中，我们优化了RSS成本函数。但是，还有其他几个成本函数可以处理不同的数据类型、分布和预测问题。无论使用哪种成本函数，GD的工作方式都是一样的。</p><h1 id="d3ec" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">批量与随机梯度下降</h1><p id="4ec7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们在这个问题中执行的操作被称为批量梯度下降，由此在计算成本函数的斜率时使用所有可用的数据点(训练数据)(上面的步骤3)。当我们有大量数据点时，这会导致问题和效率低下。</p><p id="a939" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一种叫做随机梯度下降的动物很好地处理了这种低效率。随机梯度下降在每一步使用随机选择的观测值，而不是整个数据集。这减少了计算成本函数的斜率所花费的时间(步骤3)。总体概念和方法仍然类似于批量梯度下降。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h1 id="2d86" class="me mf it bd mg mh po mj mk ml pp mn mo ki pq kj mq kl pr km ms ko ps kp mu mv bi translated">学习率</h1><p id="a29d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">你会注意到我们使用了两种不同的学习率:当只找到最优截距时，学习率较高；当同时找到最优截距和斜率时，学习率较低。</p><p id="ef9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">GD对学习率高度敏感。在实践中，通过<strong class="lk jd">学习计划</strong>，一个合理的学习率可以由大到小自动确定。在大多数机器学习算法中，学习速率和学习时间表可以作为参数使用。例如，在scikit-learn的<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" rel="noopener ugc nofollow" target="_blank"> SGDRegressor </a>和<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" rel="noopener ugc nofollow" target="_blank"> SGDClassifier </a>类中，学习速率和学习进度分别由<code class="fe nc nd ne nf b">eta0</code>和<code class="fe nc nd ne nf b">learning_rate</code>参数控制。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><h1 id="9168" class="me mf it bd mg mh po mj mk ml pp mn mo ki pq kj mq kl pr km ms ko ps kp mu mv bi translated">摘要</h1><p id="c229" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在你有了，一个关于GD如何在后台工作的非常简单的解释。现在让我们总结一下GD在优化参数时采取的步骤:</p><ol class=""><li id="b8c4" class="nh ni it lk b ll lm lo lp lr nj lv nk lz nl md pt nn no np bi translated">对成本函数中的每个参数求偏导数，从而得到与参数数量相等的多个偏导数</li><li id="3abc" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md pt nn no np bi translated">为每个参数选择一些随机初始值——称为随机初始化</li><li id="457c" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md pt nn no np bi translated">将这些随机值代入步骤1中确定的每个偏导数中，以找到成本函数的斜率</li><li id="359f" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md pt nn no np bi translated">计算每个参数的步长:<code class="fe nc nd ne nf b">step size = slope * learning rate</code></li><li id="2203" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md pt nn no np bi translated">计算新的参数:<code class="fe nc nd ne nf b">new parameter = old parameter — step size</code></li><li id="b600" class="nh ni it lk b ll nr lo ns lr nt lv nu lz nv md pt nn no np bi translated">从步骤3开始重复，直到步长非常小或达到最大步数</li></ol><p id="15b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您想讨论任何与数据分析、机器学习、金融或信用分析相关的问题，请随时联系<a class="ae lh" href="https://www.finlyticshub.com/" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><p id="30af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下次见，摇滚起来！</p></div></div>    
</body>
</html>