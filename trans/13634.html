<html>
<head>
<title>Data to Text generation with T5; Building a simple yet advanced NLG model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用T5产生数据到文本；建立一个简单而先进的NLG模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45?source=collection_archive---------2-----------------------#2020-09-19">https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45?source=collection_archive---------2-----------------------#2020-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b36d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过微调T5实现数据到文本的NLG模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/801d3082128e1566a3a03b0619b0953d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*cpwgfaZmO6SBH5nJc379Ig.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><h1 id="6466" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">简介</strong></h1><p id="4cdb" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">自从自然语言处理领域中的序列到序列模型出现以来，我就一直在探索NLG模型的数据到文本生成能力。解决这个问题的早期尝试没有显示出任何有希望的结果。像<a class="ae mi" href="https://github.com/simplenlg/simplenlg" rel="noopener ugc nofollow" target="_blank">简单NLG </a>这样的非基于ML规则的方法似乎没有很好地扩展，因为它们需要格式良好的输入，并且只能执行诸如改变句子时态之类的任务。但是在语言模型的时代，每两周就会有变形金刚的新版本发布，这样的任务不再是遥不可及的梦想。</p><p id="cb18" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在这篇博客中，我将讨论我如何用高级深度学习模型处理数据到文本的生成问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c6a939f4ec390036d4117fac5e638240.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*66olm041CMNV1Y07VWSeYw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">模因通过<a class="ae mi" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank">图像翻转</a></p></figure><p id="4375" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">openAI GPT-2似乎是一个不错的选择，因为它具有引人注目的文本生成能力。但是在NLG 2017年数据上训练它并没有让我有所收获。模型根本没有收敛。GPT-2的有条件和无条件文本生成能力相当不错，但是您很难找到可以处理这些任务的业务用例。</p><p id="1692" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">此外，根据特定领域的数据对它们进行微调有时会导致产生脱离上下文的句子</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/bfd22bd7d8566cbae68413da0c6c2abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*F71v6A6Oe8g-IG8fvvbl2Q.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae mi" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank">图像翻转</a>进行迷因</p></figure><p id="17ea" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">由于openAI(不那么开放)没有发布GPT-3的代码，我在该系列中只剩下第二好的，即<a class="ae mi" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5 </a>。</p><h1 id="0acf" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">型号:谷歌T5 </strong></h1><p id="2502" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">Google的T5是一个文本到文本转换转换器，它是一个共享的NLP框架，所有的NLP任务都被重新组织成一个统一的文本到文本格式，输入和输出都是文本串。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/c2db709c08dcdd5a9bc0daaf32835bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*60g54I1mCfKKYv3wTgfq8A.gif"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:<a class="ae mi" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌博客</a></p></figure><p id="d10b" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它与只能输出一个类标签或输入范围的BERT型模型有很大的不同。T5允许我们在任何NLP任务中使用相同的模型以及损失函数和超参数。</p><h1 id="17b7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">数据:WebNLG 2020 </strong></h1><p id="1aec" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我使用WebNLG Challenge 2020的RDF-to-text生成任务的<a class="ae mi" href="https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/release_v3.0" rel="noopener ugc nofollow" target="_blank">数据</a>来训练T5。</p><p id="5140" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">给定(a)中所示的四个RDF三元组，目标是生成一个像(b)这样的文本</p><p id="3c20" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">(a)RDF三元组</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2bb17533d8abdb8ddcf7ec0c224acc3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*1ITcdZ5WmhInjVIRCywa6A.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来源:<a class="ae mi" href="https://webnlg-challenge.loria.fr/challenge_2020/" rel="noopener ugc nofollow" target="_blank"> WebNLG </a></p></figure><p id="66da" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu"> (b)英文本</strong></p><p id="8c84" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="mw">特灵于1913年1月1日在威斯康星州的拉克罗斯成立，总部位于爱尔兰。它有29，000名员工。</em></p><h1 id="22ae" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">数据预处理</strong></h1><p id="cdde" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">要对数据进行预处理，可以使用Python中的XML WebNLG数据读取器<a class="ae mi" href="https://gitlab.com/webnlg/corpus-reader" rel="noopener ugc nofollow" target="_blank">这里是</a>，或者使用下面代码中给出的<code class="fe mx my mz na b"><a class="ae mi" href="https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree" rel="noopener ugc nofollow" target="_blank">xml.etree.ElementTree</a></code>模块。(我最终使用了后者，因为我太无知了，无法阅读整个挑战文档😐)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="2943" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在代码中，您可以看到我们保留了正常的三元组，并用“&amp;&amp;”连接多个三元组。当一个表的多行被一次输入到模型中时，它可以被认为是一个分隔符。</p><h1 id="120f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">训练模型</strong></h1><p id="c9f9" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">像往常一样，谷歌的TensorFlow实现很难解释，我继续使用<a class="ae mi" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>的PyTorch实现，并选择了T5基础模型。整个模型训练在google colab中进行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi nd"><img src="../Images/35e3240a171e4d45db4a205d93e4be17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRNgsT_or0q0pQl6sp20LA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">通过<a class="ae mi" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank">图像翻转</a>进行迷因</p></figure><p id="7e3f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">安装转换器库</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="e63e" class="ni kv it na b gy nj nk l nl nm">!pip install transformers</span></pre><p id="7f2e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">导入所需的模块</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="dfb2" class="ni kv it na b gy nj nk l nl nm">import pandas as pd</span><span id="2f50" class="ni kv it na b gy nn nk l nl nm">import torch</span><span id="baeb" class="ni kv it na b gy nn nk l nl nm">from transformers import T5Tokenizer, T5ForConditionalGeneration,Adafactor<br/></span></pre><p id="e10c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">加载预处理后的数据，并随机重排行，使不同长度的三元组(1个三元组到7个三元组)分布在数据框中，从而快速归纳损失。<br/>修剪掉一些数据点，这样一批就不会留下任何余数，因此可以避免一些代码行(好吧，这可能是一种粗略的做法)。</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="1728" class="ni kv it na b gy nj nk l nl nm">train_df=pd.read_csv(‘webNLG2020_train.csv’, index_col=[0])<br/>train_df=train_df.iloc[  :35000,:]<br/>train_df=train_df.sample(frac = 1)</span><span id="6262" class="ni kv it na b gy nn nk l nl nm">batch_size=8<br/>num_of_batches=len(train_df)/batch_size</span></pre><p id="6154" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">检测GPU。</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="48a0" class="ni kv it na b gy nj nk l nl nm">if torch.cuda.is_available():<br/>   dev = torch.device("cuda:0")<br/>   print("Running on the GPU")<br/>else:<br/>   dev = torch.device("cpu")<br/>   print("Running on the CPU")</span></pre><p id="22ee" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">加载预训练的模型、记号化器，并将模型移入GPU。</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="d521" class="ni kv it na b gy nj nk l nl nm">tokenizer = T5Tokenizer.from_pretrained(‘t5-base’)<br/>model = T5ForConditionalGeneration.from_pretrained(‘t5-base’,<br/>                                             return_dict=True)</span><span id="41ec" class="ni kv it na b gy nn nk l nl nm">#moving the model to GPU<br/>model.to(dev)</span></pre><p id="47a2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">使用推荐的T5设置启动Adafactor优化器。</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="1897" class="ni kv it na b gy nj nk l nl nm">optimizer = Adafactor(model.parameters(),lr=1e-3,<br/>                      eps=(1e-30, 1e-3),<br/>                      clip_threshold=1.0,<br/>                      decay_rate=-0.8,<br/>                      beta1=None,<br/>                      weight_decay=0.0,<br/>                      relative_step=False,<br/>                      scale_parameter=False,<br/>                      warmup_init=False)</span></pre><p id="ac2a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">基于Html的进度条。</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="fa76" class="ni kv it na b gy nj nk l nl nm">from IPython.display import HTML, display<br/>def progress(loss,value, max=100):<br/> return HTML(""" Batch loss :{loss}</span><span id="28c3" class="ni kv it na b gy nn nk l nl nm">      &lt;progress    <br/>value='{value}'max='{max}',style='width: 100%'&gt;{value}<br/>      &lt;/progress&gt;</span><span id="c917" class="ni kv it na b gy nn nk l nl nm">             """.format(loss=loss,value=value, max=max))</span></pre><p id="a677" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在，训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="b995" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我花了大约3-4个小时在Colab GPU上运行四个纪元。</p><p id="400c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">序列化模型</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="dd34" class="ni kv it na b gy nj nk l nl nm">torch.save(model.state_dict(),'pytoch_model.bin')</span></pre><p id="8b6c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">t5基础模型的配置文件可以下载并放置在与保存的模型相同的目录中。确保将其重命名为config.json</p><pre class="kj kk kl km gt ne na nf ng aw nh bi"><span id="8f1e" class="ni kv it na b gy nj nk l nl nm">!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json</span></pre><h1 id="731a" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">加载训练好的模型进行推理</strong></h1><p id="bd85" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">确保给定的路径既有保存的模型又有配置文件。此外，如果您有一个用于执行推理的GPU，请记住将模型和输入张量移动到GPU。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h1 id="dbb9" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">生成的结果</strong></h1><p id="b637" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">现在让我们看看不同输入生成的文本输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi no"><img src="../Images/09fc06e7cb60dc2c4b4ff1a2fae343e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lv076JeuEiufmSe06Y2znw.gif"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><h1 id="db37" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结论</h1><p id="e1f4" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们讨论了如何构建一个从结构化数据生成文本的高级NLG模型。T5的文本到文本架构使得将结构化数据(可以是文本和数字数据的组合)输入模型变得容易。我在huggingface的transformer上使用了原生PyTorch代码，在WebNLG 2020数据集上对其进行了微调。</p><blockquote class="np"><p id="c3b1" class="nq nr it bd ns nt nu nv nw nx ny mh dk translated">与基于GPT-2的文本生成不同，这里我们不仅触发语言生成，我们还控制它！！</p></blockquote><p id="5f04" class="pw-post-body-paragraph lm ln it lo b lp nz ju lr ls oa jx lu lv ob lx ly lz oc mb mc md od mf mg mh im bi translated">然而，这是该方法的基本实现，并且使用相对不太复杂的数据集来测试该模型。当用具有两个以上三元组的数据点测试模型时，它似乎忽略了数据输入中存在的一些信息。为了解决这个问题，需要做进一步的研究和大量的实验。</p><p id="396e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你可以在这个<a class="ae mi" href="https://github.com/MathewAlexander/T5_nlg.git" rel="noopener ugc nofollow" target="_blank">回购</a>里找到代码</p><p id="0568" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">有什么问题尽管问！谢谢大家！</p></div></div>    
</body>
</html>