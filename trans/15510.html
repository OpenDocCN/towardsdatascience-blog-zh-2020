<html>
<head>
<title>Predict Customer Churn in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python预测客户流失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-customer-churn-in-python-e8cd6d3aaa7?source=collection_archive---------0-----------------------#2020-10-26">https://towardsdatascience.com/predict-customer-churn-in-python-e8cd6d3aaa7?source=collection_archive---------0-----------------------#2020-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="209e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python中的监督机器学习算法预测客户流失的逐步方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dca17551223c0d68cf0a25a3dd606b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nPg4stkDEeFxKQMe"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Emile Perron 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ad12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">客户流失(<em class="lv">又名客户流失</em>)是任何组织最大的支出之一。如果我们能够合理准确地找出客户离开的原因和时间，这将极大地帮助组织制定多方面的保留计划。让我们利用来自<a class="ae ky" href="https://github.com/srees1988/predict-churn-py" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的客户交易数据集来理解在Python中预测客户流失所涉及的关键步骤。</p><p id="7589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有监督的机器学习只不过是学习一个基于示例输入-输出对将输入映射到输出的函数。监督机器学习算法分析训练数据并产生推断的函数，该函数可用于映射新的示例。假设我们在电信数据集中有当前和以前客户交易的数据，这是一个标准化的监督分类问题，试图预测二元结果(Y/N)。</p><p id="4a8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文结束时，让我们尝试解决一些与客户流失相关的关键业务挑战，比如说，(1)活跃客户离开组织的可能性有多大？(2)客户流失的关键指标是什么？(3)根据调查结果，可以实施哪些保留策略来减少潜在客户流失？</p><p id="1c58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实世界中，我们需要经历七个主要阶段来成功预测客户流失:</p><p id="f24b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">A部分:数据预处理</p><p id="82e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">B部分:数据评估</p><p id="7284" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">C部分:型号选择</p><p id="7037" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">D部分:模型评估</p><p id="4e6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">E部分:模型改进</p><p id="036a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">F部分:未来预测</p><p id="6ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">G部分:模型部署</p><p id="163c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了了解业务挑战和建议的解决方案，我建议您<a class="ae ky" href="https://github.com/srees1988/predict-churn-py" rel="noopener ugc nofollow" target="_blank">下载</a>数据集，并与我一起编写代码。如果你在工作中有任何问题，请随时问我。让我们在下面详细研究一下上述每一个步骤</p><h1 id="55e4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">A部分:数据预处理</h1><p id="0d98" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如果你问20岁的我，我会直接跳到模型选择，作为机器学习中最酷的事情。但是就像在生活中一样，智慧是在较晚的阶段发挥作用的！在目睹了真实世界的机器学习业务挑战之后，我不能强调数据预处理和数据评估的重要性。</p><p id="4efd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">永远记住预测分析中的以下黄金法则:</p><blockquote class="mt mu mv"><p id="9037" class="kz la lv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">“你的模型和你的数据一样好”</p></blockquote><p id="3d2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解数据集的端到端结构并重塑变量是定性预测建模计划的起点。</p><p id="e9ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤0:重启会话:</strong>在我们开始编码之前，重启会话并从交互式开发环境中移除所有临时变量是一个好的做法。因此，让我们重新启动会话，清除缓存并重新开始！</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="3794" class="ne lx it na b gy nf ng l nh ni">try:<br/>    from IPython import get_ipython<br/>    get_ipython().magic('clear')<br/>    get_ipython().magic('reset -f')<br/>except:<br/>    pass</span></pre><p id="300b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步:导入相关库:</strong>导入所有相关的python库，用于构建有监督的机器学习算法。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e404" class="ne lx it na b gy nf ng l nh ni">#Standard libraries for data analysis:<br/>    <br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from scipy.stats import norm, skew<br/>from scipy import stats<br/>import statsmodels.api as sm</span><span id="0772" class="ne lx it na b gy nj ng l nh ni"># sklearn modules for data preprocessing:</span><span id="b162" class="ne lx it na b gy nj ng l nh ni">from sklearn.impute import SimpleImputer<br/>from sklearn.preprocessing import LabelEncoder, OneHotEncoder<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import OneHotEncoder<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler</span><span id="e115" class="ne lx it na b gy nj ng l nh ni">#sklearn modules for Model Selection:</span><span id="2904" class="ne lx it na b gy nj ng l nh ni">from sklearn import svm, tree, linear_model, neighbors<br/>from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>from xgboost import XGBClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import SVC<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="d261" class="ne lx it na b gy nj ng l nh ni">#sklearn modules for Model Evaluation &amp; Improvement:<br/>    <br/>from sklearn.metrics import confusion_matrix, accuracy_score <br/>from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import ShuffleSplit<br/>from sklearn.model_selection import KFold<br/>from sklearn import feature_selection<br/>from sklearn import model_selection<br/>from sklearn import metrics<br/>from sklearn.metrics import classification_report, precision_recall_curve<br/>from sklearn.metrics import auc, roc_auc_score, roc_curve<br/>from sklearn.metrics import make_scorer, recall_score, log_loss<br/>from sklearn.metrics import average_precision_score</span><span id="5079" class="ne lx it na b gy nj ng l nh ni">#Standard libraries for data visualization:</span><span id="1b10" class="ne lx it na b gy nj ng l nh ni">import seaborn as sn<br/>from matplotlib import pyplot<br/>import matplotlib.pyplot as plt<br/>import matplotlib.pylab as pylab<br/>import matplotlib <br/>%matplotlib inline<br/>color = sn.color_palette()<br/>import matplotlib.ticker as mtick<br/>from IPython.display import display<br/>pd.options.display.max_columns = None<br/>from pandas.plotting import scatter_matrix<br/>from sklearn.metrics import roc_curve</span><span id="850c" class="ne lx it na b gy nj ng l nh ni">#Miscellaneous Utilitiy Libraries:<br/>    <br/>import random<br/>import os<br/>import re<br/>import sys<br/>import timeit<br/>import string<br/>import time<br/>from datetime import datetime<br/>from time import time<br/>from dateutil.parser import parse<br/>import joblib</span></pre><p id="22ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步:设置当前工作目录:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="bf8f" class="ne lx it na b gy nf ng l nh ni">os.chdir(r”C:/Users/srees/Propensity Scoring Models/Predict Customer Churn/”)</span></pre><p id="a60f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三步:导入数据集:</strong>让我们将输入数据集加载到当前工作目录下的python笔记本中。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="99c8" class="ne lx it na b gy nf ng l nh ni">dataset = pd.read_csv('1.Input/customer_churn_data.csv')</span></pre><p id="e929" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第4步:评估数据结构:</strong>在这一部分，我们需要大致了解数据集以及每一列的细节，以便更好地理解输入数据，从而在需要时聚合字段。</p><p id="5f0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从head &amp; column方法中，我们了解到这是一个电信客户流失数据集，其中每个记录都包含订阅、任期、付款频率和流失的性质(表示他们的当前状态)。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d58a" class="ne lx it na b gy nf ng l nh ni">dataset.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/6d3905d0152d0d2c4c8bb59dff0fdfa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uwa0b31nlOBRLGSAARTtgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输入数据集的快照(图片由作者提供)</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f020" class="ne lx it na b gy nf ng l nh ni">dataset.columns</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/430ada9ca04ce1570985523e70621020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p84rvSohGeqoDDDL6MCq5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">列名列表(作者图片)</p></figure><p id="6085" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速描述法显示，电信客户平均停留32个月，每月支付64美元。但是，这可能是因为不同的客户有不同的合同。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cbd1" class="ne lx it na b gy nf ng l nh ni">dataset.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ddd84090493f41d84a10b9b4a6782179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*kzqSNq9-PNp60ODqhwCh_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">描述方法(图片由作者提供)</p></figure><p id="be68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从表面上看，我们可以假设数据集包含几个数字和分类列，提供关于客户交易的各种信息。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="960e" class="ne lx it na b gy nf ng l nh ni">dataset.dtypes</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/335ff07cc3dc93455eb4d6dbeb820c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*qoSETCSTkwCTeaL7vV_95g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">列数据类型(作者图片)</p></figure><p id="2df4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重新验证列数据类型和缺失值:</strong>始终关注数据集中缺失的值。缺少的值可能会扰乱模型的构建和准确性。因此，在比较和选择模型之前，我们需要考虑缺失值(如果有的话)。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="1450" class="ne lx it na b gy nf ng l nh ni">dataset.columns.to_series().groupby(dataset.dtypes).groups</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/f97958b2d044b482b3a47c07feddb2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwvcnFa3V-UiKbiM1JjDww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">聚合列数据类型(按作者分类的图片)</p></figure><p id="6f05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集包含7043行和21列，数据集中似乎没有缺失值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8192" class="ne lx it na b gy nf ng l nh ni">dataset.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/3eb22a878a43e7202d9d878b5d1d187d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGEwX5he6YJu0QGWit5iqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据结构(图片由作者提供)</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="49d8" class="ne lx it na b gy nf ng l nh ni">dataset.isna().any()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bc583b5eef5501f444671c8b7f5f4b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*n-7yiANomgoSjqEXt8TWIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">检查NA(图片由作者提供)</p></figure><p id="3ddf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">识别唯一值:</strong>“付款方式”和“合同”是数据集中的两个分类变量。当我们查看每个分类变量中的唯一值时，我们会发现客户要么是按月滚动合同，要么是一年/两年的固定合同。此外，他们通过信用卡、银行转账或电子支票支付账单。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="db38" class="ne lx it na b gy nf ng l nh ni">#Unique values in each categorical variable:</span><span id="0ec9" class="ne lx it na b gy nj ng l nh ni">dataset["PaymentMethod"].nunique()</span><span id="e786" class="ne lx it na b gy nj ng l nh ni">dataset["PaymentMethod"].unique()</span><span id="e539" class="ne lx it na b gy nj ng l nh ni">dataset["Contract"].nunique()</span><span id="f548" class="ne lx it na b gy nj ng l nh ni">dataset["Contract"].unique()</span></pre><p id="7c1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第五步:检查目标变量分布:</strong>我们来看看流失值的分布。这是检验数据集是否支持任何类别不平衡问题的一个非常简单而关键的步骤。正如您在下面看到的，数据集是不平衡的，活跃客户的比例比他们的对手高。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="b59f" class="ne lx it na b gy nf ng l nh ni">dataset["Churn"].value_counts()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/b5eac72b13c0897b6a28fc692edd927a.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*2f0dqhq217ztqUbjNgpuvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">流失值的分布(按作者分类的图片)</p></figure><p id="f2e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第六步:清理数据集:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="f555" class="ne lx it na b gy nf ng l nh ni">dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'],errors='coerce')</span><span id="4de9" class="ne lx it na b gy nj ng l nh ni">dataset['TotalCharges'] = dataset['TotalCharges'].astype("float")</span></pre><p id="02b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第7步:处理缺失数据:</strong>正如我们前面看到的，所提供的数据没有缺失值，因此这一步对于所选的数据集是不需要的。我想在这里展示这些步骤，以供将来参考。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="76c5" class="ne lx it na b gy nf ng l nh ni">dataset.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/3eb22a878a43e7202d9d878b5d1d187d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGEwX5he6YJu0QGWit5iqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据结构(图片由作者提供)</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="227c" class="ne lx it na b gy nf ng l nh ni">dataset.isna().any()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bc583b5eef5501f444671c8b7f5f4b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*n-7yiANomgoSjqEXt8TWIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">检查NA(图片由作者提供)</p></figure><p id="7058" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">以编程方式查找平均值并填充缺失值:</strong>如果我们在数据集的数字列中有任何缺失值，那么我们应该查找每一列的平均值并填充它们缺失的值。下面是以编程方式执行相同步骤的一段代码。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="80bd" class="ne lx it na b gy nf ng l nh ni">na_cols = dataset.isna().any()</span><span id="4a27" class="ne lx it na b gy nj ng l nh ni">na_cols = na_cols[na_cols == True].reset_index()</span><span id="ee39" class="ne lx it na b gy nj ng l nh ni">na_cols = na_cols["index"].tolist()</span><span id="d32d" class="ne lx it na b gy nj ng l nh ni">for col in dataset.columns[1:]:<br/>     if col in na_cols:<br/>        if dataset[col].dtype != 'object':<br/>             dataset[col] =  dataset[col].fillna(dataset[col].mean()).round(0)</span></pre><p id="0567" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重新验证NA:</strong>重新验证并确保数据集中不再有空值总是一个好的做法。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6684" class="ne lx it na b gy nf ng l nh ni">dataset.isna().any()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bc583b5eef5501f444671c8b7f5f4b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*n-7yiANomgoSjqEXt8TWIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">重新验证NA(图片由作者提供)</p></figure><p id="7c37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤8:标签编码二进制数据:</strong>机器学习算法通常只能将数值作为它们的独立变量。因此，标签编码非常关键，因为它们用适当的数值对分类标签进行编码。这里我们对所有只有两个唯一值的分类变量进行标签编码。任何具有两个以上唯一值的分类变量都将在后面的部分中用标签编码和一键编码来处理。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="41ef" class="ne lx it na b gy nf ng l nh ni">#Create a label encoder object</span><span id="b4c2" class="ne lx it na b gy nj ng l nh ni">le = LabelEncoder()</span><span id="090f" class="ne lx it na b gy nj ng l nh ni"># Label Encoding will be used for columns with 2 or less unique </span><span id="2525" class="ne lx it na b gy nj ng l nh ni">values<br/>le_count = 0<br/>for col in dataset.columns[1:]:<br/>    if dataset[col].dtype == 'object':<br/>        if len(list(dataset[col].unique())) &lt;= 2:<br/>            le.fit(dataset[col])<br/>            dataset[col] = le.transform(dataset[col])<br/>            le_count += 1<br/>print('{} columns were label encoded.'.format(le_count))</span></pre></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="b052" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">B部分:数据评估</h1><p id="f766" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">第9步:探索性数据分析:</strong>让我们尝试通过独立变量的分布来探索和可视化我们的数据集，以更好地理解数据中的模式，并潜在地形成一些假设。</p><p id="22b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤9.1。绘制数值列的直方图:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="fcab" class="ne lx it na b gy nf ng l nh ni">dataset2 = dataset[['gender', <br/>'SeniorCitizen', 'Partner','Dependents',<br/>'tenure', 'PhoneService', 'PaperlessBilling',<br/>'MonthlyCharges', 'TotalCharges']]</span><span id="7502" class="ne lx it na b gy nj ng l nh ni">#Histogram:<br/>    <br/>fig = plt.figure(figsize=(15, 12))<br/>plt.suptitle('Histograms of Numerical Columns\n',horizontalalignment="center",fontstyle = "normal", fontsize = 24, fontfamily = "sans-serif")<br/>for i in range(dataset2.shape[1]):<br/>    plt.subplot(6, 3, i + 1)<br/>    f = plt.gca()<br/>    f.set_title(dataset2.columns.values[i])</span><span id="73bd" class="ne lx it na b gy nj ng l nh ni">vals = np.size(dataset2.iloc[:, i].unique())<br/>    if vals &gt;= 100:<br/>        vals = 100<br/>    <br/>plt.hist(dataset2.iloc[:, i], bins=vals, color = '#ec838a')<br/>plt.tight_layout(rect=[0, 0.03, 1, 0.95])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/854dafeb73ba1b8ca5423c236b464c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfebz-NGRCb4suLt7vsUwg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数字柱直方图(图片由作者提供)</p></figure><p id="5069" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以根据数值变量的直方图进行一些观察:</p><ul class=""><li id="02ac" class="of og it lb b lc ld lf lg li oh lm oi lq oj lu ok ol om on bi translated">性别分布显示，该数据集的特点是男性和女性客户的比例相对相等。在我们的数据集中，几乎一半的顾客是女性，而另一半是男性。</li><li id="38e2" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">数据集中的大多数客户都是年轻人。</li><li id="6ced" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">似乎没有多少顾客有家属，而几乎一半的顾客有伴侣。</li><li id="48a9" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">组织中有许多新客户(不到10个月),然后是平均停留超过70个月的忠诚客户群。</li><li id="b5a3" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">大多数客户似乎有电话服务，3/4的客户选择了无纸化计费</li><li id="9356" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">每个客户的月费在18美元到118美元之间，其中20美元的客户占很大比例。</li></ul><p id="e1be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第9.2步。分析分类变量的分布:</strong></p><p id="ffd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 9.2.1。合同类型分布:</strong>大多数客户似乎都与电信公司建立了预付费连接。另一方面，1年期和2年期合同的客户比例大致相同。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="0364" class="ne lx it na b gy nf ng l nh ni">contract_split = dataset[[ "customerID", "Contract"]]<br/>sectors = contract_split .groupby ("Contract")<br/>contract_split = pd.DataFrame(sectors["customerID"].count())<br/>contract_split.rename(columns={'customerID':'No. of customers'}, inplace=True)</span><span id="c415" class="ne lx it na b gy nj ng l nh ni">ax =  contract_split[["No. of customers"]].plot.bar(title = 'Customers by Contract Type',legend =True, table = False, <br/>grid = False,  subplots = False,figsize =(12, 7), color ='#ec838a', <br/>fontsize = 15, stacked=False)</span><span id="f5d5" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('No. of Customers\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="4810" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('\n Contract Type',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="dccf" class="ne lx it na b gy nj ng l nh ni">plt.title('Customers by Contract Type \n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "22", fontfamily = "sans-serif")</span><span id="0341" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="044a" class="ne lx it na b gy nj ng l nh ni">x_labels = np.array(contract_split[["No. of customers"]])</span><span id="93c3" class="ne lx it na b gy nj ng l nh ni">def add_value_labels(ax, spacing=5):   <br/>    for rect in ax.patches:      <br/>        y_value = rect.get_height()<br/>        x_value = rect.get_x() + rect.get_width() / 2       <br/>        space = spacing        <br/>        va = 'bottom'      <br/>        if y_value &lt; 0:           <br/>            space *= -1            <br/>            va = 'top'       <br/>        label = "{:.0f}".format(y_value)      <br/>        <br/>            ax.annotate(<br/>            label,                      <br/>            (x_value, y_value),         <br/>            xytext=(0, space),          <br/>            textcoords="offset points", <br/>            ha='center',                <br/>            va=va)  <br/>        <br/>add_value_labels(ax)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/09dcaf590cc4c0b210682e2385cea9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyaHYZfBycB2GMiSVn6iDw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按合同类型划分的客户分布(按作者划分的图片)</p></figure><p id="31e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 9.2.2支付方式类型分布:</strong>数据集表明，客户最喜欢以电子方式支付账单，其次是银行转账、信用卡和邮寄支票。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a553" class="ne lx it na b gy nf ng l nh ni">payment_method_split = dataset[[ "customerID", "PaymentMethod"]]<br/>sectors = payment_method_split  .groupby ("PaymentMethod")<br/>payment_method_split  = pd.DataFrame(sectors["customerID"].count())<br/>payment_method_split.rename(columns={'customerID':'No. of customers'}, inplace=True)</span><span id="4ae7" class="ne lx it na b gy nj ng l nh ni">ax =  payment_method_split [["No. of customers"]].plot.bar(title = 'Customers by Payment Method', legend =True, table = False, grid = False, subplots = False,  figsize =(15, 10),color ='#ec838a', fontsize = 15, stacked=False)</span><span id="7c9c" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('No. of Customers\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="2044" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('\n Contract Type',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="f789" class="ne lx it na b gy nj ng l nh ni">plt.title('Customers by Payment Method \n',<br/>horizontalalignment="center", fontstyle = "normal", fontsize = "22", fontfamily = "sans-serif")</span><span id="403e" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="e823" class="ne lx it na b gy nj ng l nh ni">x_labels = np.array(payment_method_split [["No. of customers"]])</span><span id="e316" class="ne lx it na b gy nj ng l nh ni">def add_value_labels(ax, spacing=5):   <br/>    for rect in ax.patches:      <br/>        y_value = rect.get_height()<br/>        x_value = rect.get_x() + rect.get_width() / 2       <br/>        space = spacing        <br/>        va = 'bottom'      <br/>        if y_value &lt; 0:           <br/>            space *= -1            <br/>            va = 'top'       <br/>        label = "{:.0f}".format(y_value)<br/>     <br/>           ax.annotate(label,<br/>           (x_value, y_value),         <br/>            xytext=(0, space),textcoords="offset points", <br/>            ha='center',va=va)</span><span id="da25" class="ne lx it na b gy nj ng l nh ni">add_value_labels(ax)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/56fdbafbc140407658cfc06ecec1a1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4hPdNnd96w6rtFh6Z0UBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按支付方式划分的客户分布(按作者划分的图片)</p></figure><p id="ee73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.2.3。标签编码分类变量的分布:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7663" class="ne lx it na b gy nf ng l nh ni">services= ['PhoneService','MultipleLines',<br/>'InternetService','OnlineSecurity',  'OnlineBackup','DeviceProtection',<br/>'TechSupport','StreamingTV','StreamingMovies']</span><span id="ae36" class="ne lx it na b gy nj ng l nh ni">fig, axes = plt.subplots(nrows = 3,ncols = 3,<br/>figsize = (15,12))<br/>for i, item in enumerate(services):</span><span id="062b" class="ne lx it na b gy nj ng l nh ni">    if i &lt; 3:<br/>    ax = dataset[item].value_counts().plot(<br/>    kind = 'bar',ax=axes[i,0],<br/>    rot = 0, color ='#f3babc' )<br/>        <br/>    elif i &gt;=3 and i &lt; 6:<br/>    ax = dataset[item].value_counts().plot(<br/>    kind = 'bar',ax=axes[i-3,1],<br/>    rot = 0,color ='#9b9c9a')<br/>        <br/>    elif i &lt; 9:<br/>    ax = dataset[item].value_counts().plot(<br/>    kind = 'bar',ax=axes[i-6,2],rot = 0,<br/>    color = '#ec838a')ax.set_title(item)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/0819665c47577699bf534d9da0f9a7de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFoqp6dl3KIJevfMyvlEng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签编码分类变量的分布(图片由作者提供)</p></figure><ul class=""><li id="99a0" class="of og it lb b lc ld lf lg li oh lm oi lq oj lu ok ol om on bi translated">大多数客户拥有电话服务，其中几乎一半的客户拥有多条线路。</li><li id="c87e" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">四分之三的客户选择了通过光纤和DSL连接的互联网服务，近一半的互联网用户订阅了流媒体电视和电影。</li><li id="ff2a" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">利用在线备份、设备保护、技术支持和在线安全功能的客户是少数。</li></ul><p id="e099" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第9.3步:通过分类变量分析流失率:</strong></p><p id="797b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 9.3.1。整体流失率:</strong>对整体流失率的初步观察显示，约74%的客户是活跃的。如下图所示，这是一个不平衡的分类问题。当每个类的实例数量大致相等时，机器学习算法工作得很好。由于数据集是倾斜的，我们在选择模型选择的指标时需要记住这一点。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6daa" class="ne lx it na b gy nf ng l nh ni">import matplotlib.ticker as mtick<br/>churn_rate = dataset[["Churn", "customerID"]]<br/>churn_rate ["churn_label"] = pd.Series(<br/>np.where((churn_rate["Churn"] == 0), "No", "Yes"))</span><span id="93c1" class="ne lx it na b gy nj ng l nh ni">sectors = churn_rate .groupby ("churn_label")<br/>churn_rate = pd.DataFrame(sectors["customerID"].count())</span><span id="49b3" class="ne lx it na b gy nj ng l nh ni">churn_rate ["Churn Rate"] = (<br/>churn_rate ["customerID"]/ sum(churn_rate ["customerID"]) )*100</span><span id="4765" class="ne lx it na b gy nj ng l nh ni">ax =  churn_rate[["Churn Rate"]].plot.bar(title = 'Overall Churn Rate',legend =True, table = False,grid = False,  subplots = False, <br/>figsize =(12, 7), color = '#ec838a', fontsize = 15, stacked=False, <br/>ylim =(0,100))</span><span id="dd78" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Proportion of Customers',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", fontfamily = "sans-serif")</span><span id="3e7f" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('Churn',horizontalalignment="center",fontstyle = "normal", fontsize = "large", fontfamily = "sans-serif")</span><span id="41ac" class="ne lx it na b gy nj ng l nh ni">plt.title('Overall Churn Rate \n',horizontalalignment="center", <br/>fontstyle = "normal", fontsize = "22", fontfamily = "sans-serif")</span><span id="9f92" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")<br/>ax.yaxis.set_major_formatter(mtick.PercentFormatter())</span><span id="e4a2" class="ne lx it na b gy nj ng l nh ni">x_labels = np.array(churn_rate[["customerID"]])</span><span id="be56" class="ne lx it na b gy nj ng l nh ni">def add_value_labels(ax, spacing=5):   <br/>    for rect in ax.patches:     <br/>        y_value = rect.get_height()<br/>        x_value = rect.get_x() + rect.get_width() / 2       <br/>        space = spacing<br/>        va = 'bottom'        <br/>        if y_value &lt; 0:           <br/>            space *= -1          <br/>            va = 'top'<br/>        label = "{:.1f}%".format(y_value)    <br/> <br/>     ax.annotate(label,<br/>                (x_value, y_value),         <br/>                 xytext=(0, space),<br/>                 textcoords="offset points", <br/>                 ha='center',va=va)</span><span id="ef1d" class="ne lx it na b gy nj ng l nh ni">add_value_labels(ax)<br/>ax.autoscale(enable=False, axis='both', tight=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/5a75e40e6ef52b0df1eda5410671c7a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5Tr5FJr21fVygFHRJyHGg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">总体流失率(图片由作者提供)</p></figure><p id="44bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.3.2。按合同类型划分的客户流失率:与1年或2年合同的同行相比，预付费或按月连接的客户流失率非常高。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="48b0" class="ne lx it na b gy nf ng l nh ni">import matplotlib.ticker as mtick<br/>contract_churn =<br/>dataset.groupby(<br/>['Contract','Churn']).size().unstack()</span><span id="8b11" class="ne lx it na b gy nj ng l nh ni">contract_churn.rename(<br/>columns={0:'No', 1:'Yes'}, inplace=True)</span><span id="5553" class="ne lx it na b gy nj ng l nh ni">colors  = ['#ec838a','#9b9c9a']</span><span id="6db1" class="ne lx it na b gy nj ng l nh ni">ax = (contract_churn.T*100.0 / contract_churn.T.sum()).T.plot(kind='bar',width = 0.3,stacked = True,rot = 0,figsize = (12,7),color = colors)</span><span id="c484" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Proportion of Customers\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="9557" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('Contract Type\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="d676" class="ne lx it na b gy nj ng l nh ni">plt.title('Churn Rate by Contract type \n',<br/>horizontalalignment="center", fontstyle = "normal", <br/>fontsize = "22", fontfamily = "sans-serif")</span><span id="e37d" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")<br/>ax.yaxis.set_major_formatter(mtick.PercentFormatter())</span><span id="1dfc" class="ne lx it na b gy nj ng l nh ni">for p in ax.patches:<br/>    width, height = p.get_width(), p.get_height()<br/>    x, y = p.get_xy() <br/>    ax.text(x+width/2, <br/>            y+height/2, <br/>            '{:.1f}%'.format(height), <br/>            horizontalalignment='center', <br/>            verticalalignment='center')</span><span id="0adc" class="ne lx it na b gy nj ng l nh ni">ax.autoscale(enable=False, axis='both', tight=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/752b6d2e82acdd7709ad375c20d10b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60RmAKn-y-JGpOwBkHs2Jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按合同类型分类的流失率(按作者分类的图片)</p></figure><p id="6902" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">9.3.3。按付款方式分类的流失率:通过银行转账付款的客户似乎在所有付款方式类别中流失率最低。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="1f3e" class="ne lx it na b gy nf ng l nh ni">import matplotlib.ticker as mtick<br/>contract_churn = dataset.groupby(['Contract',<br/>'PaymentMethod']).size().unstack()</span><span id="0267" class="ne lx it na b gy nj ng l nh ni">contract_churn.rename(columns=<br/>{0:'No', 1:'Yes'}, inplace=True)</span><span id="ab22" class="ne lx it na b gy nj ng l nh ni">colors  = ['#ec838a','#9b9c9a', '#f3babc' , '#4d4f4c']</span><span id="cf11" class="ne lx it na b gy nj ng l nh ni">ax = (contract_churn.T*100.0 / contract_churn.T.sum()).T.plot(<br/>kind='bar',width = 0.3,stacked = True,rot = 0,figsize = (12,7),<br/>color = colors)</span><span id="f826" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Proportion of Customers\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="b7ae" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('Contract Type\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="6e36" class="ne lx it na b gy nj ng l nh ni">plt.title('Churn Rate by Payment Method \n',<br/>horizontalalignment="center", fontstyle = "normal", <br/>fontsize = "22", fontfamily = "sans-serif")</span><span id="c99b" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")<br/>ax.yaxis.set_major_formatter(mtick.PercentFormatter())</span><span id="504f" class="ne lx it na b gy nj ng l nh ni">for p in ax.patches:<br/>    width, height = p.get_width(), p.get_height()<br/>    x, y = p.get_xy() <br/>    ax.text(x+width/2, <br/>            y+height/2, <br/>            '{:.1f}%'.format(height), <br/>            horizontalalignment='center', <br/>            verticalalignment='center')</span><span id="14e6" class="ne lx it na b gy nj ng l nh ni">ax.autoscale(enable=False, axis='both', tight=False</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/91fb3c819ff55c17e4400c10d8c4a448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UL9vdPW4UUI10a1Q2Mv32Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按合同类型分类的流失率(按作者分类的图片)</p></figure><p id="387a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤9.4。找到正相关和负相关:</strong>有趣的是，流失率随着月费和年龄的增长而增加。相反，伴侣、受抚养人和任期似乎与流失负相关。让我们在下一步中用图表来看看正相关和负相关。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cd1d" class="ne lx it na b gy nf ng l nh ni">dataset2 = dataset[['SeniorCitizen', 'Partner', 'Dependents',<br/>       'tenure', 'PhoneService', 'PaperlessBilling',<br/>        'MonthlyCharges', 'TotalCharges']]</span><span id="fe27" class="ne lx it na b gy nj ng l nh ni">correlations = dataset2.corrwith(dataset.Churn)<br/>correlations = correlations[correlations!=1]<br/>positive_correlations = correlations[<br/>correlations &gt;0].sort_values(ascending = False)<br/>negative_correlations =correlations[<br/>correlations&lt;0].sort_values(ascending = False)</span><span id="8fcf" class="ne lx it na b gy nj ng l nh ni">print('Most Positive Correlations: \n', positive_correlations)<br/>print('\nMost Negative Correlations: \n', negative_correlations)</span></pre><p id="c848" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第9.5步。图正&amp;负相关:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="bfae" class="ne lx it na b gy nf ng l nh ni">correlations = dataset2.corrwith(dataset.Churn)<br/>correlations = correlations[correlations!=1]</span><span id="f7c3" class="ne lx it na b gy nj ng l nh ni">correlations.plot.bar(<br/>        figsize = (18, 10), <br/>        fontsize = 15, <br/>        color = '#ec838a',<br/>        rot = 45, grid = True)</span><span id="9d90" class="ne lx it na b gy nj ng l nh ni">plt.title('Correlation with Churn Rate \n',<br/>horizontalalignment="center", fontstyle = "normal", <br/>fontsize = "22", fontfamily = "sans-serif")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/0c7d9fb57e79746d17bb370363163d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4iLwUQie9-b18RKCxSbMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与流失率的相关性(图片由作者提供)</p></figure><p id="3618" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第9.6步。绘制所有自变量的相关矩阵:</strong>相关矩阵有助于我们发现数据集中自变量之间的二元关系。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9a92" class="ne lx it na b gy nf ng l nh ni">#Set and compute the Correlation Matrix:</span><span id="6a26" class="ne lx it na b gy nj ng l nh ni">sn.set(style="white")<br/>corr = dataset2.corr()</span><span id="be31" class="ne lx it na b gy nj ng l nh ni">#Generate a mask for the upper triangle:</span><span id="dead" class="ne lx it na b gy nj ng l nh ni">mask = np.zeros_like(corr, dtype=np.bool)<br/>mask[np.triu_indices_from(mask)] = True</span><span id="c926" class="ne lx it na b gy nj ng l nh ni">#Set up the matplotlib figure and a diverging colormap:</span><span id="67b6" class="ne lx it na b gy nj ng l nh ni">f, ax = plt.subplots(figsize=(18, 15))<br/>cmap = sn.diverging_palette(220, 10, as_cmap=True)</span><span id="f71d" class="ne lx it na b gy nj ng l nh ni">#Draw the heatmap with the mask and correct aspect ratio:</span><span id="66fe" class="ne lx it na b gy nj ng l nh ni">sn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,<br/>square=True, linewidths=.5, cbar_kws={"shrink": .5})</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/853c83cb04a54d9614a108d12326f699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ai4tqo7DCQNWk4hWrVbXWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自变量的相关矩阵(图片由作者提供)</p></figure><p id="7260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤9.7:使用VIF检查多重共线性:</strong>让我们尝试使用可变通货膨胀系数(VIF)来研究多重共线性。与相关矩阵不同，VIF确定一个变量与数据集中一组其他独立变量的相关性强度。VIF通常从1开始，任何超过10的地方都表明自变量之间的高度多重共线性。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8555" class="ne lx it na b gy nf ng l nh ni">def calc_vif(X):</span><span id="69f2" class="ne lx it na b gy nj ng l nh ni"># Calculating VIF<br/>    vif = pd.DataFrame()<br/>    vif["variables"] = X.columns<br/>    vif["VIF"] = [variance_inflation_factor(X.values, i) <br/>    for i in range(X.shape[1])]</span><span id="a3f5" class="ne lx it na b gy nj ng l nh ni">return(vif)</span><span id="2eae" class="ne lx it na b gy nj ng l nh ni">dataset2 = dataset[['gender', <br/>'SeniorCitizen', 'Partner', 'Dependents',<br/>'tenure', 'PhoneService',<br/>'PaperlessBilling','MonthlyCharges',<br/>'TotalCharges']]</span><span id="4503" class="ne lx it na b gy nj ng l nh ni">calc_vif(dataset2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/53ec3e172d00754769449bfc9df5d061.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*ipvPc4q7hwTe5aled7nI5A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算VIF(图片作者)</p></figure><p id="f577" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到“每月费用”和“总费用”具有很高的VIF值。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="a1f1" class="ne lx it na b gy nf ng l nh ni">'Total Charges' seem to be collinear with 'Monthly Charges'.</span><span id="9aee" class="ne lx it na b gy nj ng l nh ni">#Check colinearity:<br/>    <br/>dataset2[['MonthlyCharges', 'TotalCharges']].<br/>plot.scatter(<br/>figsize = (15, 10), <br/>x ='MonthlyCharges',<br/>y='TotalCharges', <br/>color =  '#ec838a')</span><span id="0e1c" class="ne lx it na b gy nj ng l nh ni">plt.title('Collinearity of Monthly Charges and Total Charges \n',<br/>horizontalalignment="center", fontstyle = "normal", fontsize = "22", fontfamily = "sans-serif")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/22ac04aa0418ee7dcc94c0f61eebe537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kt6s2O1QpHxGG0KVWcnDww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每月费用和总费用的共线性(图片由作者提供)</p></figure><p id="7df3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们尝试删除其中一个相关要素，看看它是否有助于降低相关要素之间的多重共线性:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5945" class="ne lx it na b gy nf ng l nh ni">#Dropping 'TotalCharges':<br/>    <br/>dataset2 = dataset2.drop(columns = "TotalCharges")</span><span id="e23d" class="ne lx it na b gy nj ng l nh ni">#Revalidate Colinearity:</span><span id="bbeb" class="ne lx it na b gy nj ng l nh ni">dataset2 = dataset[['gender', <br/>'SeniorCitizen', 'Partner', 'Dependents',<br/>'tenure', 'PhoneService', 'PaperlessBilling',<br/>'MonthlyCharges']]</span><span id="92a0" class="ne lx it na b gy nj ng l nh ni">calc_vif(dataset2)</span><span id="5e10" class="ne lx it na b gy nj ng l nh ni">#Applying changes in the main dataset:<br/>    <br/>dataset = dataset.drop(columns = "TotalCharges")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/c1c1daa6b24f5baf3034df26817efef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*qBWgw0p0g3SUTISasrTBqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算VIF(图片作者)</p></figure><p id="60c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，在去掉“总费用”变量后，所有独立变量的VIF值都大大降低了。</p><p id="5f34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">探索性数据分析结束语:</strong></p><p id="8726" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试着总结一下这次EDA的一些关键发现:</p><ul class=""><li id="79da" class="of og it lb b lc ld lf lg li oh lm oi lq oj lu ok ol om on bi translated">数据集没有任何缺失或错误的数据值。</li><li id="c2d6" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">与目标特征最强正相关的是月费和年龄，而与伴侣、家属和任期负相关。</li><li id="6bf7" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">数据集不平衡，大多数客户都很活跃。</li><li id="2142" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">每月费用和总费用之间存在多重共线性。总费用的下降大大降低了VIF值。</li><li id="574e" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">数据集中的大多数客户都是年轻人。</li><li id="3d84" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">组织中有许多新客户(不到10个月),随后是70个月以上的忠实客户群。</li><li id="7404" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">大多数客户似乎都有电话服务，每个客户的月费在18美元到118美元之间。</li><li id="c6e0" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">如果订阅了通过电子支票支付的服务，月结用户也很有可能流失。</li></ul><p id="14e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤10:对分类数据进行编码:</strong>任何具有两个以上唯一值的分类变量都已经在pandas中使用get_dummies方法进行了标签编码和一键编码。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="e2a8" class="ne lx it na b gy nf ng l nh ni">#Incase if user_id is an object:<br/>    <br/>identity = dataset["customerID"]</span><span id="1d79" class="ne lx it na b gy nj ng l nh ni">dataset = dataset.drop(columns="customerID")</span><span id="7b3e" class="ne lx it na b gy nj ng l nh ni">#Convert rest of categorical variable into dummy:</span><span id="7686" class="ne lx it na b gy nj ng l nh ni">dataset= pd.get_dummies(dataset)</span><span id="79fa" class="ne lx it na b gy nj ng l nh ni">#Rejoin userid to dataset:</span><span id="cc9c" class="ne lx it na b gy nj ng l nh ni">dataset = pd.concat([dataset, identity], axis = 1)</span></pre><p id="991d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第11步:将数据集分成因变量和自变量:</strong>现在我们需要将数据集分成X和y值。y将是“变动”列，而X将是数据集中独立变量的剩余列表。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7ceb" class="ne lx it na b gy nf ng l nh ni">#Identify response variable:<br/>    <br/>response = dataset["Churn"]</span><span id="3125" class="ne lx it na b gy nj ng l nh ni">dataset = dataset.drop(columns="Churn")</span></pre><p id="57e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第十二步:生成训练和测试数据集:</strong>让我们将主数据集解耦为80%-20%比例的训练和测试集。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="61df" class="ne lx it na b gy nf ng l nh ni">X_train, X_test, y_train, y_test = train_test_split(dataset, response,stratify=response, test_size = 0.2, #use 0.9 if data is huge.random_state = 0)</span><span id="8aa4" class="ne lx it na b gy nj ng l nh ni">#to resolve any class imbalance - use stratify parameter.</span><span id="b175" class="ne lx it na b gy nj ng l nh ni">print("Number transactions X_train dataset: ", X_train.shape)<br/>print("Number transactions y_train dataset: ", y_train.shape)<br/>print("Number transactions X_test dataset: ", X_test.shape)<br/>print("Number transactions y_test dataset: ", y_test.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/88ee00dedeff0786ac87ddcc45ccc74d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lE4-LYtN-EU4yO-mYTuYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练和测试数据集(图片由作者提供)</p></figure><p id="2694" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤13:移除标识符:</strong>将“customerID”从训练和测试数据帧中分离出来。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="d31f" class="ne lx it na b gy nf ng l nh ni">train_identity = X_train['customerID']<br/>X_train = X_train.drop(columns = ['customerID'])</span><span id="a6ad" class="ne lx it na b gy nj ng l nh ni">test_identity = X_test['customerID']<br/>X_test = X_test.drop(columns = ['customerID'])</span></pre><p id="eb34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤14:进行特征缩放:</strong>在进行任何机器学习(分类)算法之前，标准化变量是非常重要的，以便所有的训练和测试变量都在0到1的范围内缩放。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7509" class="ne lx it na b gy nf ng l nh ni">sc_X = StandardScaler()<br/>X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))<br/>X_train2.columns = X_train.columns.values<br/>X_train2.index = X_train.index.values<br/>X_train = X_train2</span><span id="45e3" class="ne lx it na b gy nj ng l nh ni">X_test2 = pd.DataFrame(sc_X.transform(X_test))<br/>X_test2.columns = X_test.columns.values<br/>X_test2.index = X_test.index.values<br/>X_test = X_test2</span></pre></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="90eb" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">C部分:型号选择</h1><p id="7ffd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">步骤15.1:比较基线分类算法(第一次迭代):</strong>让我们在训练数据集上对每个分类算法进行建模，并评估它们的准确性和标准偏差分数。</p><p id="cc9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分类准确性是比较基准算法的最常见的分类评估指标之一，因为它是正确预测的数量与总预测的比率。然而，当我们有阶级不平衡的问题时，这不是理想的衡量标准。因此，让我们根据“平均AUC”值对结果进行排序，该值只不过是模型区分阳性和阴性类别的能力。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="be57" class="ne lx it na b gy nf ng l nh ni">models = []</span><span id="1dd7" class="ne lx it na b gy nj ng l nh ni">models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state = 0,<br/>                                                         class_weight='balanced')))</span><span id="d7b3" class="ne lx it na b gy nj ng l nh ni">models.append(('SVC', SVC(kernel = 'linear', random_state = 0)))</span><span id="6f33" class="ne lx it na b gy nj ng l nh ni">models.append(('Kernel SVM', SVC(kernel = 'rbf', random_state = 0)))</span><span id="9334" class="ne lx it na b gy nj ng l nh ni">models.append(('KNN', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))</span><span id="24ec" class="ne lx it na b gy nj ng l nh ni">models.append(('Gaussian NB', GaussianNB()))</span><span id="745e" class="ne lx it na b gy nj ng l nh ni">models.append(('Decision Tree Classifier',<br/>               DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))</span><span id="2db6" class="ne lx it na b gy nj ng l nh ni">models.append(('Random Forest', RandomForestClassifier(<br/>    n_estimators=100, criterion = 'entropy', random_state = 0)))</span><span id="0ba8" class="ne lx it na b gy nj ng l nh ni">#Evaluating Model Results:</span><span id="6520" class="ne lx it na b gy nj ng l nh ni">acc_results = []<br/>auc_results = []<br/>names = []</span><span id="d015" class="ne lx it na b gy nj ng l nh ni"># set table to table to populate with performance results<br/>col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', <br/>       'Accuracy Mean', 'Accuracy STD']</span><span id="6ff9" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame(columns=col)<br/>i = 0</span><span id="7722" class="ne lx it na b gy nj ng l nh ni"># Evaluate each model using k-fold cross-validation:</span><span id="ed0b" class="ne lx it na b gy nj ng l nh ni">for name, model in models:<br/>    kfold = model_selection.KFold(<br/>        n_splits=10, random_state=0)</span><span id="37c2" class="ne lx it na b gy nj ng l nh ni"># accuracy scoring:<br/>cv_acc_results = model_selection.cross_val_score(  <br/>model, X_train, y_train, cv=kfold, scoring='accuracy')</span><span id="c6c8" class="ne lx it na b gy nj ng l nh ni"># roc_auc scoring:<br/>cv_auc_results = model_selection.cross_val_score(  <br/>model, X_train, y_train, cv=kfold, scoring='roc_auc')</span><span id="7696" class="ne lx it na b gy nj ng l nh ni">acc_results.append(cv_acc_results)<br/>    auc_results.append(cv_auc_results)<br/>    names.append(name)<br/>    model_results.loc[i] = [name,<br/>                         round(cv_auc_results.mean()*100, 2),<br/>                         round(cv_auc_results.std()*100, 2),<br/>                         round(cv_acc_results.mean()*100, 2),<br/>                         round(cv_acc_results.std()*100, 2)<br/>                         ]<br/>    i += 1<br/>    <br/>model_results.sort_values(by=['ROC AUC Mean'], ascending=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/3626607a043e2fb6fb12cab7dd6c8949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9Br3rOfRxK2B1pCXF5X1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较基线分类算法第一次迭代(图片由作者提供)</p></figure><p id="62b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第15.2步。可视化分类算法精度对比:</strong></p><p id="eb74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用精度均值:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="81ce" class="ne lx it na b gy nf ng l nh ni">fig = plt.figure(figsize=(15, 7))<br/>ax = fig.add_subplot(111)<br/>plt.boxplot(acc_results)<br/>ax.set_xticklabels(names)</span><span id="37d6" class="ne lx it na b gy nj ng l nh ni">#plt.ylabel('ROC AUC Score\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="123e" class="ne lx it na b gy nj ng l nh ni">#plt.xlabel('\n Baseline Classification Algorithms\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="7814" class="ne lx it na b gy nj ng l nh ni">plt.title('Accuracy Score Comparison \n',<br/>horizontalalignment="center", fontstyle = "normal", <br/>fontsize = "22", fontfamily = "sans-serif")</span><span id="6726" class="ne lx it na b gy nj ng l nh ni">#plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="adb7" class="ne lx it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/d5fef9538d657f6f20386f79ac26b7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BbYNhAn5vfwI6MEWcLwWrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">准确度分数比较(图片由作者提供)</p></figure><p id="fe89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用ROC曲线下面积:</strong>从基线分类算法的第一次迭代中，我们可以看到，对于具有最高平均AUC分数的所选数据集，逻辑回归和SVC已经优于其他五个模型。让我们在第二次迭代中再次确认我们的结果，如下面的步骤所示。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="078d" class="ne lx it na b gy nf ng l nh ni">fig = plt.figure(figsize=(15, 7))<br/>ax = fig.add_subplot(111)<br/>plt.boxplot(auc_results)<br/>ax.set_xticklabels(names)</span><span id="9a93" class="ne lx it na b gy nj ng l nh ni">#plt.ylabel('ROC AUC Score\n',<br/>horizontalalignment="center",fontstyle = "normal",<br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="3b5a" class="ne lx it na b gy nj ng l nh ni">#plt.xlabel('\n Baseline Classification Algorithms\n',<br/>horizontalalignment="center",fontstyle = "normal", <br/>fontsize = "large", fontfamily = "sans-serif")</span><span id="89d7" class="ne lx it na b gy nj ng l nh ni">plt.title('ROC AUC Comparison \n',horizontalalignment="center", fontstyle = "normal", fontsize = "22", <br/>fontfamily = "sans-serif")</span><span id="a09c" class="ne lx it na b gy nj ng l nh ni">#plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="913c" class="ne lx it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/7454630c59633fa90ee8dad5cf11f135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPjSil3a7PyAumON4VTuTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ROC AUC比较(图片由作者提供)</p></figure><p id="4d08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.3。为基线模型获取正确的参数:</strong>在进行第二次迭代之前，让我们优化参数并最终确定模型选择的评估指标。</p><p id="6a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">为KNN模型确定K个邻居的最佳数量:</strong>在第一次迭代中，我们假设K = 3，但实际上，我们不知道为所选训练数据集提供最大准确度的最佳K值是什么。因此，让我们编写一个迭代20到30次的for循环，并给出每次迭代的精度，以便计算出KNN模型的K个邻居的最佳数量。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9da6" class="ne lx it na b gy nf ng l nh ni">score_array = []<br/>for each in range(1,25):<br/>    knn_loop = KNeighborsClassifier(n_neighbors = each) </span><span id="21a5" class="ne lx it na b gy nj ng l nh ni">#set K neighbor as 3<br/>    knn_loop.fit(X_train,y_train)<br/>    score_array.append(knn_loop.score(X_test,y_test))</span><span id="2f5b" class="ne lx it na b gy nj ng l nh ni">fig = plt.figure(figsize=(15, 7))<br/>plt.plot(range(1,25),score_array, color = '#ec838a')</span><span id="1796" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Range\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="b0eb" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('Score\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="abe5" class="ne lx it na b gy nj ng l nh ni">plt.title('Optimal Number of K Neighbors \n',<br/>horizontalalignment="center", fontstyle = "normal",<br/> fontsize = "22", fontfamily = "sans-serif")</span><span id="b1bb" class="ne lx it na b gy nj ng l nh ni">#plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="0e3b" class="ne lx it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/bbf4a071d3836e4f9137cc3922b02e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPvNXvgyPKB7OjCDQgxo7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">K近邻的最佳数量(图片由作者提供)</p></figure><p id="a191" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的迭代可以看出，如果我们使用K = 22，那么我们将得到78%的最高分。</p><p id="d9a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">确定随机森林模型的最佳树木数量:</strong>与KNN模型中的迭代非常相似，这里我们试图找到最佳决策树数量，以组成最佳随机森林。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="7680" class="ne lx it na b gy nf ng l nh ni">score_array = []<br/>for each in range(1,100):<br/>    rf_loop = RandomForestClassifier(<br/>n_estimators = each, random_state = 1) </span><span id="4ef0" class="ne lx it na b gy nj ng l nh ni">    rf_loop.fit(X_train,y_train)</span><span id="a61a" class="ne lx it na b gy nj ng l nh ni">    score_array.append(rf_loop.score(X_test,y_test))<br/> <br/>fig = plt.figure(figsize=(15, 7))<br/>plt.plot(range(1,100),score_array, color = '#ec838a')</span><span id="f26d" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Range\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="22b9" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('Score\n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "large", <br/>fontfamily = "sans-serif")</span><span id="3cc9" class="ne lx it na b gy nj ng l nh ni">plt.title('Optimal Number of Trees for Random Forest Model \n',horizontalalignment="center", fontstyle = "normal", fontsize = "22", fontfamily = "sans-serif")</span><span id="2741" class="ne lx it na b gy nj ng l nh ni">#plt.legend(loc='top right', fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="51bc" class="ne lx it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/1463186d936bc3570ad12ee5ca54f17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrSJbr5hZWjr3lkvM3_GGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林模型的最佳树数(图片由作者提供)</p></figure><p id="d10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的迭代中我们可以看出，当n_estimators = 72时，随机森林模型将获得最高的精度分数。</p><p id="d6ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第15.4步。比较基线分类算法(第二次迭代):</strong></p><p id="0d51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在比较基线分类算法的第二次迭代中，我们将使用KNN和随机森林模型的优化参数。此外，我们知道，在流失中，假阴性比假阳性的成本更高，因此让我们使用精确度、召回率和F2分数作为模型选择的理想指标。</p><p id="2a0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.1。逻辑回归:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="30a3" class="ne lx it na b gy nf ng l nh ni"># Fitting Logistic Regression to the Training set<br/>classifier = LogisticRegression(random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="8d8d" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="3bd2" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="fb92" class="ne lx it na b gy nj ng l nh ni">results = pd.DataFrame([['Logistic Regression', <br/>acc, prec, rec, f1, f2]], columns = ['Model', <br/>'Accuracy', 'Precision', 'Recall', 'F1 Score', <br/>'F2 Score'])</span><span id="10a5" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="124c" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/e0651db9529752ac2434214d17fb427e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQnYo4uENQFSxuRuKKi4Yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逻辑回归结果(图片由作者提供)</p></figure><p id="70ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.2。支持向量机(线性分类器):</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5277" class="ne lx it na b gy nf ng l nh ni"># Fitting SVM (SVC class) to the Training set<br/>classifier = SVC(kernel = 'linear', random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="0d94" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results y_pred = classifier.predict(X_test)</span><span id="3ea0" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred)<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="f534" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame(<br/>[['SVM (Linear)', acc, prec, rec, f1, f2]],<br/>columns = ['Model', 'Accuracy', 'Precision', <br/>'Recall', 'F1 Score', 'F2 Score'])</span><span id="4047" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="bb3a" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="b67b" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/3c27acd679ab38ec8fd4438b743deadf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCVyNX-4VImfTb9hoJD6mw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SVM线性结果(图片由作者提供)</p></figure><p id="63d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.3。k-最近邻:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="edc3" class="ne lx it na b gy nf ng l nh ni"># Fitting KNN to the Training set:<br/>classifier = KNeighborsClassifier(<br/>n_neighbors = 22, <br/>metric = 'minkowski', p = 2)<br/>classifier.fit(X_train, y_train)</span><span id="2200" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results <br/>y_pred  = classifier.predict(X_test)</span><span id="5034" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="6c45" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame([['K-Nearest Neighbours', <br/>acc, prec, rec, f1, f2]], columns = ['Model',<br/> 'Accuracy', 'Precision', 'Recall',<br/> 'F1 Score', 'F2 Score'])</span><span id="0f4c" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="dbce" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="f0e7" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/cb37be7e57f3db3eef1c0ffe12ca1581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sg4TR0jLIfKwNIoIEIDXTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-最近邻(图片由作者提供)</p></figure><p id="eb78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.4。内核SVM: </strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6825" class="ne lx it na b gy nf ng l nh ni"># Fitting Kernel SVM to the Training set:<br/>classifier = SVC(kernel = 'rbf', random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="5295" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results <br/>y_pred = classifier.predict(X_test)</span><span id="a7b1" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="5a01" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame([[<br/>'Kernel SVM', acc, prec, rec, f1, f2]],<br/>columns = ['Model', 'Accuracy', 'Precision', <br/>'Recall', 'F1 Score', 'F2 Score'])</span><span id="1b66" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="ee0c" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="2970" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/c00030b5853e5694a33a8546781c5072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEkqwHrqyjKlwCBfUbqNJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内核SVM结果(图片由作者提供)</p></figure><p id="9219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.5。天真的轮空:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="cbee" class="ne lx it na b gy nf ng l nh ni"># Fitting Naive Byes to the Training set:<br/>classifier = GaussianNB()<br/>classifier.fit(X_train, y_train)</span><span id="cf51" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results <br/>y_pred = classifier.predict(X_test)</span><span id="cca0" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="4a2c" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame([[<br/>'Naive Byes', acc, prec, rec, f1, f2]],<br/>columns = ['Model', 'Accuracy', 'Precision',<br/>'Recall', 'F1 Score', 'F2 Score'])</span><span id="0a15" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="155b" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="a7df" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/85c9c47a2efcd63970fde99e3973a274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Kn_2UITkL4gluhr-GWhVA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">天真的Byes结果(作者图片)</p></figure><p id="1cb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.6。决策树:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="1ca9" class="ne lx it na b gy nf ng l nh ni"># Fitting Decision Tree to the Training set:</span><span id="35db" class="ne lx it na b gy nj ng l nh ni">classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="2491" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results <br/>y_pred = classifier.predict(X_test)</span><span id="322b" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="30a1" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame([[<br/>'Decision Tree', acc, prec, rec, f1, f2]],<br/> columns = ['Model', 'Accuracy', 'Precision', <br/>'Recall', 'F1 Score', 'F2 Score'])</span><span id="596f" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="4171" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="801a" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/9baf1ca29c2d6807904eda04debfffc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YiWrfsB8S8Acm_df498l-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树结果(图片由作者提供)</p></figure><p id="0794" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤15.4.7。随机森林:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="23ec" class="ne lx it na b gy nf ng l nh ni"># Fitting Random Forest to the Training set:<br/>    <br/>classifier = RandomForestClassifier(n_estimators = 72, <br/>criterion = 'entropy', random_state = 0)<br/>classifier.fit(X_train, y_train)</span><span id="29fe" class="ne lx it na b gy nj ng l nh ni"># Predicting the Test set results <br/>y_pred = classifier.predict(X_test)</span><span id="0baa" class="ne lx it na b gy nj ng l nh ni">#Evaluate results<br/>from sklearn.metrics import confusion_matrix, <br/>accuracy_score, f1_score, precision_score, recall_score<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="580b" class="ne lx it na b gy nj ng l nh ni">model_results = pd.DataFrame([['Random Forest', <br/>acc, prec, rec, f1, f2]],<br/>columns = ['Model', 'Accuracy', 'Precision', <br/>'Recall', 'F1 Score', 'F2 Score'])</span><span id="4f2e" class="ne lx it na b gy nj ng l nh ni">results = results.append(model_results, ignore_index = True)</span><span id="c4df" class="ne lx it na b gy nj ng l nh ni">results = results.sort_values(["Precision", <br/>"Recall", "F2 Score"], ascending = False)</span><span id="8853" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/5e97bd935f5088120df596b6e7ef8fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dGlTrM6H-vmcY4eAlJd8Hw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较基线分类算法第二次迭代(图片由作者提供)</p></figure><p id="a3cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从第二次迭代，我们可以明确地得出结论，逻辑回归是给定数据集的最佳选择模型，因为它具有相对最高的精确度、召回率和F2分数的组合；给出最大数量的正确肯定预测，同时最小化假否定。因此，让我们在接下来的章节中尝试使用逻辑回归并评估其性能。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="c7ee" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">D部分:模型评估</h1><p id="292c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">步骤16:训练&amp;评估选择的模型:</strong>让我们将选择的模型(在这种情况下是逻辑回归)拟合到训练数据集上，并评估结果。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="bb26" class="ne lx it na b gy nf ng l nh ni">classifier = LogisticRegression(random_state = 0,<br/>penalty = 'l2')<br/>classifier.fit(X_train, y_train)</span><span id="0f04" class="ne lx it na b gy nj ng l nh ni"># Predict the Test set results<br/>y_pred = classifier.predict(X_test)</span><span id="f2d7" class="ne lx it na b gy nj ng l nh ni">#Evaluate Model Results on Test Set:<br/>acc = accuracy_score(y_test, y_pred )<br/>prec = precision_score(y_test, y_pred )<br/>rec = recall_score(y_test, y_pred )<br/>f1 = f1_score(y_test, y_pred )<br/>f2 = fbeta_score(y_test, y_pred, beta=2.0)</span><span id="2ba6" class="ne lx it na b gy nj ng l nh ni">results = pd.DataFrame([['Logistic Regression',<br/>acc, prec, rec, f1, f2]],columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'F2 Score'])</span><span id="6135" class="ne lx it na b gy nj ng l nh ni">print (results)</span></pre><p id="63a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> k-Fold交叉验证:</strong>模型评估通常通过“k-Fold交叉验证”技术完成，该技术主要帮助我们修正方差。当我们在训练集和测试集上运行模型时获得了良好的准确性，但当模型在另一个测试集上运行时，准确性看起来有所不同，这时就会出现方差问题。</p><p id="816e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了解决方差问题，k折叠交叉验证基本上将训练集分成10个折叠，并在测试折叠上测试之前，在9个折叠(训练数据集的9个子集)上训练模型。这给了我们在所有十种9折组合上训练模型的灵活性；为最终确定差异提供了充足的空间。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6370" class="ne lx it na b gy nf ng l nh ni">accuracies = cross_val_score(estimator = classifier,<br/> X = X_train, y = y_train, cv = 10)</span><span id="abb6" class="ne lx it na b gy nj ng l nh ni">print("Logistic Regression Classifier Accuracy: <br/>%0.2f (+/- %0.2f)"  % (accuracies.mean(), <br/>accuracies.std() * 2))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/59eb6c52c68cd0e8b08534a24f200eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8JeopmIX95H1sJbyGAYSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k倍交叉验证结果(图片由作者提供)</p></figure><p id="72ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们的k-fold交叉验证结果表明，在任何测试集上运行该模型时，我们的准确率都在76%到84%之间。</p><p id="fd7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在混淆矩阵上可视化结果:</strong>混淆矩阵表明我们有208+924个正确预测和166+111个错误预测。</p><p id="0c36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">准确率=正确预测数/总预测数* 100 <br/>错误率=错误预测数/总预测数* 100</p><p id="c01f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有80%的准确率；标志着一个相当好的模型的特征。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="68ac" class="ne lx it na b gy nf ng l nh ni">cm = confusion_matrix(y_test, y_pred) <br/>df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))<br/>plt.figure(figsize = (28,20))</span><span id="475b" class="ne lx it na b gy nj ng l nh ni">fig, ax = plt.subplots()<br/>sn.set(font_scale=1.4)<br/>sn.heatmap(df_cm, annot=True, fmt='g'#,cmap="YlGnBu" <br/>           )<br/>class_names=[0,1]<br/>tick_marks = np.arange(len(class_names))</span><span id="7d68" class="ne lx it na b gy nj ng l nh ni">plt.tight_layout()<br/>plt.title('Confusion matrix\n', y=1.1)<br/>plt.xticks(tick_marks, class_names)<br/>plt.yticks(tick_marks, class_names)<br/>ax.xaxis.set_label_position("top")</span><span id="63f4" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('Actual label\n')<br/>plt.xlabel('Predicted label\n')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/cedc48f2cbb5de565e7fc050110ad06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFrYriNx0fIqgPyL0xAnZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">困惑矩阵(图片由作者提供)</p></figure><p id="2198" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用ROC图评估模型:</strong>用ROC图重新评估模型很好。ROC图向我们展示了一个模型基于AUC平均分数区分类别的能力。橙色线代表随机分类器的ROC曲线，而好的分类器试图尽可能远离该线。如下图所示，微调后的逻辑回归模型显示了更高的AUC得分。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6884" class="ne lx it na b gy nf ng l nh ni">classifier.fit(X_train, y_train) <br/>probs = classifier.predict_proba(X_test) <br/>probs = probs[:, 1] <br/>classifier_roc_auc = accuracy_score(y_test, y_pred )</span><span id="c9cb" class="ne lx it na b gy nj ng l nh ni">rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])<br/>plt.figure(figsize=(14, 6))</span><span id="f87a" class="ne lx it na b gy nj ng l nh ni"># Plot Logistic Regression ROC</span><span id="9cdd" class="ne lx it na b gy nj ng l nh ni">plt.plot(rf_fpr, rf_tpr, <br/>label='Logistic Regression (area = %0.2f)' % classifier_roc_auc)</span><span id="b6b6" class="ne lx it na b gy nj ng l nh ni"># Plot Base Rate ROC<br/>plt.plot([0,1], [0,1],label='Base Rate' 'k--')</span><span id="141a" class="ne lx it na b gy nj ng l nh ni">plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])</span><span id="2564" class="ne lx it na b gy nj ng l nh ni">plt.ylabel('True Positive Rate \n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "medium", <br/>fontfamily = "sans-serif")</span><span id="a7ba" class="ne lx it na b gy nj ng l nh ni">plt.xlabel('\nFalse Positive Rate \n',horizontalalignment="center",<br/>fontstyle = "normal", fontsize = "medium", <br/>fontfamily = "sans-serif")</span><span id="4e05" class="ne lx it na b gy nj ng l nh ni">plt.title('ROC Graph \n',horizontalalignment="center", <br/>fontstyle = "normal", fontsize = "22", <br/>fontfamily = "sans-serif")</span><span id="cf49" class="ne lx it na b gy nj ng l nh ni">plt.legend(loc="lower right", fontsize = "medium")<br/>plt.xticks(rotation=0, horizontalalignment="center")<br/>plt.yticks(rotation=0, horizontalalignment="right")</span><span id="0c4f" class="ne lx it na b gy nj ng l nh ni">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/703ac4dd2bc3f8ac4aa60eff70fbe510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUD2kRAJ-vBvIzh_eBczcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ROC图(图片由作者提供)</p></figure><p id="a28b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤17:预测特征重要性:</strong>逻辑回归允许我们确定对预测目标属性有意义的关键特征(在这个项目中为“流失”)。</p><p id="4027" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">逻辑回归模型预测，流失率将随着逐月合同、光纤互联网服务、电子支票、支付安全和技术支持的缺失而正增长。</p><p id="2a80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，如果任何客户订阅了在线证券、一年期合同或选择邮寄支票作为支付媒介，该模型预测与客户流失呈负相关。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="da50" class="ne lx it na b gy nf ng l nh ni"># Analyzing Coefficients</span><span id="8b8b" class="ne lx it na b gy nj ng l nh ni">feature_importances = pd.concat([<br/>pd.DataFrame(dataset.drop(columns = 'customerID').<br/>columns, columns = ["features"]),<br/>pd.DataFrame(np.transpose(classifier.coef_), <br/>columns = ["coef"])],axis = 1)<br/>feature_importances.sort_values("coef", ascending = False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/93dc24f8d789d0ba22482a978c5e18e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*pKJ8Y4qIe442_OpHu0M5nQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测功能的重要性(图片由作者提供)</p></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="5a89" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">E部分:模型改进</h1><p id="05d9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">模型改进基本上包括为我们提出的机器学习模型选择最佳参数。任何机器学习模型中都有两种类型的参数——第一种类型是模型学习的参数类型；通过运行模型自动找到最佳值。第二类参数是用户在运行模型时可以选择的参数。这样的参数被称为超参数；模型外部的一组可配置值，这些值无法由数据确定，我们正试图通过随机搜索或网格搜索等参数调整技术来优化这些值。</p><p id="d3c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数调整可能不会每次都改进模型。例如，当我们试图进一步调优模型时，我们最终得到的准确度分数低于默认分数。我只是在这里演示超参数调整的步骤，以供将来参考。</p><p id="769c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤18:通过网格搜索进行超参数调谐:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="de50" class="ne lx it na b gy nf ng l nh ni"># Round 1:<br/> <br/># Select Regularization Method   <br/>import time<br/>penalty = ['l1', 'l2']</span><span id="d0ea" class="ne lx it na b gy nj ng l nh ni"># Create regularization hyperparameter space<br/>C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]</span><span id="7efb" class="ne lx it na b gy nj ng l nh ni"># Combine Parameters<br/>parameters = dict(C=C, penalty=penalty)</span><span id="f298" class="ne lx it na b gy nj ng l nh ni">lr_classifier = GridSearchCV(estimator = classifier,<br/>                           param_grid = parameters,<br/>                           scoring = "balanced_accuracy",<br/>                           cv = 10,<br/>                           n_jobs = -1)<br/>t0 = time.time()<br/>lr_classifier  = lr_classifier .fit(X_train, y_train)<br/>t1 = time.time()<br/>print("Took %0.2f seconds" % (t1 - t0))</span><span id="d126" class="ne lx it na b gy nj ng l nh ni">lr_best_accuracy = lr_classifier.best_score_<br/>lr_best_parameters = lr_classifier.best_params_<br/>lr_best_accuracy, lr_best_parameters</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/2ffd176d16ba0cc6979dfab54b9271a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVisDy8AlF0sgKAbwgNo2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">超参数调整-第一轮(图片由作者提供)</p></figure><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="1c8c" class="ne lx it na b gy nf ng l nh ni"># Round 2:</span><span id="d253" class="ne lx it na b gy nj ng l nh ni"># Select Regularization Method<br/>import time<br/>penalty = ['l2']</span><span id="417d" class="ne lx it na b gy nj ng l nh ni"># Create regularization hyperparameter space<br/>C = [ 0.0001, 0.001, 0.01, 0.02, 0.05]</span><span id="a972" class="ne lx it na b gy nj ng l nh ni"># Combine Parameters<br/>parameters = dict(C=C, penalty=penalty)</span><span id="b053" class="ne lx it na b gy nj ng l nh ni">lr_classifier = GridSearchCV(estimator = classifier,<br/>                           param_grid = parameters,<br/>                           scoring = "balanced_accuracy",<br/>                           cv = 10,<br/>                           n_jobs = -1)<br/>t0 = time.time()<br/>lr_classifier  = lr_classifier .fit(X_train, y_train)<br/>t1 = time.time()<br/>print("Took %0.2f seconds" % (t1 - t0))</span><span id="2f39" class="ne lx it na b gy nj ng l nh ni">lr_best_accuracy = lr_classifier.best_score_<br/>lr_best_parameters = lr_classifier.best_params_<br/>lr_best_accuracy, lr_best_parameters</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/72c997b31f7a9da1ea8bdace6562556c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PF2KHSrUsQeAfp6HN-vgIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">超参数调整-第二轮(图片由作者提供)</p></figure><p id="365f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤18.2:最终超参数调整和选择:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="360c" class="ne lx it na b gy nf ng l nh ni">lr_classifier = LogisticRegression(random_state = 0, penalty = 'l2')<br/>lr_classifier.fit(X_train, y_train)</span><span id="e171" class="ne lx it na b gy nj ng l nh ni"># Predict the Test set results</span><span id="6ba5" class="ne lx it na b gy nj ng l nh ni">y_pred = lr_classifier.predict(X_test)</span><span id="4330" class="ne lx it na b gy nj ng l nh ni">#probability score<br/>y_pred_probs = lr_classifier.predict_proba(X_test)<br/>y_pred_probs  = y_pred_probs [:, 1]</span></pre></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="958c" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">F部分:未来预测</h1><p id="5af7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">步骤19:将预测与测试集进行比较:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2c4b" class="ne lx it na b gy nf ng l nh ni">#Revalidate final results with Confusion Matrix:</span><span id="d4fd" class="ne lx it na b gy nj ng l nh ni">cm = confusion_matrix(y_test, y_pred) <br/>print (cm)</span><span id="c751" class="ne lx it na b gy nj ng l nh ni">#Confusion Matrix as a quick Crosstab:<br/>    <br/>pd.crosstab(y_test,pd.Series(y_pred),<br/>rownames=['ACTUAL'],colnames=['PRED'])</span><span id="4faf" class="ne lx it na b gy nj ng l nh ni">#visualize Confusion Matrix:</span><span id="75c4" class="ne lx it na b gy nj ng l nh ni">cm = confusion_matrix(y_test, y_pred) <br/>df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))<br/>plt.figure(figsize = (28,20))</span><span id="e7ac" class="ne lx it na b gy nj ng l nh ni">fig, ax = plt.subplots()<br/>sn.set(font_scale=1.4)<br/>sn.heatmap(df_cm, annot=True, fmt='g'#,cmap="YlGnBu" <br/>           )<br/>class_names=[0,1]<br/>tick_marks = np.arange(len(class_names))<br/>plt.tight_layout()<br/>plt.title('Confusion matrix\n', y=1.1)<br/>plt.xticks(tick_marks, class_names)<br/>plt.yticks(tick_marks, class_names)<br/>ax.xaxis.set_label_position("top")<br/>plt.ylabel('Actual label\n')<br/>plt.xlabel('Predicted label\n')</span><span id="f7b6" class="ne lx it na b gy nj ng l nh ni">print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_pred))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/cedc48f2cbb5de565e7fc050110ad06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFrYriNx0fIqgPyL0xAnZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">困惑矩阵(图片由作者提供)</p></figure><p id="60b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤20:格式化最终结果:</strong>不可预测性和风险是任何预测模型的亲密伴侣。因此，在现实世界中，除了绝对的预测结果之外，建立一个倾向评分总是一个好的做法。<strong class="lb iu"> </strong>除了检索二进制估计目标结果(0或1)，每个“客户ID”都可以获得一个额外的倾向得分层，突出显示他们采取目标行动的概率百分比。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="538a" class="ne lx it na b gy nf ng l nh ni">final_results = pd.concat([test_identity, y_test], axis = 1).dropna()</span><span id="159f" class="ne lx it na b gy nj ng l nh ni">final_results['predictions'] = y_pred</span><span id="51da" class="ne lx it na b gy nj ng l nh ni">final_results["propensity_to_churn(%)"] = y_pred_probs</span><span id="6d38" class="ne lx it na b gy nj ng l nh ni">final_results["propensity_to_churn(%)"] = final_results["propensity_to_churn(%)"]*100</span><span id="9061" class="ne lx it na b gy nj ng l nh ni">final_results["propensity_to_churn(%)"]=final_results["propensity_to_churn(%)"].round(2)</span><span id="3f3a" class="ne lx it na b gy nj ng l nh ni">final_results = final_results[['customerID', 'Churn', 'predictions', 'propensity_to_churn(%)']]</span><span id="9453" class="ne lx it na b gy nj ng l nh ni">final_results ['Ranking'] = pd.qcut(final_results['propensity_to_churn(%)'].rank(method = 'first'),10,labels=range(10,0,-1))</span><span id="0739" class="ne lx it na b gy nj ng l nh ni">print (final_results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/c3fc1e938a936a97f072861d9f105a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEgyudpBJZlrEFI-2T-CCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高风险类别(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/69b1e1daa0f8558dbdb44cfc484a79ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M49-TNkL8OgTE_32CrEfQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">低风险类别(图片由作者提供)</p></figure></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="f7ff" class="lw lx it bd ly lz nz mb mc md oa mf mg jz ob ka mi kc oc kd mk kf od kg mm mn bi translated">G部分:模型部署</h1><p id="cc72" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最后，使用“joblib”库将模型部署到服务器上，这样我们就可以生产端到端的机器学习框架。稍后，我们可以在任何新的数据集上运行该模型，以预测未来几个月内任何客户流失的概率。</p><p id="b429" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第21步:保存模型:</strong></p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="03b8" class="ne lx it na b gy nf ng l nh ni">filename = 'final_model.model'<br/>i = [lr_classifier]<br/>joblib.dump(i,filename)</span></pre><p id="3abd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="6eed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，简而言之，我们利用Kaggle的客户流失数据集来建立一个机器学习分类器，该分类器可以预测任何客户在未来几个月内的流失倾向，准确率达到76%至84%。</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="c88f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">接下来是什么？</strong></p><ul class=""><li id="afef" class="of og it lb b lc ld lf lg li oh lm oi lq oj lu ok ol om on bi translated">与组织的销售和营销团队分享您从探索性数据分析部分获得的关于客户人口统计和流失率的关键见解。让销售团队了解与客户流失有积极和消极关系的特征，以便他们能够相应地制定保留计划。</li><li id="0ac5" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">此外，根据倾向得分将即将到来的客户分为<strong class="lb iu">高风险</strong>(倾向得分&gt; 80%的客户)、<strong class="lb iu">中风险</strong>(倾向得分在60-80%之间的客户)和最后<strong class="lb iu">低风险</strong>类别(倾向得分&lt; 60%的客户)。预先关注每个客户群，确保他们的需求得到充分满足。</li><li id="75ad" class="of og it lb b lc oo lf op li oq lm or lq os lu ok ol om on bi translated">最后，通过计算当前财务季度的流失率来衡量这项任务的投资回报(ROI)。将季度结果与去年或前年的同一季度进行比较，并与贵组织的高级管理层分享结果。</li></ul></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="e135" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> GitHub库</strong></p><p id="8e00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经从Github的许多人那里学到了(并且还在继续学到)。因此，在一个公共的<a class="ae ky" href="https://github.com/srees1988/predict-churn-py" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中分享我的整个python脚本和支持文件，以防它对任何在线搜索者有益。此外，如果您在理解Python中有监督的机器学习算法的基础方面需要任何帮助，请随时联系我。乐于分享我所知道的:)希望这有所帮助！</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="7957" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">关于作者</strong></p><div class="py pz gp gr qa qb"><a href="https://srees.org/about" rel="noopener  ugc nofollow" target="_blank"><div class="qc ab fo"><div class="qd ab qe cl cj qf"><h2 class="bd iu gy z fp qg fr fs qh fu fw is bi translated">Sreejith Sreedharan - Sree</h2><div class="qi l"><h3 class="bd b gy z fp qg fr fs qh fu fw dk translated">数据爱好者。不多不少！你好！我是Sreejith Sreedharan，又名Sree一个永远好奇的数据驱动的…</h3></div><div class="qj l"><p class="bd b dl z fp qg fr fs qh fu fw dk translated">srees.org</p></div></div><div class="qk l"><div class="ql l qm qn qo qk qp ks qb"/></div></div></a></div></div></div>    
</body>
</html>