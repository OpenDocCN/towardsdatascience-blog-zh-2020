<html>
<head>
<title>Performance Testing an ML-Serving API with Locust!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Locust对ML服务API进行性能测试！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/performance-testing-an-ml-serving-api-with-locust-ecd98ab9b7f7?source=collection_archive---------15-----------------------#2020-09-20">https://towardsdatascience.com/performance-testing-an-ml-serving-api-with-locust-ecd98ab9b7f7?source=collection_archive---------15-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e057d574ad7fb479763699821a6c13b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JmY0qUBEhd4u1MG2Sywqhg.png"/></div></div></figure><div class=""/><div class=""><h2 id="965a" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">确保您的ML-serving API在生产中使用时能够处理适当的预期性能负载</h2></div><p id="940b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">朋友们，又见面了！欢迎回到另一个数据科学快速技巧。现在，当谈到数据科学的整个领域(从发现到生产)时，这篇文章肯定会落在这个领域的末端。事实上，一些公司可能认为这是机器学习工程师的工作，而不是数据科学家的工作。作为一名机器学习工程师，我可以证明我的情况确实如此。</p><p id="e03d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尽管如此，我相信有许多数据科学家负责部署他们自己的机器学习模型，这篇文章有望为如何使用这个名为Locust的简洁工具进行简单的性能测试提供一些启示。</p><p id="b36b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我们开始性能测试之前，让我们花一点时间讨论一下API本身。使用我们在以前的一篇文章中创建的虚拟模型，我们将使用Flask和Gunicorn来服务API端点后面的模型。用户将向端点发布适当的JSON数据，并从机器学习模型接收回预期的预测。现在，这篇文章不是关于创建一个ML-serving API，所以我很快为我们的目的创建了一个。为了使用它，您需要做的就是<a class="ae lp" href="https://github.com/dkhundley/ds-quick-tips/tree/master/007_performance_testing_locust" rel="noopener ugc nofollow" target="_blank">从GitHub </a>下载我的代码，导航到“api”文件夹，并在您的终端中运行以下命令:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="ef7e" class="lz ma je lv b gy mb mc l md me">bash run.sh</span></pre><p id="6dee" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这将做的是在本地主机:5001的一个Gunicorn服务器后面运行您的API。如果你看到这个屏幕，你就在正确的轨道上。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mf"><img src="../Images/e451a70c77b7002c4352e582cf3c011c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ISHdqZUqU0_uXvLkeHWFQQ.png"/></div></div></figure><p id="34db" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在终端中保持该标签打开，并打开一个新标签。只是为了验证API是否真的在工作，我创建了一个单独的小测试，它将通过API快速运行2次观察。在同一个代码库中，导航到“test_data”目录并运行以下命令:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="bbfc" class="lz ma je lv b gy mb mc l md me">bash tests.sh</span></pre><p id="202d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果API正常工作，您应该会看到下面的屏幕:</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mg"><img src="../Images/f9c0bc2932ad028699b5896e8df5cf1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0dl6xzRHejZGVUokw-6dA.png"/></div></div></figure><p id="9321" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">好了，我们准备进入这篇文章的核心部分:性能测试！当机器学习模型在生产环境中的API上下文中使用时，确保它能够处理适当的请求负载是非常重要的。如果你有太多的用户或太多的请求，你可能会遇到一些大问题。你不想成为导致产量下降的人吧！</p><p id="ac7e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">幸运的是，人们制作了这个叫做Locust(或Locust.io)的工具来帮助解决这个问题。起初，代码可能看起来很奇怪，但是我们将在这里简单地解释一下，这样您就可以很快开始运行了。</p><p id="5a56" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，你可能需要第一次在你的机器上安装Locust。简单到可以做！只需运行以下pip命令从PyPi下载Locust:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="3e53" class="lz ma je lv b gy mb mc l md me">pip install locust</span></pre><p id="f1a2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">好了，现在我们准备好构建我们的Locustfile了！Locustfile是一个简单的Python脚本，我们将调用它来启动Locust，它的用户界面非常方便。默认情况下，Locust命令行工具会查找一个名为“locustfile.py”的文件，但是您可以随意命名它(只要您用-f标志指定它)。为了简单起见，我们将我们的文件命名为默认的locustfile.py，这是我们要放入的所有内容。</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="3cdd" class="lz ma je lv b gy mb mc l md me">from locust import HttpUser, task, between<br/>import json</span><span id="8c22" class="lz ma je lv b gy mh mc l md me"># Loading the test JSON data<br/>with open('test_data/test_1.json') as f:<br/>    test_data = json.loads(f.read())</span><span id="3773" class="lz ma je lv b gy mh mc l md me"># Creating an API User class inheriting from Locust's HttpUser class<br/>class APIUser(HttpUser):<br/>    # Setting the host name and wait_time<br/>    host = '<a class="ae lp" href="http://localhost:5001'" rel="noopener ugc nofollow" target="_blank">http://localhost:5001'</a><br/>    wait_time = between(3, 5)</span><span id="d528" class="lz ma je lv b gy mh mc l md me">    # Defining the post task using the JSON test data<br/>    <a class="ae lp" href="http://twitter.com/task" rel="noopener ugc nofollow" target="_blank">@task</a>()<br/>    def predict_endpoint(self):<br/>        self.client.post('/predict', json = test_data)</span></pre><p id="4ba2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是一个非常小的脚本，但它将为我们做一些强大的事情！现在，您第一次看到这个语法时，可能会有点奇怪，所以让我们一点一点地分解它，以便您理解这里发生了什么。从这第一点开始…</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="8269" class="lz ma je lv b gy mb mc l md me">from locust import HttpUser, task, between<br/>import json</span><span id="0bd3" class="lz ma je lv b gy mh mc l md me"># Loading the test JSON data<br/>with open('test_data/test_1.json') as f:<br/>    test_data = json.loads(f.read())</span></pre><p id="92ac" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们只是从Locust和JSON中导入我们需要的东西，并加载我已经提供的测试JSON数据。到目前为止，这可能不是你不熟悉的。但是事情开始变得有点棘手了。我们会慢慢来。</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="20d5" class="lz ma je lv b gy mb mc l md me"># Creating an API User class inheriting from Locust's HttpUser class<br/>class APIUser(HttpUser):</span></pre><p id="de8d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">好的，所以你可能熟悉Python类。这是在创建一个新类，它继承了由Locust创建的父类“HttpUser”的内容。我不打算深入探讨那个类的属性/方法，但可以说，这就是我们很快将在用户界面中使用的Locust。</p><p id="022e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">向前移动…</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="4937" class="lz ma je lv b gy mb mc l md me">    # Setting the host name and wait_time<br/>    host = '<a class="ae lp" href="http://localhost:5001'" rel="noopener ugc nofollow" target="_blank">http://localhost:5001'</a><br/>    wait_time = between(3, 5)</span></pre><p id="47a9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里的主机可能非常简单:我们只是提供API当前服务的基本URL。(回想一下，我的Gunicorn脚本服务于localhost:5001。)这段“等待_时间”可能对你来说是新的。与“between()”方法一起，它记录了Locust在产生更多用户之前应该等待多长时间。“between()”以秒为单位，所以在我们的例子中，新用户每3到5秒就会产生一段时间。</p><p id="9df8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们剧本的最后一部分:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="a936" class="lz ma je lv b gy mb mc l md me">    # Defining the post task using the JSON test data<br/>    <a class="ae lp" href="http://twitter.com/task" rel="noopener ugc nofollow" target="_blank">@task</a>()<br/>    def predict_endpoint(self):<br/>        self.client.post('/predict', json = test_data)</span></pre><p id="d884" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">那个“@task()”装饰器告诉我们的APIUser类，当Locust启动时，它需要采取什么行动。您实际上可以有多个任务，甚至可以适当地对它们进行加权，但这超出了我们在这里的范围。对于我们的目的，一个任务就可以了。我们的任务需要做的就是调用“/predict”API端点，并将我们在脚本顶部加载的JSON测试数据传递给它。</p><p id="a0f0" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在有趣的部分来了！在我们的API仍在运行的情况下，在您的终端中打开一个新标签。导航到包含locustfile.py的目录，并运行以下命令:</p><pre class="lq lr ls lt gt lu lv lw lx aw ly bi"><span id="308c" class="lz ma je lv b gy mb mc l md me">locust</span></pre><p id="0764" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">记住，默认情况下，Locust会查找locustfile.py文件，这就是为什么我们不需要在命令行中指定任何其他内容。你应该看到的是这样的东西。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mi"><img src="../Images/24511d60c2ceb0f3c2d06e3d9b013fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AejsB7jEcgN8Ksv_9xcpWQ.png"/></div></div></figure><p id="1da7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">值得注意的是，Locust已经在一个特定的端口号上启动了一个web用户界面。在我的例子中，您会注意到Locust UI在localhost:8089之后提供。打开你选择的浏览器，导航到那里。您应该会看到下面的屏幕。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/71cd718cd27d69ba106f125cd31772a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IN6JuVHEDQ4D9drNthd3QA.png"/></div></div></figure><p id="9e66" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">其实用户数和产卵率都会是空的。在这个例子中，我在这里指定的是我想要测试总共100个用户。在最开始，蝗虫将只开始测试5个用户的API。然后每隔3-5秒(我们在脚本中指定为wait_time)，Locust将添加另外5个用户，直到用户总数达到100。来吧，点击“开始蜂拥”按钮，观看蝗虫工作的魔力。这是您将看到的第一个屏幕。</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/bdb1acd1406b967126154a554de43f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9j4DBmmELrKUjgiP1gc6Tg.png"/></div></div></figure><p id="6853" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">您可以看到，当我拍摄这个截图时，Locust已经达到了100个用户，并且有1866个请求被传递给它。您还可以看到，每个请求的平均运行时间为26毫秒，API每秒可以有效地处理25个请求(RPS)。整洁！但是你是一个视觉的人吗？导航到图表！</p><figure class="lq lr ls lt gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ml"><img src="../Images/ad958bdaa831055d9f2ef8607a1553d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3B_jk-WKg-kNOF6mrzNbg.png"/></div></div></figure><p id="a4fc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">正如您在这些图表中看到的，很明显我们的API正在以非常稳定的速度运行。显然，在达到100个用户的上限之前，我们的RPS较低，但一旦我们达到100个用户的上限，一切都基本持平。(可能是少数几个你真的想看到扁平化的案例之一！)在这个页面上实际上还有第三个图表，它也将以图形方式显示不同种子点的用户数量，但是我已经没有截图的空间了。我们不会涉及其他选项卡，但您可能会猜到它们是做什么的。</p><p id="fac6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这就是蝗虫的全部！如果您的性能与预期的用户数量持平，那么您就可以开始了。如果没有，您可能需要探索不同的部署选项。也许您可能需要扩展更多的API实例，或者您可能需要探索如何进一步优化模型本身的选项。至少您可以高枕无忧，因为当您将最终的ML-serving API推向生产时，您不会导致性能瓶颈。</p><p id="12d8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这就是这篇文章的全部内容，各位！希望你喜欢这个。让我知道你还想让我在以后的帖子中涵盖哪些内容！总是喜欢听你的想法。</p></div></div>    
</body>
</html>