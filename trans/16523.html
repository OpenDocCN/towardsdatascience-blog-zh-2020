<html>
<head>
<title>Why Learned Optimizers Outperform “hand-designed” Optimizers like Adam</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么有经验的优化者优于像Adam这样“手工设计”的优化者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-learned-optimizers-outperform-standard-optimizers-like-adam-5f617b3035d?source=collection_archive---------32-----------------------#2020-11-14">https://towardsdatascience.com/why-learned-optimizers-outperform-standard-optimizers-like-adam-5f617b3035d?source=collection_archive---------32-----------------------#2020-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1935" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">反向工程学习优化揭示了谷歌大脑已知的和新的机制</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fbf67f044f6bef9d17fef7e17c05bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkf3wHZ5KN2ND7MLE_2fOg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1868049" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/pexels-2286921/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1868049" rel="noopener ugc nofollow" target="_blank">像素</a></p></figure><p id="e7d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化器，如momentum ( <em class="lv"> Polyak，1964 </em>)、AdaGrad ( <em class="lv"> Duchi等人，2011 </em>)、rms prop(<em class="lv">tie leman&amp;hint on，2012 </em>)或Adam ( <em class="lv"> Kingma &amp; Ba，2014 </em>)是几乎所有机器学习的算法基础。结合损失函数，它们是使机器学习能够工作的关键部分。这些算法使用源自直觉机制和理论原则的简单更新规则，这是一种衡量预测错误程度的数学方法，并将其调整为更好。</p><p id="50ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近的研究思路集中在基于学习的优化算法上；他们称之为习得优化。已经表明，通过直接参数化和训练任务分布的优化器，学习的优化器优于“手工设计的”优化器，如Adam(<em class="lv">Andrychowicz等人，2016；Wichrowska等人，2017年；吕等，2017；Bello等人，2017；李&amp;马利克，2016；梅斯等人，2019；2020 </em>)。</p><p id="e35d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管通过使用这些习得的优化器，性能得到了提高，但研究人员仍然缺乏对其工作原理的理解。研究人员强调，理解学习型优化器的底层机制可以提高鲁棒性(<em class="lv"> Wichrowska等人，2017；Lv等人，2017 </em>)、元训练(<em class="lv"> Metz等人，2019 </em>)、学习型优化器的泛化(<em class="lv"> Metz等人，2020 </em>)。识别它们的操作缺陷，同时加深我们对关键机器学习机制为什么有效以及如何改进它们的理解。</p><p id="64ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于momentum和其他标准优化器，状态变量是低维的，因此它们的动力学很简单，描述其行为也很简单。但相比之下，有经验的优化器具有高维状态变量，因此这些系统学习复杂的非线性动力学，这对于提取有经验的优化器的行为的直观描述是具有挑战性的。</p><p id="2df9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多年来，研究人员一直使用常见的优化技术来调整优化器，如动量、梯度裁剪、学习速率计划和学习速率适应等技术。这些是直观的机制，已被证明有助于最小化参数上的损失函数。</p><p id="139f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有经验的优化者只是在学习已知技术的巧妙组合吗？或者他们发现了优化文献中尚未提出的全新行为？</p><p id="73bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个谷歌大脑研究团队试图揭示这个问题。该团队开发了用于隔离和阐明非线性、高维学习优化算法中的机制的工具。这篇名为<a class="ae ky" href="https://arxiv.org/abs/2011.02159" rel="noopener ugc nofollow" target="_blank">逆向工程学习优化器揭示了已知和新颖的机制</a>的论文目前正在接受ICLR 2021的审查。</p><p id="d270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">研究人员在三个快速训练的任务上训练了学习过的优化器，这对于元优化来说特别重要，并且覆盖了一系列损失曲面(凸和非凸，高维和低维)。这些任务是:</p><ul class=""><li id="44f5" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">随机线性回归问题(凸，二次)</li><li id="7525" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">最小化Rosenbrock函数(非凸和低维)</li><li id="3f15" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">双月数据集(非凸高维)</li></ul><p id="458a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们将学习优化器的性能与调优的基线优化器RMSProp、Momentum和Adam进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/1265c72d254593c162a95042d6333055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PHoCClrYWvPFr04CCQmZg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在三个不同的任务上，有经验的优化者优于调整良好的基线。上排:任务示意图。底部一行:优化器性能。[ <a class="ae ky" href="https://arxiv.org/pdf/2011.02159.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="ab82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">损失曲线中显示的优化器性能是128个随机种子的平均值、标准偏差误差。有经验的优化器在所有三个任务上都优于三个调优的优化器。</p><h2 id="34c0" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">动力</h2><p id="d67f" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在实验中，每个优化器收敛到动态的单个全局不动点。作者发现，有经验的优化者使用近似线性动力学来实现动量。当他们分析表现最好的学习优化器时，有一个在线性回归任务上的学习优化器表现稍差，但非常类似于经典动量。它恢复了特定任务分配的最佳动量参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f6f39c001812cdcf1c93e4b30afc070a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6yhjqzc3kLdTx1YTNDz5zQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有学问的优化者的动力。顶行:优化器状态在收敛点周围的投影。中间一行:沿着动力学的慢模式的更新函数的可视化。底行:在复平面中绘制的收敛固定点处的线性化优化器动态特性的特征值。[ <a class="ae ky" href="https://arxiv.org/pdf/2011.02159.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="cc45" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">渐变剪辑</h2><p id="3e33" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">随着梯度幅度的增加，学习优化器也使用饱和更新函数；这类似于渐变剪辑的一种柔和形式。事实上，削波效应的强度是与训练任务相适应的。例如，在线性回归问题中，学习的优化器主要停留在更新函数的线性区域内。相比之下，对于Rosenbrock问题，学习优化器利用更新函数中更饱和的部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/f9d7eecff66fda2a0d5801748bded258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0NDGL8aKtyjsO4K06RWYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习优化器中的梯度削波。顶行:在初始状态计算的更新函数对于大的梯度幅度饱和。这种效果类似于渐变剪辑。底行:每个任务遇到的梯度的经验密度。[ <a class="ae ky" href="https://arxiv.org/pdf/2011.02159.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="ac97" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">学习费率表</h2><p id="32d5" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">随着优化的进行，我们经常调整学习率调度器来衰减学习率。作者发现，有经验的优化器可以使用自主动态实现调度器。当系统松弛到固定点时，它将特定的轨迹编码为迭代的函数。从下图中可以看出，在线性回归实验中，学习优化器最初会在25次迭代后提高学习率，然后线性衰减。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/03c62d0983c0532d4c3aefa6c23a1d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7axHUPfxWNB9jgAf6djnoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自主动力学调节的学习速率表。顶行:响应于零梯度(无输入)的已学习优化器的动态的低维投影。这些自主动态允许系统学习学习率时间表。底行:有效学习率是顶行中自主轨迹期间迭代的函数。[ <a class="ae ky" href="https://arxiv.org/pdf/2011.02159.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="cd6e" class="ml mm it bd mn mo mp dn mq mr ms dp mt li mu mv mw lm mx my mz lq na nb nc nd bi translated">学习速率适应</h2><p id="0381" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">学习率自适应的目的是在遇到大梯度时降低优化器的学习率。这是通过根据当前梯度改变系统中的固定点来实现的。作者发现这些点在所有任务中形成一条S曲线；曲线的一个臂对应于负梯度，而另一个臂对应于正梯度。更新函数的斜率类似于RMSProp状态变化时观察到的变化。这允许优化器对于较小的梯度幅度增加其学习速率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/58a02b7e73ddd341cae62c374eba2852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQnYuXfgxcrWscjJq9fI_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习优化器中的学习速率适应。顶行:为不同梯度计算的动力学的近似固定点揭示了S曲线结构。中间一行:更新沿S曲线不同点计算的函数。底行:显示沿S曲线每条臂的有效学习率的汇总图。【<a class="ae ky" href="https://arxiv.org/pdf/2011.02159.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="d357" class="nn mm it bd mn no np nq mq nr ns nt mt jz nu ka mw kc nv kd mz kf nw kg nc nx bi translated">摘要</h1><p id="63dd" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">人们对有经验的优化者如何工作知之甚少。在这项工作中，提出的分析表明，学习优化能够学习几个有趣的优化现象；这些是常用的直观优化机制。理解学习型优化器是如何工作的，可以让我们在一个环境中训练学习型优化器，并且知道何时以及如何将它们应用到新的问题中。我们可以使用从更普遍的学习优化器和元学习算法的高维非线性动力学中提取洞察力。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><div class="kj kk kl km gt of"><a rel="noopener follow" target="_blank" href="/improve-glaucoma-assessment-with-brain-computer-interface-and-machine-learning-6c3b774494f8"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">利用脑-机接口和机器学习改进青光眼评估</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">我的研究使用多任务学习来提供快速的护理点诊断，以检测周边视觉损失</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><div class="ou ov gp gr ow of"><a rel="noopener follow" target="_blank" href="/the-fascinating-relationship-between-ai-and-neuroscience-89189218bb05"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">人工智能和神经科学之间的迷人关系</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">他们如何相互激励、共同进步、相互受益</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ox l oq or os oo ot ks of"/></div></div></a></div><div class="kj kk kl km gt ab cb"><figure class="oy kn oz pa pb pc pd paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/e6191b77eb1b195de751fecf706289ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="oy kn oz pa pb pc pd paragraph-image"><a href="https://jinglesnote.medium.com/"><img src="../Images/7c898af9285ccd6872db2ff2f21ce5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*airGp_q6AXwaoL1LYXwYeQ.png"/></a></figure><figure class="oy kn oz pa pb pc pd paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/d370b96eace4b03cb3c36039b70735d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ESxUX6V6tAqj_2ZFSr-pUw.png"/></a></figure></div></div></div>    
</body>
</html>