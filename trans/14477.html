<html>
<head>
<title>SVM Hyperparameters Explained with Visualizations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化解释SVM超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b?source=collection_archive---------10-----------------------#2020-10-06">https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b?source=collection_archive---------10-----------------------#2020-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bc80" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">C和γ是用来做什么的</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a356ffcac21d27668e18ca3a8c3ed5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p3aEhqjPNeWGX1Hg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/s/photos/balance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@acharki95?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aziz Acharki </a>拍摄</p></figure><p id="f9f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">支持向量机(SVM)是一种广泛使用的监督机器学习算法。它主要用于分类任务，但也适用于回归任务。</p><p id="0eaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们深入探讨了支持向量机的两个重要的超参数，<strong class="lb iu"> C </strong>和<strong class="lb iu">γ，</strong>并用可视化的方法解释了它们的作用。所以我假设你对算法有一个基本的了解，并关注这些超参数。</p><p id="930f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM使用决策边界来分离属于不同类别的数据点。当确定决策边界时，软边界SVM(软边界意味着允许一些数据点被错误分类)试图解决具有以下目标的优化问题:</p><ul class=""><li id="b936" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">增加决策边界到类别(或支持向量)的距离</li><li id="2a19" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最大化训练集中正确分类的点数</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9746f5cfbed29b976e1ec114f77731f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*8uPA1GjjAVNuaj2D.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">软边界SVM的决策边界(图片由作者提供)</p></figure><p id="5073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，这两个目标之间有一个折衷，它由<strong class="lb iu"> C </strong>控制，对每个错误分类的数据点增加一个惩罚。</p><p id="f7d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果C很小，则对误分类点的惩罚也很低，因此以更大数量的误分类为代价选择了具有大余量的决策边界。</p><p id="a12c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果C很大，SVM会尽量减少错误分类的例子，因为高惩罚会导致决策边界具有较小的余量。对于所有分类错误的例子，惩罚是不一样的。它与到决策边界的距离成正比。</p><p id="3222" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举例之后会更清楚。让我们首先导入库并创建一个合成数据集。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="3507" class="mp mq it ml b gy mr ms l mt mu">import numpy as np<br/>import pandas as pd</span><span id="6500" class="mp mq it ml b gy mv ms l mt mu">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="0b0a" class="mp mq it ml b gy mv ms l mt mu">from sklearn.svm import SVC<br/>from sklearn.datasets import make_classification</span><span id="59cd" class="mp mq it ml b gy mv ms l mt mu">X, y = make_classification(n_samples=200, n_features=2,<br/>n_informative=2, n_redundant=0, n_repeated=0, n_classes=2,random_state=42)</span><span id="98a4" class="mp mq it ml b gy mv ms l mt mu">plt.figure(figsize=(10,6))<br/>plt.title("Synthetic Binary Classification Dataset", fontsize=18)<br/>plt.scatter(X[:,0], X[:,1], c=y, cmap='cool')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d622cc18cdf51009cebc64b966ec07ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*JT38dnZM9DJEwgz5KYrq4g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将首先训练一个只需要调整c的线性SVM，然后我们将实现一个具有RBF核的SVM，并调整伽马参数。</p><p id="6728" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了绘制决策边界，我们将使用Jake VanderPlas的<a class="ae ky" href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="noopener ugc nofollow" target="_blank"> Python数据科学手册</a>的<a class="ae ky" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html" rel="noopener ugc nofollow" target="_blank"> SVM章节</a>中的函数。</p><p id="62b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以创建两个具有不同C值的线性SVM分类器。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="e191" class="mp mq it ml b gy mr ms l mt mu">clf = SVC(C=0.1, kernel='linear').fit(X, y)</span><span id="44cc" class="mp mq it ml b gy mv ms l mt mu">plt.figure(figsize=(10,6))<br/>plt.title("Linear kernel with C=0.1", fontsize=18)<br/>plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='cool')<br/>plot_svc_decision_function(clf)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f4aa1ca33c35c4d59736f415c39b94c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*XFtyzSNjexMecQ4wmqBfgA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="581a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需将C值更改为100，即可生成如下图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/4eb0abf2a59b44b57e5220c5cc902020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*k4wh7vzjDbQWXx7wKyH0kg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="af6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们增加C值时，边距变小。因此，具有低C值的模型倾向于更一般化。随着数据集越来越大，这种差异变得越来越明显。</p><p id="13b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数的影响仅在线性核的情况下达到一定程度。对于非线性核，超参数的影响变得更加明显。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="5be7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伽马是与非线性SVM一起使用的超参数。最常用的非线性核之一是径向基函数(RBF)。RBF的Gamma参数控制单个训练点的影响距离。</p><p id="90bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">低gamma值表示较大的相似性半径，这将导致更多的点被组合在一起。对于高gamma值，这些点需要彼此非常接近，才能被视为在同一组(或类)中。因此，gamma值非常大的模型往往会过度拟合。</p><p id="73bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用不同的伽马值绘制三个支持向量机的预测。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="3ce9" class="mp mq it ml b gy mr ms l mt mu">clf = SVC(C=1, kernel='rbf', gamma=0.01).fit(X, y)<br/>y_pred = clf.predict(X)</span><span id="336c" class="mp mq it ml b gy mv ms l mt mu">plt.figure(figsize=(10,6))<br/>plt.title("Predictions of RBF kernel with C=1 and Gamma=0.01", fontsize=18)<br/>plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='cool')<br/>plot_svc_decision_function(clf)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2f911cc2d71e1a6a01bf9fe8ea34a099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*5DtPKUzLI1e-FIjC-odFiw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5d4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需改变伽马值，以产生以下图形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a40b4189a6fabed576d7f61d9261873c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*JDSwT-svWnAu69fy9oguBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cfcdda12a1eca726ca38494ee2b4e871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*faj7x1I0uFwfU6mkLfUwvg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="e281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着gamma值的增加，模型变得过拟合。数据点需要非常接近才能分组在一起，因为相似性半径随着灰度值的增加而减小。</p><p id="8da5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伽马值为0.01、1和5的数据集上的RBF核的精度分别为0.89、0.92和0.93。这些值表明随着gamma值的增加，模型过度适应训练集。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="4467" class="ne mq it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">伽马与C参数</h1><p id="edff" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">对于线性核，我们只需要优化c参数。然而，如果我们想使用RBF核，c和gamma参数需要同时优化。如果γ很大，c的影响可以忽略不计。如果gamma很小，c会像影响线性模型一样影响模型。c和γ的典型值如下。然而，根据应用可能存在特定的最佳值:</p><p id="6bc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0.0001 &lt; gamma &lt; 10</p><p id="348b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0.1 &lt; c &lt; 100</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="e84e" class="ne mq it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated"><strong class="ak">参考值</strong></h1><ul class=""><li id="c53e" class="lv lw it lb b lc nv lf nw li oa lm ob lq oc lu ma mb mc md bi translated"><a class="ae ky" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html" rel="noopener ugc nofollow" target="_blank">https://jakevdp . github . io/python datascience handbook/05.07-support-vector-machines . html</a></li></ul></div></div>    
</body>
</html>