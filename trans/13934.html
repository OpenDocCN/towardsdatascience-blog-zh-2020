<html>
<head>
<title>Cleansing and transforming schema drifted CSV files into relational data in Azure Databricks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Azure Databricks中将模式漂移的CSV文件清理并转换为关系数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff?source=collection_archive---------13-----------------------#2020-09-25">https://towardsdatascience.com/cleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff?source=collection_archive---------13-----------------------#2020-09-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c6fd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用PySpark增量处理和加载模式漂移文件到Azure Databricks中的Azure Synapse Analytics数据仓库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4a65851377af905397dd917b865a7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LzuYUI4zo54cyqBQbss5DA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@grimstad" rel="noopener ugc nofollow" target="_blank">hkon grim stad</a>拍摄的照片</p></figure><p id="0892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据是企业的血液。数据以不同的形状和大小出现，这使得寻找处理和消费的方式成为一项持续的挑战性任务，没有这种方式，数据就没有任何价值。</p><p id="b878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将介绍如何利用Apache Spark的并行分析功能，将模式漂移的CSV文件迭代地清理和转换为可查询的关系数据，并存储在数据仓库中。我们将在Spark环境中工作，并用PySpark编写代码来实现我们的转换目标。</p><p id="0ba2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">警告:</strong> <em class="lv"> Microsoft Azure是一项付费服务，遵循本文可能会导致您或您的组织承担财务责任。</em></p><p id="6033" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在继续本文之前，请阅读我们的使用条款:</em><a class="ae ky" href="https://dhyanintech.medium.com/disclaimer-disclosure-terms-of-use-fb3bfbd1e0e5" rel="noopener"><em class="lv">https://dhyanintech . medium . com/disclaimer-disclosure-disclosure-terms-of-use-fb3 BF BD 1e 0e 5</em></a></p><h1 id="1a78" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件</h1><ol class=""><li id="fe2a" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated">有效的Microsoft Azure订阅</li><li id="d763" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">带有CSV文件的Azure数据湖存储第二代帐户</li><li id="6465" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">Azure Databricks工作区(高级定价层)</li><li id="7b42" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">Azure Synapse分析数据仓库</li></ol><p id="7d7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">如果您还没有设置先决条件，请参考我们以前的文章开始:</em></p><div class="ne nf gp gr ng nh"><a href="https://medium.com/@dhyanintech/a-definitive-guide-to-turn-csv-files-into-power-bi-visuals-using-azure-4483cf406eab" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">使用Azure将CSV文件转换为Power BI视觉效果的权威指南</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">使用Microsoft Azure产品将新冠肺炎数据转化为惊人的Power BI视觉效果的分步指南。</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks nh"/></div></div></a></div><div class="ne nf gp gr ng nh"><a href="https://medium.com/@dhyanintech/using-azure-data-factory-to-incrementally-copy-files-based-on-url-pattern-over-http-569476b625fc" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">使用Azure Data Factory基于HTTP上的URL模式增量复制文件</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">一个创新的Azure数据工厂管道，通过HTTP从第三方网站增量复制多个文件…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="nw l ns nt nu nq nv ks nh"/></div></div></a></div></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="894b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">登录到<a class="ae ky" href="https://portal.azure.com/" rel="noopener ugc nofollow" target="_blank"> Azure门户</a>，找到并打开你的Azure Databricks实例，然后点击“启动工作区”我们的Databricks实例将在新的浏览器选项卡中打开；等待Azure AD SSO自动为您登录。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/8a4b505de2a8daa72dd35783da7b4348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzT93Onk2zrf7yriYXUW_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure门户:启动Databricks工作区(图片由作者提供)</p></figure><p id="b145" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要创建一个节点集群，以利用Apache Spark无与伦比的并行处理(双关语)能力来处理、清理和转换我们的半结构化数据。</p><div class="ne nf gp gr ng nh"><a href="https://spark.apache.org/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">Apache Spark面向大数据的统一分析引擎</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">闪电般快速的统一分析引擎Apache Spark是用于大规模数据处理的统一分析引擎…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">spark.apache.org</p></div></div><div class="nq l"><div class="of l ns nt nu nq nv ks nh"/></div></div></a></div><h1 id="2b6f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">旋转阿帕奇火花集群</h1><p id="0838" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">选择左侧菜单上的<strong class="lb iu">集群</strong>开始创建新的集群。通过选择<strong class="lb iu"> +创建集群</strong>开始，如图所示继续操作。这里需要注意的两个基本问题是Databricks运行时版本以及工作节点的最小和最大数量。我们的集群将在这些节点之间自动扩展以适应负载。等待创建过程完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/2bccdd37baf16864ee143eabb909a0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Baou0iBs3g9QsKA9a8hwkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure Databricks:建立一个Apache Spark集群(图片由作者提供)</p></figure><p id="67af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">点击<strong class="lb iu">开始</strong>来启动你的集群。Azure可能需要几分钟来配置和设置您的群集资源。密切关注集群状态指示器，查看实时状态。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/fae02a9332fdcca858f6a401f4508c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iWUEB6bSYQgo8lSmuHIkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure Databricks:启动Spark集群(图片由作者提供)</p></figure><p id="9ef3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据砖块的真正魔力发生在<a class="ae ky" href="https://docs.databricks.com/notebooks/index.html" rel="noopener ugc nofollow" target="_blank">笔记本</a>中。Azure Databricks支持用Python、Scala、SQL和r编写的笔记本，在我们的项目中，我们将使用Python和PySpark来编码所有的转换和清理活动。让我们开始创建一个Python笔记本。</p><p id="4c0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">笔记本是一个基于网络的文档界面，其中包含可运行的代码、叙述性文本和可视化效果。</em></p><p id="5842" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> PySpark是Apache Spark的Python API。Apache Spark是用Scala写的。PySpark已经发布，支持Apache Spark和Python的协作。</em></p><p id="4596" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在左侧菜单中选择<strong class="lb iu">工作区</strong>,并按照所示步骤操作。你的笔记本创建后会打开；花点时间四处看看，熟悉一下UI和我们可用的各种选项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/5f98f13f860a1a73bd4b55a4114f9ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSHNWvfqkdd4z1ahlTqTBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Azure Databricks:创建一个Python笔记本(图片由作者提供)</p></figure><p id="0154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">笔记本的前几行应该告诉数据块我们的数据在哪里以及如何访问它。我们将<em class="lv">将我们的存储帐户挂载到Databricks文件系统，并将其作为本地存储进行访问。</em></p><h1 id="85b7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">将存储帐户装载到数据块文件系统</h1><p id="07c8" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">请阅读我们的文章，了解在Azure Databricks中安装和访问ADLS第二代存储的详细步骤。我们会尽量简短。</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/@dhyanintech/mounting-accessing-adls-gen2-in-azure-databricks-using-service-principal-and-secret-scopes-96e5c3d6008b" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">使用服务主体和秘密范围在Azure数据块中挂载和访问ADLS Gen2</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">关于使用Azure密钥库支持的秘密范围和服务从数据块访问Azure数据湖存储Gen2的指南…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="om l ns nt nu nq nv ks nh"/></div></div></a></div><h1 id="7f55" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">从数据块连接和访问Azure Synapse分析数据仓库</h1><p id="9e1a" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">我们的最终目标是将数据加载到数据仓库中，从数据中获得洞察力，并构建报告来做出决策。让我们在继续之前设置连接。</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/@dhyanintech/a-credential-safe-way-to-connect-and-access-azure-synapse-analytics-in-azure-databricks-1b008839590a" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">在Azure Databricks中连接和访问Azure Synapse Analytics的凭据安全方式</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">关于如何在PySpark中使用秘密作用域设置SQL Server防火墙和从数据块连接的指南</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="on l ns nt nu nq nv ks nh"/></div></div></a></div><p id="3f3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的连接都设置好了；让我们继续清理刚刚挂载的CSV文件。我们将简要解释语句的目的，并在最后展示整个代码。</p><h1 id="0070" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使用PySpark进行转化和净化</h1><p id="bbe2" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">首先，让我们将一个文件读入PySpark并确定模式。我们将设置一些选项来告诉PySpark列的类型和结构。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="cb3b" class="ot lx it op b gy ou ov l ow ox"># Read the csv files with first line as header, comma (,) as separator, and detect schema from the file<br/>csvDf = spark.read.format("csv") \<br/>.option("inferSchema", "true") \<br/>.option("header", "true") \<br/>.option("sep", ",") \<br/>.load("dbfs:/mnt/csvFiles/01-22-2020.csv")</span><span id="caac" class="ot lx it op b gy oy ov l ow ox">csvDf.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/298c66b59aa9e064090fad85987ace59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J86Zil864hXNg9yiwxmXKQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:确定文件的模式(图片由作者提供)</p></figure><p id="f641" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个文件的列数比我们从<a class="ae ky" href="https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data#daily-reports-csse_covid_19_daily_reports" rel="noopener ugc nofollow" target="_blank"> GitHub源</a>得到的少，列名也不同。我们对最终功率BI可视化的突出显示列感兴趣。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/d3bba7acce1b312060e55961d805712c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eD2m7SDEF8iSPSQ_Wwv9rw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据源:预期的架构(图片由作者提供)</p></figure><p id="40f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们读取一个较新的文件并检查其结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/45171c753fabb75ff19ede65691e2374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GDNL3d7CvsP87mjyIIFTuQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:确定文件的模式(图片由作者提供)</p></figure><p id="1151" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个文件的结构更接近我们的来源的描述。图式的差异并没有让我们的事情变得简单。如果我们所有的文件都有相同的模式，我们可以一次加载并清理所有的文件。我们的例子是典型的<em class="lv">模式漂移</em>，我们必须恰当地处理它；否则，我们的ELT(提取、加载和转换)过程将会失败。我们将设计我们的转换来解释这种漂移，并使它不会因模式改变而出错。</p><blockquote class="pc"><p id="b351" class="pd pe it bd pf pg ph pi pj pk pl lu dk translated">模式漂移是源经常改变元数据的情况。字段、列和类型可能会被更改、添加或删除。</p></blockquote><p id="9492" class="pw-post-body-paragraph kz la it lb b lc pm ju le lf pn jx lh li po lk ll lm pp lo lp lq pq ls lt lu im bi translated">我们将通过重命名列来开始清理，以匹配数据库中表的属性，从而在表和数据之间建立一对一的映射。我们将通过将所有字母转换为小写并删除空格、正斜杠('/')和下划线(' _ ')来实现这一点。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="be7e" class="ot lx it op b gy ou ov l ow ox"># Function to flatten the column names by removing (' ', '/', '_') and converting them to lowercase letters<br/>def rename_columns(rename_df):<br/>  for column in rename_df.columns:<br/>    new_column = column.replace(' ','').replace('/','').replace('_','')<br/>    rename_df = rename_df.withColumnRenamed(column, new_column.lower())<br/>  return rename_df</span><span id="bd15" class="ot lx it op b gy oy ov l ow ox">csvDf = rename_columns(csvDf)<br/>csvDf.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/6254b5d330bfdc733bdee5ac78dc75eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIWDwqEYXlrtQ8nr0oHm5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:展平列名(作者图片)</p></figure><p id="3790" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的列名现在看起来好多了。我们将添加一些新的列来处理我们缺少列的情况；<em class="lv">活动</em>、<em class="lv">经度</em>、<em class="lv">纬度</em>和<em class="lv">源文件</em>。我们将使用文件名作为<em class="lv">源文件</em>列的值。本专栏将有助于设置数据库中数据的增量加载。</p><p id="80ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将把数据中存在的<em class="lv"> lat </em>和<em class="lv"> long </em>列名重命名为<em class="lv"> latitude </em>和<em class="lv">经度</em>。接下来，我们将使用PySpark中的<code class="fe ps pt pu op b">lit()</code>来添加缺少的<em class="lv">活动</em>、<em class="lv">纬度</em>和<em class="lv">经度</em>列，其中<em class="lv">为空</em>值，而<em class="lv">源文件</em>的文件名为列值。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="fee5" class="ot lx it op b gy ou ov l ow ox"># lit() function to create new columns in our datafram<br/>from pyspark.sql.functions import lit</span><span id="f799" class="ot lx it op b gy oy ov l ow ox"># Check dataframe and add/rename columns to fit our database table structure<br/>if 'lat' in csvDf.columns:<br/>  csvDf = csvDf.withColumnRenamed('lat', 'latitude')<br/>    <br/>if 'long' in csvDf.columns:<br/>  csvDf = csvDf.withColumnRenamed('long', 'longitude')<br/>  <br/>if 'active' not in csvDf.columns:<br/>  csvDf = csvDf.withColumn('active', lit(None).cast("int"))<br/>   <br/>if 'latitude' not in csvDf.columns:<br/>  csvDf = csvDf.withColumn('latitude', lit(None).cast("decimal"))<br/>  <br/>if 'longitude' not in csvDf.columns:<br/>  csvDf = csvDf.withColumn('longitude', lit(None).cast("decimal"))</span><span id="32ab" class="ot lx it op b gy oy ov l ow ox"># Add the source file name (without the extension) as an additional column to help us keep track of data source<br/>csvDf = csvDf.withColumn("sourcefile", lit('01-22-2020.csv'.split('.')[0]))</span><span id="f4f5" class="ot lx it op b gy oy ov l ow ox">csvDf = csvDf.select("provincestate", "countryregion", "lastupdate", "confirmed", "deaths", "recovered", "active", "latitude", "longitude", "sourcefile")</span><span id="b60d" class="ot lx it op b gy oy ov l ow ox">csvDf.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/41875e62868ec0ca287f3867c0b89bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKKpSdyLJ575NOsyLO9OxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:添加额外的栏目(图片由作者提供)</p></figure><p id="bc13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用<code class="fe ps pt pu op b">display(DATAFRAME)</code>来看看我们在清理活动开始时查看的两个文件的数据</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/49c66ab9d8ea988399b457974b43f0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3MWOHCgEdDd79uyRWCSTQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:显示数据框的数据(图片由作者提供)</p></figure><p id="925e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个文件现在都为我们提供了固定结构的格式化数据，并准备好插入到我们的数据库中。我们已经成功地处理了漂移模式。</p><p id="e6fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们手动运行了两个文件的代码；我们应该自动处理一个接一个的文件。我们可以使用Databricks文件系统实用程序来遍历所有文件。</p><p id="e21c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">关于Databricks文件系统实用程序的进一步阅读:</em></p><div class="ne nf gp gr ng nh"><a href="https://docs.databricks.com/dev-tools/databricks-utils.html#dbutilsfsls-command" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">数据块工具</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">Databricks实用程序(DBUtils)使执行强大的任务组合变得容易。您可以使用这些实用程序来…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">docs.databricks.com</p></div></div></div></a></div><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="f247" class="ot lx it op b gy ou ov l ow ox"># List all the files we have in our store to iterate through them<br/>file_list = [file.name for file in dbutils.fs.ls("dbfs:{}".format(mountPoint))]</span><span id="e728" class="ot lx it op b gy oy ov l ow ox">for file in file_list:<br/>  print(file)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/7a94c2238a1be6093a83acc69e93e4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvSWXKncZ5Sbl3bQC0stzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark:使用Databricks工具列出文件(图片由作者提供)</p></figure><p id="0bf6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只需要处理尚未加载到数据库中的文件(增量加载)。我们可以通过查询数据库找出最后加载的文件的名称，并调整迭代器代码以忽略已经加载的文件。</p><pre class="kj kk kl km gt oo op oq or aw os bi"><span id="9571" class="ot lx it op b gy ou ov l ow ox"># Find out the last file we loaded into the database<br/># This will return <em class="lv">null</em> if there's no data in the table<br/>lastLoadedFileQuery = "(SELECT MAX(sourcefile) as sourcefile FROM csvData.covidcsvdata) t"</span><span id="50ca" class="ot lx it op b gy oy ov l ow ox">lastFileDf = spark.read.jdbc(url=jdbcUrl, table=lastLoadedFileQuery, properties=connectionProperties)</span><span id="8912" class="ot lx it op b gy oy ov l ow ox">lastFile = lastFileDf.collect()[0][0]</span><span id="da97" class="ot lx it op b gy oy ov l ow ox"># List all the files we have in our store to iterate through them<br/>file_list = [file.name for file in dbutils.fs.ls("dbfs:{}".format(mountPoint))]</span><span id="bbfc" class="ot lx it op b gy oy ov l ow ox"># Find the index of the file from the list<br/>loadFrom = file_list.index('{}.csv'.format(lastFile)) + 1 if lastFile else 0</span><span id="7410" class="ot lx it op b gy oy ov l ow ox"># Trim the list keeping only the files that should be processed<br/>file_list = file_list[loadFrom:]<br/>for file in file_list:<br/>  print(file)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/536d920755921bcefd7f9ad10af15157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5yfW5UH33yzJ1PTbu0TjQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PySpark —概念验证:增量处理(图片由作者提供)</p></figure><p id="de75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">组合和重组我们到目前为止编写的所有代码将允许我们通过增加数据库负载来清理模式漂移文件。试试看。您可以在文章末尾找到GitHub的整个笔记本，用于任何故障排除目的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pz qa l"/></div></figure><h1 id="9e05" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="da59" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">我们查看了我们的CSV文件，意识到它们有不同的模式，需要不同的处理方法，然后才能将它们加载到我们的数据仓库中。我们使用PySpark创建了一个创造性的解决方案来增量处理我们的文件，并设计了一个满足我们需求的解决方案。</p><h1 id="9c8c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">后续步骤</h1><p id="3d5b" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">如果您正在关注我们关于将CSV数据转化为Power BI视觉效果的系列文章，或者有兴趣了解如何在您的数据工厂管道中添加和执行Databricks notebook，请阅读我们的下一篇文章继续您的旅程。</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/@dhyanintech/executing-azure-databricks-notebook-in-azure-data-factory-pipeline-using-access-tokens-3326b8703432" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">使用访问令牌在Azure数据工厂管道中执行Azure Databricks笔记本</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">关于如何使用Azure Key Vault安全访问在数据工厂管道中添加和执行Databricks笔记本的指南…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="qb l ns nt nu nq nv ks nh"/></div></div></a></div></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="21a8" class="ot lx it bd ly qc qd dn mc qe qf dp mg li qg qh mi lm qi qj mk lq qk ql mm qm bi translated">喜欢这个帖子？与Dhyan联系</h2><p id="4bb0" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">让我们做朋友吧！你可以在<a class="ae ky" href="https://www.linkedin.com/in/dhyans/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我或者在<a class="ae ky" href="https://dhyanintech.medium.com/membership" rel="noopener"> Medium </a>上<strong class="lb iu">加入</strong>我。</p></div></div>    
</body>
</html>