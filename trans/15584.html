<html>
<head>
<title>Text Generation Using N-Gram Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用N元模型的文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=collection_archive---------4-----------------------#2020-10-27">https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0?source=collection_archive---------4-----------------------#2020-10-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/554950aaada6ae1eb951207c723419aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*azU2PUVNUzTM5Mcu"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">迪伦·卢在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b7d6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我了解到机器能够通过使用一些简单的统计和概率技术来生成新文本时，我对人工智能，特别是自然语言处理(NLP)的兴趣就爆发了。在这篇文章中，我想分享这个非常简单和直观的创建文本生成模型的方法。</p><h1 id="ddd3" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">背景</h1><p id="1ff0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">众所周知，语言具有顺序性，因此单词在文本中出现的顺序非常重要。这个特性允许我们理解一个句子的上下文，即使缺少一些单词(或者偶然发现一个意思未知的单词)。考虑下面的例子:</p><blockquote class="me mf mg"><p id="d88e" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">"玛丽被卓帕卡布拉发出的可怕声音吓坏了."</p></blockquote><p id="395c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有一些先前的背景，我们不知道“卓帕卡布拉”到底是什么，但我们可能会说，我们不会高兴在现实生活中遇到这种生物。</p><p id="8d86" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">句子中单词的这种依赖性可以给我们一些关于丢失单词性质的线索，有时我们甚至不需要知道整个上下文。在上面的例子中，通过只看由发出的<em class="mh">噪声，我们在直觉层面上可以说下面的单词应该是一个<strong class="kf ir">名词</strong>而不是某个其他词类。</em></p><p id="ff90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这让我们想到了N元语法背后的概念，它的正式定义是“来自给定文本样本的N个项目的连续序列”。主要的想法是，给定任何文本，我们可以把它分成一个一元(1-gram)、二元(2-gram)、三元(3-gram)等的列表。</p><blockquote class="me mf mg"><p id="2578" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">例如:</p><p id="ac66" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">文字:“我去跑步了”</p><p id="9745" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">单字:[(I)，(got)，(running)]</p><p id="4d83" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">二元模型:[(我，去，(去，运行)]</p></blockquote><p id="ae5a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你所注意到的，单词“go”出现在两个二元模型中:(I，<strong class="kf ir">go</strong>)和(<strong class="kf ir">go</strong>，running)。</p><p id="255a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种更直观的方式来看待它</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/04e41617e1a9f64d37a525128974d17e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EdZ1FVrVS7zzHFLkaZJyVQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">句子中三元组的例子。图片由奥列格·鲍里索夫拍摄。</p></figure><h1 id="fb06" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">理论</h1><p id="d97f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">使用n-gram生成文本的主要思想是假设n-gram的<strong class="kf ir">最后一个单词</strong> (x^{n})可以从同一个n-gram中出现的其他单词(x^{n-1}，x^{n-2}，… x)推断出来，我称之为<strong class="kf ir">上下文</strong>。</p><p id="86af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，该模型的主要简化是，我们不需要跟踪整个句子来预测下一个单词，我们只需要回头寻找<em class="mh"> n-1 </em>标记。这意味着主要的假设是:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d08cc754e56307b4da60d7599e944bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*kuHNmNYfr342haz6vsPaSA.png"/></div></figure><p id="1a96" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！最美妙的是，为了计算上面的概率，我们只需要应用简单的条件概率规则</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/ee166ef648af2b7e1eaf8bcace2cb25d.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/1*lTsmIj4PQfC0P0HkKG2nLA.gif"/></div></figure><blockquote class="me mf mg"><p id="d3c4" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">例如:使用三元模型(n=3)</p><p id="8416" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">正文:“玛丽害怕是因为__”</p><p id="ceb1" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">因为我们使用三元模型，所以我们删除了句子的开头:“Mary被吓到了”，并且我们只需要计算“因为”的可能延续。假设从数据集我们知道有以下可能的延续:“我”，“噪音”。因此，我们需要计算:</p><p id="85a4" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">P(噪音|因为)和P(我|因为)</p></blockquote><p id="c438" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在计算出概率后，有多种方法来选择给定所有候选的最终单词。<strong class="kf ir">一种</strong>方法是产生具有最高条件概率的单词(如果<em class="mh"> n </em>很小，这个选项可能不是最好的，因为它会陷入循环)。</p><p id="9a07" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">另一个</strong>(也是更好的)选项是根据条件概率半随机地输出单词“<em class="mh"/>”。从而具有较高概率的单词将有较高的机会被产生，而具有较低概率的其他单词有机会被产生。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/8367d2d900dfb9772d967afe717140de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G1QQQOoB4v5kIDZU"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@clemhlrdt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Clément H </a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="1b8b" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">密码</h1><p id="2cdf" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">理论到此为止，让我们进入实现部分。<a class="ae kc" href="https://github.com/olegborisovv/NGram_LanguageModel" rel="noopener ugc nofollow" target="_blank">我的github上有源代码</a>。</p><p id="02c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要一些源文本，从中我们将训练我们的<em class="mh">语言模型</em>。理想情况下，我们希望有一些大的书(甚至多本书)，因为我们不仅希望有大量的词汇，而且我们有兴趣看到尽可能多的不同排列或单词。这里我将使用玛丽·雪莱写的《弗兰肯斯坦》,但是你可以使用你选择的任何其他书(为了方便你可以使用<a class="ae kc" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">古腾堡计划</a>链接)。</p><p id="684f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望代码尽可能简单，这样就不需要安装任何外部包。除此之外，我还做了一些代码优化，以减少计算时间，使有效的推理过程。</p><p id="a69e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们实现N元语法语言模型之前，让我们实现一些辅助函数:一个用于执行标记化(拆分句子中的单词)，另一个用于将结果标记合并到N元语法中。</p><p id="15f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于标记化，最好使用一些外部库，如NLTK或spaCy，但是对于我们的目的，自定义标记化器就足够了。</p><p id="d969" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们来看看<code class="fe na nb nc nd b">get_ngrams</code>函数。</p><figure class="mm mn mo mp gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="6f3a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们之前提到的，为了预测位置<em class="mh"> n </em>的标记，我们必须查看我们的N-gram的所有先前的<em class="mh"> n-1 </em>标记。所以我们要用一个类型为<code class="fe na nb nc nd b">((context), current_word)</code>的元组来表示我们的N-gram:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5205fe1d5b4f8d3b6f8a4e500e80521e.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/1*cPt_5j0ggOnPWNv0yN9FaQ.gif"/></div></figure><p id="c388" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们需要生成一个句子的第一个标记时，问题就出现了，因为没有前面的上下文可用。为了解决这个问题，我们可以引入一些像<code class="fe na nb nc nd b">&lt;START&gt;</code>这样的前导标签，这将确保在任何推断点上，我们总是使用正确的N元语法。例如，如果我们想从文本“<em class="mh">我买了一辆红色汽车</em>”中获取所有的3克:</p><blockquote class="me mf mg"><p id="8c04" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">[((' <start>'，'<start>')，'我')，</start></start></p><p id="36c7" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">(('<start>'，'我'，'买')，</start></p><p id="8466" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">(('我'，'买'，'甲')，</p><p id="7af4" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">(('买了'，' a '，'红')，</p><p id="ba4c" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">(('甲'，'红'，'车')，</p></blockquote><p id="7fc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很好，现在让我们创建我们的N元模型:</p><figure class="mm mn mo mp gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="f489" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在初始化中，我们必须指定，什么是我们的n元文法的<em class="mh"> n </em>值，除此之外，我还创建了<em class="mh">上下文</em>和<em class="mh"> ngram_counter </em>字典。字典作为键，它有上下文，作为值，它存储给定上下文的可能延续的列表。例如:</p><pre class="mm mn mo mp gt nh nd ni nj aw nk bi"><span id="bb4d" class="nl lc iq nd b gy nm nn l no np">&gt;&gt; self.context<br/>&gt;&gt; {(red, car): [names, wallpapers, game, paint ...],<br/>    (weather, in): [London, Bern, Paris, ...]}</span></pre><p id="b515" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在某种程度上，这本<em class="mh">上下文</em>词典立即向我们展示了我们可以使用哪些候选词来生成相对随机的、有一定意义的文本(仍然比盲目生成一堆单词要好)。</p><p id="c66e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">字典只是计算一个特定的N-gram在我们的训练集中出现过多少次。</p><p id="fb07" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了更新我们的语言模型，我们将提供书中的每个单独的句子，并将使用与我们的N元语法相对应的信息来更新字典。</p><figure class="mm mn mo mp gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="0dd0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如在本文的理论部分提到的，在给定上下文的情况下，要计算下一个单词的概率，我们只需要应用简单的条件概率规则。</p><p id="de70" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们转到文本生成部分。</p><figure class="mm mn mo mp gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="f0a7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始文本生成之前，我们首先必须为我们的系统提供一些<strong class="kf ir">上下文</strong>，在我们的例子中，它将是重复<em class="mh">n–1</em>次的开始标记<code class="fe na nb nc nd b">&lt;START&gt;</code>。</p><p id="8396" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，我们可以通过使用我们的“半随机”方法生成我们的第一个<code class="fe na nb nc nd b">random_token</code>。然后，我们重复我们的过程，直到生成了一定数量的令牌(由<code class="fe na nb nc nd b">token_count</code>变量指定)。</p><p id="1aea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">注意</strong>:当我们到达生成中的句号<code class="fe na nb nc nd b">.</code>时，由于我们更新语言模型的方式，系统将不知道如何继续。因此，每次我们的模型生成一个句号时，我们都需要重新初始化上下文队列<code class="fe na nb nc nd b">context_queue</code>。</p><p id="3b6f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！一切都设置好了，让我们运行我们的模型，看看我们能产生什么句子！</p><figure class="mm mn mo mp gt jr"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="83a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">部分结果如下:</p><blockquote class="me mf mg"><p id="ba64" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">n=1，“我的关心。这是命中注定的。。对于费利克斯的话，安不哪措激动”</p><p id="fcc1" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">n=2，“我同情他，多么沉重；但我已经到达，但他们断言，我不允许潮湿和”</p><p id="8826" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">n=3，”我继续他们的单个后代。最后，我从一个生于自由的人那里得知</p><p id="8bda" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">n=4，”我继续以这种方式走着，在此期间，我享受着幸福的感觉。你仍能聆听”</p><p id="bf1e" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">n=5，“我继续以这种方式走了一段时间，我担心德蒙的失望的影响。”</p></blockquote><p id="7685" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看到生成的文本如何随着我们增加<em class="mh"> n </em>而改进是很酷的，然而这并不奇怪，因为随着N元模型大小的增加，我们开始从训练集复制原始文本，在我们的例子中是book。</p><p id="edca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，对于<em class="mh"> n &gt; 5 </em>生成的文本仍然是“我继续以这种方式行走了一段时间，试图通过身体锻炼来减轻负重”。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="b78d" class="lb lc iq bd ld le nq lg lh li nr lk ll lm ns lo lp lq nt ls lt lu nu lw lx ly bi translated">结束语</h1><p id="7c2f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我仍然感到印象深刻的是，我们使用N元语法和简单概率规则的过于简化的语言模型仍然能够产生一些新的文本，对读者来说有一定的意义。</p><p id="bb0f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，这样一个天真的模型不可能写出像人类一样的文章，也不可能像GPT 3号那样表演，因为它有很多缺点。这种系统可能陷入循环，或者不能跟踪长期的上下文关系。但这是对自然语言处理和语言建模的第一次很好的介绍，因为它强调了句子中上下文的重要性。</p><p id="8202" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">敬请关注更多关于人工智能和自然语言处理的文章。</p></div></div>    
</body>
</html>