<html>
<head>
<title>How to Solve Data Loading Bottlenecks in Your Deep Learning Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何解决你深度学习训练中的数据加载瓶颈</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-solve-data-loading-bottlenecks-in-your-deep-learning-training-1ddfcc24449b?source=collection_archive---------14-----------------------#2020-09-27">https://towardsdatascience.com/how-to-solve-data-loading-bottlenecks-in-your-deep-learning-training-1ddfcc24449b?source=collection_archive---------14-----------------------#2020-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8acd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">即使你没有固态硬盘</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1f9f181cb472c668501ea8936fbcb585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U_C8CbwlWpOCBgPh"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@varunkgaba?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Varun Gaba </a>拍摄的照片</p></figure><p id="07e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习训练中的每一次迭代，你都形成一批数据，并传递给模型。从HDD中读取数据是一个非常耗时的过程，因为机械致动器臂会穿过HDD板来寻找数据点的目标块。你知道，我们通常混洗数据集，并从这个混洗的数据集形成批次。这意味着批次中的数据点随机分布在硬盘中。这称为随机读取，与顺序读取相比非常慢。如果您要训练大型数据集(如ImageNet ),您应该避免这些类型的瓶颈。下面是一个例子:<strong class="ky ir">使用RTX2080 GPU和7200 RPM HDD </strong>，ResNet18模型在ImageNet数据上的一次迭代需要32个批处理大小，耗时0.44秒。<strong class="ky ir">100个纪元，需要20天！</strong>当我们测量函数的计时时，数据加载+预处理需要<strong class="ky ir"> 0.38秒</strong>(其中90%的时间属于数据加载部分)，而优化(向前+向后传递)时间只需要<strong class="ky ir"> 0.055秒</strong>。如果将数据加载时间减少到合理的时间，完全训练可以轻松减少到<strong class="ky ir"> 2.5天！</strong></p><div class="kg kh ki kj gt ab cb"><figure class="ls kk lt lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c1819fd0caa8f69b7dc0f007ae38439b.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*eWpfc1PjyS3LujMY"/></div></figure><figure class="ls kk ly lu lv lw lx paragraph-image"><img src="../Images/fa804bf0252a12c4c35eaa60b2bde9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*7XCWErEVa7CU0i0ylIhJvA.png"/><p class="kr ks gj gh gi kt ku bd b be z dk lz di ma mb translated">硬盘内部。我们在深度学习训练中形成批次时执行随机访问。照片由弗兰克R在Unsplash。</p></figure></div><h2 id="70e1" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">最基本、最快的解决方案:购买NVMe固态硬盘</h2><p id="b7b3" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">如果你的硬件系统支持固态硬盘，购买一个<strong class="ky ir"> NVMe M2型固态硬盘</strong>将立即解决你的问题。为什么？让我们检查一些存储设备的读取速度。</p><p id="287b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 7200 RPM硬盘:</strong>100 MB/秒</p><p id="fdcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">采用SATA速度的固态硬盘:</strong>600 MB/秒</p><p id="8e5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SSD (NVMe型)通过M2:</strong>3500 MB/秒</p><p id="cf0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以在NVMe型固态硬盘和你的7200 RPM硬盘之间，有35倍的差别。因此，购买和NVMe类型的固态硬盘是最基本和最快的解决方案。</p><p id="f22d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果固态硬盘不适合您，该怎么办？然后你必须继续使用你的硬盘。这是一些收据，可以让你用硬盘更快地加载数据。</p><h2 id="d3c5" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">1.如果你的数据是在硬盘上，它适合内存</h2><p id="01a6" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">在训练开始时将所有数据集加载(读取)到RAM中，这样在每次迭代中就不会消耗从磁盘读取的时间。你必须为此编写一个自定义数据集类(对于PyTorch，请查看这里:<a class="ae kv" href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/beginner/data _ loading _ tutorial . html</a>)。</p><h2 id="00ab" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">2.如果您的数据存储在硬盘中，但不适合存储在RAM中</h2><p id="1e41" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">如果您正在处理大型数据集，如ImageNet(大约150 GB)，您不能将所有数据加载到RAM中。所以，你必须从硬盘上一批一批地读取数据。然而，我们怎样才能让它更快呢？提示在本文的第一段:我们应该执行<strong class="ky ir">顺序读取</strong>而不是<strong class="ky ir">随机读取</strong>。</p><p id="9a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你混洗你的数据集，并将混洗后的数据集再次写入磁盘，这样，如果你按顺序读取每一批，你将得到混洗后的批。您可以使用<strong class="ky ir"> Tensorpack </strong>库来完成此操作(https://tensor pack . readthedocs . io/tutorial/efficient-data flow . html)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/91a7e2578fc3d052a9f6da12705ce28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oNRjxDoUWCHZtUeJ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">当您没有固态硬盘时，Tensorpack库是一个救命恩人[1]。</p></figure><p id="9837" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们比较这三种不同的数据加载方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/40aafa6bdcba52053c8f80e16729e57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1z4KfBHfTUJA6_jMBOvnLA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三种不同数据加载方法的数据加载时间比较。</p></figure><p id="bf39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> HDD，<em class="nc"> </em>随机读取</strong> <em class="nc"> </em>果然最慢。然而，这里有一个惊喜！<strong class="ky ir">硬盘，顺序读取</strong> <em class="nc">，</em>比<strong class="ky ir"> SSD快一圈，随机读取</strong> <em class="nc">。</em>所以，我们不花钱买固态硬盘，就不能用硬盘吗？</p><p id="df5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是没有。</p><p id="7d29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，读写混洗数据需要时间。对于ImageNet 2012数据(150 GB)，读取、混洗和写入需要4个小时。因此，您不能对每个时期执行此操作。如果你没有固态硬盘，这是唯一的救命稻草。</p><p id="d644" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二，将数据加载时间从0.370秒减少到0.018秒是重要的，但是对于训练来说将其从0.018秒减少到0.003秒是不必要的，因为存在额外的优化时间(向前传递+向后传递)。一旦数据加载时间减少，优化时间就会成为主要瓶颈。此外，许多框架提供异步数据加载；因此，这种差异不会显著影响整体训练表现。</p><h2 id="2f3b" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">预处理呢？</h2><ol class=""><li id="e42a" class="nd ne iq ky b kz mv lc mw lf nf lj ng ln nh lr ni nj nk nl bi translated">裁剪、翻转、规格化等基本操作不会显著影响训练时间；因此，大多数时候，您可以动态地执行这些操作。但是，如果您有一长串预处理函数或自定义预处理函数会降低处理速度，您可以对原始数据集执行一次这些操作并将其写入磁盘，然后您可以读取预处理文件并直接将其提供给模型。</li><li id="9e25" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">可以使用GPU张量进行预处理。<strong class="ky ir"> NVIDIA DALI </strong> [2]设计用于在GPU上进行预处理，它也具有快速的CPU实现。</li><li id="f350" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">可以使用低级语言(比如C++)进行预处理，充分利用线程机制。</li></ol><h2 id="2212" class="mc md iq bd me mf mg dn mh mi mj dp mk lf ml mm mn lj mo mp mq ln mr ms mt mu bi translated">额外收获:了解你的图书馆。</h2><p id="82c5" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">PyTorch和Tensorflow等框架将复杂的操作和GPU、CPU数据传输与用户隔离开来。这是这些图书馆的魅力之一。然而，它可能是有害的。例如，您想在每次迭代中获得一个张量的值，您可以通过下面的代码行(在PyTorch中)轻松获得它:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ca52" class="mc md iq ns b gy nw nx l ny nz">aValue = aTensor.item()</span></pre><p id="7f96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然看起来是很无辜的一行，但是每次执行这一行，都会把GPU RAM中的张量复制到主RAM中，这是一个很耗时的操作。它就像一个<em class="nc"> print() </em>函数，我们不能随时使用它(尤其是在循环中)。因此，我们必须知道正在发生什么，以及函数如何在后台运行。了解这一点有助于我们编写更有效的代码。</p><p id="cfcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是本文中提到的方法的简单流程图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/527548167471c0fbee272d1a74958eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axeqAw0R7wU85iT8CZG_zQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="d342" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">觉得这篇文章有用？看看我下面的其他文章:</p><div class="oi oj gp gr ok ol"><a href="https://medium.com/swlh/deep-learning-architectures-that-you-can-use-with-a-very-few-data-8e5b4fa1d5da" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">你可以用很少的数据使用深度学习架构</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">传统的CNN(Alex net，VGG，GoogLeNet，ResNet，DenseNet …)在样本较多的情况下有很好的表现…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz kp ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/effective-deep-learning-development-environment-with-pycharm-and-docker-34018f122d92"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">使用PyCharm和Docker的有效深度学习开发环境</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">借助全面承诺的IDE和虚拟化提高您的工作效率。</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz kp ol"/></div></div></a></div><h1 id="30e9" class="pb md iq bd me pc pd pe mh pf pg ph mk jw pi jx mn jz pj ka mq kc pk kd mt pl bi translated">参考</h1><p id="b393" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">[1]吴，庾信。“Tensorpack。”(2016).(【https://github.com/tensorpack/tensorpack】T4)</p><p id="68fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://github.com/NVIDIA/DALI" rel="noopener ugc nofollow" target="_blank">https://github.com/NVIDIA/DALI</a></p></div></div>    
</body>
</html>