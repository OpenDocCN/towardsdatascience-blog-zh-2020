<html>
<head>
<title>Bird’s-Eye View of Reinforcement Learning Algorithms Taxonomy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习算法分类的鸟瞰图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/birds-eye-view-of-reinforcement-learning-algorithms-landscape-2aba7840211c?source=collection_archive---------28-----------------------#2020-10-30">https://towardsdatascience.com/birds-eye-view-of-reinforcement-learning-algorithms-landscape-2aba7840211c?source=collection_archive---------28-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad75" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“邀请所有有志RL从业者”系列第3集</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7c228ffe1a7876390b4cdcde13fabac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ptspml4K_IF6wjn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@inakihxz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">伊尼基·德尔·奥尔莫</a>拍摄的照片</p></figure><p id="a28b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本系列的<a class="ae kv" rel="noopener" target="_blank" href="/invitation-to-all-aspiring-reinforcement-learning-practitioner-5f87384cee67">第一部分中，我们已经了解了强化学习(RL)中的一些重要术语和概念。在第二部</a>的<a class="ae kv" rel="noopener" target="_blank" href="/reinforcement-learning-in-autonomous-race-car-c25822def9f8">中，我们还学习了RL是如何应用在自主赛车上的。</a></p><blockquote class="ls lt lu"><p id="589f" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">在本文中，我们将了解强化学习算法的分类。我们不仅会学习一个分类法，还会从许多不同的角度学习几个分类法。</p></blockquote><p id="3633" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们熟悉了分类法之后，我们将在以后的章节中学习更多关于每个分支的知识。不浪费更多的时间，让我们深呼吸，做一杯巧克力，我邀请你和我一起学习RL算法分类的鸟瞰图！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/52955f18a0655001068e968f58bc5f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FM4Xa4K5yRoT6zcj"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@americanheritagechocolate?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">美国传统巧克力</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="106a" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">无模型与基于模型</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/8b8c710ac8f87ef61bdb6a55e7431620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yknXrUYahqjDCc28xHk_sQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">无模型分类法与基于模型分类法。[图片由作者提供，转载自OpenAI Spinning Up]</p></figure><p id="240b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对RL算法进行分类的一种方式是询问代理是否可以访问环境的模型。换句话说，通过询问我们是否能确切地知道环境将如何对我们的代理人的行动作出反应。</p><p id="d58c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于这个观点，我们有两个RL算法分支:无模型的和基于模型的:</p><ul class=""><li id="4503" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><strong class="ky ir">基于模型的</strong>是RL算法的一个分支，它试图根据学习到的环境模型来选择最佳策略。</li><li id="9399" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">在<strong class="ky ir">无模型</strong>算法中，基于代理经历的试错法选择最优策略。</li></ul><p id="e83e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无模型算法和基于模型的算法都有各自的优缺点，如下表所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/ddd8947a6436bc2cb70bcac8f1b6bbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xs84s-vVxQwmFW-z6Bkukg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">无模型算法和基于模型算法的优缺点。[图片由作者提供]</p></figure><blockquote class="ni"><p id="918e" class="nj nk iq bd nl nm nn no np nq nr lr dk translated">事实:无模型方法比基于模型的方法更受欢迎。</p></blockquote></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="3ec5" class="ma mb iq bd mc md nz mf mg mh oa mj mk jw ob jx mm jz oc ka mo kc od kd mq mr bi translated">基于价值与基于政策</h1><p id="40e7" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">对RL算法进行分类的另一种方式是通过考虑算法优化了什么组件——值函数还是策略。</p><blockquote class="ls lt lu"><p id="12df" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">在我们深入探讨之前，让我们先了解一下政策和价值函数。</p></blockquote><h2 id="6e22" class="oj mb iq bd mc ok ol dn mg om on dp mk lf oo op mm lj oq or mo ln os ot mq ou bi translated">政策</h2><p id="7c86" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">策略π是从状态<em class="lv"> s </em>到动作<em class="lv"> a，</em>的映射，其中π( <em class="lv"> a|s </em>)是当处于状态<em class="lv"> s. </em>时采取动作<em class="lv"> a </em>的概率。策略可以是确定性的，也可以是随机的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/0af474eefad94a310badc6c95f3f12db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FV0FKaHALmAhcUGW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·沃利斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5fab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们想象一下，我和你在玩石头剪刀布的游戏。如果你不知道这个游戏是什么，这是一个非常简单的游戏，两个人通过同时执行三个动作(石头/布/剪刀)中的一个向对方竞争。规则很简单:</p><ul class=""><li id="d962" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">剪刀打败了布</li><li id="5913" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">石头打败剪刀</li><li id="cf36" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">纸打败了石头</li></ul><p id="be37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑<em class="lv">迭代</em>石头剪子布的策略</p><ul class=""><li id="9543" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">确定性策略很容易被利用——如果你选择“石头”的频率高于其他选择，并且我意识到了你的行为，那么我就可以利用这一点，这样我就有更大的概率获胜。</li><li id="3f8f" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">一致随机策略是最优的——如果你的行为完全是随机的，那么我不知道应该采取什么行动才能打败你。</li></ul><h2 id="4c94" class="oj mb iq bd mc ok ol dn mg om on dp mk lf oo op mm lj oq or mo ln os ot mq ou bi translated">价值函数</h2><p id="673f" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">价值函数是一个基于对未来回报的预测或称为<strong class="ky ir">回报</strong>来衡量一个状态有多好的函数。基本上，回报(Gt)是从时间t开始的“贴现”回报的总和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/b60251df78dd6695dbab85c5d18a8aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*UIQUXwFU70Gq5V7CzQXTpw.png"/></div></div></figure><p id="9585" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">，其中<em class="lv"> γ </em> ∈ [0，1]为折现因子。贴现因子旨在惩罚未来的奖励，原因有几个:</p><ol class=""><li id="983b" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr ox mz na nb bi translated">在数学方面很方便</li><li id="723b" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated">打破状态转换图中的无限循环</li><li id="cf64" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated">未来回报的不确定性更高(即股票价格变动)</li><li id="332f" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated">未来的回报不会带来直接的好处(也就是说，人们倾向于在今天而不是10年后享受快乐)</li></ol><p id="6c84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在知道什么是回报了。我们来定义一下价值函数的数学形式！</p><p id="e001" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">价值函数有两种形式:</p><ul class=""><li id="7db8" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><strong class="ky ir">状态-价值函数</strong>(通常称为价值函数)是一个状态在时间<em class="lv"> t: </em>的期望收益</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e8b5c75fb4e6b4dd3d5da56afbdbca13.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*UHmzmcWu9uhPQcKtNaRWNg.png"/></div></figure><ul class=""><li id="78cc" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><strong class="ky ir">状态-行为价值函数</strong>(通常称为Q值)是一个状态-行为对在时间<em class="lv"> t: </em>的期望收益</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/5d379e8fdcaa50758f09c42c0cba7f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*fdH4j5SRQ1W4yi4Fh6JJjA.png"/></div></figure><p id="a351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Q值与值函数的区别是动作<strong class="ky ir">优势</strong>函数(通常称为A值):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/1159402b2e057b2beea122ad9f42d220.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*lCY5ZHp1_7ayiLE2wcMOBg.png"/></div></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="7be7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好了，我们已经学习了什么是价值函数和行为状态价值函数。现在，我们准备学习更多关于RL算法的另一个分支，该分支专注于算法优化的组件。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/91c8d1119a669a5eab481628099d31d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*7QvVhjAnxx3orxG2_yC1-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于值和基于策略的算法。[图片由作者提供，转载自大卫·西尔弗的RL课程]</p></figure><ul class=""><li id="e8fb" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated"><strong class="ky ir">基于价值的</strong> RL旨在学习价值/动作-价值函数，以便生成最佳策略(即，最佳策略是隐式生成的)。</li><li id="481d" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><strong class="ky ir">基于策略</strong> RL旨在使用参数化函数直接学习策略。</li><li id="1981" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><strong class="ky ir">演员兼评论家</strong> RL旨在学习价值函数和政策。</li></ul><p id="203c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下表列出了基于价值和基于策略的方法的优缺点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/48a0257dc4813cf7ea5946f9d152b39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66cmFofH9atJYDDrHlaoOg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于值和基于策略的算法的优缺点。[图片由作者提供]</p></figure><ol class=""><li id="8cf1" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr ox mz na nb bi translated"><strong class="ky ir">基于值的</strong>算法必须挑选<em class="lv">最大化</em>动作-状态值函数的动作，如果动作空间非常高维甚至是连续的，代价会很大，而<strong class="ky ir">基于策略的</strong>算法通过直接调整策略的参数来工作，不需要做<em class="lv">最大化</em>计算。</li><li id="a7cb" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated"><strong class="ky ir">基于值的</strong>算法如果以“错误的方式”做事，可能会振荡/颤动/发散(更差的收敛特性/不太稳定)，而<strong class="ky ir">基于策略的</strong>算法更稳定，具有更好的收敛特性，因为它们只对策略梯度做很小的增量改变。</li><li id="0972" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated"><strong class="ky ir">基于策略的</strong>算法可以学习确定性和随机性策略，而<strong class="ky ir">基于值的</strong>算法只能学习确定性策略。</li><li id="3855" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated">与基于值的算法相比，简单的基于策略的算法可能更慢，并且方差更大。<strong class="ky ir">基于值的方法</strong>试图挑选<em class="lv">最大化</em>动作-状态值函数的动作，该动作-状态值函数将朝着最佳策略(更快和更低的方差)的方向改进策略，而基于策略的方法只是迈出一小步，并朝着更稳定的方向平滑更新，但同时效率较低，有时会导致更高的方差。</li><li id="2057" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr ox mz na nb bi translated">基于策略的方法通常收敛于局部最优，而不是全局最优。</li></ol></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="799b" class="ma mb iq bd mc md nz mf mg mh oa mj mk jw ob jx mm jz oc ka mo kc od kd mq mr bi translated">符合政策与不符合政策</h1><p id="f557" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">还有另一种分类RL算法的方法。这次分类是基于策略的来源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/6d23c2a38a14416d2dc363ac5c72c8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*-GQfMEudKqM6Otm1vVciPA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">策略内与策略外算法。[图片由作者提供]</p></figure><p id="199d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以说被归类为<strong class="ky ir"> on-policy </strong>的算法是在工作中学习的<em class="lv">。</em>“换句话说，算法试图从π采样的经验中学习策略π。</p><p id="aa61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而被归类为<strong class="ky ir">非策略</strong>的算法是通过<em class="lv">监视某人来工作的算法。</em>“换句话说，算法试图从μ采样的经验中学习策略π。例如，机器人通过观察另一个人的行为来学习如何操作。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="de45" class="ma mb iq bd mc md nz mf mg mh oa mj mk jw ob jx mm jz oc ka mo kc od kd mq mr bi translated">最后的话</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/6d89f77b8c7c2dfe38c8fc5d2a47d313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*umXeKbNmRa2wQBvl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔丹·沃兹尼亚克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8217" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">恭喜你坚持到这一步！！</p><p id="51d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读完本文后，您应该已经知道RL算法是如何基于几种观点进行分类的。在未来的几集里，我们会学到更多关于基于值和基于策略的算法。</p><p id="898c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，我们的RL之旅仍处于早期阶段！我还有很多材料要和大家分享。所以，如果你喜欢这些内容，并想在接下来的两个月里继续和我一起学习，请关注我的媒体账号，以获得关于我未来帖子的通知！</p><h1 id="6e9a" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">关于作者</h1><p id="af11" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated">Louis Owen是一名数据科学爱好者，他总是渴望获得新知识。他获得了最后一年的全额奖学金，在印尼顶尖大学<a class="ae kv" href="https://www.itb.ac.id/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">万隆技术学院</em> </a>攻读数学专业。最近，2020年7月，他刚刚以优异的成绩毕业。</p><p id="79a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Louis曾在多个行业领域担任分析/机器学习实习生，包括OTA ( <a class="ae kv" href="https://www.linkedin.com/company/traveloka-com/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> Traveloka </em> </a>)、电子商务(<a class="ae kv" href="https://www.linkedin.com/company/pt--tokopedia/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> Tokopedia </em> </a>)、fin tech(<a class="ae kv" href="https://www.linkedin.com/company/doitglotech/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Do-it</em></a>)、智慧城市App ( <a class="ae kv" href="https://www.linkedin.com/company/qluesmartcity/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> Qlue智慧城市</em> </a>)，目前在<a class="ae kv" href="https://www.linkedin.com/company/the-world-bank/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">世界银行</em> </a>担任数据科学顾问</p><p id="b00b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看路易斯的网站，了解更多关于他的信息！最后，如果您有任何疑问或任何要讨论的话题，请通过<a class="ae kv" href="https://www.linkedin.com/in/louisowen/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系路易斯。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="2324" class="ma mb iq bd mc md nz mf mg mh oa mj mk jw ob jx mm jz oc ka mo kc od kd mq mr bi translated">参考</h1><p id="5aab" class="pw-post-body-paragraph kw kx iq ky b kz oe jr lb lc of ju le lf og lh li lj oh ll lm ln oi lp lq lr ij bi translated"><a class="ae kv" href="https://donnie.id/posts/on-policies-in-reinforcement-learning-algorithms-and-aws-deepracer/" rel="noopener ugc nofollow" target="_blank">https://donnie . id/posts/on-policies-in-reinforcement-learning-algorithms-and-AWS-deepracer/</a></p><p id="82a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html" rel="noopener ugc nofollow" target="_blank">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html </a></p><p id="9371" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf" rel="noopener ugc nofollow" target="_blank">https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf </a></p><p id="9227" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.youtube.com/watch?v=bRfUxQs6xIM&amp;list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb&amp;index=6" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=bRfUxQs6xIM &amp; list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb &amp; index=6 </a></p></div></div>    
</body>
</html>