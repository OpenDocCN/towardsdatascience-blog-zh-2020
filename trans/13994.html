<html>
<head>
<title>Big Data Transformations with Complex and Nested Data Types</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有复杂和嵌套数据类型的大数据转换</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/big-data-transformations-with-complex-and-nested-data-types-c1b9c45ca792?source=collection_archive---------22-----------------------#2020-09-26">https://towardsdatascience.com/big-data-transformations-with-complex-and-nested-data-types-c1b9c45ca792?source=collection_archive---------22-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="37f3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Apache Spark编程技巧和诀窍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8d4bef324763be62c0f624781bd8fe4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pUjzmBEsljoyyLHA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">劳拉·奥克尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2009" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="5ee0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> <em class="mn"> Apache Spark </em> </strong>是一个分布式计算大数据分析框架，旨在跨机器集群转换、设计和处理海量数据(想想万亿字节和千兆字节)。经常使用不同的数据集，您会遇到复杂的数据类型和格式，需要昂贵的计算和转换(想想<em class="mn">物联网</em>设备)。极其复杂和专业，在扩展大数据工程工作方面，<strong class="lt iu"> <em class="mn"> Apache Spark </em> </strong>是其工艺的大师。在这篇博客中，我将使用原生的<em class="mn"> Scala </em> API向你展示1。)如何用嵌套schema ( <em class="mn"> array </em>和<em class="mn"> struct </em>)对半结构化<em class="mn"> JSON </em>数据进行扁平化和规范化，2。)如何透视数据，以及3 .)如何将数据作为<em class="mn">拼花</em>模式保存到存储中，以便进行下游分析。注意，同样的练习可以使用<em class="mn"> Python </em> API和<em class="mn"> Spark SQL </em>来完成。</p><h1 id="45af" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">步骤1:规范化半结构化嵌套模式</h1><p id="2c9f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1a。)让我们来看看我们漂亮的多行<em class="mn"> JSON </em> schema(来自我最喜欢的视频游戏的虚拟数据)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="f980" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ms mc md me mt mg mh mi mu mk ml mm im bi translated">1b。)接下来，为了提高性能，我将把我们的模式映射并构造为新的<code class="fe mv mw mx my b">StructType()</code>，以避免在读取<em class="mn"> JSON </em>数据时触发不必要的<strong class="lt iu"> <em class="mn"> Spark </em> </strong>作业。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="c022" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ms mc md me mt mg mh mi mu mk ml mm im bi translated">1c。)现在，我们可以打印我们的模式并检查数据，您可以看到，由于涉及到的数据类型，这是一件令人欣喜的事情。该数据集中总共有12行5列，但是我们在本机模式中看到2行2列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="d5a9" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ms mc md me mt mg mh mi mu mk ml mm im bi translated">1d。)是时候使用内置的<strong class="lt iu"><em class="mn">Spark</em></strong><em class="mn">data frame</em>API函数(包括<code class="fe mv mw mx my b">explode</code>)将<code class="fe mv mw mx my b">array</code>数据类型中的元素转换为单独的行和点<code class="fe mv mw mx my b">*</code>，这些函数解包<code class="fe mv mw mx my b">struct</code>数据类型中的子字段。由于<em class="mn">子类</em>和<em class="mn">超级</em>列具有一对一的元素对映射，slick <code class="fe mv mw mx my b">arrays_zip</code>函数也将与点选择一起使用，以避免在分解期间创建额外的行组合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="fe71" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ms mc md me mt mg mh mi mu mk ml mm im bi translated">1e。)是时候检查转换后的数据集及其模式了。正如所料，它返回12行5列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h1 id="c5aa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第2步:转换和重塑数据</h1><p id="f4b7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">2a。)下一个练习将采用我们的展平数据集并应用<code class="fe mv mw mx my b">pivot</code>函数，这将触发大范围转换，将特定列的不同值转置到各个列中。可以在聚合或不聚合的情况下执行透视。对于使用许多列<em class="mn">也称为</em>功能作为学习算法输入的数据科学用例，无聚合通常是必需的模式。一个有效的性能提示是在<code class="fe mv mw mx my b">pivot</code>函数输入中指定您的唯一值，这样<strong class="lt iu"> <em class="mn"> Spark </em> </strong>就不必触发额外的作业。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h1 id="9cd5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第三步:写入拼花格式</h1><p id="93e8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">3a。)最后一个练习将简单地把我们的数据写到存储器中。<em class="mn">将使用Parquet </em>格式，因为它是一种可拆分的文件格式，经过高度压缩以提高空间效率，并针对列存储进行了优化，因此非常适合下游大数据分析。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mo mp l"/></div></figure><h1 id="0279" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="86a5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这些练习只是触及了<strong class="lt iu"> <em class="mn"> Apache Spark </em> </strong>对于大数据工程和高级分析用例的皮毛。感谢你阅读这篇博客。</p></div></div>    
</body>
</html>