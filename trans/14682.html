<html>
<head>
<title>The Exploding and Vanishing Gradients Problem in Time Series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列中的爆炸梯度和消失梯度问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22?source=collection_archive---------2-----------------------#2020-10-10">https://towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22?source=collection_archive---------2-----------------------#2020-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="fb9e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="f5a6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在这篇文章中，我们处理时间序列中的爆炸和消失梯度，特别是在递归神经网络(RNN)中，通过时间和梯度裁剪截断反向传播。</h2></div><h1 id="82e6" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">介绍</h1><p id="f26e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这篇文章中，我们将重点关注顺序数据技术的深度学习。我们都熟悉这类数据。例如，文本是单词序列，视频是图像序列。更具挑战性的例子是来自时间序列数据的分支，具有诸如心率、血压等医学信息。、或金融，提供股票价格信息。深度学习时间序列任务最常见的<em class="mc"> AI </em>方法是递归神经网络(RNNs)。使用RNN的动机在于解决方案对时间的概括。由于序列具有不同的长度(大多数情况下)，经典的深度学习架构(如多层感知器(MLP))不加修改就无法应用。而且，MLP的砝码数量绝对庞大！因此，通常使用RNN，在整个架构中共享权重。下面显示了一个简单的RNN架构，其中V、W和U是权重矩阵，b是偏置向量。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/a439caa5161fcf1a07d77731ad0a09dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pfDZQ6GIiZ0MGHjG.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="5352" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">如果你不熟悉RNN、反向传播或MLP，请随意阅读本文末尾的参考文献[1]-[3]来填补空白。</p><h1 id="a5ca" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">穿越时间的反向传播(BPTT)</h1><p id="0dbf" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">通过定义损失函数(<em class="mc"> L </em>)来训练RNN，该损失函数测量真实标签和输出之间的误差，并通过使用向前传递和向后传递来最小化该误差。下面简单的RNN架构总结了整个通过时间反向传播的想法。</p><p id="f596" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">对于单个时间步长，执行以下程序:首先，输入到达，然后通过隐藏层/状态进行处理，并计算估计的标签。在这个阶段，计算损失函数以评估真实标签和估计标签之间的差异。计算总损失函数<strong class="li ja"> L </strong>，并由此完成正向传递。第二部分是向后传递，在这里计算各种导数。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/5aaf0f0c56933d9951783b1e6ca5b288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SKflG0ykj6EWPnm4.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="5ba7" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">RNN的训练并不简单，因为我们通过层和时间反向传播梯度<strong class="li ja">。因此，在每个时间步中，我们必须将所有之前的贡献相加，直到当前贡献，如下式所示:</strong></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi my"><img src="../Images/58967fc5dab3a5ea0e251b3c01b8378a.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/0*1elGSq59CbTaKXme.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="1fc2" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">在该等式中，计算时间步长<strong class="li ja"> k </strong>处的状态对时间步长T = T<strong class="li ja">T5处的整个损失函数<strong class="li ja"> L、</strong>的梯度的贡献。训练期间的挑战在于隐藏状态的比率:</strong></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a438092e27e8471dc4774aba0f63b57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*bOv7Ls0Uz1URRacD.png"/></div></figure><h1 id="ebab" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">消失和爆炸梯度问题</h1><p id="5d4c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">时间序列数据反向传播过程中出现的两个常见问题是梯度的消失和爆炸。上面的等式有两个问题:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/9fa8881ca3d3ff8c3f6a7ebdd983239d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ET_iHvyIb0eNBzD9.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="15ba" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">在第一种情况下，这一项以指数形式快速趋向于零，这使得学习一些长周期相关性变得困难。这个问题叫做<em class="mc">消失渐变</em>。在第二种情况下，这一项以指数方式快速趋近于无穷大，由于过程不稳定，它们的值变成了NaN。这个问题叫做<em class="mc">爆炸梯度</em>。在接下来的两节中，我们将回顾处理这些问题的两种方法。</p><h1 id="392c" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">随时间截断反向传播(截断BPTT)。</h1><p id="aff9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">下面的“技巧”试图通过在训练过程中考虑移动窗口来克服消失梯度问题。众所周知，在反向传播训练方案中，通过<strong class="li ja">整个序列</strong>有一个正向传递和一个反向传递来计算损失和梯度。通过采用一个窗口，我们还从训练持续时间方面提高了训练性能——我们简化了它。</p><p id="abf4" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">这个窗口被称为“<em class="mc">块</em>”。在反向传播过程中，我们向前和向后运行这个特定大小的块，而不是整个序列。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/eb36342a1486ba4c721f1950a9db32bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ctBCh2RrzCY4zvrV.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="b74a" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">截断的BPTT比简单的BPTT要快得多，也不太复杂，因为我们不需要从远处的步骤做梯度的贡献。这种方法的缺点是，在训练过程中不会教授大于组块长度的依赖性。另一个缺点是消失梯度的检测。通过观察学习曲线，人们可以假设梯度消失，但是，也许任务本身是困难的。</p><p id="b80e" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">对于消失梯度问题，已经提出了许多其他方法，仅举几个例子:</p><ol class=""><li id="2698" class="na nb iq li b lj mt lm mu lp nc lt nd lx ne mb nf ng nh ni bi translated">使用ReLU激活功能。</li><li id="8793" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">长短期记忆(LSTM)架构，其中遗忘门可能有所帮助。</li></ol><p id="bc77" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">3.用正交矩阵初始化权重矩阵，<strong class="li ja"> W </strong>，<em class="mc"> </em>，并在整个训练中使用它(正交矩阵的乘法不会爆炸或消失)。</p><h1 id="a9ec" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">渐变剪辑</h1><p id="28c2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">将<strong class="li ja"> g </strong>视为损失函数相对于所有网络参数的梯度。现在，定义一些<strong class="li ja">阈值</strong>，并在训练过程的后台运行以下剪辑条件。这是一个非常简单且非常有效的条件。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c21c4d358dd4764357b74fe87eb3c5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*1JbmuzbGnqJ7yTO-.png"/></div></figure><p id="631e" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">通过应用渐变裁剪，我们不会改变渐变的方向，而只是改变它的大小。由于隐藏状态(<strong class="li ja"> h) </strong>导数<strong class="li ja"> </strong>是引起爆炸渐变的部分，它足以剪辑下面的实体:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3eb3f09c5c050ea66debb4ae3ad6c866.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/format:webp/0*TU8IKwuU3n_5nZsf.png"/></div></figure><p id="5fde" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated"><strong class="li ja"> <em class="mc">阈值</em> </strong> <em class="mc"> </em>是设计者需要手动定义的关键参数。我们的目标是通过查看梯度范数的曲线来选择解决爆炸梯度问题的最高阈值:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/cccca4544173a50f6902f5f2cb291ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p0GIi2qgj5dfIrQv.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><h1 id="ad64" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">摘要</h1><p id="fdee" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这篇文章中，我们探讨了简单的RNN建筑中渐变的消失和爆炸问题。这两个问题属于机器学习中的开放问题，这方面的研究非常活跃。讨论了截断BPTT和梯度裁剪方法，并给出了一些实现技巧。</p><h1 id="3499" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">关于作者</h1><p id="9df7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Barak获得了以色列理工学院的航空工程学士学位(2016年)、硕士学位(2018年)以及经济和管理学士学位(2016年，成绩优异)。他曾在高通工作(2019-2020)，在那里他主要研究机器学习和信号处理算法。巴拉克目前正在海法大学攻读博士学位。他的研究兴趣包括传感器融合、导航、深度学习和估计理论。</p><p id="9164" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated"><a class="ae nq" href="http://www.barakor.com/" rel="noopener ugc nofollow" target="_blank">www.barakor.com</a></p><p id="b056" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated"><a class="ae nq" href="https://www.linkedin.com/in/barakor/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/barakor/</a></p><h1 id="fe67" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考资料和进一步阅读</h1><p id="35ef" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">[1] <a class="ae nq" href="https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f" rel="noopener">对多层感知器的理解(MLP) </a>。<a class="ae nq" href="https://medium.com/@AI_with_Kain?source=post_page-----8f179c4a135f--------------------------------" rel="noopener">尼廷·库马尔·凯恩</a>，中等。2018.</p><p id="0e9e" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">[2] <a class="ae nq" href="https://medium.com/@AI_with_Kain/understanding-of-multilayer-perceptron-mlp-8f179c4a135f" rel="noopener">了解神经网络。从神经元到RNN、CNN和深度学习</a>。维博·尼甘，中等。2018.</p><p id="6153" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">反向传播非常简单。谁让它变得复杂了？ <a class="ae nq" href="https://medium.com/@14prakash?source=post_page-----97b794c97e5c--------------------------------" rel="noopener">普拉卡什杰伊</a>，中等。2017.</p><p id="0b38" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">[4]张，京兆，等.“为什么梯度裁剪加速训练:适应性的理论辩护”<em class="mc"> arXiv预印本arXiv:1905.11881 </em> (2019)。</p><p id="b832" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">[5]陈，向毅，志伟，洪."理解私人SGD中的渐变剪辑:几何透视."arXiv预印本arXiv:2006.15429  (2020)。</p><p id="5cc4" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated"><strong class="li ja"> [6]帕斯卡努，拉兹万，托马斯·米科洛夫，约舒阿·本吉奥。"关于训练递归神经网络的难度."<em class="mc">机器学习国际会议</em>。2013.</strong></p><p id="57fb" class="pw-post-body-paragraph lg lh iq li b lj mt ka ll lm mu kd lo lp mv lr ls lt mw lv lw lx mx lz ma mb ij bi translated">[7] Ribeiro，António H .等人，“超越爆炸和消失梯度:使用吸引子和平滑度分析RNN训练”人工智能与统计国际会议。2020.</p></div></div>    
</body>
</html>