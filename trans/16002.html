<html>
<head>
<title>How Neural Networks Solve the XOR Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络如何解决异或问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7?source=collection_archive---------0-----------------------#2020-11-04">https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7?source=collection_archive---------0-----------------------#2020-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2954" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">以及为什么隐藏层如此重要</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2e5c8bd6c502a551bc3c247d8ec8da17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7YhLRr9Xz4wFECQ-0Q3Spg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="793d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">感知器</strong>是一种分类算法。具体来说，它是一个线性二元分类器。它是由弗兰克·罗森布拉特在20世纪50年代末发明的。</p><p id="132f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感知器基本上是作为一个阈值函数工作的——非负输出被放入一个类，而负输出被放入另一个类。</p><p id="f4dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然有很多关于神经网络及其变体的讨论，但我们将讨论一个突出单层感知器和多层感知器之间主要差异的具体问题。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="8ca4" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">目录</h1><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="e82f" class="mv lz iq mr b gy mw mx l my mz"><strong class="mr ir">The Perceptron</strong><br/>   Structure and Properties<br/>   Evalutation<br/>   Training algorithm</span><span id="8e50" class="mv lz iq mr b gy na mx l my mz"><strong class="mr ir">2d Xor problem  <br/></strong>   The XOR function</span><span id="b6ef" class="mv lz iq mr b gy na mx l my mz"><strong class="mr ir">Attempt #1: The Single Layer Perceptron   <br/>   </strong>Implementing the Perceptron algorithm<br/>   Results<br/>   The need for non-linearity</span><span id="fe78" class="mv lz iq mr b gy na mx l my mz"><strong class="mr ir">Attempt #2: Multiple Decision Boundaries<br/></strong>   Intuition<br/>   Implementing the OR and NAND parts</span><span id="380e" class="mv lz iq mr b gy na mx l my mz"><strong class="mr ir">The Multi-layered Perceptron<br/></strong>   Structure and Properties<br/>   Training algorithm</span><span id="07f4" class="mv lz iq mr b gy na mx l my mz"><strong class="mr ir">Attempt #3: The Multi-layered Perceptron<br/></strong>   Implementing the MLP<br/>   Results</span></pre></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="1b20" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated"><strong class="ak">组织和性能</strong></h1><p id="58f4" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">感知器有以下组件:</p><ul class=""><li id="5246" class="ng nh iq kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated">输入节点</li><li id="0691" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">输出节点</li><li id="f5fc" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">激活功能</li><li id="7328" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">权重和偏差</li><li id="2c9e" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">误差函数</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/128132eaa63ad5ce351e4d21a2eb7213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tzjf5GuL-1xu2tMb2Rsg1g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有2个输入节点的单层感知器的表示——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><h2 id="e1af" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">输入节点</h2><p id="c30f" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">这些节点包含网络的输入。在任何迭代中——无论是测试还是训练——这些节点都被传递来自我们数据的输入。</p><h2 id="83c3" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated"><strong class="ak">权重和偏差</strong></h2><p id="6844" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">当我们谈论“训练”一个模型时，这些参数是我们更新的。它们被初始化为某个随机值或设置为0，并随着训练的进行而更新。偏差类似于独立于任何输入节点的权重。基本上，它使模型更加灵活，因为你可以“移动”激活函数。</p><h2 id="4b18" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">估价</h2><p id="3763" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">输出计算很简单。</p><ul class=""><li id="d092" class="ng nh iq kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated">计算输入和权重向量的点积</li><li id="3fe5" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">加上偏见</li><li id="3286" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">应用激活功能。</li></ul><p id="98ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这可以这样表达:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/f2a500139e1000dbaf5d0e0393366f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgmKKBVoFIXJC0qiqpC50A.png"/></div></div></figure><p id="770b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这通常被简化为权重和输入向量加上偏差的点积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/0829a38f152ee3f3cfa5e0b181765680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0GjDWpgA_x2a4QbjNh6HKQ.png"/></div></div></figure><h2 id="0f73" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">激活功能</h2><p id="01b2" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">该函数允许我们以更有意义的方式拟合输出。例如，在简单分类器的情况下，比如说<code class="fe oj ok ol mr b">-2.5</code>或<code class="fe oj ok ol mr b">8</code>的输出对于分类没有多大意义。如果我们使用称为sigmoidal激活函数的东西，我们可以在0到1的范围内拟合它，这可以直接解释为属于特定类的数据点的概率。</p><p id="7807" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管有很多种激活函数，我们将为我们的感知器使用一个简单的线性激活函数。线性激活函数对其输入没有影响，并按原样输出。</p><h2 id="b02a" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">分类</h2><p id="25e2" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">感知器如何给一个数据点分配一个类？</p><p id="3ef3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们知道一个数据点的评估由关系式<code class="fe oj ok ol mr b">wX + b</code>表示。我们定义一个阈值(<strong class="kx ir"> θ </strong>)来对我们的数据进行分类。通常，对于感知器，该阈值被设置为0。</p><p id="3031" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，<code class="fe oj ok ol mr b">wX + b</code>大于或等于0的点将属于一个类别，而其余的点(<code class="fe oj ok ol mr b">wX + b</code>为负)被分类为属于另一个类别。我们可以这样表达:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/7c42dc31b387fe7039605363eacaa0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzNeD1NCpKEpOWUMI_-xiQ.png"/></div></div></figure><h2 id="b652" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">训练算法</h2><p id="d564" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">为了训练我们的感知机，我们必须确保我们正确地对所有的训练数据进行分类。请注意，这与您训练神经网络的方式不同，在神经网络中，您不会尝试正确分类您的整个训练数据。在大多数情况下，这将导致所谓的过度拟合。</p><p id="426e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们通过计算<strong class="kx ir">梯度</strong>或δw来开始训练算法。它是下式的乘积:</p><ul class=""><li id="9d9e" class="ng nh iq kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated">对应于该权重的输入节点的值</li><li id="df81" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">实际值和计算值之间的差异。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/4abfe1c403e4249cca0e20c4465640dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoaCWleG8stS4AobGX_H_w.png"/></div></div></figure><p id="2d01" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们通过简单地用计算出的梯度乘以学习率来增加我们的原始权重，从而得到我们的新权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/78444c83240948dbacbff0d71d7c8046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7h2ZWZXOr_Ae5ZERGRpLLg.png"/></div></div></figure><p id="7833" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是如何工作的一个简单的直觉:如果我们的感知器正确地分类了一个输入数据点，<code class="fe oj ok ol mr b">actual_value — computed_value</code>将是<code class="fe oj ok ol mr b">0</code>，并且我们的权重不会有任何变化，因为梯度现在是<code class="fe oj ok ol mr b">0</code>。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="cb64" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">2D异或问题</h1><p id="f44f" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在异或问题中，我们试图训练一个模型来模仿2D异或函数。</p><h2 id="2b75" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">异或函数</h2><p id="b23a" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">该函数定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/fbe504da415120e99e9055ea3bb1abc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*bpc3E6NlkPQpyGtL-azRsw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">XOR真值表—作者图片</p></figure><p id="0290" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们画出来，我们会得到下面的图表。这就是我们想要分类的。您在图例中看到的⊕(“o-plus”)符号通常用于表示XOR布尔运算符。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/c24772a8221873a9e742505f37d3f267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aN7_uKSN8iWUktGOKa1Vgg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">XOR输出图—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><p id="12e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的算法——不管它是如何工作的——必须正确地输出4个点中每一个点的XOR值。我们将把这建模为一个分类问题，因此<code class="fe oj ok ol mr b">Class 1</code>将表示XOR值1，而<code class="fe oj ok ol mr b">Class 0</code>将表示值0。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="f23c" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">尝试1:单层感知器</h1><p id="7160" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">让我们用单层感知器来模拟这个问题。</p><h2 id="13db" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated"><strong class="ak">输入数据</strong></h2><p id="5777" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我们将在其上训练模型的数据是我们看到的XOR函数的表。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="70ab" class="mv lz iq mr b gy mw mx l my mz"><strong class="mr ir">Data         Target<br/>[0, 0]         0<br/>[0, 1]         1<br/>[1, 0]         1<br/>[1, 1]         0</strong></span></pre><h2 id="8592" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">履行</h2><p id="6d84" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated"><strong class="kx ir">进口</strong></p><p id="5a5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了通常的可视化(<code class="fe oj ok ol mr b">matplotlib</code>和<code class="fe oj ok ol mr b">seaborn</code>)和数字库(<code class="fe oj ok ol mr b">numpy</code>)，我们将使用<code class="fe oj ok ol mr b">itertools</code>中的<code class="fe oj ok ol mr b">cycle</code>。这是因为我们的算法无限地循环通过我们的数据，直到它成功地正确分类整个训练数据，而中间没有任何错误。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="8777" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">数据</strong></p><p id="131f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们创建训练数据。这些数据对于每种逻辑门都是一样的，因为它们都接受两个布尔变量作为输入。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="d075" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">培训功能</strong></p><p id="12f6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，我们无限地循环数据，跟踪我们正确分类了多少连续的数据点。如果我们设法在一段时间内对所有东西进行分类，我们就终止了我们的算法。</p><p id="09a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果没有，我们重置我们的计数器，更新我们的权重并继续算法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="11e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了可视化我们的模型是如何执行的，我们创建了一个数据点的网格，或者说一个网格，并在网格中的每个点上评估我们的模型。最后，我们根据模型的分类给每个点着色。因此,<code class="fe oj ok ol mr b">Class 0</code>区域将被分配给属于该类的点的颜色填充。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="8dc8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">感知器类</strong></p><p id="b9f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了把所有的东西放在一起，我们用我们刚刚讨论过的函数创建了一个简单的<code class="fe oj ok ol mr b">Perceptron</code>类。我们有一些实例变量，如训练数据、目标、输入节点数和学习率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="78e2" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">结果</h2><p id="9cfe" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">让我们创建一个感知器对象，并在XOR数据上训练它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="3667" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你会注意到训练循环永远不会终止，因为感知器只能收敛于线性可分的数据。线性可分离数据基本上意味着您可以用1D的一个点、2D的一条线、3D的一个平面等来分离数据。</p><blockquote class="ot"><p id="ab64" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">感知器只能收敛于可线性分离的数据。因此，它不能模仿异或函数。</p></blockquote><p id="d015" class="pw-post-body-paragraph kv kw iq kx b ky pd jr la lb pe ju ld le pf lg lh li pg lk ll lm ph lo lp lq ij bi translated">请记住，感知器必须一次性正确地对整个训练数据进行分类。如果我们连续记录有多少点被正确分类，我们会得到这样的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/a8e03c5dce95871922b699053d753a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylW5N5ObDpDf6TxzCeJDXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">超过100个训练周期的correct_counter的值-作者提供的图像</p></figure><p id="e02e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该算法仅在<code class="fe oj ok ol mr b">correct_counter</code>达到4时终止——这是训练集的大小——因此这将无限期地继续下去。</p><h2 id="2265" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">对非线性的需求</h2><p id="9196" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">很明显，单个感知器不能满足我们的目的:类不是线性可分的。这归结为一个事实，即单一的线性决策边界是行不通的。</p><p id="af8b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">非线性允许更复杂的决策边界。我们的XOR数据的一个潜在决策边界可能是这样的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/bbdd61e1fe85a7bc6a96582f1b742199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgcoOFn-gfKnA_U5BOw-XA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的XOR模型的潜在非线性决策边界—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="13ba" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">2d XOR问题—尝试2</h1><p id="ef65" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我们知道模拟XOR函数需要非线性的判定边界。</p><blockquote class="ot"><p id="32d6" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">但是为什么我们必须坚持单一的决策边界呢？</p></blockquote><h2 id="2fe3" class="mv lz iq bd ma nw pk dn me ny pl dp mi le pm ob mk li pn od mm lm po of mo og bi translated">直觉</h2><p id="4405" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">让我们首先将XOR函数分解成它的AND和OR对应物。</p><p id="9f87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两个布尔变量A和B上的XOR函数被定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/39e99d191796a04e1403ed47744568a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LuINCL4Tf_yDGUlMD44T4A.png"/></div></div></figure><p id="2d78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将<code class="fe oj ok ol mr b">A.~A</code>和<code class="fe oj ok ol mr b">B.~B</code>加到等式中。因为它们都等于0，所以等式仍然有效。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/00cfd69058538e6fbe553129bd2e7211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRRn_PvwGlMego2Xxe_LIg.png"/></div></div></figure><p id="0721" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们重新安排术语，这样我们可以从第一部分中抽出<code class="fe oj ok ol mr b">A</code>，从第二部分中抽出<code class="fe oj ok ol mr b">B</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pr"><img src="../Images/feae7a862c7254dd61240244d958908b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMB4L2oK5PJVYqGQrm5y0A.png"/></div></div></figure><p id="6a36" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">进一步简化，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ps"><img src="../Images/84070b198ed89db7bc1f3ad4d3cf28c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9nguc5_2v1A1ABR-IUF0g.png"/></div></div></figure><p id="368d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用布尔代数的德摩根定律:<code class="fe oj ok ol mr b">~A + ~B = ~(AB)</code>，我们可以这样替换上式中的第二项:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pt"><img src="../Images/77f1d1d1942aff6a5c8eb018bff9e990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBrvvYfm-jVFmgtGW-LiIw.png"/></div></div></figure><p id="87d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们分别用x1和x2代替A和B，因为这是我们在数据中使用的约定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pu"><img src="../Images/695bfbb16526c3f7272964b0be04eedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HutPiBbJJ2iafsRo_g0O2Q.png"/></div></div></figure><p id="6baa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">XOR函数可以浓缩为两部分:<strong class="kx ir">一个NAND和一个OR </strong>。如果我们可以分别计算这些，我们可以使用<strong class="kx ir">AND门合并结果。</strong></p><p id="fe8d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们把公式的OR部分称为第一部分，把NAND部分称为第二部分。</p><h2 id="a332" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated"><strong class="ak">建模OR零件</strong></h2><p id="4ee6" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我们将像以前一样使用相同的感知器类，只是我们将在训练数据上训练它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/f53d079cb0c837e5bbec6f8dac37cc18.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*roabWS1sgBDoqKHtCde0oQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">OR真值表—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="b766" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是收敛的，因为or函数的数据是线性可分的。如果我们像第一次尝试那样绘制正确分类的连续数据点的数量，我们会得到这个图。很明显，在迭代50次左右，它达到了值4，这意味着它对整个数据集进行了正确的分类。</p><blockquote class="ot"><p id="0e1e" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">correct_counter测量被我们的感知器正确分类的连续数据点的数量</p></blockquote><figure class="px py pz qa qb kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/61724aef83d6289b5ef51f8bb2f62de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IybVY22exz_e-AoTJeIF9g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们或感知器的正确计数图——作者图片</p></figure><p id="db6f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策边界图如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/d7ebe642d22940adc9dd3e7964e9da00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7r0FU4_2YZJzPixFXExcA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的或感知器的输出图—作者的图像</p></figure><h2 id="28b4" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">模拟NAND部分</h2><p id="b395" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">让我们进入第二部分。我们需要模拟一个与非门。就像OR部分一样，我们将使用相同的代码，但在NAND数据上训练模型。所以我们的输入数据应该是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/e59d587ab068707ca9d593081e323d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*0J2g4A35NfCUT82y3GZcVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NAND真值表—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>制作的图片</p></figure><p id="145c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">经过训练后，下面的图表明，我们的模型收敛于NAND数据，并完美地模拟了NAND门。</p><div class="kg kh ki kj gt ab cb"><figure class="qd kk qe qf qg qh qi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0cd27706b7e230c1e5598c9853b2915c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*_FSuCnOqRtg1uXYSgnD9KQ.png"/></div></figure><figure class="qd kk qj qf qg qh qi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/69236d58f5206adfbadd946d8e9bc7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*q4H9dJUjyDkAcUO72R3Nkg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk qk di ql qm translated">NAND感知器的决策边界和正确计数器图—作者图片</p></figure></div><h2 id="b0e2" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">将所有东西整合在一起</h2><p id="7bf6" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">从中可以清楚地看出两件事:</p><ul class=""><li id="3981" class="ng nh iq kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated">我们对两个逻辑门的输出执行逻辑“与”(其中第一个是“或”,第二个是“与非”)</li><li id="7504" class="ng nh iq kx b ky np lb nq le nr li ns lm nt lq nl nm nn no bi translated">并且两个函数被传递相同的输入(x1和x2)。</li></ul><p id="63c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将这个模型应用到我们的网络中。首先，让我们把我们的两个感知机看作是黑盒。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qn"><img src="../Images/8df0214cf4b9db5194759d4cb8a3e6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6E1lvXJv9phURco9mBr08A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们模型的平面图——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><p id="04ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在添加了我们的输入节点x_1和x_2之后，我们最终可以通过一个简单的函数来实现这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qo"><img src="../Images/1f39e3fd47b2c7e1e4690faf781148f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTBFlBgXysCfR2OHXKWP_A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">添加输入节点—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>生成的图像</p></figure><p id="5136" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们需要一个AND门，我们将像刚才一样训练它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><div class="kg kh ki kj gt ab cb"><figure class="qd kk qe qf qg qh qi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ab48ee8657f17e5f1bf0dd03eafdfc94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*pc4V2KACa6MBjHdI5Y3bAg.png"/></div></figure><figure class="qd kk qj qf qg qh qi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a4bc9f70906acc3f28ed8b9182661557.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*Z5FCj0Nhk31n3XDX9SZWdA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk qk di ql qm translated">我们的和感知器的正确计数和输出图。—作者图片</p></figure></div><p id="23cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在拥有的是一个模拟XOR函数的模型。</p><p id="9c13" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们要实现我们的XOR模型，它看起来会像这样:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="6ec8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们从我们的模型(基本上是我们的OR和n AND模型的AND)中绘制决策边界，我们会得到这样的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qp"><img src="../Images/2aba73aebdaa55757c0f952295afff7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yVX7bID4pp0svaEOAPuZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们第二次尝试的输出图，显示了我们的XOR数据的正确分类——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>生成的图像</p></figure><blockquote class="ot"><p id="7c9f" class="ou ov iq bd ow ox qq qr qs qt qu lq dk translated">在所有2个输入逻辑门中，XOR和XNOR门是唯一不可线性分离的。</p></blockquote><p id="b851" class="pw-post-body-paragraph kv kw iq kx b ky pd jr la lb pe ju ld le pf lg lh li pg lk ll lm ph lo lp lq ij bi translated">尽管我们的模型是可行的，但对于大多数非线性分类或回归任务来说，它似乎不是一个可行的解决方案。真的是具体到这个案例，绝大多数问题都不能拆分成仅仅是简单的中间问题，可以单独解决，然后再组合。对于这样的事情:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qv"><img src="../Images/a2fa13baf13d6f61ed860e103c925d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--0y9w95J7lZAUqjGetHVQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个二维的二元分类问题——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>的图片</p></figure><p id="84b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">潜在的决策边界可能是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qw"><img src="../Images/f85208efc3879df4fc2ee63ffc1beeff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0fYqaCJIGZLsoiKGZgP7Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">符合我们示例的潜在决策边界—作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><p id="599b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们需要寻找一个更通用的模型，它将允许非线性的决策边界，就像上面的曲线一样。让我们看看MLP是如何解决这个问题的。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="2db1" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">多层感知器</h1><p id="2efa" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">MLP的所有组件，如输入和输出节点、激活函数、权重和偏差，都与我们刚刚在感知器中讨论的相同。</p><blockquote class="ot"><p id="05f4" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">最大的区别？MLP可以有隐藏图层。</p></blockquote><h2 id="2e37" class="mv lz iq bd ma nw pk dn me ny pl dp mi le pm ob mk li pn od mm lm po of mo og bi translated">隐藏层</h2><p id="5187" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">隐藏层是那些具有除输入和输出节点之外的节点的层。</p><blockquote class="ot"><p id="d050" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">MLP通常被限制为只有一个隐藏层。</p></blockquote><p id="aa5d" class="pw-post-body-paragraph kv kw iq kx b ky pd jr la lb pe ju ld le pf lg lh li pg lk ll lm ph lo lp lq ij bi translated"><strong class="kx ir">隐藏层允许非线性。</strong>隐藏层中的节点与输出节点没有太大的不同:先前层中的节点用它们自己的权重和偏差连接到它，并且通常用激活函数计算输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qx"><img src="../Images/eadec97f7eb8469fdb48a9b02965f0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iB0NCmG2OFUGI-DCokHT2g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多层感知器的一般结构——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><h2 id="1095" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">激活功能</h2><p id="780e" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">还记得我们在感知器模型的输出节点上使用的线性激活函数吗？还有几个更复杂的激活函数。你可能听说过<code class="fe oj ok ol mr b">sigmoid</code>和<code class="fe oj ok ol mr b">tanh</code>函数，它们是一些最流行的非线性激活函数。</p><blockquote class="ot"><p id="e889" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">激活函数应该是可微的，这样网络的参数可以用反向传播来更新。</p></blockquote><h2 id="3779" class="mv lz iq bd ma nw pk dn me ny pl dp mi le pm ob mk li pn od mm lm po of mo og bi translated">训练算法</h2><p id="5101" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">虽然输出生成过程是感知器的直接扩展，但更新权重并不那么简单。这就是反向传播的由来。</p><p id="8575" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">反向传播</strong>是一种从输出层一直到起点更新模型权重和偏差的方法。其背后的主要原理是，每个参数的变化与其对网络输出的影响程度成比例。对模型输出几乎没有任何影响的权重将显示出非常小的变化，而具有较大负面影响的权重将显著变化，以提高模型的预测能力。</p><blockquote class="ot"><p id="6bf3" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated"><strong class="ak">反向传播</strong>是一种算法，用于根据误差函数的梯度更新模型的权重和偏差，从输出层一直到第一层。</p></blockquote><p id="5623" class="pw-post-body-paragraph kv kw iq kx b ky pd jr la lb pe ju ld le pf lg lh li pg lk ll lm ph lo lp lq ij bi translated">更新权重的方法直接来自求导和链式法则。</p><p id="d36f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当谈到反向传播时，有很多内容需要讨论。它有自己的文章。所以如果你想了解更多，可以看看Simeon Kostadinov的这篇精彩文章。</p><div class="qy qz gp gr ra rb"><a rel="noopener follow" target="_blank" href="/understanding-backpropagation-algorithm-7bb3aa2f95fd"><div class="rc ab fo"><div class="rd ab re cl cj rf"><h2 class="bd ir gy z fp rg fr fs rh fu fw ip bi translated">理解反向传播算法</h2><div class="ri l"><h3 class="bd b gy z fp rg fr fs rh fu fw dk translated">了解神经网络最重要组成部分的具体细节</h3></div><div class="rj l"><p class="bd b dl z fp rg fr fs rh fu fw dk translated">towardsdatascience.com</p></div></div><div class="rk l"><div class="rl l rm rn ro rk rp kp rb"/></div></div></a></div></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="12a5" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">尝试3:多层感知器</h1><h2 id="3081" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">建筑</h2><p id="9935" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">对于网络的隐藏层数或每层中的节点数没有固定的规则。最佳性能的模型是通过反复试验获得的。</p><blockquote class="ot"><p id="975a" class="ou ov iq bd ow ox oy oz pa pb pc lq dk translated">网络的架构指的是它的一般结构——隐藏层的数量、每层中节点的数量以及这些节点如何相互连接。</p></blockquote><p id="0967" class="pw-post-body-paragraph kv kw iq kx b ky pd jr la lb pe ju ld le pf lg lh li pg lk ll lm ph lo lp lq ij bi translated">让我们用一个有两个节点的隐藏层。我们将在每个隐藏层节点中使用sigmoid函数，当然还有输出节点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi rq"><img src="../Images/b654f37f087bb82e48534aa6a6202203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kAZGp-basYjFSHexmQteUA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们MLP的最终建筑——作者使用<a class="ae nv" href="https://app.diagrams.net/" rel="noopener ugc nofollow" target="_blank"> draw.io </a>绘制的图像</p></figure><h2 id="e38b" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">履行</h2><p id="16e9" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">这里使用的库如NumPy和pyplot与感知器类中使用的相同。</p><p id="f79e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">训练算法</strong></p><p id="c186" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里的算法略有不同:我们对训练数据进行固定次数的迭代——准确地说是<code class="fe oj ok ol mr b">num_epochs</code>。在每次迭代中，我们进行一次前向传递，然后进行一次后向传递，根据需要更新权重和偏差。这被称为反向传播。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="815b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">乙状结肠激活功能</strong></p><p id="bb00" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，我们定义一个sigmoid函数。如前所述，它应用于每个隐藏层节点和输出节点的输出。它是可微的，因此它允许我们舒适地执行反向传播来改进我们的模型。</p><p id="dfef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它的派生也是通过<code class="fe oj ok ol mr b">_delsigmoid</code>函数实现的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="1153" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">向前和向后传球</strong></p><p id="812a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在前向传递中，我们多次应用<code class="fe oj ok ol mr b">wX + b</code>关系，并在每次调用后应用sigmoid函数。</p><p id="6fb7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在反向传递中，实现为<code class="fe oj ok ol mr b">update_weights</code>函数，我们计算6个权重和3个偏差相对于误差函数的梯度，并通过因子<code class="fe oj ok ol mr b">learning rate * gradient.</code>更新它们</p><p id="ac0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，classify函数按预期工作:由于sigmoid函数输出0到1之间的值，我们简单地将它们解释为属于特定类的概率。因此，大于或等于0.5的输出被归类为属于<code class="fe oj ok ol mr b">Class 1</code>，而那些小于0.5的输出被称为属于<code class="fe oj ok ol mr b">Class 0</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="7211" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">MLP级</strong></p><p id="2451" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们通过创建一个MLP类将所有的东西集合在一起。我们刚才讨论的所有功能都放在里面了。<code class="fe oj ok ol mr b">plot</code>函数与<code class="fe oj ok ol mr b">Perceptron</code>类中的函数完全相同。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><h2 id="b0d7" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">结果</h2><p id="c165" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">让我们用<code class="fe oj ok ol mr b">0.2</code>倍于<code class="fe oj ok ol mr b">5000</code>倍的学习率来训练我们的MLP。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="6611" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们绘制损失函数的值，我们在大约5000次迭代后得到下面的图，表明我们的模型确实收敛了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/352eb7ef648091ed98fd6ced91a4eea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDHpv6hzQ7Mo17auo5G3VQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们MLP 5000多个时代的损失图—作者图片</p></figure><p id="a319" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里用我们的广义神经网络或MLP创建了一个清晰的非线性决策边界。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/bf9915f891eac50eff14e932eb5cee88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Trja6lUuVl71Xn1w27XRSQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">决策边界图，显示决策边界和类别——由作者提供的图像</p></figure><h2 id="19cb" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">注意#1:添加更多的层或节点</h2><p id="3ee3" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">添加更多的层或节点会产生越来越复杂的决策边界。但这也可能导致所谓的过度拟合，即模型在训练数据上达到非常高的精度，但无法推广。</p><p id="61d6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Tensorflow神经网络游乐场是一个很好的资源，您可以在那里尝试不同的网络架构并查看结果。</p><div class="qy qz gp gr ra rb"><a href="http://playground.tensorflow.org/#activation=sigmoid&amp;batchSize=30&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.1&amp;regularizationRate=0&amp;noise=0&amp;networkShape=2&amp;seed=0.21709&amp;showTestData=false&amp;discretize=true&amp;percTrainData=70&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;batchSize_hide=false" rel="noopener  ugc nofollow" target="_blank"><div class="rc ab fo"><div class="rd ab re cl cj rf"><h2 class="bd ir gy z fp rg fr fs rh fu fw ip bi translated">张量流-神经网络游乐场</h2><div class="ri l"><h3 class="bd b gy z fp rg fr fs rh fu fw dk translated">这是一种构建从数据中学习的计算机程序的技术。它非常松散地基于我们如何思考…</h3></div><div class="rj l"><p class="bd b dl z fp rg fr fs rh fu fw dk translated">playground.tensorflow.org</p></div></div><div class="rk l"><div class="rr l rm rn ro rk rp kp rb"/></div></div></a></div><h2 id="7a38" class="mv lz iq bd ma nw nx dn me ny nz dp mi le oa ob mk li oc od mm lm oe of mo og bi translated">注意#2:选择损失函数</h2><p id="aa12" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">我们在MLP模型中使用的损失函数是均方损失函数。虽然这是一个非常流行的损失函数，但它对数据做了一些假设(比如它是高斯型的)，并且在分类问题上并不总是凸的。这里使用它是为了更容易理解感知机如何工作，但对于分类任务，有更好的替代方法，如<strong class="kx ir">二进制交叉熵损失。</strong></p><div class="qy qz gp gr ra rb"><a href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="rc ab fo"><div class="rd ab re cl cj rf"><h2 class="bd ir gy z fp rg fr fs rh fu fw ip bi translated">训练深度学习神经网络——机器学习掌握时如何选择损失函数</h2><div class="ri l"><h3 class="bd b gy z fp rg fr fs rh fu fw dk translated">使用随机梯度下降优化算法来训练深度学习神经网络。作为…的一部分</h3></div><div class="rj l"><p class="bd b dl z fp rg fr fs rh fu fw dk translated">machinelearningmastery.com</p></div></div><div class="rk l"><div class="rs l rm rn ro rk rp kp rb"/></div></div></a></div></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="2f1d" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">代码</h1><p id="9741" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">你可以在这里找到这篇文章的全部代码。</p><div class="qy qz gp gr ra rb"><a href="https://github.com/Polaris000/BlogCode/blob/main/XOR_Perceptron/xorperceptron.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="rc ab fo"><div class="rd ab re cl cj rf"><h2 class="bd ir gy z fp rg fr fs rh fu fw ip bi translated">Polaris 000/blog code/xorperceptron . ipynb</h2><div class="ri l"><h3 class="bd b gy z fp rg fr fs rh fu fw dk translated">这篇文章的示例代码可以在这里找到。</h3></div><div class="rj l"><p class="bd b dl z fp rg fr fs rh fu fw dk translated">github.com</p></div></div><div class="rk l"><div class="rt l rm rn ro rk rp kp rb"/></div></div></a></div></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="6092" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">结论</h1><p id="0ce9" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">生产或研究中使用的神经网络从来没有这么简单，但它们几乎总是建立在这里概述的基础之上。希望这篇文章能给你一些关于如何建立和训练感知器和香草网络的想法。</p><p id="33fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>