<html>
<head>
<title>A Practical Demonstration of Using Vision Transformers in PyTorch: MNIST Handwritten Digit Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中使用视觉转换器的实际演示:MNIST手写数字识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-demonstration-of-using-vision-transformers-in-pytorch-mnist-handwritten-digit-recognition-407eafbc15b0?source=collection_archive---------15-----------------------#2020-10-09">https://towardsdatascience.com/a-demonstration-of-using-vision-transformers-in-pytorch-mnist-handwritten-digit-recognition-407eafbc15b0?source=collection_archive---------15-----------------------#2020-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7810" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我将给出一个实际操作的示例(带代码)，说明如何使用流行的PyTorch框架将视觉转换器应用到实际的计算机视觉任务中，该框架是在论文“<a class="ae ko" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">中提出的，一幅图像相当于16x16个单词:按比例进行图像识别的转换器</strong> </a>(我在<a class="ae ko" href="https://medium.com/@stankriventsov/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-brief-review-of-the-8770a636c6a8" rel="noopener"> <strong class="js iu">的另一篇文章</strong> </a>中对此进行了评论)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/adc34b7f73630b80ac55a2f49d9ae59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DBSfgxHUuknIqmyDVKwCg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">视觉转换器的原理图(来自<a class="ae ko" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">https://openreview.net/pdf?id=YicbFdNTTy</a>)</p></figure><p id="59f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为此，我们将使用众所周知的<a class="ae ko" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> MNIST数据集</strong> </a>来研究手写数字识别的问题。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lf"><img src="../Images/2407d2535720a56e1630d58536934d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJSNwwDZ6RPuKrB--HqyZQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">使用Pyplot生成的MNIST手写数字示例</p></figure><p id="33b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我想马上提供一个警告，只是为了说明这一点。我选择MNIST数据集进行这次演示，因为它足够简单，可以在几分钟内，而不是几小时或几天内，在没有任何专门硬件的情况下，从头开始训练模型并用于预测，因此几乎任何有计算机的人都可以做到这一点，并看看它是如何工作的。我没有尝试过优化模型的超参数，我当然也没有用这种方法达到最先进的精确度(目前这个数据集的精确度约为99.8%)的目标。</p><p id="6dc0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，虽然我将展示视觉转换器可以在MNIST上获得令人尊敬的98%以上的准确性，但可以说它不是这项工作的最佳工具。由于该数据集中的每个图像都很小(只有28x28像素),并且由单个对象组成，因此应用全局注意力只能具有有限的效用。稍后我可能会写另一篇文章，研究如何在包含更大图像和更多种类的更大数据集上使用该模型。现在，我只想展示它是如何工作的。</p><p id="2b90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在实现方面，我将依赖来自王飞的<a class="ae ko" href="https://github.com/lucidrains/vit-pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">这个</strong> </a>开源库的代码，特别是来自<em class="lg"> vit_pytorch.py </em>文件的以下Vision Transformer (ViT)类:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="937f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与任何PyTorch神经网络模块类一样，它具有初始化(<strong class="js iu"> __init__ </strong>)函数，其中定义了所有可训练参数和层，以及<strong class="js iu">转发</strong>函数，该函数建立了将这些层组装到整个网络架构中的方式。</p><p id="deb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了简洁起见，这里只给出了ViT类本身的定义，没有给出依赖类。如果你想在你的电脑上使用这段代码，你需要导入整个<a class="ae ko" href="https://github.com/kriventsov/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lg">vit _ py torch . py</em></strong></a>文件(这个文件出奇的小，只有一百行左右的代码；我在GitHub上给出了我自己的分叉版本的链接，以防将来原始文件发生变化)，以及最近版本的<a class="ae ko" href="https://pypi.org/project/torch/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> PyTorch </strong> </a>(我使用的是1.6.0)和用于张量操作的<a class="ae ko" href="https://pypi.org/project/einops/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> einops </strong> </a>库。</p><p id="20f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要开始使用MNIST数据集，我们需要首先加载它，我们可以这样做(从这一点开始，这篇文章中的所有代码都是我的，尽管其中很多都很标准):</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="7e9f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面代码中的<strong class="js iu"> transform_mnist </strong>转换用于归一化图像数据，使其均值为零，标准差为1，这有助于神经网络训练。<strong class="js iu"> train_loader </strong>和<strong class="js iu"> test_loader </strong>对象包含已经随机分成批次的MNIST图像，以便它们可以方便地输入到训练和验证程序中。</p><p id="b506" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集中的每个项目都包含一个带有相应地面实况标签的图像。一旦在数据集的训练部分(60，000个手写数字图像)上进行训练，我们的转换器的目标将是基于图像来预测测试部分(10，000个图像)中每个样本的正确标签。</p><p id="2e8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用以下函数为每个时期训练我们的模型:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="b9e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该函数对<a class="ae ko" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> data_loader </strong> </a>对象中的每个批次进行循环。对于每一批，它计算模型的输出(作为一个<a class="ae ko" href="https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.log_softmax" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> log_softmax </strong> </a>)和这个输出的<a class="ae ko" href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">负对数似然损失</strong> </a>，然后通过<strong class="js iu"> loss.backward() </strong>计算这个损失关于每个可训练模型参数的梯度，并通过<strong class="js iu"> optimizer.step()更新参数。</strong>每第100批，它提供一份关于训练进度的打印更新，并将当前损失值附加到<strong class="js iu"> loss_history </strong>列表中。</p><p id="c577" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在每个时期的训练之后，我们将能够使用以下函数来查看我们的当前模型在测试集上的表现:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="955a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然这类似于上面的训练程序，但现在我们不计算任何梯度，而是将模型的输出与地面实况标签进行比较，以计算准确性并更新损失历史。</p><p id="331e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦定义了所有的函数，就该初始化我们的模型并运行训练了。我们将使用以下代码:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="4402" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，我们用7x7的面片大小(对于28x28的图像，这意味着4 x 4 =每个图像16个面片)、10个可能的目标类别(0到9)和1个颜色通道(因为图像是灰度)来定义我们的视觉转换器模型。</p><p id="1464" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在网络参数方面，我们使用64个单元的嵌入维数，6个变换器块的深度，8个变换器头，以及输出MLP头的隐藏层中的128个单元。对于优化器，我们将使用学习率为0.003的Adam(如论文中所示)。我们将对我们的模型进行25个时期的训练，并观察结果。</p><p id="8dc3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用上面的超参数值没有特别的理由。我只是选了一些看起来合理的东西。当然有可能优化这些将导致更高的精度和/或更快的收敛。</p><p id="d29b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦代码运行了25个时期(在一个普通的免费<a class="ae ko" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">谷歌实验室</strong> </a>笔记本电脑上，配有一个特斯拉T4 GPU)，它会产生以下输出:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/9e20c2f682cc032193a93c5cf513e5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*SK0uhW5Pm6Sh8rtqsdGPgA.png"/></div></figure><p id="65b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，98.36%的准确率还不算太差。这比人们期望从全连接网络中得到的要好得多(我在那里得到了大约97.8-97.9%，没有任何技巧)，所以注意力层肯定有好处。当然，正如我上面提到的，视觉转换器并不特别适合这项任务，即使是几层的简单卷积网络也可以达到99%或以上的精度。在优化了超参数之后，这个变压器模型可能会做得更好。</p><p id="c9eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但关键是，它是可行的，而且，正如<a class="ae ko" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">论文</strong> </a>中所描述的，当应用于更大、更复杂的问题时，它可以与最好的卷积模型相媲美。希望这篇简短的教程向读者展示了如何在自己的工作中使用它。</p></div></div>    
</body>
</html>