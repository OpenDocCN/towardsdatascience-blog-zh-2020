<html>
<head>
<title>1. Introduction To Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">1.Apache Spark简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/1-introduction-to-apache-spark-299db7a4b68d?source=collection_archive---------17-----------------------#2020-11-12">https://towardsdatascience.com/1-introduction-to-apache-spark-299db7a4b68d?source=collection_archive---------17-----------------------#2020-11-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="88dc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a>，探索Spark性能优化</h2><div class=""/><div class=""><h2 id="0e57" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">开始探索Spark性能优化新系列的启动帖子</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a236c4434a28eb15ac0098e02deab02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FjiITr5rmU9T4r6Y"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@nasa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> NASA </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ee76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Apache Spark是大数据领域的一个流行框架。由于我有Python和SQL编码的背景，我很快就掌握了Spark的使用方法。然而，由于不了解这些机制，我在开始时经常感到困惑。我以前在单机上运行代码的经验向使用集群的转变，加上处理的数据大小变化从MB向GB(甚至TB)的转变，促使我开始学习Spark。我将在这个系列中讨论我对Spark性能优化的探索，从第一篇介绍文章开始。这篇文章将涉及一些与Spark相关的关键概念、API和工具。</p><p id="5e35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">事不宜迟，我们开始吧！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="d220" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Spark </strong> </a>是一个分布式集群计算软件框架。它提供了简单的API来计算大量数据，而最终用户几乎不需要了解跨机器的任务和资源管理，这些都是由Spark在幕后完成的。</p><h2 id="4a50" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">1.分布式计算</h2><p id="f1d0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">要实现分布式计算，需要在一个机器集群上进行资源和任务管理。资源管理包括为当前任务获取可用的机器，而任务管理包括协调集群中的代码和数据。</p><p id="bbe3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Spark应用程序由驱动程序组成，在集群上执行并行计算。为了启动Spark应用程序，在主机上运行的驱动程序将首先启动一个<code class="fe ni nj nk nl b">SparkContext</code>对象。这个<code class="fe ni nj nk nl b">SparkContext</code>对象将与一个集群管理器通信，这个集群管理器可以是Spark自己的独立集群管理器、Mesos、YARN或Kubernetes，以获取这个应用程序的资源。然后，<code class="fe ni nj nk nl b">SparkContext</code>对象将把应用程序代码和任务发送给工作节点。</p><p id="d68d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于一个应用程序，一个worker节点可以有多个执行器，这取决于该worker节点上可用的CPU数量。在应用程序的计算过程中，每个执行器将数据保存在内存或磁盘存储器中，并运行任务。这样，执行者就相互隔离了，同一个应用程序的任务并行运行。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/afde587281ce3d826aa7b549a74dc53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*EzZs4uEuO30lV51KV07_RA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">火花簇模式概述</a></p></figure><h2 id="ad01" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">2.弹性分布式数据集(RDD)</h2><p id="2f03" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated"><a class="ae lh" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/></a>是Spark中的核心抽象，代表弹性分布式数据集。它能够将大数据分割成适合每台机器的较小数据，因此计算可以在多台机器上并行进行。此外，rdd会自动从节点故障中恢复，以确保存储的弹性。</p><p id="7b63" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> HDFS </strong> </a> (Hadoop分布式文件系统)是我在使用Spark时经常碰到的另一个重要概念。尽管RDD和HDFS都是关于弹性分布式存储的，但它们是为处理不同的问题而设计的。RDD的弹性观点指的是计算失败的自动处理。虽然HDFS是关于存储管理的，但它是为处理存储故障而设计的。</p><h2 id="5e14" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">3.Spark APIs</h2><p id="c64c" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Spark提供了三个API:<a class="ae lh" href="https://databricks.com/glossary/what-is-rdd" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd"/></a><a class="ae lh" href="https://databricks.com/glossary/what-are-dataframes" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">data frames</strong></a><a class="ae lh" href="https://databricks.com/glossary/what-are-datasets" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">Datasets</strong></a>。所有这三个API都确保了分布式、弹性的数据计算，但是它们适用于不同的应用场景。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/a5dd5427de17940e45360037947bc19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Lm6Q8uYLY2o9lVl30Dcuw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://databricks.com/glossary/what-is-rdd" rel="noopener ugc nofollow" target="_blank">三个Spark API</a></p></figure><p id="9420" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RDD是Spark提供的底层API，它支持非结构化或半结构化数据的操作。使用RDD就像告诉Spark如何执行任务，而不是简单地告诉Spark执行什么任务。因此，在这三个API中，RDD提供了最好的编码灵活性和数据控制。然而，与此同时，在没有利用Spark内部优化的情况下，一个好的RDD大师对程序员的经验提出了更高的要求。</p><p id="e358" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了低级的RDD API，Spark还提供高级的DataFrames API。数据框架强调数据结构。因此，DataFrames对有关系数据库经验的程序员来说是友好的。当使用DataFrame时，感觉非常类似于使用Pandas DataFrame或Excel电子表格，但Spark在幕后处理集群计算。</p><p id="56fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们说RDD和数据框架API位于倾斜的两侧，RDD位于灵活的低级控制一侧，数据框架位于简单的高级编码一侧，那么数据集API位于其他两个API的中间。在DataFrames API之上，Datasets API强加了类型安全以避免运行时错误。</p><p id="aba2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RDD和数据集API都需要类型安全，并且只支持Java和Scala。但是DataFrame API支持动态类型语言，比如Python和r。</p><p id="d08a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Jules Damji有一个关于RDD、数据框架和数据集的精彩博客。如果你有兴趣，别忘了去看看。</p><h2 id="4368" class="ml mm it bd mn mo mp dn mq mr ms dp mt lr mu mv mw lv mx my mz lz na nb nc iz bi translated">4.Spark SQL</h2><p id="3290" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated"><a class="ae lh" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Spark SQL </strong> </a>是Spark处理结构化数据的模块。</p><p id="59ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Spark SQL使用两种结构化API，即数据集和数据帧。利用仅在数据集或数据帧中可用的模式信息，Spark SQL代码以声明的方式告诉Spark做什么，而不是在使用低级RDD API时告诉Spark如何做。这样，用Spark SQL编写的代码受益于Spark的catalyst，它优化了性能。因此，使用Spark SQL和结构化API更容易编写高性能代码。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/3db90716bd948754d93b46352af21b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXp6jD-GevTqMoETQZpwcw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://databricks.com/glossary/catalyst-optimizer" rel="noopener ugc nofollow" target="_blank"> Catalyst优化SQL查询性能</a></p></figure><p id="2450" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用Spark SQL和DataFrames API相当于在关系数据库上运行SQL查询。常用的SQL函数，如filter、join、aggregation、window函数等，在<a class="ae lh" href="https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions" rel="noopener ugc nofollow" target="_blank"> Spark SQL </a>中也有。Spark SQL和DataFrames API支持几种编程语言，包括Python、R、Scala和Java。</p><p id="5239" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Spark SQL、<a class="ae lh" href="https://prestodb.io/docs/current/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Presto </strong> </a>和<a class="ae lh" href="https://cwiki.apache.org/confluence/display/Hive" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> Hive </strong> </a>都支持使用SQL语法查询驻留在分布式存储中的大规模数据，但用于不同的场景。</p><p id="9285" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Spark SQL是Spark中的核心模块，而Presto在Hadoop生态中。Spark SQL强调计算，通常用于大规模ETA和流水线。然而，Presto强调查询，更常用于特别分析。Spark SQL和Presto都在内存中计算。当谈到内存短缺时，Spark SQL允许溢出到磁盘中，而Presto将遭受OOM问题。Spark SQL中也考虑了容错，但Presto中没有。</p><p id="66ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Hive是一个数据仓库软件，管理Hadoop生态系统中的大规模结构化数据。Hive查询可以通过Spark或MapReduce来执行。Hive有自己的SQL引擎，名为HiveQL。正如我们上面提到的，Spark SQL是Spark处理结构化数据的模块。类似地，Hive是Hadoop处理结构化数据的模块。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="154d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">总结</strong></p><p id="9ac7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我讨论了Spark的一些基本概念。尽管高级DataFrame API和Spark SQL使得编写高性能代码更加容易，但是理解Spark的工作方式有助于进一步提高性能。在下一篇文章中，我将以YARN为例，讨论如何使用YARN web UI理解Spark资源和任务管理。如果你对本次探索星火性能优化系列感兴趣，敬请关注！</p></div></div>    
</body>
</html>