<html>
<head>
<title>How to enforce the outcome of your ML Classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何执行ML分类器的结果</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2?source=collection_archive---------22-----------------------#2020-10-20">https://towardsdatascience.com/how-to-enforce-the-outcome-of-your-ml-classifiers-b5f6163d68c2?source=collection_archive---------22-----------------------#2020-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7ab3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何校准机器学习分类器的分步指南</h2></div><p id="20e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">根据分类器的输出做出决定可能会很棘手。幸运的是，通过校准工具的使用，预测概率的解释最近变得更容易了。我接触过这个问题，因为我工作的很大一部分要求我不仅要解释模型的输出，还要根据这些结果推荐产品策略。事实上，许多企业依靠预测的概率来做决定(例如，打赌的几率，广告定位…)。如果这也是你的情况，这篇文章描述了一个系统的方法来确保你的分类器与理论，代码和可视化的健康组合校准！</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/866bbaa884101eae1e0c413a342d8a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fic1PqDxjtvdsyqji55EwQ.jpeg"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated"><a class="ae ma" href="https://unsplash.com/@nci?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">国立癌症研究所</a>在<a class="ae ma" href="https://unsplash.com/s/photos/lab?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="72c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于具有工程背景的人来说，术语“校准”可能会让您的系统1 ( <a class="ae ma" href="https://www.scientificamerican.com/article/kahneman-excerpt-thinking-fast-and-slow/" rel="noopener ugc nofollow" target="_blank"> Daniel Kahnema </a> n)兴奋不已，并引发了一个装满测量仪器的实验室的可视化，如图所示。嗯，校准你的ML分类器是非常相似的。<br/> <strong class="kh ir">感兴趣？</strong></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="b696" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在整篇文章中，最基本的是要记住校准的目标是获得“<strong class="kh ir">接近经验概率[即观察到的类频率]的预测概率，正如<a class="ae ma" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf" rel="noopener ugc nofollow" target="_blank">de groot&amp;fien Berg(1983)</a>【1】所定义的那样。</strong></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="b60f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文结束时，目标是让您知道如何做3件事:</p><ol class=""><li id="5c35" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated"><strong class="kh ir">评估</strong>校准不良概率的风险</li><li id="23ce" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kh ir">使用合适的工具调整</strong></li><li id="fe92" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">更加自信地解读它们</li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="2020" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">一、为什么校准很重要</strong></h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi no"><img src="../Images/3302184c61528420f292190fe791776a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4ZCQNKCtTiR9cIz-DlQoA.jpeg"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">米娅似乎知道答案！(照片由<a class="ae ma" href="https://unsplash.com/@camylla93?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Camylla Battani </a>在<a class="ae ma" href="https://unsplash.com/s/photos/question?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure><p id="55ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类可以简单地理解为用一条“线”(2D的平面或&gt; 3D+的超平面)分隔数据点，正如吴恩达在Coursera 上的第一个<a class="ae ma" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习课程中所解释的那样。算法负责寻找最佳“路线”。然而，这些算法采用不同的方法，同时依赖不同的假设。在推理过程中，分类器可以直接预测标签，也可以预测属于每个类别标签的概率。</a></p><p id="f214" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">校准之所以重要，是因为在理想情况下，我们希望能够将这些概率解释为<a class="ae ma" href="https://en.wikipedia.org/wiki/Ex-ante" rel="noopener ugc nofollow" target="_blank">事前概率</a>。</p><p id="bf9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，当概率的分布与数据中观察到的分布不匹配时，如何做到这一点呢？<br/>答:校准。</p><p id="e2a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但具体来说，这对你和你的分类器意味着什么？</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="37d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再来看<strong class="kh ir">逻辑回归</strong>。回想一下，分类器试图最小化损失函数(在这种情况下是交叉熵),以便找到将数据分成两部分(二元分类)的最佳“线”。当损失函数的导数等于0时，可以找到损失函数的最小值。我将省去3-4行数学公式，方便地得出以下等式:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c64f394b9abfaef8fdb2c1a1b088006e.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/1*ko1maFu6Rzqd5CM_J5awWg.gif"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="170d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中p是类概率，y表示观察到的类值。</p><p id="3183" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设数据集是线性可分的，这个等式似乎告诉我们，类概率将与期望值完全对应。因此，<strong class="kh ir">逻辑回归很可能提供校准概率</strong>。这使得它在应用用例中非常有吸引力。</p><p id="0db5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，其他(更复杂的)分类器没有这个特征。支持向量机就是一个很好的例子。事实上，支持向量机通过最大化决策边界周围的边界(即“线”)来找到最优的“线”。它没有输出任何可能性，而是使用了边际的概念(这是斯坦福大学一门课程的一些材料)。</p><p id="93f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一点上，你们中的一些人可能会感到困惑，并在scikit-learn (如果你是python用户)上打开一个新的标签页，查看是否可以从SVM输出概率。让我为你节省一些时间:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nq"><img src="../Images/191216e799cf97552fb13e2f177aa7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgxlsQ3GbVVoXzSi3NMC8Q.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">Scikit-Learn ( <a class="ae ma" href="https://scikit-learn.org/stable/modules/svm.html#scores-probabilities" rel="noopener ugc nofollow" target="_blank">开源库</a>)的文档快照</p></figure><p id="6cd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在线性SVM估计量上sklearn predict_proba方法的输出概率似乎实际上来自对SVM分数的逻辑回归<strong class="kh ir"/>(对描述这种方法的<a class="ae ma" href="https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的作者来说，这种方法通常被称为普拉特标度)。</p><p id="c09b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣吧。看起来scikit-learn已经帮你做了。但是对所有的量词都成立吗？决策树怎么样？<br/>让我再为您节省一些时间:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nr"><img src="../Images/9047a67fb44c672f519eb577a4adf0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOJaZP0NZeGvF1PDD4CRxA.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">Scikit的文档快照——了解DecisionTreeClassifier ( <a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba" rel="noopener ugc nofollow" target="_blank">开源库</a>)</p></figure><p id="bee1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于单个决策树，输出的概率代表一个<strong class="kh ir">频率</strong>(由你的训练集决定，这就是为什么单个决策树很可能过度拟合你的训练数据，也就是高方差)，而不是一个可能性。<br/>让我为您节省更多时间，RandomForestClassifiers也是如此。<br/>事实上，返回的概率是所有树中预测感兴趣类别的树的分数。gradientboosting分类器的类似推理(在scikit-learn github repo 中的<a class="ae ma" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/ensemble/_gb.py#L1200" rel="noopener ugc nofollow" target="_blank"> _stagged_raw_predict函数中描述)</a></p><p id="a795" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:<br/> -没有<em class="ns"> </em>提到scikit-learn上基于树的分类器的校准</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="5fd8" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated">二。评估校准的需要</h1><ul class=""><li id="3285" class="mi mj iq kh b ki nt kl nu ko nv ks nw kw nx la ny mo mp mq bi translated"><strong class="kh ir">一个</strong> <a class="ae ma" href="http://www.bom.gov.au/wmo/lrfvs/reliability.shtml#:~:text=Reliability%20diagrams%20(Hartmann%20et%20al,a%20forecast%20probability%20actually%20occurred." rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">可靠性图</strong> </a>作为一个强大的工具，帮助我们对错误校准的存在及其性质(信心不足或信心过度)进行定性评估。</li></ul><p id="c848" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，可靠性图是感兴趣类别的观察频率作为预测概率的函数的图。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/d6fc71c36dec52cd8e09b99634043941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*ygEroiImJLUAmtUqnvAdAg.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="e7c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图似乎显示了标准GB校准不佳的明显情况。曲线似乎遵循S形趋势。对于此图，我选择显示10个箱，因为我有一个相对较大的数据集(400k+数据点)用于校准。</p><p id="f45f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，我们是否可以用更少数量的箱得到相同的结论🤔？</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8dfd197fa3908f1e6f412e5c6b0da95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Qy3ovMvZMmQTtbRSQcA8Mg.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="7087" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是完全相同的图，但是使用4个箱而不是10个箱。我们仍然可以注意到标准GB的校准不佳。然而，曲线的形状不是很明显。在这种情况下，这不是有害的，因为数据集足够大，我们可以使用保序回归来校准模型。更多关于它的信息。</p><p id="39fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在您训练了您的模型并根据您的测试数据进行了预测之后，现在是时候构建您的可靠性图表了。您可以手动操作:</p><p id="ddd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">步骤1 </em>:选择面元数<br/> <em class="ns">步骤2 </em>:将数据点和预测概率桶入相应的面元。可以通过确保它们都包含相同数量的数据点(统一策略)或者使用分位数遵循更复杂的分割来确定容器的大小。<br/> <em class="ns">步骤3 </em>:计算相应仓中的平均概率和平均频率<br/> <em class="ns">步骤4 </em>:绘制作为平均预测概率的函数的平均观察频率</p><p id="058e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者您可以选择使用scikit-learn上的现成方法:</p><pre class="ll lm ln lo gt ob oc od oe aw of bi"><span id="bc52" class="og mx iq oc b gy oh oi l oj ok">#Path of least resistance: Use Sklearn [2]</span><span id="e65b" class="og mx iq oc b gy ol oi l oj ok">from sklearn.calibration import calibration_curve<br/>import matplotlib.pyplot as plt</span><span id="8623" class="og mx iq oc b gy ol oi l oj ok">observed_pos_frequency, mean_pred_proba  = calibration_curve(y_true, y_proba, n_bins=10, strategy='uniform')<br/>plt.plot(mean_pred_proba, observed_pos_frequency)</span></pre><p id="4d70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ns">提示</em></strong><em class="ns">:</em><br/>——⚠️警告:一个常见的错误是使用平均精度绘制可靠性图表👎而不是平均预测概率👍正如向江也指出的那样[3]<br/>——箱子的宽度不必一致。您可以选择使用遵循分位数分布的分割<br/> -我建议使用相对较高数量的箱(取决于您的数据集大小)，以便更好地可视化较差校准的存在。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><ul class=""><li id="ffce" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la ny mo mp mq bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Brier_score" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">欧石南损失</strong> </a> <strong class="kh ir"> </strong>是校准拟合的定量度量。它被定义为标签和感兴趣的数据集中的数据点的预测概率之间的均方差。差异越小越好。</li></ul><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9f540a6b172ccbec21a901c6ab9f0364.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/1*ZOlEGt6J9N3Qnm2bUrKC8Q.gif"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">n是数据点的数量(图片由作者提供)</p></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi on"><img src="../Images/899e63f2ea9b12d9c0e5d2ca69bad564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*N-WACwN9nyeaDs87z_ROgw.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="ef93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，早期的大幅波动是由于样本量小(方差大)。事实上，越多的数据被用来计算短暂的损失，它就变得越稳定。</p><pre class="ll lm ln lo gt ob oc od oe aw of bi"><span id="b995" class="og mx iq oc b gy oh oi l oj ok">#Path of least resistance: Use Sklearn [4]</span><span id="39e6" class="og mx iq oc b gy ol oi l oj ok">from sklearn.metrics import brier_score_loss</span><span id="5ed3" class="og mx iq oc b gy ol oi l oj ok">brier_loss  = brier_score_loss(y_true, y_proba)</span></pre><p id="c9c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ns">注:</em> <br/> </strong>前面的公式不包括样品重量。<br/>如果您使用类别权重(阳性和阴性类别的数据点比例)，则以下公式更适合计算Brier损失。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0a39e7c8328bae30180d83310ecf4cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/1*wu2sHdN-89L20u3Py-Z-QQ.gif"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="851f" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">三世。如何校准您的模型</strong></h1><p id="0b84" class="pw-post-body-paragraph kf kg iq kh b ki nt jr kk kl nu ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">到目前为止，大家都知道，校准您的模型可以被描述为拟合回归变量的行为。回归器被期望找到将分类器的输出概率映射到观察到的类频率的函数。</p><p id="080b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，可能会出现两种情况:</p><ul class=""><li id="92d4" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la ny mo mp mq bi translated">您已经安装了分类器(方法1和2适用)</li><li id="a49d" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la ny mo mp mq bi translated">您尚未安装分类器(所有方法都适用)</li></ul><ol class=""><li id="7433" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated"><strong class="kh ir"> Platt Scaling </strong>:通过最大似然法(此处有<a class="ae ma" href="https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf" rel="noopener ugc nofollow" target="_blank">的论文</a>)拟合回归变量，找到将模型输出映射成概率的sigmoid函数的参数。</li></ol><pre class="ll lm ln lo gt ob oc od oe aw of bi"><span id="deb9" class="og mx iq oc b gy oh oi l oj ok"># Path of least resistance: Use Sklearn [5]<br/># Case with a classifier already fitted</span><span id="4fde" class="og mx iq oc b gy ol oi l oj ok">from sklearn.calibration import CalibratedClassifierCV<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="4cbe" class="og mx iq oc b gy ol oi l oj ok">rf_est = RandomForestClassifier()<br/>rf_est.fit(X_train, y_train)</span><span id="6c8a" class="og mx iq oc b gy ol oi l oj ok">platt_rf = CalibratedClassifierCV(rf_est, cv="prefit", method=’sigmoid’)<br/>platt_rf = platt_rf.fit(X_test, y_test)<br/>calibrated_yhat = platt_rf.predict_proba(X_test)</span><span id="f2e7" class="og mx iq oc b gy ol oi l oj ok">brier_loss = brier_loss_score(y_test, calibrated_yhat[:,1])</span></pre><p id="91a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">注意:<br/> </strong> - ⚠️当校准来自已经拟合的分类器的概率时，拟合步骤仅应用于用于校准的回归器，而不是底层分类器(如Sklearn校准文件[6]的源代码的第237行所述)<br/> - ⚠️当校准来自已经拟合的分类器的概率时，必须用与用于拟合分类器的数据集不相交的<strong class="kh ir"/>数据集来拟合<strong class="kh ir">回归器</strong>(如指南[7]所述)</p><p id="6281" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2。保序回归</strong>有助于对数据进行单调拟合。请记住，完美校准概率的定义是可靠性图中的对角线。因此，我们知道最好的情况是曲线趋势是单调的。因此，通过设计，保序回归可以在模型输出单调的任何情况下充当强大的平滑器。这里有一个有用的可视化来理解它的机制:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi os"><img src="../Images/6c822875a9ccbd1bec93dfc049d69a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*yC6_u9woUQ_ZtU-40t6snw.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">比较等渗拟合与线性拟合的图表(图片由作者和来源提供<a class="ae ma" href="https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_isotonic_regression.html" rel="noopener ugc nofollow" target="_blank">代码</a>由n .<em class="ot">Varoquaux&amp;a . gram fort根据</em> <a class="ae ma" href="https://en.wikipedia.org/wiki/BSD_licenses#:~:text=The%20BSD%20license%20is%20a,code%20be%20distributed%20at%20all." rel="noopener ugc nofollow" target="_blank"> <em class="ot"> BSD许可</em> </a></p></figure><p id="a6c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于保序回归有趣的事实是没有参数可寻。这是一种利用“智能”平滑方法的非参数方法，如合并相邻违规者算法(PAVA)。事实上，scikit-learn [8]中的保序回归方法使用了PAVA。</p><pre class="ll lm ln lo gt ob oc od oe aw of bi"><span id="2bd7" class="og mx iq oc b gy oh oi l oj ok"># Path of least resistance: Use sklearn [5]<br/># Case with a classifier not yet fitted</span><span id="28bc" class="og mx iq oc b gy ol oi l oj ok">from sklearn.calibration import CalibratedClassifierCV<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="def7" class="og mx iq oc b gy ol oi l oj ok">rf_est = RandomForestClassifier()<br/>isotonic_rf = CalibratedClassifierCV(rf_est, cv=5, method=’isotonic’)<br/>isotonic_rf.fit(X_train, y_train)<br/>calibrated_yhat = isotonic_rf.predict_proba(X_test)</span><span id="cf0d" class="og mx iq oc b gy ol oi l oj ok">brier_loss = brier_loss_score(y_test, calibrated_yhat[:,1])</span></pre><p id="3b1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:<br/>—💢如果某件事好得不像真的，它可能就是真的。事实上，通过设计，保序回归有过度拟合的趋势。为了减少过度拟合，您应该考虑仅对大型数据集使用保序回归。</p><p id="0f81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3。拉普拉斯估计&amp; m估计</strong> : <br/>你也可以选择改变你的基础模型的估计量，比如拉普拉斯估计量或者m估计量。<a class="ae ma" href="https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees" rel="noopener ugc nofollow" target="_blank"> N. Chawla在【9】</a>中描述了RandomForestClassifier的这种变化的一个例子:</p><blockquote class="ou ov ow"><p id="aa39" class="kf kg ns kh b ki kj jr kk kl km ju kn ox kp kq kr oy kt ku kv oz kx ky kz la ij bi translated">“[预测概率]可以是基于叶频率的估计，或者[可以]通过拉普拉斯或m估计来平滑。”</p></blockquote><p id="f8fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，他的工作还没有被实现成一个软件包(据我所知)。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="c713" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">四。框架</strong></h1><p id="63a5" class="pw-post-body-paragraph kf kg iq kh b ki nt jr kk kl nu ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">最后但同样重要的是，这是您可以用于案例的框架:</p><p id="ef1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">第0步:您应该担心校准吗？</em> <br/>您是否在使用逻辑回归之外的分类器？<br/>如果是，继续步骤1。</p><p id="58f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">步骤1:风险评估</em> <br/> 1.1绘制您的可靠性图<br/> 1.2计算Brier损失</p><p id="6359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">第二步:采取行动</em> <br/> 2.1如果可靠性图上的失真看起来像S形，那么拟合一个利用sigmoid函数的回归变量可能是一个好主意。<br/>你猜对了→使用<strong class="kh ir">普拉特缩放</strong> <br/> 2.2否则，在使用<strong class="kh ir">保序回归</strong>之前，你应该验证你正在使用一个大数据集进行校准。</p><p id="3351" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">第三步:</em> <em class="ns">解读</em></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi pa"><img src="../Images/3de7443c4d747013c3774808fa7c7120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jEmA_rcbzcxLs7s5u-8pWw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="93c9" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated">动词 （verb的缩写）摘要</h1><p id="983c" class="pw-post-body-paragraph kf kg iq kh b ki nt jr kk kl nu ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">在本文中，我们看到:</p><ul class=""><li id="8f47" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la ny mo mp mq bi translated">通过设计，非线性和非参数模型很可能呈现出校准不良的概率。</li><li id="bdf8" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la ny mo mp mq bi translated">有可用的工具来评估校准不良概率的情况，例如:<br/> 1。可靠性图<br/> 2。Brier损失</li><li id="6825" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la ny mo mp mq bi translated">可以使用<br/> 1进行校准。普拉特缩放(现成的API可用)<br/> 2。等渗回归(现成的API可用)<br/> 3。自定义评估员</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="0990" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这样结束了！</p><p id="5e09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这个指南对你有用。随着更多的ML模型被部署，我们(创建者)有责任确保它们的结果被正确地解释。鼓励责任感，永远！</p><p id="42f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我很想听听大家对这个话题的看法。不要像陌生人一样，用你的经验/方法发表评论。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4517" class="mw mx iq bd my mz na nb nc nd ne nf ng jw nh jx ni jz nj ka nk kc nl kd nm nn bi translated"><strong class="ak">六。参考文献</strong></h1><p id="c985" class="pw-post-body-paragraph kf kg iq kh b ki nt jr kk kl nu ju kn ko op kq kr ks oq ku kv kw or ky kz la ij bi translated">阅读材料:</p><ul class=""><li id="0e75" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la ny mo mp mq bi translated"><a class="ae ma" href="https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf" rel="noopener ugc nofollow" target="_blank">尼古列斯库-米齐尔&amp;用监督学习预测好的概率卡鲁阿纳(2005) </a> <strong class="kh ir">(必读)</strong></li><li id="a1da" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la ny mo mp mq bi translated"><a class="ae ma" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf" rel="noopener ugc nofollow" target="_blank">比较概率预测:基本二元概念和多元扩展&amp;芬伯格(1982) </a></li><li id="05f9" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la ny mo mp mq bi translated"><a class="ae ma" href="https://www.researchgate.net/publication/228965727_Evaluating_probability_estimates_from_decision_trees" rel="noopener ugc nofollow" target="_blank">n . Chawla(2006年)评估决策树的概率估计值</a></li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="ce9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提及的参考文献:</p><p id="99af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] <a class="ae ma" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a135966.pdf" rel="noopener ugc nofollow" target="_blank">比较概率预测者:基本二元概念和多元扩展由德根&amp;芬伯格(1982)</a><br/>【2】<a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html" rel="noopener ugc nofollow" target="_blank">sk Learn校准曲线法</a><br/>【3】<a class="ae ma" rel="noopener" target="_blank" href="/introduction-to-reliability-diagrams-for-probability-calibration-ed785b3f5d44">简介不确定度校准和可靠性图表</a>由向江<br/>【4】<a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html" rel="noopener ugc nofollow" target="_blank">sk Learn Brier评分法</a><br/>【5】<a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV" rel="noopener ugc nofollow" target="_blank">sk Learn校准法</a><br/>【6】<a class="ae ma" href="https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/calibration.py#L33" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>