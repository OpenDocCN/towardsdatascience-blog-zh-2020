<html>
<head>
<title>Build Better Pipelines With TensorFlow Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow数据集构建更好的管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-better-pipelines-with-tensorflow-dataset-328932b16d56?source=collection_archive---------12-----------------------#2020-11-07">https://towardsdatascience.com/build-better-pipelines-with-tensorflow-dataset-328932b16d56?source=collection_archive---------12-----------------------#2020-11-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9f19" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="ecce" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">简洁高效的数据处理可视化指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b053dffe84d194d4e89cf1b305b9d1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*94yu1e80Siiuwn7H"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lh" href="https://unsplash.com/@romanenko29061983?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> roman pentin </a>拍摄</p></figure><p id="e43c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> D </span>在任何可扩展的、生产质量的ML解决方案中，ata管道虽然不那么迷人，但仍然是基本的构建模块。事实上，绝大多数ML实际上是数据争论——因此，强大的管道是构建强大解决方案的重要因素是有道理的。</p><p id="78a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">聚焦TensorFlow 2，我们有一个奇妙的东西叫做<code class="fe mn mo mp mq b">Dataset</code>对象，内置在库中。使用数据集对象，我们可以用少得多的努力来设计高效的数据管道——结果是一个更干净、更合理和高度优化的管道。</p><p id="d1af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将深入研究数据集对象。包括它们是什么，为什么我们应该使用它们，以及我们如何使用它们。</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="c1ab" class="mv mw it mq b gy mx my l mz na"><strong class="mq jd">What Are Datasets?</strong><br/>  - Why Use Them?</span><span id="7369" class="mv mw it mq b gy nb my l mz na"><strong class="mq jd">Reading Data</strong><br/>  - In-memory<br/>  - From File</span><span id="45eb" class="mv mw it mq b gy nb my l mz na"><strong class="mq jd">Basic Operations</strong><br/>  - Batching and Shuffling<br/>  - Data Transformations with Map</span><span id="8ebc" class="mv mw it mq b gy nb my l mz na"><strong class="mq jd">Feeding Datasets into a Model</strong><br/>  - Multiple Inputs/Outputs<br/>  - Train-Validation Split</span></pre><p id="fb28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您喜欢视频，我也在这里介绍了本文中的所有内容:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="865e" class="nl mw it bd nm nn no np nq nr ns nt nu ki nv kj nw kl nx km ny ko nz kp oa ob bi translated">什么是数据集？</h1><p id="6d82" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">数据集是包含在<code class="fe mn mo mp mq b">tf.data</code> API中的一个对象，它表示一系列元素。每个元素都被赋予了特定的结构，这取决于我们的模型所要求的格式。</p><p id="cbef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe mn mo mp mq b">tf.data</code> API是TensorFlow构建输入数据管道的内置方法——提供用更少代码开发更高效管道的方法。</p><h2 id="6ca6" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">为什么使用它们？</h2><p id="601e" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">数据集抽象使得数据提取、转换和加载非常容易。</p><p id="a642" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，我们可以构建一个管道来迭代地从文件中加载(而不是将所有内容都保存在内存中)，执行一些给定的转换(如文本数据的标记化)，批处理，混洗，并将数据加载到我们的模型中进行训练。</p><p id="b5c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有这些通常只需要几行代码。批处理和混洗我们的数据集只需要<code class="fe mn mo mp mq b">dataset.shuffle(10000).batch(64)</code>。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="901a" class="nl mw it bd nm nn no np nq nr ns nt nu ki nv kj nw kl nx km ny ko nz kp oa ob bi translated">读取数据</h1><p id="186a" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">如前所述，关于如何将数据读入数据集，我们有两种选择，(1)从内存中或(2)从磁盘中。</p><h2 id="c21d" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">在记忆中</h2><p id="252e" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">最简单的方法是使用dataframe、数组、列表或其他数据结构将数据直接读入Python。这种方法绝对可以使用，但是对于较大的数据集来说可能很快变得难以管理。</p><p id="5b6b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，如果使用较小的数据集，您有以下选择:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="9ecc" class="mv mw it mq b gy mx my l mz na">import tensorflow as tf</span><span id="e183" class="mv mw it mq b gy nb my l mz na"># read from lists<br/>inputs = [0, 1, 2, 3]<br/>labels = [1, 0, 1, 0]<br/>dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))</span><span id="85a6" class="mv mw it mq b gy nb my l mz na"># the exact same approach for Numpy arrays<br/>inputs = np.asarray([0, 1, 2, 3])<br/>labels = np.asarray([1, 0, 1, 0])<br/>dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))</span><span id="0717" class="mv mw it mq b gy nb my l mz na"># and from dataframes<br/>df = pd.DataFrame({<br/>    'inputs': [0, 1, 2, 3],<br/>    'labels': [1, 0, 1, 0]<br/>})<br/>dataset = tf.data.Dataset.from_tensor_slices(df)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/eeaadc43b8158d3019d0a1abbe2fe449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Tsj9SEHf-GNXqTk_ReNMg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用上面的list/NumPy方法创建的<strong class="bd os">数据集</strong>对象内的内部记录。</p></figure><p id="5a12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些方法产生如上所示的<code class="fe mn mo mp mq b">dataset</code>结构。显然，从内存对象中读取非常简单，但是对于大型数据集来说并不理想。</p><h2 id="4384" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">从文件</h2><p id="fb17" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">第二种方法是从内存不足的地方读入数据，比如本地硬盘。</p><p id="658e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">采用这种方法时，我们受益于只在内存中加载我们需要的数据，这意味着对于一个20M的样本数据集，我们可以避免一次加载整个数据集，只迭代加载我们需要的数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/5b1c551ff35e0d9698f0819bdd24f2b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ZWhJsKYYaC7uvceBo5yS7Q.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们的数据集(右)被分成几批。在任何时候，只有一个批次被载入内存。然后，活动批次被转换并输入到模型中进行训练。</p></figure><p id="b8aa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个例子中，我们将使用IMDB电影评论数据集。你可以从<a class="ae lh" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data" rel="noopener ugc nofollow" target="_blank"> Kaggle这里</a>下载。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/99d62cb935c433b836a300469ef4ea31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAyjaKS4Bn32yEuyeIubIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自<strong class="bd os"> train.tsv </strong>数据的前四行。</p></figure><p id="19f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的训练数据包含在<code class="fe mn mo mp mq b">train.tsv</code>中，我们将使用它来训练一个情感分类器。为此，我们需要将包含在<code class="fe mn mo mp mq b">Content</code>中的文本作为我们的输入特征，将<code class="fe mn mo mp mq b">Sentiment</code>作为我们的目标标签。我们不需要任何其他列。</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="0ec2" class="mv mw it mq b gy mx my l mz na">dataset = tf.data.experimental.make_csv_dataset(<br/>    "train.tsv",<br/>    batch_size=8,<br/>    field_delim='\t',<br/>    select_columns=['Phrase', 'Sentiment'],<br/>    label_name='Sentiment')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/a0fab40df9e5693e8c6a72703835255c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xKeO8rfMEKGpJlfJiJWWw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd os"> 2 </strong>和<strong class="bd os"> 3 </strong>显示一批八个<strong class="bd os">短语</strong>和<strong class="bd os">情感</strong>样本。编号<strong class="bd os"> 1 </strong>是<strong class="bd os"> PhraseId </strong>，它被添加到<strong class="bd os"> select_columns </strong>中，以表明数据在默认情况下是混洗的。</p></figure><p id="463f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过这种方法，我们可以轻松地将存储在内存中的数据保持在较低水平，并且仍然可以轻松地洗牌和批量处理数据——这是一个非常方便的选择。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="8bd0" class="nl mw it bd nm nn no np nq nr ns nt nu ki nv kj nw kl nx km ny ko nz kp oa ob bi translated">基本操作</h1><p id="a132" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">现在我们已经讲述了读写数据集对象的基础知识；我们可以开始转换加载的数据集。</p><h2 id="5eef" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">批处理和洗牌</h2><p id="49ac" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">当从文件中读取时，不需要这些操作，因为它们是内置的，可以读取像<code class="fe mn mo mp mq b">tf.data.experimental.make_csv_dataset</code>这样的函数——但是当我们从内存中构建数据集时，这种情况不会发生。</p><p id="0a55" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">幸运的是，这仍然非常简单。我们给<code class="fe mn mo mp mq b">shuffle</code>一个大数字(<code class="fe mn mo mp mq b">10000</code>很常见)作为参数，给<code class="fe mn mo mp mq b">batch</code>我们的训练批量:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="53de" class="mv mw it mq b gy mx my l mz na">dataset = dataset.shuffle(10000).batch(64)</span></pre><p id="0fed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">只需这一行代码就可以对我们的数据集进行洗牌和批处理！</p><h2 id="0748" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">使用地图进行数据转换</h2><p id="ef8d" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">我们可以使用<code class="fe mn mo mp mq b">map</code>函数对xour数据集中的每个样本执行操作。例如，为了预测序列中的下一个时间步，我们可能需要训练输入数据，输入数据由时间步<em class="ow"> n </em>到<em class="ow"> n+8 </em>组成，输出数据由时间步<em class="ow"> n+1 </em>到<em class="ow"> n+9 </em>组成。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/727427f9f41df28b2d3007d1f26ede11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4l0qAIvXa6N_lXDArVbyw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最初，我们的数据由包含代表10个时间步长的10个元素的记录组成。在基于时间的预测中，我们通常将这些序列分成输入(X)和目标(y)数据，如图所示。我们可以使用<strong class="bd os"> map </strong>方法来做到这一点。</p></figure><p id="8fae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最初，我们的数据集可能由许多包含10个时间步序列的样本组成。为了训练我们的模型，我们需要将这些10长的序列分成两组9长的序列，一组是输入数据，另一组是我们的目标数据。</p><p id="cb07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了执行这个操作，我们使用了<code class="fe mn mo mp mq b">map</code>方法，就像这样:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="4238" class="mv mw it mq b gy mx my l mz na">def Xy_split(x):<br/>    X = x[:-1]  # [0, 1, 2, 3, 4, 5, 6, 7, 8]<br/>    y = x[1:]   # [1, 2, 3, 4, 5, 6, 7, 8, 9]<br/>    return X, y</span><span id="94d2" class="mv mw it mq b gy nb my l mz na">dataset = dataset.map(Xy_split)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/172da39ea1d2484ee580a8046110eca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9BfXD9dsfQ46U83noIzsw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们可以根据需要使用<strong class="bd os"> map </strong>方法来拆分或重新格式化我们的数据。</p></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="ca86" class="nl mw it bd nm nn no np nq nr ns nt nu ki nv kj nw kl nx km ny ko nz kp oa ob bi translated">将数据集输入模型</h1><p id="7a0b" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">当把我们的数据集输入到训练模型中时，默认的格式是<code class="fe mn mo mp mq b">(input, output)</code>。这意味着数据集中包含的每个记录/批次都应该包含输入张量和标签/输出张量。</p><p id="5702" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这种格式，我们可以将数据集对象传递给我们的训练方法，如下所示:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="4009" class="mv mw it mq b gy mx my l mz na">model.fit(dataset, epochs=2)</span></pre><h2 id="3c95" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">多输入/输出</h2><p id="62a0" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">单一输入/输出格式并不总是我们需要使用的格式。对于任何重组，我们都可以再次依赖<code class="fe mn mo mp mq b">map</code>方法。</p><p id="16a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以通过将数据集映射到字典格式来映射多个输入和输出，在字典格式中，我们的输入名称(键)指向我们将使用的一组值(值)。</p><p id="0695" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们将输入/输出层名称匹配到输入/输出数据集字典键，我们可以像往常一样将<code class="fe mn mo mp mq b">dataset</code>传递给我们的模型！</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="3c01" class="mv mw it mq b gy mx my l mz na"><em class="ow"># current dataset format is (inputs1, inputs2, labels)</em></span><span id="e52a" class="mv mw it mq b gy nb my l mz na">def map_func(x):<br/>    return {<br/>        <strong class="mq jd">'inputs1'</strong>: x[0],<br/>        <strong class="mq jd">'inputs2'</strong>: x[1]<br/>    }, x[2]</span><span id="d810" class="mv mw it mq b gy nb my l mz na">dataset = dataset.map(map_func)</span><span id="6bb7" class="mv mw it mq b gy nb my l mz na">input_layer_1 = tf.keras.layers.Input(shape=(100,), name=<strong class="mq jd">'inputs1'</strong>)<br/>input_layer_2 = tf.keras.layers.Input(shape=(100,), name=<strong class="mq jd">'inputs2'</strong>)</span></pre><p id="1866" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个常见的用例是为transformer模型创建两个输入层。许多这些模型需要一个输入ID张量和相应的注意屏蔽张量，我们可以使用相同的逻辑:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="10df" class="mv mw it mq b gy mx my l mz na">def transformer_fmt(x):<br/>    return {<br/>        <strong class="mq jd">'input_ids'</strong>: x[0],<br/>        <strong class="mq jd">'attention_mask'</strong>: x[1],<br/>    }, x[2]  <em class="ow"># x[2] are the output labels</em></span><span id="7eb6" class="mv mw it mq b gy nb my l mz na">dataset = dataset.map(transformer_fmt)</span><span id="5a44" class="mv mw it mq b gy nb my l mz na">input_ids = tf.keras.layers.Input(shape=(100,), name=<strong class="mq jd">'input_ids'</strong>)<br/>mask = tf.keras.layers.Input(shape=(100,), name=<strong class="mq jd">'attention_mask'</strong>)</span><span id="3bee" class="mv mw it mq b gy nb my l mz na">...</span><span id="09a7" class="mv mw it mq b gy nb my l mz na">model.fit(dataset, epochs=2)</span></pre><h2 id="b6e3" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated">训练-验证分割</h2><p id="172d" class="pw-post-body-paragraph li lj it lk b ll oc kd ln lo od kg lq lr oe lt lu lv of lx ly lz og mb mc md im bi translated">这里的另一个要点是训练集和验证集(测试集)的分离。在数据集中实现拆分的最简单方法是使用<code class="fe mn mo mp mq b">take</code>和<code class="fe mn mo mp mq b">split</code>方法。</p><p id="6d8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了获取给定数量的记录，我们使用<code class="fe mn mo mp mq b">take</code>——就像这样:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="ef59" class="mv mw it mq b gy mx my l mz na">first5 = dataset.take(5)</span></pre><p id="fd9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们可以使用<code class="fe mn mo mp mq b">skip</code>跳过给定数量的记录，返回以下所有记录——如下所示:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="b964" class="mv mw it mq b gy mx my l mz na">miss5 = dataset.skip(5)</span></pre><p id="d509" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结合这两种方法，我们可以在数据中创建一个分割。例如，对于70–30%的培训验证分割，我们会:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="e02f" class="mv mw it mq b gy mx my l mz na">train = dataset.take(round(length*0.7))<br/>val = dataset.skip(round(length*0.7))</span></pre><p id="367b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">并创建另一个分割来添加测试集。</p><p id="9b0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当我们将训练集和验证集输入到我们的训练模型中时，我们是这样做的:</p><pre class="ks kt ku kv gt mr mq ms mt aw mu bi"><span id="5865" class="mv mw it mq b gy mx my l mz na">history = model.fit(<br/>    train,<br/>    validation_data=val,<br/>    epochs=2)</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="7fe8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经介绍了如何使用数据集对象在TF2构建更干净、更高效的数据输入管道！包括:</p><ul class=""><li id="6678" class="oy oz it lk b ll lm lo lp lr pa lv pb lz pc md pd pe pf pg bi translated">它们是什么以及我们为什么要使用它们</li><li id="3695" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">将内存中的数据加载到数据集</li><li id="8164" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">从文件读入数据集</li><li id="ddce" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">批处理和洗牌</li><li id="c8f6" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">使用<code class="fe mn mo mp mq b">map</code>转换数据集</li><li id="16db" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">将数据集输入模型</li><li id="1988" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">如何处理多个输入/输出层</li><li id="e71c" class="oy oz it lk b ll ph lo pi lr pj lv pk lz pl md pd pe pf pg bi translated">列车价值测试拆分</li></ul><p id="8789" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是我们需要知道的大部分ML输入管道。由于清晰高效的<code class="fe mn mo mp mq b">tf.data</code> API，我们所做的一切都非常容易执行，而且快如闪电。</p><p id="475a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你喜欢这篇文章！如果你有任何问题或想法，请通过<a class="ae lh" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank">推特</a>或在下面的评论中告诉我。</p><p id="391a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢阅读！</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="13dd" class="mv mw it bd nm oh oi dn nq oj ok dp nu lr ol om nw lv on oo ny lz op oq oa iz bi translated"><a class="ae lh" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》NLP课程70%的折扣</a></h2></div></div>    
</body>
</html>