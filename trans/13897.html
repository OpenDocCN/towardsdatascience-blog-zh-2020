<html>
<head>
<title>Neural Networks with Memory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有记忆的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-with-memory-27528a242b78?source=collection_archive---------19-----------------------#2020-09-24">https://towardsdatascience.com/neural-networks-with-memory-27528a242b78?source=collection_archive---------19-----------------------#2020-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/015e7daaebcb0f4a7b2ec1932886aed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s4l2rX9CIsn90dEk"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@infralist?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Infralist.com</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="a5a9" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="c236" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">了解LSTM RNN不到5分钟</h2></div><p id="ae89" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们总是听说<strong class="lg jq">神经网络</strong> ( <strong class="lg jq"> NNs </strong>)是受生物神经网络的启发。这个巨大的展示是以一种奇妙的方式完成的。</p><p id="f39c" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">图1显示了单个神经元的解剖结构。中心部分称为细胞核所在的细胞体。有各种导线将刺激传递到细胞体，少数导线将输出发送到其他神经元。树突的厚度暗示了刺激的重量/偏向/力量。许多具有不同细胞体的神经元堆叠在一起，形成了生物神经网络。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/18660fd5b7e3af316df61317a135aa34.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*z3aVSuzJM_2IoGVQljhGlw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:单个神经元的解剖(<a class="ae jd" href="https://en.wikipedia.org/wiki/Unipolar_neuron#:~:text=A%20unipolar%20neuron%20is%20a,form%20dendritic%20and%20axonal%20processes." rel="noopener ugc nofollow" target="_blank">来源</a>，作者编辑)</p></figure><p id="8469" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在神经网络中也实现了同样的结构。输入通过带加权边的激活函数传递。生成的输出可以传递给另一个激活函数。许多激活功能可以堆叠起来，其中的每一个都被称为一个<strong class="lg jq">层</strong>。在一层中，我们可以有多个神经元。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/bbf25af96c783a0a8a4ebe92ba62ae74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*rKhR9nGR6KftxoHq23NutA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:单神经元神经网络(图片由作者创建)</p></figure><p id="0970" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些激活函数可以像sigmoid函数一样简单</p><h2 id="10c9" class="mg mh jg bd mi mj mk dn ml mm mn dp mo ln mp mq mr lr ms mt mu lv mv mw mx jm bi translated">神经网络相对于传统机器学习算法的优势</h2><ul class=""><li id="9b46" class="my mz jg lg b lh na lk nb ln nc lr nd lv ne lz nf ng nh ni bi translated">可以处理各种类型和大小的数据</li><li id="60f1" class="my mz jg lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">可以轻松配置多种功能</li><li id="7538" class="my mz jg lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">可以有效地处理非线性数据</li></ul><h1 id="6fc9" class="no mh jg bd mi np nq nr ml ns nt nu mo kv nv kw mr ky nw kz mu lb nx lc mx ny bi translated">具有记忆的神经网络</h1><p id="e091" class="pw-post-body-paragraph le lf jg lg b lh na kq lj lk nb kt lm ln nz lp lq lr oa lt lu lv ob lx ly lz ij bi translated">神经网络和生物神经网络的主要区别在于记忆。虽然人脑和神经网络都有能力从可用的记忆中读取和写入，但大脑也可以创建/存储记忆。<a class="ae jd" href="https://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">研究人员</a>发现这一关键差异是当今人工智能系统达到人类智能水平的主要障碍。</p><p id="7240" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">DeepMind的研究人员旨在通过构建一个神经网络并将其连接到外部存储器来构建一台可区分的计算机。神经网络将充当带有存储器的CPU。这种可微分计算机旨在从输入和输出数据中学习程序(算法)。</p><p id="fc67" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当数据量巨大时，使用神经网络。例如，文本数据具有大量的维度，或者图像数据被分割成大量的像素。</p><h1 id="2335" class="no mh jg bd mi np nq nr ml ns nt nu mo kv nv kw mr ky nw kz mu lb nx lc mx ny bi translated">递归神经网络</h1><p id="0640" class="pw-post-body-paragraph le lf jg lg b lh na kq lj lk nb kt lm ln nz lp lq lr oa lt lu lv ob lx ly lz ij bi translated">一部电影由一系列场景组成。当我们观看一个特定的场景时，我们不会试图孤立地理解它，而是联系以前的场景来理解。以类似的方式，机器学习模型必须通过利用已经学习的文本来理解文本，就像人类神经网络一样。</p><p id="c068" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在传统的机器学习模型中，我们无法存储模型的先前阶段。然而，递归神经网络(通常称为RNN)可以为我们做到这一点。下面我们来仔细看看RNNs。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2421749fbb00038f572720b29f659cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*gf2jUGt1cKecrP2DUaVTmg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:基本RNN的工作原理(图片由作者提供)</p></figure><p id="494b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">RNN有一个重复模块，它接收前一级的输入，并将其输出作为下一级的输入。然而，在RNNs中，我们只能保留最近阶段的信息。这就是LSTM的问题所在。</p><h1 id="33d5" class="no mh jg bd mi np nq nr ml ns nt nu mo kv nv kw mr ky nw kz mu lb nx lc mx ny bi translated">长短期记忆网络</h1><p id="4b87" class="pw-post-body-paragraph le lf jg lg b lh na kq lj lk nb kt lm ln nz lp lq lr oa lt lu lv ob lx ly lz ij bi translated">为了了解长期依赖关系，我们的网络需要记忆能力。LSTMs是可以做到这一点的rnn的特例。它们具有与RNNs相同的链状结构，但具有不同的重复模块结构。</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/a5d4b878847cd50dad56d37af6fb6c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2jae9lGhq1JkqpXOay4OuQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4:LSTM的工作(图片由作者提供)</p></figure><p id="6cec" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">LSTM在序列到序列建模任务中有广泛的应用，如语音识别、文本摘要、视频分类等。</p><p id="d5e1" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">要想快速了解这些网络在现实生活中的应用，请阅读下面的文章。</p><div class="ip iq gp gr ir oe"><a rel="noopener follow" target="_blank" href="/spam-detection-in-emails-de0398ea3b48"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jq gy z fp oj fr fs ok fu fw jp bi translated">检测电子邮件中的垃圾邮件</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">应用自然语言处理和深度学习进行垃圾邮件检测</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="oo l op oq or on os ix oe"/></div></div></a></div><p id="ada7" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">垃圾邮件检测模型可以通过将文本数据转换成向量、创建LSTM模型并用向量拟合该模型来实现。</p><h1 id="c262" class="no mh jg bd mi np nq nr ml ns nt nu mo kv nv kw mr ky nw kz mu lb nx lc mx ny bi translated">履行</h1><p id="250d" class="pw-post-body-paragraph le lf jg lg b lh na kq lj lk nb kt lm ln nz lp lq lr oa lt lu lv ob lx ly lz ij bi translated">为了实现这些完整的神经网络，Keras和TensorFlow将其简化。</p><p id="5dd7" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们正在实现一个带有ReLU激活功能的双向LSTM。</p><pre class="mb mc md me gt ot ou ov ow aw ox bi"><span id="6434" class="mg mh jg ou b gy oy oz l pa pb">#Importing necessary libraries<br/>import tensorflow as tf<br/>from keras.layers import LSTM, Activation, Bidirectional</span><span id="2f1a" class="mg mh jg ou b gy pc oz l pa pb">#Addding Bi-directional LSTM<br/>model.add(Bidirectional(tf.keras.layers.LSTM(64)))</span><span id="5a52" class="mg mh jg ou b gy pc oz l pa pb">#Relu allows converging quickly and allows backpropagation<br/>model.add(Dense(16, activation='relu'))</span><span id="14e7" class="mg mh jg ou b gy pc oz l pa pb">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="4ab8" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">创建每一层只是一行代码。</p><h1 id="b6ba" class="no mh jg bd mi np nq nr ml ns nt nu mo kv nv kw mr ky nw kz mu lb nx lc mx ny bi translated">摘要</h1><p id="3673" class="pw-post-body-paragraph le lf jg lg b lh na kq lj lk nb kt lm ln nz lp lq lr oa lt lu lv ob lx ly lz ij bi translated">在这篇文章中，我们学习了神经网络如何与生物神经网络相联系，以及具有记忆的神经网络的工作原理(即RNN，LSTM)。</p><p id="7d0d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">谢谢你的阅读。我也将在未来写更多初学者友好的帖子。请在<a class="ae jd" href="https://medium.com/@ramyavidiyala" rel="noopener">媒体</a>上关注我，以便了解他们。我欢迎反馈，可以通过Twitter <a class="ae jd" href="https://twitter.com/ramya_vidiyala" rel="noopener ugc nofollow" target="_blank"> ramya_vidiyala </a>和LinkedIn <a class="ae jd" href="https://www.linkedin.com/in/ramya-vidiyala-308ba6139/" rel="noopener ugc nofollow" target="_blank"> RamyaVidiyala </a>联系我。快乐学习！</p></div></div>    
</body>
</html>