<html>
<head>
<title>Deep learning pipeline for Natural Language Processing (NLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理(NLP)的深度学习管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb?source=collection_archive---------10-----------------------#2020-11-11">https://towardsdatascience.com/deep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb?source=collection_archive---------10-----------------------#2020-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dca1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">NLP、无监督机器学习和深度学习概念在无标签文本数据上的实际实现。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fb3af188cb7f5b4af1e78ed59ade1b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jMim6tBDb8F3_ciL"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> h heyerlein </a>拍摄</p></figure><p id="08ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将探索自然语言处理(NLP)的基础知识，并演示如何实现一个管道，该管道将传统的无监督学习算法与深度学习算法相结合，以训练无标签的大型文本数据。因此，主要目标是演示如何建立管道，以促进原始文本数据的收集和创建，预处理和分类未标记的文本数据，最终在Keras中训练和评估深度学习模型。</p><p id="9c00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读完本教程后，您将能够执行以下操作:</p><ol class=""><li id="732a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如何通过Twitter API和Tweepy Python包从Twitter收集数据</li><li id="c852" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何用pandas高效地读取和清理大型文本数据集</li><li id="a589" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何使用基本的自然语言处理技术预处理文本数据并生成特征</li><li id="da76" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何对未标记的文本数据进行分类</li><li id="dfc0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何在Keras中训练、编译、拟合和评估深度学习模型</li></ol><p id="1505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的GitHub <a class="ae kv" href="https://github.com/bauyrjanj/NLP-TwitterData" rel="noopener ugc nofollow" target="_blank">中找到我的带有Python代码的Jupyter笔记本。</a></p><p id="0c61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷起袖子，我们有很多工作要做，让我们开始吧…..</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="f372" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">数据</h1><p id="78d8" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在做这个项目的时候，美国2020年大选即将到来，对与即将到来的选举相关的推文进行情感分析，以了解在选举日之前大约两周Twitter世界中正在讨论的观点和话题是有意义的。Twitter是一个未经过滤的意见的伟大来源，而不是我们从主要媒体渠道看到的典型的过滤新闻。因此，我们将通过使用Twitter API和python包Tweepy从Twitter收集tweets来构建我们自己的数据集。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="6bc1" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">步骤1:数据收集</h1><h2 id="5e81" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">先决条件</h2><p id="542c" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在开始使用来自Twitter的流数据之前，您必须具备以下条件:</p><ol class=""><li id="b18f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">Twitter帐户和Twitter API消费者密钥(访问令牌密钥、访问令牌秘密密钥、消费者密钥和消费者秘密密钥)</li><li id="89e8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">安装在您的Jupyter笔记本中的Tweepy软件包</li></ol><p id="0405" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设置Twitter帐户和检索您的Twitter API消费者密钥超出了本文的范围。如果你在这些方面需要帮助，看看这个<a class="ae kv" href="https://medium.com/@divyeshardeshana/create-twitter-developer-account-app-4ac55e945bf4" rel="noopener">帖子</a>。</p><p id="bc53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tweepy可以通过Jupyter笔记本中的pip install进行安装，下面一行代码就可以完成。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="1972" class="nk mo iq nx b gy ob oc l od oe"># Install Tweepy<br/>!pip install tweepy</span></pre><p id="094f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装完成后，继续将软件包导入您的笔记本电脑。</p><h2 id="c3bc" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">1.1设置数据流管道</h2><p id="ab20" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在这一节中，我将向您展示如何使用Twitter API、Tweepy和一个自定义函数来设置您的数据流管道。我们可以通过三个步骤实现这一目标:</p><ol class=""><li id="effc" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">设置您的Twitter API消费者密钥</li><li id="cf48" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">设置Twitter API授权处理程序</li><li id="b50a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">编写一个自定义函数来监听和流式传输实时推文</li></ol><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="f855" class="nk mo iq nx b gy ob oc l od oe"># Twitter API consumer keys<br/>access_token = "  insert your key here  "<br/>access_token_secret = "  insert your key here  "<br/>consumer_key = "  insert your key here  "<br/>consumer_secret = "  insert your key here  "</span><span id="07e5" class="nk mo iq nx b gy of oc l od oe"># Twitter API authorization<br/>auth = tweepy.OAuthHandler(consumer_key, consumer_secret)<br/>auth.set_access_token(access_token, access_token_secret)</span><span id="ff54" class="nk mo iq nx b gy of oc l od oe"># Custom function that streams data from Twitter (20,000 tweets at most per instance)<br/>class MyStreamListener(tweepy.StreamListener):<br/>    """Function to listen and stream Twitter data"""<br/>    def __init__(self, api=None):<br/>        super(MyStreamListener, self).__init__()<br/>        self.num_tweets = 0<br/>        self.file = open("tweets.txt", "w")</span><span id="de9b" class="nk mo iq nx b gy of oc l od oe">def on_status(self, status):<br/>        tweet = status._json<br/>        self.file.write( json.dumps(tweet) + '\n' )<br/>        self.num_tweets += 1<br/>        if self.num_tweets &lt; 20000: <br/>            return True<br/>        else:<br/>            return False<br/>        self.file.close()</span><span id="9a84" class="nk mo iq nx b gy of oc l od oe">def on_error(self, status):<br/>        print(status)</span></pre><h2 id="8361" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">1.2开始直播推文</h2><p id="c558" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">既然环境已经设置好了，您就可以开始从Twitter流式传输实时推文了。在此之前，确定一些你想用来收集你感兴趣的相关推文的关键词。由于我将发布与美国大选相关的推文，我选择了一些相关的关键词，如“美国大选”、“特朗普”、“拜登”等。</p><p id="e5f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的目标是收集至少400，000条推文，使其成为足够大的文本数据，一次性收集所有这些数据的计算量很大。因此，我将建立一个管道，以高效地传输数据。请注意上面的自定义函数，它将在每个块中最多监听和传输20，000条tweets。因此，为了收集超过40万条推文，我们需要运行至少20个区块。</p><p id="397b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是代码如何寻找用于监听和流式传输实时推文到熊猫数据帧的块:</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="62a2" class="nk mo iq nx b gy ob oc l od oe"># Listen and stream live tweets<br/>listener = MyStreamListener()<br/>stream = tweepy.Stream(auth, listener)<br/>stream.filter(track = ['US Election', 'election', 'trump', 'Mike Pence', 'biden', 'Kamala Harris', 'Donald Trump', 'Joe Biden'])</span><span id="557f" class="nk mo iq nx b gy of oc l od oe"># Read the tweets into a list<br/>tweets_data_path = 'tweets.txt'<br/>tweets_data=[]<br/>tweets_file_1 = open(tweets_data_path, 'r')</span><span id="d3e8" class="nk mo iq nx b gy of oc l od oe"># Read in tweets and store in list: tweets_data<br/>for line in tweets_file_1:<br/>    tweet = json.loads(line)<br/>    tweets_data.append(tweet)</span><span id="dfea" class="nk mo iq nx b gy of oc l od oe"># Close connection to file<br/>tweets_file_1.close()</span><span id="8f87" class="nk mo iq nx b gy of oc l od oe"># Print the keys of the first tweet dict<br/>print(tweets_data[0].keys())</span><span id="fb21" class="nk mo iq nx b gy of oc l od oe"># Read the data into a pandas DataFrame<br/>names = tweets_data[0].keys()<br/>df1 = pd.DataFrame(tweets_data, columns= names)</span></pre><p id="73c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，为了收集400，000条推文，您必须运行上述代码至少20次，并将收集的推文保存在单独的pandas dataframe中，该dataframe将在以后连接起来，以将所有推文整合到单个数据集。</p><h2 id="aa64" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">1.3将所有数据块组合成一个数据集</h2><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="705f" class="nk mo iq nx b gy ob oc l od oe"># Concatenate dataframes into a single pandas dataframe<br/>list_of_dataChunks = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20]<br/>df = pd.concat(list_of_dataChunks, ignore_index=True)</span><span id="5d5a" class="nk mo iq nx b gy of oc l od oe"># Export the dataset into a CSV file<br/>df.to_csv('tweets.csv', index=False)</span></pre><p id="fe46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您已经将组合数据集导出到CSV文件中，您可以在接下来的数据清理和可视化步骤中使用它。</p><p id="d256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经捐赠了我创建的数据集，并在<a class="ae kv" href="https://www.kaggle.com/bauyrjanj/2020-us-election-tweets-unlabeled" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>中公开。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="bac9" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">第二步:数据争论</h1><p id="8ed9" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在本节中，我们将清理刚刚收集的数据。在可视化数据之前，必须清理数据集并将其转换为可以高效可视化的格式。给定具有440，000行的数据集，必须找到一种有效的方法来读取和清理它。为此，pandas chunksize属性可用于将CSV文件中的数据以块的形式读入pandas数据帧。此外，我们可以指定感兴趣的列的名称，而不是读取包含所有列的数据集。使用chunksize和更少量的感兴趣的列，大型数据集可以非常高效和快速地读入数据帧，而不需要其他替代方案，例如在集群上使用PySpark的分布式计算。</p><p id="fccd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了将数据集转换成可视化所需的形状，将应用以下基本NLP技术:</p><ol class=""><li id="ca6d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">提取仅有的英语推特</li><li id="cac1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除重复项(如果有)</li><li id="70ba" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除丢失的值(如果有)</li><li id="8b97" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">标记化(将推文分成单个单词)</li><li id="1d65" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将单词转换成小写</li><li id="6180" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除标点符号</li><li id="0cdf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除停用词</li><li id="e629" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">删除网址,“twitter”和其他缩写词</li></ol><p id="2784" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将遵循以下方法来实现上述步骤:</p><ol class=""><li id="7f77" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">编写一个自定义函数来标记推文</li><li id="a3ff" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">编写另一个自定义函数，对数据应用上述所有清理步骤。</li><li id="44b9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">最后，读取数据块，并在读取数据块时，通过自定义函数将这些争论步骤应用于每个数据块。</li></ol><p id="e002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看所有这些都在起作用…</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="e81a" class="nk mo iq nx b gy ob oc l od oe"># Function to tokenize the tweets<br/>def custom_tokenize(text):<br/>    """Function that tokenizes text"""<br/>    from nltk.tokenize import word_tokenize<br/>    if not text:<br/>        print('The text to be tokenized is a None type. Defaulting to blank string.')<br/>        text = ''<br/>    return word_tokenize(text)</span><span id="d6e5" class="nk mo iq nx b gy of oc l od oe"># Function that applies the cleaning steps<br/>def clean_up(data):<br/>    """Function that cleans up the data into a shape that can be further used for modeling"""<br/>    english = data[data['lang']=='en'] # extract only tweets in english language<br/>    english.drop_duplicates() # drop duplicate tweets<br/>    english['text'].dropna(inplace=True) # drop any rows with missing tweets<br/>    tokenized = english['text'].apply(custom_tokenize) # Tokenize tweets<br/>    lower_tokens = tokenized.apply(lambda x: [t.lower() for t in x]) # Convert tokens into lower case<br/>    alpha_only = lower_tokens.apply(lambda x: [t for t in x if t.isalpha()]) # Remove punctuations<br/>    no_stops = alpha_only.apply(lambda x: [t for t in x if t not in stopwords.words('english')]) # remove stop words<br/>    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='rt']) # remove acronym "rt"<br/>    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='https']) # remove acronym "https"<br/>    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='twitter']) # remove the word "twitter"<br/>    no_stops.apply(lambda x: [x.remove(t) for t in x if t=='retweet']) # remove the word "retweet"<br/>    return no_stops</span><span id="2b3d" class="nk mo iq nx b gy of oc l od oe"># Read and clean the data<br/>warnings.filterwarnings("ignore")<br/>use_cols = ['text', 'lang'] # specify the columns<br/>path = 'tweets.csv' # path to the raw dataset<br/>data_iterator = pd.read_csv(path, usecols=use_cols, chunksize=50000)<br/>chunk_list = []<br/>for data_chunk in data_iterator:<br/>    filtered_chunk = clean_up(data_chunk)<br/>    chunk_list.append(filtered_chunk)<br/>tidy_data = pd.concat(chunk_list)</span></pre><p id="7879" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，块大小是50，000，这就是熊猫如何读取每个块中的50，000条推文，并在读取下一批推文之前对它们应用清理步骤，等等。</p><p id="d6ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在此过程之后，数据集将变得干净，并为可视化做好准备。为了避免每次打开笔记本时执行数据争论步骤，您可以简单地将整齐的数据导出到一个外部文件中，以便将来使用。对于大型数据集，将其导出到JSON文件比导出到CSV文件更有效。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="baef" class="nk mo iq nx b gy ob oc l od oe"># Explort the tidy data to json file for ease of use in the next steps<br/>tidy_data.to_json('tidy_tweets.json', orient='table')</span></pre><p id="fb03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些整洁的数据看起来是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/830c29d4ed1be6c5c99f4104da4e509b.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*pjGtTOhcy9mMngQjBsQDjA.png"/></div></div></figure><h1 id="66aa" class="mn mo iq bd mp mq oh ms mt mu oi mw mx jw oj jx mz jz ok ka nb kc ol kd nd ne bi translated">步骤3:探索性数据分析(可视化)</h1><p id="df63" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">既然数据是干净的，让我们可视化并理解我们的数据的本质。我们可以关注的几个明显的事情如下:</p><ol class=""><li id="a9f4" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">每条推文的字数</li><li id="cb73" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">一条推文中单词的平均长度</li><li id="0229" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">Unigram</li><li id="673e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">二元模型</li><li id="7226" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">三元模型</li><li id="1227" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">Wordcloud</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e92e53ac43d3440dd6e97110baaa53f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*oB1VlyDm_gaAUIoUYkdoMg.png"/></div></figure><p id="b55b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎每条推文的字数在1到19个词之间，平均在10到12个词之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/06777f86741d3615370829876c206e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Wi7rdSa6ff7IzXCzZLWdMw.png"/></div></div></figure><p id="1d57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推特中一个单词的平均字符数似乎在3到14个字符之间，平均出现在5到7个字符之间。人们可能会在Twitter设定的280个字符的限制内，选择简短的词语来以最佳方式表达自己的观点。</p><h2 id="6d18" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">Unigram</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/81bceca97fa9524d8b40e02c828f8f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6Lco1Y29HoOE2NWTTo6pw.png"/></div></div></figure><p id="5bfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如所料，“特朗普”和“拜登”这两个词主导了10月15日至10月16日期间发布的2020年美国大选相关推文。</p><h2 id="9290" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">二元模型(最常出现的一对连续单词)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/0cd4de236cf36783983ab399f5cebffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUMOSdrZcQfIZWVT6tIyYg.png"/></div></div></figure><h2 id="a10e" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">三元模型(三个单词的最常见序列)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/6987d8010088418bf89efbd272e1c1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dMi3vn0fGUu_X6DINMaXA.png"/></div></div></figure><h2 id="ba7c" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">Wordcloud</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/1744aff5543c19417058a522f8da54a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qD_7ixLQFYq-XijZNJGigQ.png"/></div></div></figure><p id="faa4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过可视化数据，注意单词没有被词条化。词汇化是将单词转换成基本形式或词典形式的过程。这是NLP和机器学习中常用的技术。因此，在下一步中，我们将使用以下代码对令牌进行符号化。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="62bf" class="nk mo iq nx b gy ob oc l od oe"># Convert tokens into format required for lemmatization<br/>from nltk.corpus import wordnet<br/>def get_wordnet_pos(word):<br/>    """Map POS tag to first character lemmatize() accepts"""<br/>    tag = nltk.pos_tag([word])[0][1][0].upper()<br/>    tag_dict = {"J": wordnet.ADJ,<br/>                "N": wordnet.NOUN,<br/>                "V": wordnet.VERB,<br/>                "R": wordnet.ADV}</span><span id="7e6f" class="nk mo iq nx b gy of oc l od oe">return tag_dict.get(tag, wordnet.NOUN)</span><span id="5874" class="nk mo iq nx b gy of oc l od oe"># Lemmatize tokens<br/>lemmatizer = WordNetLemmatizer()<br/>tidy_tweets['lemmatized'] = tidy_tweets['text'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x])</span><span id="e9b1" class="nk mo iq nx b gy of oc l od oe"># Convert the lemmatized words back to the text format<br/>tidy_tweets['tokens_back_to_text'] = [' '.join(map(str, l)) for l in tidy_tweets['lemmatized']]</span></pre><p id="c59c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将符号化的标记保存到另一个JSON文件中，以便在管道的下一步中使用。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="8538" class="nk mo iq nx b gy ob oc l od oe"># Explort the lemmatized data to json file for ease of use in the next steps<br/>tidy_tweets.to_json('lemmatized.json', orient='table')</span></pre><h1 id="ff5c" class="mn mo iq bd mp mq oh ms mt mu oi mw mx jw oj jx mz jz ok ka nb kc ol kd nd ne bi translated">方法</h1><p id="a2c3" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在进行预处理和建模的步骤之前，让我们回顾并明确我们在管道中的下一个步骤的方法。在我们可以预测一条推文属于哪个类别之前，我们必须首先用类别来标记原始推文。请记住，我们从Twitter上以原始推文的形式传输我们的数据，因此数据并没有标记出来。因此，实施以下方法是合适的:</p><ol class=""><li id="fa5a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">用k-means聚类算法标记数据集</li><li id="afdb" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">训练深度学习模型来预测推文的类别</li><li id="0983" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">评估模型并确定潜在的改进</li></ol></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="14ba" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">步骤4:标记未标记的文本数据并预处理</h1><p id="68d6" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在这一部分，我们的目标是给推文贴上两个标签，分别对应积极或消极的情绪。然后进一步预处理，将标注后的文本数据转换成可以进一步用于训练深度学习模型的格式。</p><p id="7fcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多不同的方法来对未标记的文本数据进行分类，这些方法包括但不限于使用SVM、分层聚类、余弦相似度甚至亚马逊Mechanical Turk。在这个例子中，我将向您展示另一种更简单的，也许不是最准确的，对文本数据进行分类的快速而又麻烦的方法。为此，我将首先对VADER进行情绪分析，以确定推文是积极的、消极的还是中立的。接下来，我将使用一个简单的k-means聚类算法，根据从推文的积极、消极和中立程度得出的计算复合值对推文进行聚类。</p><h2 id="27c6" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated"><strong class="ak"> 4.1创造情绪</strong></h2><p id="6393" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">让我们先来看看数据集</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/580983251857ff0b9a55d1ab65dab2a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kzjlg11eA4QTYQZTI5iN2A.png"/></div></div></figure><p id="1a58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">列“tokens_back_to_text”是转换回文本格式的词汇化标记，我将使用tidy数据集中的此列来创建VADER包中的SenitmentIntensityAnalyzer情感。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="1488" class="nk mo iq nx b gy ob oc l od oe"># Extract lemmatized text into a list<br/>tweets = list(df['tokens_back_to_text'])</span><span id="1bba" class="nk mo iq nx b gy of oc l od oe"># Create sentiments with SentimentIntensityAnalyzer<br/><strong class="nx ir">from</strong> <strong class="nx ir">vaderSentiment.vaderSentiment</strong> <strong class="nx ir">import</strong> SentimentIntensityAnalyzer <br/>sid = SentimentIntensityAnalyzer()<br/>sentiment = [sid.polarity_scores(tweet) <strong class="nx ir">for</strong> tweet <strong class="nx ir">in</strong> tweets]</span></pre><p id="1fc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是情绪的前5行</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/77c1fff2960f7910b9311b7eeaef9e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*BfS1Bbmd7QVTJSzsFcXMRw.png"/></div></figure><h2 id="03f6" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated"><strong class="ak"> 4.2用k-means聚类对未标记数据进行标记</strong></h2><p id="449b" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，我将使用上述数据帧中的“复合”列，并将其输入k-means聚类算法，以0或1分别代表“消极”或“积极”情绪。也就是说，我会将相应复合值大于或等于0.05的推文标记为积极情绪，而小于0.05的值将被标记为消极情绪。这里没有硬性规定，只是我如何设置我的实验。</p><p id="a486" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是如何用python中的scikit-learn中的k-means聚类算法实现文本标注工作。记住，给标签和原始数据框都指定相同的索引，在那里你有你的推文/文本。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="71e9" class="nk mo iq nx b gy ob oc l od oe"><em class="ou"># Tag the tweets with labels using k-means clustering algorithm</em><br/><strong class="nx ir">from</strong> <strong class="nx ir">sklearn.cluster</strong> <strong class="nx ir">import</strong> KMeans<br/>kmeans = KMeans(n_clusters=2, random_state=0).fit(compound)<br/>labels = pd.DataFrame(kmeans.labels_, columns=['label'], index=df.index)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2fa9dd7eaddb7061ca72f160e7aa3f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*xfZTS0Nv9OdaaMt6s9YJIw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标签计数</p></figure><p id="ed65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看0和1的标签计数，注意数据集是不平衡的，其中超过两倍的推文被标记为1。这将影响模型的性能，因此我们必须在训练模型之前平衡数据集。</p><p id="7e9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们还可以借助一种称为“潜在狄利克雷分配”的强大NLP算法，从每个类别中识别推文的主题，这种算法可以提供负面和正面推文中主题的直觉。稍后，我将在另一篇文章中展示这一点。现在，为了这个练习，让我们使用类别0和1。所以现在，我们已经成功地将我们的问题转化为监督学习问题，接下来我们将继续使用我们现在标记的文本数据来训练深度学习模型。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="64f9" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">第五步:建模</h1><p id="3376" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">我们有一个相当大的数据集，有超过400，000条推文，超过60，000个独特的单词。在如此大的数据集上训练具有多个隐藏层的rnn在计算上是繁重的，如果你试图在CPU上训练它们，可能需要几天(如果不是几周的话)。训练深度学习模型的一种常见方法是使用GPU优化的机器来获得更高的训练性能。在本练习中，我们将使用预装了TensorFlow后端和CUDA的Amazon SageMaker p2.xlarge实例。我们将使用Keras接口到张量流。</p><p id="8165" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧，我们将应用以下步骤。</p><h2 id="21c1" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">培训步骤</h2><ol class=""><li id="0e49" class="ls lt iq ky b kz nf lc ng lf ow lj ox ln oy lr lx ly lz ma bi translated">对数据集进行标记化、填充和排序</li><li id="58d3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">用SMOTE平衡数据集</li><li id="a198" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将数据集分成训练集和测试集</li><li id="3556" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">训练简单神经网络和LSTM模型</li><li id="3731" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">评估模型</li></ol><p id="ffa3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集必须转换成数字格式，因为机器学习算法不理解自然语言。在对数据进行矢量化之前，我们先来看看数据的文本格式。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="d402" class="nk mo iq nx b gy ob oc l od oe">tweets.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b20b55232812598d6a8f171429ecfbad.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*zMn9b1LU_32QRjIb9E__XA.png"/></div></figure><h2 id="f7af" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated"><strong class="ak"> 5.1对数据集进行标记化、填充和排序</strong></h2><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="70ed" class="nk mo iq nx b gy ob oc l od oe"><em class="ou"># prepare tokenizer</em> tokenizer = Tokenizer() tokenizer.fit_on_texts(tweets)</span><span id="18a8" class="nk mo iq nx b gy of oc l od oe"><em class="ou"># integer encode the documents</em><br/>sequences = tokenizer.texts_to_sequences(tweets)</span><span id="2ccc" class="nk mo iq nx b gy of oc l od oe"><em class="ou"># pad documents to a max length of 14 words</em> maxlen = 14 X = pad_sequences(sequences, maxlen=maxlen)</span></pre><h2 id="1df6" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">5.2用SMOTE平衡不平衡的数据</h2><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="9bf7" class="nk mo iq nx b gy ob oc l od oe"><strong class="nx ir">from</strong> <strong class="nx ir">imblearn.over_sampling</strong> <strong class="nx ir">import</strong> SMOTE<br/><strong class="nx ir">from</strong> <strong class="nx ir">imblearn.under_sampling</strong> <strong class="nx ir">import</strong> RandomUnderSampler<br/><strong class="nx ir">from</strong> <strong class="nx ir">imblearn.pipeline</strong> <strong class="nx ir">import</strong> Pipeline</span><span id="28bc" class="nk mo iq nx b gy of oc l od oe"><em class="ou"># define pipeline</em><br/>over = SMOTE(sampling_strategy=0.5)<br/>under = RandomUnderSampler(sampling_strategy=0.8)<br/>steps = [('o', over), ('u', under)]<br/>pipeline = Pipeline(steps=steps)</span><span id="a4ec" class="nk mo iq nx b gy of oc l od oe"><em class="ou"># transform the dataset</em><br/>X, y = pipeline.fit_resample(X, labels['label'])</span><span id="7068" class="nk mo iq nx b gy of oc l od oe"><em class="ou"># One-hot encoding of labels</em><br/><strong class="nx ir">from</strong> <strong class="nx ir">keras.utils.np_utils</strong> <strong class="nx ir">import</strong> to_categorical<br/>y = to_categorical(y)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/5b2ebb610b5c0b6ecf89752794a01379.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*26U2tnoC_gXvHVhTTEwsGw.png"/></div></figure><p id="266a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面0和1之间的数据分布可以看出，与以前相比，现在的数据看起来相当平衡。</p><h2 id="ed97" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">5.3将数据分为训练集和测试集</h2><p id="22c8" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">既然数据已经平衡，我们就可以将数据分成训练集和测试集了。我将收集30%的数据集进行测试。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="9fb0" class="nk mo iq nx b gy ob oc l od oe"># Split the dataset into train and test sets<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)<br/></span></pre><h2 id="bd10" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated">5.4培训注册护士</h2><p id="6d3d" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在这一部分，我将向您展示如何实现RNN深度学习架构的几个变体，3层SimpleRNN和3层LSTM架构。默认情况下，SimpleRNN和LSTM图层的激活函数都设置为“tanh ”,所以让我们保留其默认设置。我将使用所有65，125个唯一单词作为词汇表的大小，将每个输入的最大长度限制为14个单词，因为这与tweet中的最大单词长度一致，并将嵌入矩阵的输出维度设置为32。</p><p id="6998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SimpleRNN </strong></p><p id="4894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">脱落层将用于强制正则化项以控制过度拟合。由于我的数据集被标记为二进制类，我将使用二进制交叉熵作为损失函数。就优化器而言，Adam optimizer是一个不错的选择，我将准确性作为衡量标准。我将在训练集上运行10个时期，其中70%的训练集将用于训练模型，而剩余的30%将用于验证。这不能和我们放在一边的测试设备混淆。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="3309" class="nk mo iq nx b gy ob oc l od oe"><em class="ou"># SimpleRNN</em><br/>model = Sequential()<br/>model.add(Embedding(input_dim = vocab_size, output_dim = output_dim, input_length = maxlen, embeddings_constraint=maxnorm(3)))<br/>model.add(SimpleRNN(output_dim=output_dim, return_sequences=True, kernel_constraint=maxnorm(3)))<br/>model.add(Dropout(0.2))<br/>model.add(SimpleRNN(output_dim=output_dim, return_sequences=True, kernel_constraint=maxnorm(3)))<br/>model.add(Dropout(0.2))<br/>model.add(SimpleRNN(output_dim=output_dim))<br/>model.add(Dense(2,activation='softmax'))</span></pre><p id="0d2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">型号汇总如下:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/f43e7be302ced289f30a771d45aeb8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*EFUVmNppjpvzMzh7uYsZOA.png"/></div></figure><p id="b5ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">SimpleRNN模型结果如下— 10个历元:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/641265957d4cd3362781c7be091c917e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Krc-K5rTACcsx6E-6JF-ug.png"/></div></div></figure><p id="2925" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> LSTM </strong></p><p id="8d98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3层LSTM模型将使用漏失层进行训练。我将在训练集上运行10个时期，其中70%的训练集将用于训练模型，而剩余的30%将用于验证。这不能和我们放在一边的测试设备混淆。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="948a" class="nk mo iq nx b gy ob oc l od oe"># LSTM<br/>model.add(Embedding(input_dim = vocab_size, output_dim = output_dim, input_length = maxlen, embeddings_constraint=maxnorm(3)))<br/>model.add(LSTM(output_dim=output_dim, return_sequences=True, kernel_constraint=maxnorm(3)))<br/>model.add(Dropout(0.2))<br/>model.add(LSTM(output_dim=output_dim, return_sequences=True, kernel_constraint=maxnorm(3)))<br/>model.add(Dropout(0.2))<br/>model.add(LSTM(output_dim=output_dim, kernel_constraint=maxnorm(3)))<br/>model.add(Dense(2,activation='softmax'))</span></pre><p id="984b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">型号汇总如下:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/dbaae0aa41beb6a257caca85399ba2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*wnvahoNFKmDf9zjHb7k7hw.png"/></div></figure><p id="9a9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> LSTM模型结果如下— 10个历元:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/ab2d771b4d82f3e06765cfecd1b9f032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hX4GJ71b1amKsP4SuC-abQ.png"/></div></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c89c" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">第六步:模型评估</h1><p id="9365" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">现在，让我们绘制模型随时间变化的性能图，并查看它们在10个时期的精度和损失。</p><p id="21a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">简单:精度</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cd564883ce26418b7d17c84c0e5f619a.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*D3j4H_BbdlPqSEGkcJcazQ.png"/></div></figure><p id="dee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">简单:损失</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/2471950c4bb57af9d0c30c476e8695af.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*StAzi1J2tQOaCZdYhvqCMw.png"/></div></figure><p id="ac16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意训练精度，SimpleRNN模型很快开始过度拟合，并且由于相同的原因，验证精度具有很高的方差。</p><p id="0d11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> LSTM:精确度</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/1320670b41271935871758f65f0c42b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*BquUkGUsXQfyBsyYQ7Bmdg.png"/></div></figure><p id="3aba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> LSTM:损失</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/f1c75b7e983a19a90c81f98b44622ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*Tv6tnU51usgJy8PYKOF_Lg.png"/></div></figure><p id="ec59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从LSTM的精度和损失图来看，模型过度拟合，验证精度不仅方差高，而且由于同样的原因下降很快。</p><h1 id="7606" class="mn mo iq bd mp mq oh ms mt mu oi mw mx jw oj jx mz jz ok ka nb kc ol kd nd ne bi translated">结论</h1><p id="f314" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在这个项目中，我试图演示如何建立一个深度学习管道，来预测与2020年美国大选相关的推文的情绪。为此，我首先通过Twitter API和Tweepy包抓取原始推文，创建了自己的数据集。</p><p id="8c6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">超过440，000条推文通过Twitter API传输，并存储在一个CSV文件中。在争论和可视化数据之后，传统的聚类算法，在这种情况下是k-means聚类，被用来给推文贴上两个不同的标签，代表积极或消极的情绪。也就是说，在用数据训练深度学习模型之前，该问题被转换成监督学习问题。然后数据集被分成训练集和测试集。</p><p id="e224" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随后，训练集分别用于训练SimpleRNN和LSTM模型，并使用每个时期模型性能的损失和精度曲线进行评估。总的来说，这两个模型都表现出应有的性能，并且根据从精度图中看到的情况，可能会过度拟合数据，因此，我为下一步提出以下建议。</p><h2 id="9ec8" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated"><strong class="ak">建议:</strong></h2><ul class=""><li id="3b31" class="ls lt iq ky b kz nf lc ng lf ow lj ox ln oy lr pj ly lz ma bi translated">找到另一种方法或不同的学习算法来标记数据集</li><li id="e7e0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated">尝试亚马逊机械土耳其人或地面真相来标记数据集</li><li id="92d2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated">尝试不同的RNN建筑</li><li id="e1f7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated">对RNN架构执行更高级的超参数调整</li><li id="0a8f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated">执行交叉验证</li><li id="37c6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated">使数据成为多类问题</li></ul><h2 id="6cb3" class="nk mo iq bd mp nl nm dn mt nn no dp mx lf np nq mz lj nr ns nb ln nt nu nd nv bi translated"><strong class="ak">本项目/教程中练习的技能:</strong></h2><ol class=""><li id="cba8" class="ls lt iq ky b kz nf lc ng lf ow lj ox ln oy lr lx ly lz ma bi translated">如何通过Tweepy和Twitter API高效地从Twitter收集数据</li><li id="494a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如何有效地处理大型数据集</li></ol><p id="c487" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.如何构建深度学习架构，编译并适应Keras</p><p id="f68b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.如何将基本的自然语言处理概念和技术应用于文本数据</p><p id="8729" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，在我的GitHub <a class="ae kv" href="https://github.com/bauyrjanj/NLP-TwitterData" rel="noopener ugc nofollow" target="_blank">中找到我的带有Python代码的Jupyter笔记本，在这里</a>，让我们在<a class="ae kv" href="https://www.linkedin.com/in/bjenis/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接。</p><p id="144c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">享受深度学习:)我很想听到你的反馈和建议，所以请使用鼓掌按钮或在下面的部分评论。</p><p id="337b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢大家！</p><h1 id="959e" class="mn mo iq bd mp mq oh ms mt mu oi mw mx jw oj jx mz jz ok ka nb kc ol kd nd ne bi translated">在线参考和有用资料:</h1><ul class=""><li id="76b8" class="ls lt iq ky b kz nf lc ng lf ow lj ox ln oy lr pj ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66">使用RNN和LSTM的NLP</a></li><li id="9bee" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">用LSTM进行序列分类</a></li><li id="e22b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47">了解LSTM </a></li><li id="3b9b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/" rel="noopener ugc nofollow" target="_blank">Gensim中的单词嵌入</a></li><li id="9951" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr pj ly lz ma bi translated"><a class="ae kv" href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/" rel="noopener ugc nofollow" target="_blank">词汇化方法</a></li></ul></div></div>    
</body>
</html>