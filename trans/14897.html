<html>
<head>
<title>A New Way to BOW Analysis &amp; Feature Engineering — Part2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种新的BOW分析方法&amp;特征工程(二)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-way-to-bow-analysis-feature-engineering-part2-451d586566c6?source=collection_archive---------50-----------------------#2020-10-13">https://towardsdatascience.com/a-new-way-to-bow-analysis-feature-engineering-part2-451d586566c6?source=collection_archive---------50-----------------------#2020-10-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d82f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用统计技术为您的模型选择正确的特征。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/736a2b10f089884a287fad3213ab2e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RySHqHgyEv3e1-vB"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@kasiape?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Katarzyna Pe </a>拍摄</p></figure><p id="af40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">这是由两部分组成的系列文章的第二部分。你应该先读完</em> <a class="ae kv" href="https://medium.com/@prateekkrjain/a-new-way-to-bow-analysis-feature-engineering-part1-e012eba90ef" rel="noopener"> <em class="ls">第一部分</em> </a> <em class="ls">。</em></p><p id="50e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们正在讨论一种方法，在不建立机器学习模型的情况下，跨类别比较单词包，并进行特征工程。</p><p id="4bef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经了解了:</p><ol class=""><li id="35a5" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">将跨类别的词频视为单独的分布</li><li id="b174" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">应用Mann-Whitney U检验——对每个词的分布进行非参数检验，以检验它们的显著性</li><li id="93ff" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">分析结果，即比较重要单词的频率和p值</li></ol><p id="717d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看了结果之后，我们确实对这种方法有了一些信心，但是它还不完整。我的意思是使用这种技术来减少我们必须衡量模型性能影响的特征，无论它们是否是训练数据集的一部分。这是我们在这一部分将要研究的内容。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="4677" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">测试1-用所有特征训练模型</h1><ul class=""><li id="960d" class="lt lu iq ky b kz ng lc nh lf ni lj nj ln nk lr nl lz ma mb bi translated">首先，我将使用从CountVectorizer获得的所有特征建立一个模型，并检查单词的系数，这些系数在它们的频率分布中没有显著差异。</li><li id="9afb" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nl lz ma mb bi translated">这样做的目的是检查我们是否得到非常低的系数值(接近0 ),并且这将证明我们的假设，即这些特征/单词是否确实可以从模型中排除。</li><li id="6b76" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nl lz ma mb bi translated">此外，我将使用套索，因为它给不重要的功能0权重</li></ul><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="f984" class="nr mp iq nn b gy ns nt l nu nv"># split the data into test and train</span><span id="3541" class="nr mp iq nn b gy nw nt l nu nv">X = train_df_w_ft.iloc[:, 1:]<br/>Y = train_df_w_ft.iloc[:, 0]<br/><br/>print("Shape of X: ", X.shape)<br/>print("Shape of Y: ", Y.shape)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8f677fc359696e229a116474935eaba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*5Mtc5AmPxwTHSCPTUl1z8A.png"/></div></figure><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="dfd9" class="nr mp iq nn b gy ns nt l nu nv">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=42)<br/><br/>print("Size of X_train - <strong class="nn ir">{}</strong> and Y_train - <strong class="nn ir">{}</strong>".format(X_train.shape[0], Y_train.shape[0]))<br/>print("Size of X_test - <strong class="nn ir">{}</strong> and Y_test - <strong class="nn ir">{}</strong>".format(X_test.shape[0], Y_test.shape[0]))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1b683d7ce4b83d4f1db797399e8c18d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*cHvLcQrr-PJbPsHSO_si_w.png"/></div></figure><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="dab5" class="nr mp iq nn b gy ns nt l nu nv"># training a Logistic Regression model with L1-penalty (Lasso)</span><span id="bdf3" class="nr mp iq nn b gy nw nt l nu nv">log_reg1 = LogisticRegression(penalty='l1', random_state=42)\<br/>            .fit(X_train, Y_train)<br/><br/>print("Accuracy on Train dataset: ", accuracy_score(Y_train, log_reg1.predict(X_train)))<br/>print("Accuracy on Test dataset: ", accuracy_score(Y_test, log_reg1.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/88d4cd058f8b98d3efb603dc9bbe6d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*bIAkzTeYmQOCkbEZnjtR_g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练和测试数据集的准确性</p></figure><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="f2d6" class="nr mp iq nn b gy ns nt l nu nv">print("Test Data Classification Report:<strong class="nn ir">\n\n</strong>", classification_report(Y_test, log_reg1.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7f03e37ea3f6a4737dc89c59d2cfbe0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*ZuCoDNTQ5FzXfQ11phQAaA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">跨目标的精确度、召回率和F1分数</p></figure><p id="05bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将比较重要和不重要单词的系数。</p><p id="4900" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LR的系数在范围(-INF，+INF)内，其中-INF系数表示这种特征与目标/因变量成反比关系。远离0的系数表示特征越重要，0表示根本不重要。因此，我们将取系数的绝对值，然后将它们归一化，使它们具有可比性。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="6c3d" class="nr mp iq nn b gy ns nt l nu nv">def normalize_coefs(coefs):<br/>    <em class="ls"># normalizing the coefficients</em><br/>    <br/>    abs_coef = [abs(x) for x <strong class="nn ir">in</strong> coefs]<br/>    <br/>    _max = max(abs_coef)<br/>    _min = min(abs_coef)<br/>    <br/>    return [(x - _min)/(_max - _min) for x <strong class="nn ir">in</strong> abs_coef]<br/><br/><br/>feature_coef = dict([(ft, coef) for ft, coef <strong class="nn ir">in</strong> zip(X_train.columns, normalize_coefs(log_reg1.coef_[0]))])<br/></span><span id="c12a" class="nr mp iq nn b gy nw nt l nu nv"><em class="ls">## get the list of words which were not significant and their coefficients from the mode</em><br/><br/>top_x = 50<br/><br/><em class="ls"># get the list of significant words</em><br/>sig_words= [x[0] for x <strong class="nn ir">in</strong> words_significance]<br/>sig_words_coef = [feature_coef[ft] for ft <strong class="nn ir">in</strong> X_train.columns if ft <strong class="nn ir">in</strong> sig_words]<br/><br/><em class="ls"># get the list of insignificant words and their coefficients</em><br/>insig_words = [ft for ft <strong class="nn ir">in</strong> X_train.columns if ft <strong class="nn ir">not</strong> <strong class="nn ir">in</strong> sig_words]<br/>insig_words_coef = [feature_coef[ft] for ft <strong class="nn ir">in</strong> X_train.columns if ft <strong class="nn ir">not</strong> <strong class="nn ir">in</strong> sig_words]<br/><br/><em class="ls"># plot the words and their coefficients</em><br/>plot_bar_graph([sig_words[: top_x], insig_words[: top_x]], [sig_words_coef[: top_x], insig_words_coef[: top_x]], <br/>               ['Significant', 'Insignificant'], "Insignificant Words", "Model Coefficients", "")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/f74baea7f29d41da2581b4b98df69287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-h-uCVRPq_ytnv3tuAtig.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输入要素的系数图</p></figure><p id="1c4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面是一些重要和不重要特征的图，可以观察到，与不重要的特征相比，更重要的特征(蓝色)的值更接近1。同样，这些只是一些特征，可能有点偏差，因此，下面是重要和不重要特征的系数直方图，以获得更多的清晰度。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="969e" class="nr mp iq nn b gy ns nt l nu nv"># to plot the histograms<br/>def plot_histograms(xs, names, xlabel, ylabel, title, _min=0.0, _max=1.0, step=0.05):</span><span id="903c" class="nr mp iq nn b gy nw nt l nu nv"># create figure object<br/>    fig = go.Figure()</span><span id="f17b" class="nr mp iq nn b gy nw nt l nu nv"># create bar chart for each of the series provided <br/>    for x, name in zip(xs, names):</span><span id="8232" class="nr mp iq nn b gy nw nt l nu nv">fig.add_trace(go.Histogram(<br/>            x=x, <br/>            histnorm='percent', <br/>            name=name, <br/>            xbins=dict(start=_min, end=_max, size=step), <br/>            opacity=0.75)<br/>        )</span><span id="8427" class="nr mp iq nn b gy nw nt l nu nv"># Here we modify the tickangle of the xaxis, resulting in rotated labels.<br/>    fig.update_layout(<br/>        barmode='group',<br/>        autosize=False,<br/>        width=1300,<br/>        height=500,<br/>        margin=dict(l=5, r=5, b=5, t=50, pad=5),<br/>        yaxis_title=ylabel,<br/>        xaxis_title=xlabel,<br/>        title=title,<br/>        bargap=0.2,<br/>        bargroupgap=0.1<br/>    )<br/>    fig.show()</span><span id="f5d2" class="nr mp iq nn b gy nw nt l nu nv">plot_histograms([sig_words_coef, insig_words_coef], ['Significant', 'Insignificant'], "Coefficients", "Percentage of occurances", "")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/efdfb87ea8d5a2db1a777d2666ca905d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2MRpChnkCOJNhB87Asy8Mg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">重要和不重要特征的系数直方图</p></figure><p id="1f70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，左侧不重要特征的密度较高，表示不重要集合中有更多系数非常低的特征，其中低系数值表示预测/分类目标值的重要性较低。</p><p id="a97f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，这表明，从模型本身来看，这些特征的系数确实很低。现在，剩下的就是检查对模型性能的影响，我们将在下面看到。</p><h1 id="9173" class="mo mp iq bd mq mr oc mt mu mv od mx my jw oe jx na jz of ka nc kc og kd ne nf bi translated">测试2-仅使用重要特征训练模型，并比较精确度</h1><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="94da" class="nr mp iq nn b gy ns nt l nu nv"># split the data into train and test</span><span id="2d8e" class="nr mp iq nn b gy nw nt l nu nv">X = train_df_w_ft.loc[:, sig_words]<br/>Y = train_df_w_ft.iloc[:, 0]<br/><br/>print("Shape of X: ", X.shape)<br/>print("Shape of Y: ", Y.shape)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/7daf1e0c583928109714516a7b08e1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*TxFZ50EAYEbZvy308-La6g.png"/></div></div></figure><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="af30" class="nr mp iq nn b gy ns nt l nu nv">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=42)</span><span id="262e" class="nr mp iq nn b gy nw nt l nu nv">print("Size of X_train - {} and Y_train - {}".format(X_train.shape, Y_train.shape[0]))</span><span id="cfe2" class="nr mp iq nn b gy nw nt l nu nv">print("Size of X_test - {} and Y_test - {}".format(X_test.shape, Y_test.shape[0]))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/3ed41a44bac33d1b0e0feb63f249a660.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*J_DoXS1CInF9JuETMONK1g.png"/></div></figure><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="3b33" class="nr mp iq nn b gy ns nt l nu nv"># train the Logistic Regression only on the Significant features</span><span id="c3ed" class="nr mp iq nn b gy nw nt l nu nv">log_reg_sig = LogisticRegression(penalty='l1', random_state=42)\<br/>            .fit(X_train, Y_train)</span><span id="adc8" class="nr mp iq nn b gy nw nt l nu nv">print("Accuracy on Train dataset: ", accuracy_score(Y_train, log_reg_sig.predict(X_train)))</span><span id="3d2b" class="nr mp iq nn b gy nw nt l nu nv">print("Accuracy on Test dataset: ", accuracy_score(Y_test, log_reg_sig.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0c78bc5ee30e56f82286aa59ba51673c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*51V2xmXiBimoK0-OrIZMRQ.png"/></div></figure><p id="4f03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之前我们的训练精度是0.824，测试精度是0.778。因此，训练数据集的准确性降低了大约1%。但是，在测试数据的准确性方面有大约1%的改进。还有，我们来看看分类报告。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="c9bb" class="nr mp iq nn b gy ns nt l nu nv">print("Test Data Classification Report:\n\n", classification_report(Y_test, log_reg_sig.predict(X_test)))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/94c7ee59fc62607e7a6ee9b7f681bea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*kLFXwD5EEw-EAevxO-uYGA.png"/></div></figure><p id="363c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之前，我们对目标0的f1评分为0.81，对目标1的f1评分为0.73。在这里，我们可以清楚地看到两个目标班级的f1分数都提高了0.01。</p><p id="144e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们对这种方法的最终结论是:</p><ol class=""><li id="17f1" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">我们观察到精确度有微小的提高和下降，但是，当我们比较两个目标的f1分数(“0”和“1”)时，它们有0.01的微小提高。</li><li id="8b2a" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">我们可以得出结论，在使用问题中的技术移除特征之后，对模型的性能没有显著影响，因此，它可以用作特征选择的技术之一。在我们有成千上万个特性的情况下，它肯定可以减少训练时间，而不会对性能产生太大影响。</li><li id="14f0" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">我相信这也适用于连续变量，我一定会在这样的数据上进行测试，并与大家分享结果。</li></ol><p id="807b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分析的工作代码保存在Kaggle上，可以使用链接<a class="ae kv" href="https://www.kaggle.com/pikkupr/a-new-way-to-bow-analysis-and-feature-engg" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/pikk UPR/a-new-way-to-bow-analysis-and-feature-engg</a>进行参考</p><p id="ba34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望你发现这个分析是有帮助的，如果你最终在你的分析中实现了这个，请分享你的故事。</p><p id="6045" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我很想听听你的想法和建议，所以，请留下你的评论。</p></div></div>    
</body>
</html>