<html>
<head>
<title>Practical Machine Learning Tutorial: Part.1 (Exploratory Data Analysis)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实用机器学习教程:第1部分(探索性数据分析)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-machine-learning-tutorial-part-1-data-exploratory-analysis-c13d39b8f33b?source=collection_archive---------16-----------------------#2020-10-12">https://towardsdatascience.com/practical-machine-learning-tutorial-part-1-data-exploratory-analysis-c13d39b8f33b?source=collection_archive---------16-----------------------#2020-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9805" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多类分类问题:地球科学示例(相)</h2></div><p id="f8f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然外面有大量伟大的书籍和论文来练习机器学习，但我总是想看一些简短、简单、带描述性手稿的东西。我一直想看到一个对手术过程进行适当解释并附有详细结果解释的例子。模型评估指标也需要被清楚地阐述。</p><p id="cd16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这项工作中，我将尝试包括ML建模的所有重要步骤(即使有些步骤对该数据集来说不是必需的)，以制作一个一致且切实的示例，尤其是对地球科学家而言。八个重要的最大似然算法将被检查和结果将被比较。我会试着做一个议论文式的模型评估讨论。我不会深入算法的基础。</p><p id="b05f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要访问数据集和jupyter笔记本，请找到我的<a class="ae lb" href="https://github.com/mardani72/Practical_ML_Tutorial_Facies_examp" rel="noopener ugc nofollow" target="_blank"> Git </a>。<br/> <strong class="kh ir"> <em class="lc">注1 </em> </strong>:本手稿中嵌入的代码是为了理解工作程序而给出的。如果你想自己锻炼，我强烈推荐使用<a class="ae lb" href="https://github.com/mardani72/Practical_ML_Tutorial_Facies_examp/blob/main/Part1_practical_Tut_ML_facies.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter笔记本文件</a>。<br/> <strong class="kh ir"> <em class="lc">注2: </em> </strong>混洗数据会导致您的跑步记录与此处显示的内容有所不同。</p><p id="3425" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本教程有四个部分:<br/>第一部分:探索性数据分析，<br/>第二部分:建立模型&amp;验证，<br/>第三部分:模型评估-1，<br/>第四部分:模型评估-2</p><h2 id="e16a" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">1-探索性数据分析</h2><p id="9294" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated"><strong class="kh ir"><em class="lc">1–1数据可视化</em></strong><em class="lc"><br/>1–1对数图<br/>1–1–2柱状图<br/>1–1–3交会图<br/></em><strong class="kh ir"><em class="lc">1–2特征工程</em></strong><em class="lc"><br/>1–2–1 NaN插补<br/>1–2–2特征提取<br/>1–2–3过采样<br/></em><strong class="kh ir"><strong class="kh ir"/></strong></p><h2 id="c9f0" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">2-构建模型并验证</h2><p id="6d4d" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated"><strong class="kh ir"><em class="lc">2–1基线模型</em></strong><em class="lc"><br/></em><strong class="kh ir"><em class="lc">2–2超参数</em></strong><em class="lc"><br/>2–2–1网格搜索</em></p><h2 id="79f6" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">3-模型评估-1</h2><p id="78b8" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated"><strong class="kh ir"><em class="lc">3–1模型度量图</em></strong><em class="lc"><br/></em><strong class="kh ir"><em class="lc">3–2混淆矩阵</em> </strong></p><h2 id="aff6" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">4-模型评估-2</h2><p id="59cf" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated"><strong class="kh ir"><em class="lc">4–1学习曲线<br/>4–2 ROC图<br/>4–3盲井预测与评价</em> </strong></p><p id="5d10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你对python和ML概念完全陌生，你需要熟悉一些基础知识才能从本教程中获益。由于我们将在这里处理的数据集是一个包括测井记录和相类的表格CSV文件，我之前的两篇文章(熊猫中的<a class="ae lb" rel="noopener" target="_blank" href="/10-steps-in-pandas-to-process-las-file-and-plot-610732093338"> 10个步骤，熊猫</a>中的<a class="ae lb" rel="noopener" target="_blank" href="/5-steps-in-pandas-to-process-petrophysical-well-logs-de6c8d03d9b1"> 5个步骤)可能对测井记录数据处理、加工和绘图有所帮助。所有实现都基于scikit-learn库。</a></p><h1 id="7b85" class="mb le iq bd lf mc md me li mf mg mh ll jw mi jx lo jz mj ka lr kc mk kd lu ml bi translated"><strong class="ak">数据汇总</strong></h1><p id="cf4b" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">本研究的数据集(<a class="ae lb" href="https://github.com/mardani72/Practical_ML_Tutorial_Facies_examp/blob/main/facies_vectors.csv" rel="noopener ugc nofollow" target="_blank">phase _ vectors . CSV</a>)来自北美的Hugoton和Panoma油田，在堪萨斯大学(Dubois et。阿尔，2007)。它由九口井的测井数据(岩石物理性质的测量)组成。我们将使用这些测井数据来训练监督分类器，以便预测离散相组。更多细节，你可以看看<a class="ae lb" href="https://github.com/mardani72/Facies-Classification-Machine-Learning/blob/master/Facies_Classification_Various_ML_Final.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。这七个特征是:</p><ol class=""><li id="bbc4" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">GR :这种电缆测井工具测量伽马辐射</li><li id="3415" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> ILD_log10 </strong>:这是电阻率测量</li><li id="8373" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> PE </strong>:光电效应测井</li><li id="dca2" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir">δφ</strong>:φ是岩石物理学中的孔隙度指标。</li><li id="7d58" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> PNHIND </strong>:中子和密度测井的平均值。</li><li id="c924" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">NM_M :非海相-海相标志</li><li id="a453" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> RELPOS </strong>:相对位置</li></ol><p id="2de7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">九个离散相(岩石类别)是:</p><ol class=""><li id="8cd0" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated"><strong class="kh ir"> (SS) </strong>陆相砂岩</li><li id="e48b" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (CSiS) </strong>非海相粗粉砂岩</li><li id="fb2f" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (FSiS) </strong>非海相粉细砂岩</li><li id="0193" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (SiSH) </strong>海相粉砂岩和页岩</li><li id="75cb" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (MS) </strong>泥岩(石灰岩)</li><li id="a6aa" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (WS) </strong>瓦克斯通(石灰岩)</li><li id="6e9c" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (D) </strong>白云石</li><li id="905d" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (PS) </strong>细粒砂岩(石灰岩)</li><li id="77ee" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated"><strong class="kh ir"> (BS) </strong>叶状藻障石(石灰岩)</li></ol><p id="7e3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Pandas将数据读入python后，我们可以将其可视化，以便更好地理解数据。在绘制之前，我们需要定义一个颜色图(这一步应该在特征工程部分，但我们需要在这里为相类绘制颜色),并为每个相指定颜色代码。</p><p id="a32f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="lc">1–1数据可视化<br/></em></strong><em class="lc">1–1–1对数图</em></p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="452e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个创建情节的功能。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="0d71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和井的图<em class="lc">施林普林:</em></p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/b1347fba0be0fcc0c7c15c1df796dc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoGMUrDw89qpOz_6vwPtXA.png"/></div></div></figure><p id="2d8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–1–2条形图</em></p><p id="0189" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用计数器函数来定量评估每个类的贡献。为了查看相频率分布，我们可以使用柱状图，如下所示:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi no"><img src="../Images/8e2588841f313b46553b10f634b44370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*klumAd6BfLl8HIxcb89m4Q.png"/></div></div></figure><p id="2064" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个不平衡的数据集。白云石的会员参与度最低。与粗粉砂岩相比，白云石比粗粉砂岩少8倍。</p><p id="96b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–1–3交会图</em></p><p id="504f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了可视化数据集中的多个成对二元分布，我们可以使用seaborn库中的pairplot()函数。它以矩阵形式显示了数据集中变量组合的关系，并在对角线上显示了单变量分布图。很明显，PE测井与平均孔隙度呈非线性关系。其他对没有显示出清晰的模式。对角线上的分布模式显示，尽管各种类别有很强的重叠，但每个特征的每个标签类别(相)都有可接受的分离。理想模式可以假设为高钟形正态分布图中分布图的清晰分离。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi np"><img src="../Images/bae3dc6761fe4effedd61103e491f97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lj5Vd6ORCrXL6DiLAs3bQ.png"/></div></div></figure><p id="653c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">高亮显示</strong>:共线特征是彼此高度相关的特征。在机器学习中，由于高方差和较低的模型可解释性，这些会导致测试集上的泛化性能下降。在这个数据集中，我们没有面临共线性。使用data.corr()命令:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nq"><img src="../Images/5b9aaefd5c83cbe0e1506073549b0d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQhHYCEBCrdAnWAmCS6dJg.png"/></div></div></figure><p id="cbd7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="lc">1–2特征工程<br/></em></strong><em class="lc">1–2–1特征插补</em></p><p id="a6e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集中缺少值是很常见的。要查看每列要素的空值总和:</p><pre class="na nb nc nd gt nr ns nt nu aw nv bi"><span id="3df1" class="ld le iq ns b gy nw nx l ny nz"><em class="lc">DataFrame</em>.isna().sum()</span><span id="c307" class="ld le iq ns b gy oa nx l ny nz"># to find out which wells do not have PE<br/>df_null = data_fe.loc[data_fe.PE.isna()]<br/>df_null['Well Name'].unique()</span><span id="df79" class="ld le iq ns b gy oa nx l ny nz">#Categories (3, object): [ALEXANDER D, KIMZEY A, Recruit F9]</span></pre><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ob"><img src="../Images/494486a74b1c1a177dcd5809b3fbcce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwBmlxABo3h2wdiYdZgPJQ.png"/></div></div></figure><p id="d666" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，PE有917个空值。<br/>有几种方法可以处理数据集中的空值。最简单的方法是删除至少包含一个空值的行。对于较大的数据集，这可能是合理的，但在小数据框中，单点非常重要。我们可以用平均值或从列中相邻的数据点估算空值。用平均值填充不会影响数据方差，因此不会影响预测精度，但会产生数据偏差。如果我们有地质上均质的介质，如大量纯碳酸盐岩，则用列值的相邻单元填充可能是合适的。</p><p id="afc7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将在这里实现的另一种方法是使用机器学习模型来预测缺失值。这是处理该数据集的最佳方式，因为我们的数据集中只缺少一个要素，PE。另一方面，用最大似然预测填充比单个平均值好得多，因为我们能够通过将数据划分为训练集和测试集来查看最大似然相关性和准确性。</p><p id="b47f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我将使用scikit-learn的多层感知器神经网络来预测目标值。我不打算深入研究这种方法，并简单地使用它来预测缺失值。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/e07c1f29eff28dfb6f97a9df5e46055c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yDVHEml61W7eIqbH1yeofA.png"/></div></div></figure><p id="9d4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ALEXANDER D井的预测PE显示了正常范围和变化。预测准确率为77%。</p><p id="531a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–2–2特征提取</em></p><p id="9de6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在该数据集中拥有有限的一组要素可以让我们考虑从现有数据集中提取一些数据。首先，我们可以将地层分类数据转换成数值数据。我们的背景知识可以帮助我们猜测某些相可能更多地出现在特定地层中，而不是其他地层中。我们可以使用LabelEncoder函数:</p><pre class="na nb nc nd gt nr ns nt nu aw nv bi"><span id="ef2f" class="ld le iq ns b gy nw nx l ny nz">data_fe[‘Formation_num’] = LabelEncoder().fit_transform(data_fe[‘Formation’].astype(‘str’)) + 1</span></pre><p id="8667" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将地层类别数据转换为数值，用作预测值，并从1而不是零开始增加1作为预测值。为了查看新特征提取是否有助于预测改进，我们应该定义一个基线模型，然后将其与提取的特征模型进行比较。</p><p id="d76d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lc">基线模型性能</em> </strong></p><p id="fc95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为简单起见，我们将使用逻辑回归分类器作为基线模型，并用交叉验证概念来检查模型性能。数据将被分成10个小组，该过程将重复3次。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="9041" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们可以探讨特征提取是否可以提高模型性能。有许多方法，我们将使用一些变换来链接输入变量的分布，如分位数变换器和KBins离散化器。然后，将使用PCA和TruncatedSVD移除输入变量之间的线性相关性。要了解更多信息，请参考此处的<a class="ae lb" href="https://machinelearningmastery.com/quantile-transforms-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"/>。<br/>使用特征联合类，我们将定义一个转换列表，以执行聚合在一起的结果。这将创建一个包含许多特征列的数据集，而我们需要降低维数以获得更快更好的性能。最后，递归特征消除或RFE技术可用于选择最相关的特征。我们选择了30个特征。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="5336" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确度的提高表明，当我们处理数据集中有限的特征时，特征提取是一种有用的方法。</p><p id="9bfe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–2–3过采样</em></p><p id="f77d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在不平衡数据集中，我们可以使用重采样技术来添加更多的数据点，以增加少数群体的成员。每当少数民族标签目标具有特殊重要性时，如信用卡欺诈检测，这可能是有帮助的。在这个例子中，欺诈可能发生在不到0.1%的交易中，而检测欺诈非常重要。<br/>在这项工作中，我们将为数量最少的白云石类添加伪观测值</p><p id="8e0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">合成少数过采样技术，SMOTE </strong>:该技术用于在特征空间中选择最近的邻居，通过添加一条线来分离样本，并沿着该线产生新的样本。该方法不仅从数量上超过的类生成副本，而且应用K-最近邻生成合成数据。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="cdd6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精度提高了3 %,但在多类分类中，精度不是最佳的评估指标。我们将在本部分中介绍其他内容</p><h2 id="f68b" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">1–3功能重要性</h2><p id="59b3" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">一些机器学习算法(并非全部)提供重要性分数，以帮助用户选择最有效的特征进行预测。</p><p id="29e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–3–1特征线性相关</em></p><p id="a06b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">概念很简单:与目标值相关系数较高的特征对于预测很重要。我们可以提取这些系数，比如:</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/50d8e3334704d31b0e0e3b16311362fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*yABbQwBpneGmvYtY3kMZ5Q.png"/></div></figure><p id="29ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–3–2决策树</em></p><p id="b179" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该算法基于用于在每个节点中分裂的标准(例如熵或基尼)的减少来提供重要性分数。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/03d2edf81e53f375bb2e0c33afd6d1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*vdbaXeAIhsxw8ajgA5I6yw.png"/></div></figure><p id="f5d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">1–3–3排列特征重要性</em></p><p id="7b44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://scikit-learn.org/stable/modules/permutation_importance.html" rel="noopener ugc nofollow" target="_blank">排列特征重要性</a>是一种模型检验技术，当数据为表格形式时，可用于任何拟合的估计量。这对于非线性或不透明的估计器尤其有用。置换特征重要性被定义为当单个特征值被随机打乱时模型得分的减少。</p><figure class="na nb nc nd gt ne"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d9f027f021490c7193c5c485ca5df10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*idjNWI8Qep_sbwvpvfysvg.png"/></div></figure><p id="1551" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在所有这些特征重要性图中，我们可以看到6号预测器(PE log)在标签预测中最重要。基于我们选择来评估结果的模型，我们可以根据它们的重要性来选择特征，并忽略其余的以加速训练过程。如果我们的特征数量丰富，这是很常见的，尽管在我们的示例数据集中，我们将使用所有特征作为预测器是有限的。</p><h1 id="308c" class="mb le iq bd lf mc md me li mf mg mh ll jw mi jx lo jz mj ka lr kc mk kd lu ml bi translated">摘要</h1><p id="a03c" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">数据准备是机器学习中最重要也是最耗时的步骤之一。数据可视化可以帮助我们理解数据的性质、边界和分布。特征工程是必需的，尤其是当我们有空值和分类值时。在小型数据集中，特征提取和过采样有助于提高模型性能。最后，我们可以分析数据集中的要素，以了解要素对于不同模型算法的重要性。</p><p id="86f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您有任何问题，请通过我的LinkedIn联系我:<a class="ae lb" href="https://www.linkedin.com/in/amardani/" rel="noopener ugc nofollow" target="_blank"> Ryan A. Mardani </a></p></div></div>    
</body>
</html>