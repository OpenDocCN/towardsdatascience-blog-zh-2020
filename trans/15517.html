<html>
<head>
<title>How to use a pre-trained model (VGG) for image classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用预训练模型(VGG)进行图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-a-pre-trained-model-vgg-for-image-classification-8dd7c4a4a517?source=collection_archive---------7-----------------------#2020-10-26">https://towardsdatascience.com/how-to-use-a-pre-trained-model-vgg-for-image-classification-8dd7c4a4a517?source=collection_archive---------7-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d6c0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么要重新发明轮子？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/11c5b81e0ef68d9f9c5219dda4916536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ii0LJETIl96DtE25"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://unsplash.com/photos/LJ9KY8pIH3E" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/LJ9KY8pIH3E</a></p></figure><p id="02d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嗨，伙计们，今天我要谈谈如何使用VGG模型作为预训练模型。让我们一小步一小步来</p><h1 id="636d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">这些VGG模型是什么？</h1><ul class=""><li id="9eb5" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">VGG模型是由牛津大学视觉几何小组(VGG)的卡伦·西蒙扬和安德鲁·齐泽曼提出的一种CNN架构，它为ImageNet挑战赛带来了显著的成绩。</li><li id="d44f" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">他们试验了6个模型，有不同数量的可训练层。根据型号数量，最受欢迎的两种型号是VGG16和VGG19。</li></ul><p id="202c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们继续之前，我们应该回答什么是CNN架构以及ImageNet。</p><p id="b924" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于感兴趣的读者，可以参考下表来了解作者实验的所有ConvNet系列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/45577a9e05238ec9de2629b951e72d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*GgefggWwErPxwl-zKhBpfw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:不同的ConvNet架构(图片来源:Simonyan、Karen和Andrew Zisserman。“用于大规模图像识别的非常深的卷积网络。”<em class="nb"> arXiv预印本arXiv:1409.1556 </em> (2014)。)</p></figure><h1 id="c1fe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">这是什么CNN架构？</h1><p id="34df" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">CNN是一个专门处理图像数据的深度神经网络模型。</p><ul class=""><li id="6a27" class="mk ml iq ky b kz la lc ld lf nf lj ng ln nh lr mr ms mt mu bi translated">它不需要传统的图像处理滤波器，如边缘、直方图、纹理等。，而不是在美国有线电视新闻网，过滤器是可以学习的。所以，这些不需要通过反复试验来确定。</li><li id="6e53" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">CNN有两个部分，第一部分是特征学习部分，然后是分类层(通常称为全连接层)</li><li id="b0d2" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">特征学习部分的两个主要构建块是卷积层和池层</li><li id="2d6b" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated"><strong class="ky ir">卷积层:</strong>我们讨论过的可学习过滤器或特征提取器。</li><li id="1b59" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated"><strong class="ky ir">池层:</strong>这做了一些空间压缩，也带来了不变性。一辆车就一辆车，哪怕旋转一点点。</li></ul><p id="c5ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图2给出了CNN的架构概述。卷积创建特征图，汇集是通过二次采样实现的。</p><p id="9914" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你需要更详细的解释，你可以看这里的<a class="ae kv" href="https://www.youtube.com/watch?v=mV1eoWcx14M" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/b328a49236fd70af8843fee5a79c55c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsmiX29OAbYcvj4sibp3Jw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2: CNN架构(来源:维基百科<a title="”Aphex34," cc="" by-sa="" class="ae kv" href="https://creativecommons.org/licenses/by-sa/4.0&amp;gt" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/licenses/by-sa/4.0&amp;gt</a>；，via Wikimedia Commons " href = "<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Typical_cnn.png" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Typical_cnn.png</a>"&gt;</p></figure><h1 id="3984" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预训练模型的原因和用途？</h1><ul class=""><li id="c014" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">这些是模型，是具有大量参数的网络(一个恰当的例子是VGG16，它具有1.38亿个参数)</li><li id="3336" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">一般来说，训练这样的网络是耗时且耗费资源的</li><li id="9786" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">CV的预训练模型大多也非常通用</li><li id="1dd0" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">我们可以直接使用这些模型，如果我们选择1000个训练类中的任何一个</li><li id="214d" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">即使有一点点不同，我们也可以去掉顶层，只训练那一层的权重(迁移学习)</li></ul><h1 id="d96b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">这个ImageNet数据集是什么？</h1><p id="c050" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">这是斯坦福大学教授费-李非从2006年开始与wordnet合作的一个项目。图像注释是众包的。这实际上使得计算机视觉任务的试验台变得非常健壮、庞大和昂贵。基于ImageNet的1000类分类挑战始于ImageNet大规模视觉识别挑战(ILSVRC)。</p><p id="e69c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，这种竞争是CNN大多数杰出模特诞生的原因。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/36f1ab16c5fff9c77a439bc090c923a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*F44vWGSVkwbeKwgAh0vFoA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ImageNet与其他图像数据集(来源:林等。艾尔。<a class="ae kv" href="https://www.dbs.ifi.lmu.de/~yu_k/cvpr11_0694.pdf" rel="noopener ugc nofollow" target="_blank">https://www.dbs.ifi.lmu.de/~yu_k/cvpr11_0694.pdf</a></p></figure><h1 id="70ee" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">现在实施</strong></h1><p id="c4ba" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated"><strong class="ky ir">T5第一步:导入模型T7】</strong></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="02e0" class="nq lt iq nm b gy nr ns l nt nu">from keras.applications.vgg16 import VGG16<br/>model = VGG16(weights='imagenet')<br/>print(model.summary())</span></pre><p id="3401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有很多其他CNN的型号可供选择，可以在这里<a class="ae kv" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank">找到</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/931e8105f6380eee2e5045818d6d2dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*2LF-kDP5qmEeqJEAd8nk0g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4: VGG 16摩尔(图片来源:作者)</p></figure><p id="70d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nk">第二步:加载样本图像</em> </strong></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="474f" class="nq lt iq nm b gy nr ns l nt nu">from tensorflow.keras.preprocessing import image<br/>from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions<br/>import numpy as np<br/><br/>img_path = '/kaggle/input/images/dog.jpg'<br/><em class="nk">#There is an interpolation method to match the source size with the target size</em><br/><em class="nk">#image loaded in PIL (Python Imaging Library)</em><br/>img = image.load_img(img_path,color_mode='rgb', target_size=(224, 224))<br/>display(img)</span></pre><p id="c04b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试图像，我们使用的是黄金猎犬，也请注意图像是以Python图像库(PIL)格式加载的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b0608419ea8bbc2d88aaa3ccfab8f8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uC0r2wJEPqZj6kdg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试图片: (图片来源:<a class="ae kv" href="https://unsplash.com/photos/x5oPmHmY3kQ" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/x5oPmHmY3kQ</a>)</p></figure><p id="5767" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nk">第三步:使图像尺寸与VGG16输入兼容</em> </strong></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="54b8" class="nq lt iq nm b gy nr ns l nt nu"><em class="nk"># Converts a PIL Image to 3D Numy Array</em><br/>x = image.img_to_array(img)<br/>x.shape<br/><em class="nk"># Adding the fouth dimension, for number of images</em><br/>x = np.expand_dims(x, axis=0)</span></pre><p id="b8d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，PIL图像首先被转换成3d阵列，RGB格式的图像是3D阵列。然后为多个图像添加另一维度。因此，输入实际上是一个4D数组。</p><p id="eb30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nk">第四步:做预测</em> </strong></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="bf20" class="nq lt iq nm b gy nr ns l nt nu"><em class="nk">#mean centering with respect to Image</em><br/>x = preprocess_input(x)<br/>features = model.predict(x)<br/>p = decode_predictions(features)</span></pre><p id="7b94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在该步骤中，进行简单的均值居中预处理，然后进行预测，最后，将概率分布的预测解码为可理解的类名。我们已经在默认的前5名可能类模式中使用了它。</p><p id="7161" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出</strong></p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="cbbf" class="nq lt iq nm b gy nr ns l nt nu">[[('n02099601', 'golden_retriever', 0.8579672),<br/>  ('n02099267', 'flat-coated_retriever', 0.018425034),<br/>  ('n04409515', 'tennis_ball', 0.01615624),<br/>  ('n02099712', 'Labrador_retriever', 0.015078514),<br/>  ('n02099849', 'Chesapeake_Bay_retriever', 0.012522769)]]</span></pre><p id="09bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们使用条形图，这就是它的样子</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/62df4d6da3d43cadd61513f1646af24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*UrVhYZEL_F1XYPdN1gnNNw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:最有可能的5类(图片来源:作者)</p></figure><p id="94aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，无需创建模型并训练它，我们就可以完美地对一张金毛寻回犬的图像进行分类。</p><h1 id="f40d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">尾注:</h1><ul class=""><li id="c73c" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">预先训练的模型就像魔术一样，我们可以下载模型并开始使用它们，即使没有任何数据和训练。</li><li id="75da" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">如果源任务和目标任务是不同的，那么在域之间有一些相似性，那么我们可能必须训练几层，但是仍然，它不会像从头开始训练那样广泛，并且需要更少的数据</li></ul><h1 id="c5d8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考:</h1><p id="f430" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">[1]<a class="ae kv" href="https://www.kaggle.com/saptarsi/using-pre-trained-vgg-model" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/saptarsi/using-pre-trained-vgg-model</a></p><p id="260a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]西蒙扬、卡伦和安德鲁·齐泽曼。“用于大规模图像识别的非常深的卷积网络。”<em class="nk"> arXiv预印本arXiv:1409.1556 </em> (2014)。</p></div></div>    
</body>
</html>