<html>
<head>
<title>The Intuition and Applications Behind Autoencoders &amp; Variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器和变体背后的直觉和应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4?source=collection_archive---------52-----------------------#2020-09-21">https://towardsdatascience.com/the-intuition-and-applications-behind-autoencoders-variants-4afcd45559d4?source=collection_archive---------52-----------------------#2020-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8eec2ebddfbb03ee480d976c11b50aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tIEEdQwkCxlNp9iS8mYFuA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://unsplash.com/photos/dOhJtfXJZfw" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><div class=""/><div class=""><h2 id="14e5" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">无监督深度学习的美妙之处</h2></div><p id="6c2f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经网络<em class="lu">从根本上受到监督——它们接受一组输入，执行一系列复杂的矩阵运算，并返回一组输出。随着世界越来越多地被无监督数据填充<a class="ae jg" rel="noopener" target="_blank" href="/the-future-of-deep-learning-can-be-broken-down-into-these-3-learning-paradigms-e7970dec5502">，简单和标准的无监督算法不再能够满足需求。我们需要以某种方式将神经网络的强大功能应用于无人监管的数据。</a></em></p><p id="c95e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，<a class="ae jg" rel="noopener" target="_blank" href="/the-fascinating-blueprint-for-efficient-ai-self-supervised-learning-954f919f0d5d">自我监督学习</a>的创造性应用——从自然无监督的数据中人工创建标签，如倾斜图像和训练网络来确定旋转的程度——已经成为无监督深度学习应用的一个巨大组成部分。</p><p id="0cbd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自动编码器被训练来重新创建输入；换句话说，<em class="lu"> y </em>标签就是<em class="lu"> x </em>输入。因为自动编码器被构建为具有瓶颈(网络的中间部分)，其神经元比输入/输出少，所以网络必须找到一种方法来压缩信息(编码)，这需要重建(解码)。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lv"><img src="../Images/8b0bcd251f2d0455bc04f3ea5b5a6cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8kP9jYIKCKCTj4KTU5vAVA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由作者创建</p></figure><p id="7fc4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些类型的模型可以有多个隐藏层，用于携带和转换压缩信息。因为这些节点中的空间有限，所以它们通常被称为“潜在表示”。</p><p id="7262" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“潜伏”一词来自拉丁语，意思是“隐藏起来”。潜在变量和表示就是这样——它们携带间接的编码信息，可以解码并在以后使用。人们可以将迁移学习视为利用潜在变量:尽管像ImageNet上的Inception这样的预训练模型可能不会直接在数据集上表现良好，但它已经建立了关于图像识别动态的某些规则和知识，这使得进一步的训练更加容易。</p><p id="9483" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">创建自动编码器时，有几个组件需要注意:</p><ul class=""><li id="79aa" class="ma mb jj la b lb lc le lf lh mc ll md lp me lt mf mg mh mi bi translated">损失函数非常重要，它量化了“重建损失”。由于这是一个回归问题，损失函数通常是二进制交叉熵(对于二进制输入值)或均方误差。</li><li id="097a" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">确定代码大小—这是第一个隐藏层(紧随输入层的层)中神经元的数量。这可以说是最重要的一层，因为它直接决定了有多少信息将通过该层的其余部分传递。</li><li id="3e98" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">自动编码器的性能高度依赖于架构。平衡表示大小，可以通过隐藏层传递的信息量；和要素重要性，确保隐藏层足够紧凑，以便网络需要确定重要的要素。</li><li id="fc5c" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">对不同类型的数据使用不同的图层。例如，可以使用一维卷积层来处理序列。自动编码器与神经网络相同，只是在架构上有瓶颈。</li></ul><p id="8350" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">普通自动编码器的一个应用是异常检测。例如，我可以构建一个使用1-d conv的一维卷积自动编码器。层(有架构瓶颈)并训练它重建输入序列。</p><p id="eae0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在经过相当长时间的训练后，自动编码器学习序列的潜在表示——它能够拾取重要的区别性方面(序列中的哪些部分对精确重建更有价值),并能够假设整个序列中通用的某些特征。</p><p id="fa1c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当它对一个测试序列进行预测时，重建损失决定了它与先前序列的相似程度。如果自动编码器可以正确地重建序列，那么它的基本结构与以前看到的数据非常相似。另一方面，如果网络不能很好地重建输入，它就不遵守已知的模式。</p><p id="6631" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自动编码器的另一个应用是图像去噪。通过添加噪声，图像被人为破坏，并被送入自动编码器，该编码器试图复制原始的未被破坏的图像。自动编码器最擅长去噪，因为网络只学习让图像的结构元素——而不是无用的噪声——通过瓶颈。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mo"><img src="../Images/1ec45c27caa20765b037c2849f54f611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGUTzRkJt5rdRb5T8E6jIg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由作者创建</p></figure><p id="0d45" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">稀疏自动编码器类似于自动编码器，但是隐藏层的节点数至少与输入层和输出层的节点数相同(如果不是更多的话)。但是，L1正则化用于隐藏层，这导致不必要的节点被停用。为了获得为什么会发生这种情况的直觉，<a class="ae jg" rel="noopener" target="_blank" href="/exploring-the-simple-satisfying-math-behind-regularization-2c947755d19f">阅读这个</a>。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/55282eb17121a0c5c743475f380c51ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKwvmJGDwp366dV1hBT1OQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由作者创建。</p></figure><p id="2a97" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在某种意义上，架构是由“模型”选择的。然而，让模型自己选择并不总是好的；一般来说，L1正则化倾向于消除比可能需要的更多的神经元。</p><p id="b312" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请记住，正则化的目标不是<em class="lu">找到性能最佳的架构</em>，而是主要为了减少参数的数量，甚至以牺牲一些性能为代价。然而，在需要稀疏架构的情况下，稀疏自动编码器是一个不错的选择。</p><p id="46a0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可变自动编码器，通常缩写为VAEs，是自动编码器生成内容的扩展。正如之前在异常检测中所看到的，自动编码器擅长的一件事是拾取模式，本质上是通过将输入映射到一个缩减的潜在空间。这并没有带来多少创意。</p><p id="08aa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">任何生成模型的一个组成部分就是随机性。变分自动编码器将输入映射到多维高斯分布，而不是潜在空间中的点。然后，解码器从该分布中随机采样一个矢量以产生输出。</p><figure class="lw lx ly lz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/623f9593059d312f62bbdd391ab3c9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6_IbEDJKrixUKVcF.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:<a class="ae jg" href="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs.f8AX~1c8f3/w:1200/h:226/q:auto/https://www.unite.ai/wp-content/uploads/2020/09/variational.png" rel="noopener ugc nofollow" target="_blank"> Unite AI </a>。图片免费分享。</p></figure><p id="d7f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">变分自动编码器和自动编码器之间的主要区别在于vae基本上是概率性的。他们建立由概率分布形成的一般规则来解释输入并产生输出。</p><p id="c284" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当VAEs编码器一个输入时，它被映射到一个分布；因此，这里有随机性和“创造性”的空间。另一方面，必须识别复杂模式的自动编码器必须确定性地接近潜在空间，以获得良好的结果。</p><p id="3631" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，自动编码器实际上更多的是一个概念，而不是任何一种算法。这是一个以瓶颈和重建为特征的架构决策，其驱动力是迫使模型将信息压缩到潜在空间并对其进行解释。编码器-解码器的思维模式可以进一步以创造性的方式应用于几个监督问题，这已经取得了相当大的成功。</p><h2 id="32d4" class="mr ms jj bd mt mu mv dn mw mx my dp mz lh na nb nc ll nd ne nf lp ng nh ni nj bi translated">摘要</h2><ul class=""><li id="f2ad" class="ma mb jj la b lb nk le nl lh nm ll nn lp no lt mf mg mh mi bi translated">自动编码器是深度学习对无监督问题的创造性应用；对快速增长的未标记数据量的一个重要回答。</li><li id="702e" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">自动编码器的特点是输入与输出大小相同，并且存在体系结构瓶颈。它们将压缩信息存储在潜在空间中，并被训练以最小化重建损失。</li><li id="c569" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">标准自动编码器可用于异常检测或图像去噪(当用卷积层替代时)。</li><li id="0e17" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">稀疏自动编码器具有与输入和输出神经元数量相同的隐藏层，但是使用L1正则化来消除不必要的神经元。在某种意义上，网络“选择”在最终架构中保留哪些和多少神经元。</li><li id="83f0" class="ma mb jj la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated">变分自动编码器用于生成。因此，它们将输入表示为概率分布，而不是潜在空间中确定性的点。解码器从这些分布中采样，以产生随机(因此，创造性)输出。</li></ul><p id="9eec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p><div class="is it gp gr iu np"><a rel="noopener follow" target="_blank" href="/obtaining-top-neural-network-performance-without-any-training-5af0af464c59"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd jk gy z fp nu fr fs nv fu fw ji bi translated">无需任何训练即可获得顶级神经网络性能</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">我们对神经网络的了解比我们想象的要少</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od ja np"/></div></div></a></div></div></div>    
</body>
</html>