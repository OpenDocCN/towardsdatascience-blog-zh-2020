<html>
<head>
<title>Learning to Rank for Information Retrieval: A Deep Dive into RankNet.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习为信息检索排序:深入研究RankNet。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-to-rank-for-information-retrieval-a-deep-dive-into-ranknet-200e799b52f4?source=collection_archive---------12-----------------------#2020-09-26">https://towardsdatascience.com/learning-to-rank-for-information-retrieval-a-deep-dive-into-ranknet-200e799b52f4?source=collection_archive---------12-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e462" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入了解可用于信息检索的最新排名系统。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ac5167409db88970a33d81ef0d4689af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q-o2XbdqSomH6Vmi"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文档排序是信息检索中的一项重要任务。(图片由<a class="ae ky" href="https://unsplash.com/@floschmaezz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗洛里安·施梅兹</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="7939" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">机器学习和人工智能目前正在推动计算机科学领域的创新，它们正被应用于跨学科的多个领域。然而，传统的最大似然模型仍然可以大致分为两类问题的解决方案。</p><ol class=""><li id="eea7" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">分类——旨在根据各种特征将特定的数据实例标记到桶中。</li><li id="af15" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">回归——我们希望得到一个连续的实数作为给定特性集的输出。</li></ol><p id="30a9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">机器学习的一个相对较少探索的应用是根据相关性对数据进行排序，这在搜索引擎等信息检索系统中变得有用。这些类型的模型更关注项目的相对排序，而不是单个标签(分类)或分数(回归)，并被归类为<a class="ae ky" href="https://en.wikipedia.org/wiki/Learning_to_rank" rel="noopener ugc nofollow" target="_blank"> <strong class="li iu">学习排序</strong> </a>模型。</p><h1 id="d15a" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated"><strong class="ak">rank net简介</strong></h1><p id="97d2" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">2005年，克里斯·伯格斯等人。艾尔。微软研究院推出了一种新颖的方法来创建学习排名模型。他们的方法(在这里可以找到<a class="ae ky" href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf" rel="noopener ugc nofollow" target="_blank"/>)采用了一个概率成本函数，该函数使用一对样本项目来学习如何对它们进行排序。该功能主要是尝试最小化纠正所选项目的错误排序所需的交换次数。他们的论文进一步探索了这种方法，通过神经网络实现这种成本函数，并通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point." rel="noopener ugc nofollow" target="_blank">梯度下降</a>进行优化。这个名为“<em class="mq"> RankNet </em>”的网络也在一些真实世界的数据上进行测试，以显示其有效性。</p><p id="03b3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如本文中所实现的，RankNet的工作总结如下。</p><p id="4aa7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">训练网络</strong></p><ol class=""><li id="a218" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">构造了一个具有一个输出节点的两层神经网络。输出值对应于该项与集合的相关性，并且输入层可以基于特征向量的大小而具有多个节点。</li><li id="c75d" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">从数据集中选择两个随机样本，并分别对这两个样本进行前向传播，从而产生两个输出值，每个项目一个。</li><li id="7551" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">确定成本，该成本是这两个输出值之差的激活函数(e <em class="mq"> g: sigmoid </em>)。假设第一个样本的排名高于第二个样本，并计算适当的损失。</li><li id="5038" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">这种损失被反向传播到网络中以学习所选择的例子。</li><li id="fd73" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">执行步骤2-4，直到训练完成(基于时期数)。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9083d5d356780f716a6c54dd15b50ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*s7f9HMecjGzY7FW1dlvMnQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片:所描述网络的前向传播演示。(礼貌:自己)</p></figure><p id="dd66" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">步骤3中提到的假设只不过是确定两个选定项目的预期等级。该等级可以通过比较特定项目相对于整个集合的相关性评级来确定。此外，可以通过基于一些假设手动设置相关性，或者通过使用人工评级者基于相关性对结果进行分类，来确定该相关性评级。</p><p id="1ed5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">检索一个集合的排名</strong></p><ol class=""><li id="f6e8" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">为了对特定集合中的项目进行排序，每个项目的特征向量通过网络传播，并且输出被存储。</li><li id="fb5f" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">现在，只需按照输出的降序排列项目，就可以对项目进行排序。</li></ol><p id="aff9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Burges和他的同事用一些人工数据证明了这种方法的有效性。他们采用了两个排序函数，一个随机2层神经网络，以及一个随机多项式函数。在对5000个特征向量输入运行100个时期的数据后，他们获得了如下所示的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/03b0d39ff660416b609d4abcb19bd0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*3ivXDu8ukMUuYEEZGSejUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图:两个排名函数的成对%正确结果。(礼貌:<a class="ae ky" href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf" rel="noopener ugc nofollow" target="_blank">学习使用梯度下降法排名</a>)</p></figure><p id="7e42" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，这种方法在真实世界的数据——给定查询的搜索引擎结果——上进行了测试。需要注意的一点是，由于该模型不执行传统的分类或回归，因此其准确性必须基于排名质量的度量来确定。现实世界示例的排名准确度度量被选择为“<a class="ae ky" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain" rel="noopener ugc nofollow" target="_blank">NDCG</a>”(<em class="mq">正常贴现累积收益</em>)，这是用于评估特定排名集合的有效性的流行方法。NDCG得出一个介于0和1之间的结果，1代表项目的最佳排序。在训练了六个不同的排名函数(包括RankNet的两个不同实现)之后，对于搜索结果的给定查询/文档特征向量，在测试集上获得了以下结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e1f832017dc2d9fc937c9e07845fc618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*ibNA__R3Y7LBG8cxWkLf4Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片:RankNet与其他系统的比较。(礼貌:<a class="ae ky" href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf" rel="noopener ugc nofollow" target="_blank">学习使用梯度下降法排名</a>)</p></figure><p id="c784" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，我们看到RankNet(一层和两层)的平均NDCG明显高于其他排名函数。</p><h1 id="5090" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">未来的改进</h1><p id="94dd" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">Burges和他的同事进一步将RankNet的概念发展成训练时间更短、精度更高的模型。</p><h2 id="5693" class="nr ms it bd mt ns nt dn mx nu nv dp nb lp nw nx nd lt ny nz nf lx oa ob nh oc bi translated"><a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">λrank</strong>T9】</a></h2><p id="e4ee" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">在原始RankNet的训练过程中，发现不需要计算成本本身。相反，成本的梯度足以确定这对项目的预测排名。这可以被视为指示特定项目的移动方向的箭头。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/738485bfd1a7b44d0d65afe0d93d3e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*yn_nL_mbOkdS9PYRiuoB_w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像:成本梯度的可视化，用箭头指示移动方向。(承蒙:<a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf" rel="noopener ugc nofollow" target="_blank">从兰克内到兰达兰克到兰达玛特:概述</a>)</p></figure><p id="1a57" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，当他们通过交换文件导致的NDCG变化来放大这个梯度时，他们获得了更好的结果。因此，LambaRank被证明训练速度更快，准确性也有所提高。</p><h2 id="42cc" class="nr ms it bd mt ns nt dn mx nu nv dp nb lp nw nx nd lt ny nz nf lx oa ob nh oc bi translated"><a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LambdaMART_Final.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">λmart</strong></a></h2><p id="7cca" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">在另一个版本中，引入了<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting#:~:text=Gradient%20boosting%20is%20a%20machine,prediction%20models%2C%20typically%20decision%20trees." rel="noopener ugc nofollow" target="_blank">渐变增强的</a>树版本的LambaRank】。在实验数据集中，这已被证明比前两个模型更加准确。</p><h1 id="e6dd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结论</h1><p id="40ae" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">因此，我们已经看到了一些最先进的排序技术，当我们想要在信息检索系统中对一组项目进行排序时，这些技术非常有用。所以问题来了，是什么阻止我们去实现这些模型呢？答案很简单——<strong class="li iu">没事</strong>！</p><p id="3418" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mq">(对于感兴趣的人，我自己使用Keras和TensorFlow实现的RankNet可以在</em><a class="ae ky" href="https://github.com/devanshgoenka97/RankNet" rel="noopener ugc nofollow" target="_blank">https://github.com/devanshgoenka97/RankNet</a><em class="mq">)</em></p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="07c9" class="mr ms it bd mt mu oe mw mx my of na nb jz og ka nd kc oh kd nf kf oi kg nh ni bi translated">参考</h1><p id="f3bb" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated"><a class="ae ky" href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf" rel="noopener ugc nofollow" target="_blank">【1】:Burges，c .，Shaked，t .，Renshaw，e .，Lazier，a .，Deeds，m .，Hamilton，n .，&amp;Hu lender，G. (2005)。<strong class="li iu">学习使用梯度下降进行排序</strong>。<em class="mq">ICML 05年</em> </a> <em class="mq">。</em></p><p id="cb43" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf" rel="noopener ugc nofollow" target="_blank">【2】:布格斯，c，拉格诺，r . j .&amp;勒，Q.V. (2007)。<strong class="li iu">学习用非光滑代价函数排序</strong>。</a></p><p id="3bdb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LambdaMART_Final.pdf" rel="noopener ugc nofollow" target="_blank">【3】:吴，q，Burges，c，Svore，k，&amp;高，J. (2009)。<strong class="li iu">适应信息检索措施的助推</strong>。<em class="mq">信息检索，13 </em>，254–270。</a></p><p id="4b9b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf" rel="noopener ugc nofollow" target="_blank">【4】:布格斯，C. (2010)。<strong class="li iu">从兰克内到兰达兰克再到兰达玛特:概述。</strong> </a></p></div></div>    
</body>
</html>