<html>
<head>
<title>Keeping up with the BERTs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">跟上贝茨</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766?source=collection_archive---------12-----------------------#2020-10-26">https://towardsdatascience.com/keeping-up-with-the-berts-5b7beb92766?source=collection_archive---------12-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c0e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">NLP镇最受欢迎的家庭</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/354c3f6ca741d99f2e8ddeece93d6856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CpbAKL6DzAEAKTxW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">莱昂纳多·大久保俊郎在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bb2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你稍微关注一下NLP世界，或者甚至是ML新闻，你最有可能遇到Google的BERT模型或者它的一个亲戚。如果您还没有并且仍然偶然看到这篇文章，那么让我荣幸地向您介绍BERT——强大的NLP怪兽。</p><h1 id="1549" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">伯特是什么？</h1><p id="5ba8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">BERT代表<strong class="ky ir"><em class="mp">B</em></strong><em class="mp">I directional</em><strong class="ky ir"><em class="mp">E</em></strong><em class="mp">n coder</em><strong class="ky ir"><em class="mp">R</em></strong><em class="mp">表示来自</em><strong class="ky ir"><em class="mp">T</em></strong><em class="mp">transformers</em>，是Google的一个语言表示模型。它使用两个步骤，预训练和微调，为广泛的任务创建最先进的模型。</p><p id="0904" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它与众不同的特点是跨不同下游任务的统一架构——这些是什么，我们将很快讨论。这意味着相同的预训练模型可以针对各种最终任务进行微调，这些最终任务可能与训练的任务模型不相似，并给出接近最先进的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/e7be39e8160ec25a76fee59ad2b1304d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGg2gOPc-gzfvDofWDIbqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BERT由两个步骤组成。来源:<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><p id="407f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，我们首先在预训练任务上同时训练模型。一旦预训练完成，同一模型可以针对各种下游任务进行微调。请注意，单独的模型针对特定的下游任务进行了微调。因此，单个预训练模型可以在微调后生成多个下游任务特定模型。</p><h1 id="d422" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">伯特建筑</h1><p id="4b86" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">简单来说就是<strong class="ky ir"> <em class="mp">一堆变压器的编码器</em> </strong>。你可以在<a class="ae kv" rel="noopener" target="_blank" href="/transformers-89034557de14">我之前的文章</a>中详细了解变形金刚。或者，如果你已经对它有了一些模糊的想法，请查看<a class="ae kv" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder" rel="noopener ugc nofollow" target="_blank">这个</a>绝对是BERT中使用的编码器块的3D炸弹图。说真的，你不能错过这个！</p><div class="mr ms gp gr mt mu"><a rel="noopener follow" target="_blank" href="/transformers-89034557de14"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">变形金刚(电影名)</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">或者我喜欢称之为类固醇引起的注意。💉💊</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kp mu"/></div></div></a></div><p id="68e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来看看一些我们都不会记得的数字，但是如果没有它们，我们的理解会感到不完整，所以这里什么都不说:</p><p id="bbc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> L </strong> =层数(即堆栈中的#个变换器编码器块)。<br/> <strong class="ky ir"> H </strong> =隐藏大小(即<em class="mp"> q，k </em>和<em class="mp"> v </em>向量的大小)。<br/> <strong class="ky ir"> A </strong> =关注头数。</p><ul class=""><li id="15df" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated"><strong class="ky ir"> <em class="mp">伯特基</em> </strong> : L=12，H=768，A=12。<br/>总参数=110M！</li><li id="b0b2" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><strong class="ky ir"> <em class="mp">伯特大号</em> </strong> : L=24，H=1024，A=16。<br/>总参数=340M！！</li></ul><h2 id="464d" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">什么使得它是双向的？</h2><p id="f440" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们通常通过在一些不相关的任务上训练它来创建语言模型，但是这些任务有助于开发对模型中单词的上下文理解。这种任务通常包括预测下一个单词或彼此非常接近的单词。这种训练方法不能扩展并用于双向模型，因为它会让每个单词间接“看到自己”——当你从相反的方向再次接近同一个句子时，你已经知道会发生什么了。一个数据泄露的案例。</p><p id="0c7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，模型可以轻易地预测目标单词。此外，我们不能保证该模型，如果经过完全训练，在某种程度上已经学会了单词的上下文含义，而不仅仅是专注于优化琐碎的预测。</p><p id="a176" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么伯特是如何做到双向预训练的呢？它通过使用一个称为屏蔽LM的过程来实现这一点。稍后会有更多的细节，所以请继续读下去，我的朋友。</p><h1 id="84ce" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预训练伯特</h1><p id="2768" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">BERT模型在以下两个无监督的任务上被训练。</p><h2 id="ef2d" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">1.掩蔽语言模型(MLM)</h2><p id="ad8d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">此任务启用模型的深度双向学习方面。在此任务中，随机屏蔽(用[<em class="mp">MASK</em>token替换)一定百分比的输入标记，模型尝试预测这些屏蔽的标记，而不是整个输入序列。然后，来自该模型的预测标记被馈送到词汇表上的输出softmax中，以获得最终的输出单词。</p><p id="ef3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这在预训练和微调任务之间产生了不匹配，因为后者在大多数下游任务中不涉及预测屏蔽词。这通过我们如何屏蔽输入令牌的微妙变化得以缓解。</p><p id="3ee4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大约15%的单词在训练时被屏蔽，但是所有被屏蔽的单词都不会被[<em class="mp">MASK</em>token替换。</p><ul class=""><li id="6a21" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">80%的时间用[ <em class="mp">屏蔽</em>令牌。</li><li id="b7ab" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">10%的时候用随机代币。</li><li id="5037" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">10%的时间，未改变的输入令牌被屏蔽。</li></ul><h2 id="e729" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">2.下一句预测(NSP)</h2><p id="1280" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">LM并不直接捕捉两个句子之间的关系，而这种关系在许多下游任务中是相关的，例如<a class="ae kv" rel="noopener" target="_blank" href="/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5">问答(QA)和自然语言推理(NLI) </a>。通过对二值化NSP任务的训练，该模型被教导句子关系。</p><p id="aa18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个任务中，选择了两个句子——A和B——进行预训练。</p><ul class=""><li id="8348" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">50%的时间B实际上是a后面的下一句话。</li><li id="e17b" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">50%的时间B是从语料库中随机抽取的句子。</li></ul><h2 id="b9bd" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">培训——输入和输出。</h2><p id="c33a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">该模型同时在上述两个任务上被训练。这是通过输入和输出的巧妙运用而成为可能的。</p><h2 id="0e17" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">输入</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/527599e41dcfbc09dd35d874634c037c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_d5e72O312RkQkasUJ3EnQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伯特的输入表示。来源:<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><p id="e552" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型需要接受单个句子或明确打包在一个标记序列中的两个句子的输入。作者注意到“句子”可以是连续文本的任意跨度，而不是实际的语言句子。一个[SEP]标记用于分隔两个句子，以及使用一个学习过的片段嵌入来指示一个标记作为片段A或b的一部分。</p><p id="6c8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">问题#1: </em>所有的输入都是在一个步骤中馈入的——与顺序馈入输入的RNNs相反，模型<strong class="ky ir"> <em class="mp">不能保持输入令牌的排序</em> </strong>。每种语言的词序都很重要，无论是语义上还是句法上。</p><p id="8ac9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">问题#2: </em>为了正确执行下一个句子预测任务，我们需要能够<strong class="ky ir"> <em class="mp">区分句子A和B </em> </strong>。固定句子的长度可能限制太多，并且是各种下游任务的潜在瓶颈。</p><p id="e9dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个问题都是通过向我们的原始令牌添加包含所需信息的嵌入并将结果用作我们的BERT模型的输入来解决的。以下嵌入被添加到令牌嵌入中:</p><ul class=""><li id="6128" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated"><strong class="ky ir"> <em class="mp">片段嵌入</em> </strong>:它们提供了特定标记所属句子的信息。</li><li id="88a3" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><strong class="ky ir"> <em class="mp">位置嵌入</em> </strong>:它们提供了单词在输入中的顺序信息。</li></ul><h2 id="d51d" class="nx lt iq bd lu ny nz dn ly oa ob dp mc lf oc od me lj oe of mg ln og oh mi oi bi translated">输出</h2><p id="a15e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如何同时预测两个不同任务的产出？答案是通过使用不同的FFNN + Softmax层，该层建立在来自最后一个编码器的输出的顶部，对应于期望的输入令牌。我们将最后一个编码器的输出称为最终状态。</p><p id="c9b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个输入令牌总是一个特殊分类<em class="mp">【CLS】</em>令牌。对应于该令牌的最终状态被用作分类任务的聚集序列表示，并被用于下一句预测，其中它被馈送到预测标签“<em class="mp"> IsNext </em>或“<em class="mp"> NotNext </em>”的概率的FFNN + Softmax层。</p><p id="750e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对应于[ <em class="mp"> MASK </em> ] tokens的最终状态被输入FFNN+Softmax，以从我们的词汇表中预测下一个单词。</p><h1 id="76ec" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">微调伯特</h1><p id="d9d1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">通过交换适当的输入或输出，可以对各种下游任务进行微调。在一般情况下，为了训练特定于任务的模型，我们向现有的BERT添加一个额外的输出层，并对结果模型进行微调——所有参数，端到端。增加输入/输出层而不改变BERT模型的一个积极结果是，只需要从头开始学习最少数量的参数，使得过程快速、成本和资源高效。</p><p id="7bd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了让你了解它的速度和效率，作者声称，从完全相同的预训练模型开始，论文中的所有结果在<em class="mp">单云TPU </em>上最多可以在<em class="mp">1小时</em>内复制，在GPU 上最多可以在<em class="mp">几小时内复制。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/914db5d7c90c535151c8397aa638f68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKcrS0VO_ZBEysVh-AvL7w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在各种下游任务上微调BERT。来源:<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><p id="dfee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在句子对分类和单句分类中，对应于[ <em class="mp"> CLS </em>记号的最终状态被用作进行预测的附加层的输入。</p><p id="81ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在QA任务中，在微调期间引入了开始(S)和结束(E)向量。问题作为句子A输入，答案作为句子b输入。单词<em class="mp"> i </em>作为答案区间开始的概率计算为T <em class="mp"> i </em>(对应于第<em class="mp"> i </em>个输入标记的最终状态)和S(开始向量)之间的点积，后面是段落中所有单词的softmax。端跨采用类似的方法。<br/>从位置<em class="mp"> i </em>到位置<em class="mp"> j </em>的候选区间的得分定义为S T <em class="mp"> i </em> + E T <em class="mp"> j </em>，以<em class="mp"> j </em> ≥ <em class="mp"> i </em>的最大得分区间作为预测</p><h1 id="4811" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">GPT——远房表亲</h1><p id="a158" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">伯特是产生这些突破性成果的唯一模型吗？不是。OpenAI的另一个模型，叫做GPT，已经在网上引起了轰动。</p><p id="0fa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是很多人没有意识到这两个模型有一个共同点，那就是这两个模型都重用了一个Transformer组件。如前所述<strong class="ky ir"> BERT将变压器的编码器部分</strong>作为其构建模块。同时，<strong class="ky ir"> GPT使用变压器的解码器部分</strong>作为其构建模块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/0a9cd8deebf7597ef151389b28fe67ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAjV_FHxqONhyFvxO_sBxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文。</a></p></figure><p id="398a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，由于编码器的双向自我关注，BERT中的双向连接。与此同时，GPT的连接只是单向的，从左到右，由于解码器的设计防止查看未来的预测——更多信息请参考<a class="ae kv" rel="noopener" target="_blank" href="/transformers-89034557de14">变形金刚</a>。</p><h1 id="a7de" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">伯特一家</h1><p id="3d1a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果我们不把一些工作得很好的东西拿来，并试图重新创造或修改它，那就不是21世纪了。伯特建筑也不例外。以下是一些最流行的变体:</p><ul class=""><li id="2e2a" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated"><a class="ae kv" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">阿尔伯特</strong> </a>由谷歌和更多-本文描述了参数减少技术，以降低内存减少和提高BERT模型的训练速度。</li><li id="8cf5" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">罗伯塔</strong> </a>脸书——本文认为最初的BERT模型训练不足，并表明经过更多的训练/调整，它可以超越最初的结果。</li><li id="c331" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" href="https://arxiv.org/abs/1904.09223" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> ERNIE </strong> </a>:百度通过知识整合增强表示——受BERT掩蔽策略启发，学习通过知识掩蔽策略增强的语言表示，包括实体级掩蔽和短语级掩蔽。</li><li id="a8f6" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">蒸馏伯特</strong></a>——使用Huggingface模型蒸馏的较小伯特。</li></ul><p id="6db1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在<a class="ae kv" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank"> the GLUE排行榜</a>查看更多伯特风格的模特。</p><h1 id="0dd3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><ol class=""><li id="f8fc" class="nj nk iq ky b kz mk lc ml lf om lj on ln oo lr op np nq nr bi translated">BERT是一个堆叠变压器的编码器模型。</li><li id="d485" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr op np nq nr bi translated">它有两个阶段——预训练和微调。</li><li id="db73" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr op np nq nr bi translated">预训练是计算和时间密集型的。</li><li id="8a6a" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr op np nq nr bi translated">然而，它独立于它最终完成的任务，因此相同的预训练模型可以用于许多任务。</li><li id="9d4d" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr op np nq nr bi translated">GPT和伯特没什么不同，是一个堆叠变压器的解码器模型。</li><li id="4c84" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr op np nq nr bi translated">伯特有许多变种。</li></ol></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><h1 id="e431" class="ls lt iq bd lu lv ox lx ly lz oy mb mc jw oz jx me jz pa ka mg kc pb kd mi mj bi translated">参考文献+推荐阅读</h1><ul class=""><li id="f4a4" class="nj nk iq ky b kz mk lc ml lf om lj on ln oo lr no np nq nr bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/transformers-89034557de14">变压器</a>——如果您想更深入地了解上述编码器/解码器架构。</li><li id="e989" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">论文——很容易阅读，而且他们也稍微阐述了一些实际的细节。值得一读。</li><li id="124c" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛的博客</a>。</li><li id="611d" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">官方GitHub回购。</a></li><li id="fd92" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/2019-year-of-bert-and-transformer-f200b53d05b9">更多BERT车型</a>。</li></ul></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><blockquote class="pc pd pe"><p id="8bac" class="kw kx mp ky b kz la jr lb lc ld ju le pf lg lh li pg lk ll lm ph lo lp lq lr ij bi translated">我很高兴你坚持到这篇文章结束。🎉我希望你的阅读体验和我写这篇文章时一样丰富。💖</p><p id="0509" class="kw kx mp ky b kz la jr lb lc ld ju le pf lg lh li pg lk ll lm ph lo lp lq lr ij bi translated"><em class="iq">尽请查看我的其他文章</em> <a class="ae kv" href="https://medium.com/@ria.kulshrestha16" rel="noopener"> <em class="iq">这里</em> </a> <em class="iq">。</em></p><p id="b711" class="kw kx mp ky b kz la jr lb lc ld ju le pf lg lh li pg lk ll lm ph lo lp lq lr ij bi translated"><em class="iq">如果你想联系我，我会选择</em><a class="ae kv" href="https://twitter.com/ree_____ree" rel="noopener ugc nofollow" target="_blank"><em class="iq">Twitter</em></a><em class="iq">。</em></p></blockquote></div></div>    
</body>
</html>