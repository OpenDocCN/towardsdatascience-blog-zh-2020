<html>
<head>
<title>Learning Advanced Mathematics behind Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习机器学习背后的高等数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-advanced-mathematics-behind-machine-learning-7582e0b3be1a?source=collection_archive---------16-----------------------#2020-10-02">https://towardsdatascience.com/learning-advanced-mathematics-behind-machine-learning-7582e0b3be1a?source=collection_archive---------16-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f24e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习机器学习高等数学的全面资源列表</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/31810a59d28308a3420e226026149f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cHO2B6El_p3X6jzW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@thoughtcatalog?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">思想目录</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d117" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学构成了大多数机器学习算法的基础。所以，掌握好数学对理解机器学习是势在必行的。而大多数数据科学家都知道线性代数、统计学等基本数学概念。但他们中的许多人并没有意识到一些深层的数学概念，这些概念可以帮助他们更清楚地了解一种算法是如何工作的，或者让他们了解机器学习的最新研究。</p><p id="b69d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我分享了高等数学课程的资源，这些资源有助于机器学习。本文讨论的主题是<strong class="lb iu">凸和非凸优化、信息论、概率图形模型等。</strong></p><p id="6152" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给出资源列表是为了假设读者熟悉基本概念，如<strong class="lb iu">线性代数、概率论、多变量微积分和多变量统计</strong>。理解这些基本主题对于理解本文中的高级课程中的内容是至关重要的。</p><p id="40f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还写了另一篇关于这些主题的文章，可以作为这篇文章的前身。请随意查看！</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/three-month-plan-to-learn-mathematics-behind-machine-learning-74335a578740"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">三个月计划学习机器学习背后的数学</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">为期3个月的计划，学习机器学习背后的数学知识</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="00e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中的资源可用于攻读博士学位，学生需要对与研究主题相关的数学概念有透彻的理解。</p><p id="6a12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该计划主要分为以下几个部分</p><ul class=""><li id="16b8" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated"><strong class="lb iu">凸优化</strong></li><li id="0074" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated"><strong class="lb iu">概率图形模型</strong></li><li id="ee4f" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated"><strong class="lb iu">非凸优化</strong></li><li id="c440" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated"><strong class="lb iu">信息论</strong></li></ul><p id="8190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个列表是永无止境的，但我在这里讨论的以下四个主题在机器学习中是必不可少的，并且可以高度移植到其他工程领域。</p><h1 id="c4a0" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">充分利用资源:</h1><ul class=""><li id="8843" class="mn mo it lb b lc nt lf nu li nv lm nw lq nx lu ms mt mu mv bi translated">建议不要听课，即使你知道内容。这样的讲座可以用来修改概念。</li><li id="cf37" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">一边看讲座一边做手写笔记。它有助于更好地理解和记忆概念。</li><li id="c176" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">练习书本或课程作业中的问题。如果有许多类似的问题，每种类型做1-2个问题。</li><li id="dbf9" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">在本书的每个讲座/章节之后，尝试推导所讨论的定理/证明，并相应地修改。</li></ul><h1 id="bdff" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">凸优化:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c37b4fa9e7c8271de7e44f24659892e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*fYfrRxzWH7D-bC-0lPWPlQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://theinf2.informatik.uni-jena.de/Lectures/Convex+Optimization.html" rel="noopener ugc nofollow" target="_blank">http://Thein F2 . informatik . uni-jena . de/Lectures/Convex+optimization . html</a></p></figure><p id="9eb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习中的大多数方法都是基于寻找使一些目标函数(如二进制交叉熵、均方误差等)最小化的最佳参数。由于这一事实，大多数机器学习问题可以被视为优化问题。使用凸优化，一些机器学习算法使分析变得简单，并确保解的唯一性。凸优化应用于许多工程领域，并且是任何工程学位的基本课题之一。</p><p id="6453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习该主题最经典的书是史蒂芬·博伊德博士和列文·范德伯格博士的<strong class="lb iu">凸优化书。</strong>世界各地的不同大学也提供非常棒的讲座。其中一些列举如下:</p><ul class=""><li id="9b71" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated">YouTube上<strong class="lb iu">史蒂芬·博伊德博士</strong>的演讲</li><li id="90f3" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">YouTube上的CMU 10–725讲座</li><li id="f81f" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated">【Joydeep Dutta教授在NPTEL 上的凸优化</li></ul><h1 id="0f02" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">概率图形模型:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/3e58564f1a0f98076128272effa51c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBY4ZIwvhgL9CpMy7Y_NJA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www2.cs.arizona.edu/~pachecoj/courses.html" rel="noopener ugc nofollow" target="_blank">https://www2.cs.arizona.edu/~pachecoj/courses.html</a></p></figure><p id="ee0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概率图形模型提供了一种数学方法来编码图形中随机变量之间的概率关系。通常，图中的节点是变量，边编码了它们之间的概率独立性或依赖性。在大多数现代机器学习研究中，许多机器学习模型是用图形模型定义的。该方法用于描述模型下的概率推理的结构。它也用于其他领域，如生物，金融等。</p><p id="20b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">达芙妮·柯勒和Nir Friedman的概率图形模型</strong>是学习PGM的标准书籍。除了这本书，还有一些关于PGMs的漂亮课程，列举如下:</p><ul class=""><li id="5805" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated">Tom Mitchell博士和Lella Wehbe博士的讲座。这是一门博士水平的课程。</li><li id="ceba" class="mn mo it lb b lc mw lf mx li my lm mz lq na lu ms mt mu mv bi translated"><strong class="lb iu">概率图形模型专业化</strong>在<strong class="lb iu"> Coursera </strong>由<strong class="lb iu">斯坦福</strong>由<strong class="lb iu">达弗·柯勒</strong>博士提供。</li></ul><h1 id="a47c" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">非凸优化:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/551c6093457ff6d6810c116b06f29602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ol_sMQBYUtbu5b8LcSd8rA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Rastrigin_function.png" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:rastri gin _ function . png</a></p></figure><p id="5b5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与凸优化相比，非凸优化是一个相对较新的领域，在神经网络等算法中非常重要。使用凸优化技术来解决非凸问题会导致局部最优而不是全局最优，这是深度学习模型中最大的问题之一。非凸问题有许多可行且非常平坦的区域，曲率变化很大，每个区域有几个鞍点。</p><p id="8d38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很少有足够的资源可用于非凸优化，因为它不是一个既定的领域，仍然是一个活跃的研究领域。最好的阅读资源是【Prateek Jain博士和Purushottam Kar博士的《<strong class="lb iu">机器学习的非凸优化</strong>》。没有任何课程详细介绍非凸优化。最好的视频资源是在<strong class="lb iu"> NIPS 2015非凸优化研讨会</strong>上的演讲。</p><h1 id="df7c" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">信息论:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/7d93827969d5d1b233e28bea659e9675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXCPMbEDw0walchMQ24xnA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">istock.com</p></figure><p id="d24b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息论与机器学习有着牢固的关系。许多机器学习算法中使用的交叉熵损失是信息论的直接应用。决策树也使用信息论的概念，如熵、互信息等。拆分树形数据结构中的节点。信息论还用于数据压缩、信号处理、信息检索、数据挖掘等。</p><p id="bb2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习信息论的经典书籍是<strong class="lb iu"> David MacKay </strong>的《<strong class="lb iu">信息论、推理和学习算法</strong>》。除了这本书，作者在剑桥大学还有一门名为“<strong class="lb iu">信息论、模式识别和神经网络</strong>的课程，和<strong class="lb iu">讲座</strong>可在<strong class="lb iu"> YouTube </strong>上获得。</p><h1 id="4b8e" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">下一步做什么？</h1><p id="602d" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li oc lk ll lm od lo lp lq oe ls lt lu im bi translated">完成上面提到的课程后，可以尝试学习更多与机器学习相关的数学课程。比如<strong class="lb iu">测度论、张量代数、数学建模</strong>等。都是这样的其他话题。</p><p id="e442" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了课程之外，你现在可以选择在顶级会议上发表的任何理论机器学习论文，如<strong class="lb iu"> NIPS、ICML、ICLR </strong>等。阅读并尝试复制论文的结果。从这个计划中学到的东西也可以用来在任何公司开始你的研究，或者开始攻读博士学位。</p></div></div>    
</body>
</html>