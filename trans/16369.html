<html>
<head>
<title>Racial Bias in BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特的种族偏见</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/racial-bias-in-bert-c1c77da6b25a?source=collection_archive---------31-----------------------#2020-11-11">https://towardsdatascience.com/racial-bias-in-bert-c1c77da6b25a?source=collection_archive---------31-----------------------#2020-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c49" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">BERT嵌入向量中不公正偏差的理解和可视化</h2></div><p id="a599" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习的公平性和偏见是一个越来越多地讨论的问题。随着迁移学习应用中使用的预训练语言模型的发展，理解预训练模型继承的不公正偏见至关重要。</p><p id="9273" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将重点关注英语BERT [1]，这是一个在多伦多图书语料库[2]和维基百科上训练的语言模型。为了检测模型中的偏见，我将使用Sweeney在谷歌搜索中显示种族偏见时使用的相同的一组黑人和白人、女性和男性的名字[3]。她作品中的名字基于Bertran等人[4]和Fryer和Levitt [5]。这篇文章的灵感来自雷切尔·托马斯的实用数据伦理课程。</p><h1 id="7a5c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">方法学</h1><p id="8e6d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">演示使用了<a class="ae lb" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸变压器</a>包和<code class="fe lz ma mb mc b">bert-base-cased</code>网络。下面的代码显示了网络的用法(基于Transformers GitHub页面的示例)。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mi mj l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">使用HuggingFace transformers的BERT模型——基于GitHub示例</p></figure><p id="9101" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个网络为句子中的每个标记产生768维向量。该研究测量如下:给定句子<em class="mo"> s </em>，对于记号<em class="mo"> t∈s </em>如果我们用掩蔽记号<code class="fe lz ma mb mc b">[MASK]</code>替换记号<em class="mo"> t </em>两个记号的嵌入向量彼此有多接近，<em class="mo">余弦_相似度(f(t)，f(</em><code class="fe lz ma mb mc b"><em class="mo">[MASK]</em></code><em class="mo">)</em>。</p><p id="a49c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">BERT的训练步骤试图为屏蔽的令牌识别正确的令牌，因此，该测量提供了对模型认为给定令牌可以替代<code class="fe lz ma mb mc b">[MASK]</code>的可能性的洞察。为了能够比较这些标量值，该研究调查了成对的标记。例如在句子<em class="mo">“艾米丽是个女人。”</em>，单个令牌字<em class="mo">女</em>换成令牌【面具】和令牌<em class="mo">男</em>。通过计算这两个分数，这项研究寻求了一个问题的答案，“这个模型更可能把单词<em class="mo">女</em>还是<em class="mo">男</em>放在句子的末尾？”</p><p id="f856" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lz ma mb mc b">m(p,q) = cosine_similarity(f(p),f(MASK))-cosine_similarity(f(q),f(MASK))</code></p><p id="37ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，实验将名字列表放入两种类型的句子中:<code class="fe lz ma mb mc b">&lt;NAME&gt; is &lt;ADJ&gt;.</code>和<code class="fe lz ma mb mc b">&lt;NAME&gt; is a &lt;NOUN&gt;.</code>，其中标记对是某种类型的词对，如<em class="mo">女人-男人</em>、<em class="mo">演员-女演员</em>或<em class="mo">富人-穷人</em>。</p><h1 id="05e1" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">每个名称的令牌数</h1><p id="fdc2" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">虽然没有必要运行这个实验，但我认为完整的图片包括一个关于名称表示所需的标记数量的注释。</p><p id="d3bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一方面，每个白人男性的名字用一个记号表示，而只有一个白人女性的名字用两个记号表示:<em class="mo"> Katelyn。</em>另一方面，黑人男性姓名的平均标记数为2.29，黑人女性姓名的平均标记数为2.35。只有两个黑人女性的名字用一个token代表，而且两个名字都有不同的含义:<em class="mo">肯尼亚</em>和<em class="mo">钻石</em>。这不是由名字的长度引起的:Nia这个名字只有两个字母，却用两个符号来表示！</p><p id="d79c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果一个人理解了BERT的tokenizer词块[6]背后的算法，她可以得出结论，这是数据集(书籍和维基百科)中黑人名字出现次数少的结果。这意味着</p><blockquote class="mp"><p id="b1c9" class="mq mr iq bd ms mt mu mv mw mx my la dk translated">伯特患有针对黑人名字的“T21偏见”。</p></blockquote><p id="cbec" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">为了评估实验结果，我们必须记住这一点。预训练的伯特可能没有足够的黑人名字样本来做我们将在下面看到的表示。</p><h1 id="045e" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">表象中想要的相似性</h1><p id="0a35" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">当研究人员发明单词的嵌入向量表示时，focus的目标是建立一个向量空间，其中相似的单词彼此靠近。记住这一点，第一个实验包括象征性的男女和演员对。一个合理的假设是，模特更有可能选择男<em class="mo">男</em>和男<em class="mo">演员</em>作为男性名字，女<em class="mo">女</em>和女<em class="mo">演员</em>作为女性名字。第一对句子是<code class="fe lz ma mb mc b">&lt;NAME&gt; is a [MASK].</code>、<code class="fe lz ma mb mc b">&lt;NAME&gt; is a man.</code>和<code class="fe lz ma mb mc b">&lt;NAME&gt; is a woman.</code>，第二对句子是<code class="fe lz ma mb mc b">&lt;NAME&gt; is an [MASK].</code>、<code class="fe lz ma mb mc b">&lt;NAME&gt; is an actor.</code>和<code class="fe lz ma mb mc b">&lt;NAME&gt; is an actress.</code>。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/e23de0a3f9e3b31514c5ff5037cb2a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFdPLwfnuBxsZemNVd4k8Q.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">男-女，男-女演员投影中的白人名字</p></figure><p id="ee8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在一个二维图形中表示白人的名字，我们可以沿着两个轴在视觉上将女性和男性的名字分开。由于所有的名字都在右上方的四分之一平面上，模型为每个名字确定了更靠近<em class="mo">男主角</em>和<em class="mo">男主角</em>的<code class="fe lz ma mb mc b">[MASK]</code>。因此，我建议在看数字时，应该分析相互比较的结果，而不是看原始数字。</p><p id="9fe1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个很好的例子，它符合人们对模型的期望。让我们把黑人的名字放在桌子上！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nl"><img src="../Images/d3b9ec20c3e5a583ab613ce45e39658d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPu6CEjdG80Waqt2FAbGCA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">男女演员投影中的所有名字</p></figure><p id="6f47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我想指出的是，在投影中，黑人的名字比白人的名字更接近。其次，尽管有一些例外，但演员-演员轴和上一个差不多(不过，没有以前那么容易画门槛线了)。最后，原来在男女轴心上的分离完全消失了。</p><blockquote class="mp"><p id="c890" class="mq mr iq bd ms mt mu mv mw mx my la dk translated">对白人名字的假设对黑人名字会失效。</p></blockquote><h1 id="4c12" class="lc ld iq bd le lf lg lh li lj lk ll lm jw nm jx lo jz nn ka lq kc no kd ls lt bi translated">不公正的偏见</h1><p id="71b3" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">虽然我们希望从模型中找到与性别相关的名词作为名字，但是在其他一些例子中，模型中的偏见是不必要的。比如用令牌对<em class="mo">贫富</em>和<em class="mo">对错</em>。</p><p id="2c5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一个实验中，我举例说明了句子对<em class="mo">“X是一个演员。”</em>——<em class="mo">“X是个女演员。”</em>和<em class="mo">“X有钱。”</em>——<em class="mo">“X差。”</em>。请注意，有一个象征性的区别，单词“an”在新句子中缺失。让我们看看白人的名字！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi np"><img src="../Images/f5c87356f18fecbca8fda96f8679eb24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y2_GvdM_EpF2Exy5osM_jQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">贫富、男女演员投影中的白人名字</p></figure><p id="1f35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个图中，我沿着男演员-女演员轴展示了贫富轴。虽然人们可以沿着演员轴看到前一节中讨论的可能的分离，但人们不能沿着贫富轴看到同样的分离。我觉得不错，模型在这种新的情况下不能区分男女名字，<em class="mo">穷富</em>句对(一切都更接近于<em class="mo">穷</em>尾)。这是我们应该从公平模型中期待的。让我们看看其他的名字！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nq"><img src="../Images/a8f3239e7096fbc3e7d049914249655c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRSK66W6p5ST9Ru0dIGe5A.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">所有的名字都在贫富、男女演员的投影中</p></figure><p id="074f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我想指出，许多黑人的名字在投影上的位置与白人的名字相似。有趣的是，尽管我之前有所期待，黑人名字比白人名字更接近富人阶层。但最重要的是，这句话里有很大一部分黑人名字的表现和白人名字完全不同！在男女演员轴上，最大的差距不到0.05，而在贫富轴上，拉坦亚和沙尼斯之间的差距大于0.1。计算黑白女性-男性的平均值，人们可以得出这样的结论:在贫富句子中，该模型使黑白女性名字之间的差异是男演员-女演员句子中白人女性和男性名字之间的差异的1.8倍。</p><blockquote class="mp"><p id="620a" class="mq mr iq bd ms mt mu mv mw mx my la dk translated">在伯特模型中，不公正的偏差大于期望的差异。</p></blockquote><figure class="ns nt nu nv nw mh gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nr"><img src="../Images/60203d8189a65412aa43428645eb0f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5KYqTYNfEIyfW_3bF97Tg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">所有的名字在对错，演员演员投影</p></figure><h1 id="1b6e" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">可能的解决方案</h1><p id="d760" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">如果我们详述前面的例子，我们可以发现关于名字有许多异常。这可能导致基于预先训练的BERT网络的模型中的遗传偏差。为了发展一个更公平的网络，我建议考虑以下几点:</p><ol class=""><li id="e954" class="nx ny iq kh b ki kj kl km ko nz ks oa kw ob la oc od oe of bi translated">在输入预训练的BERT模型之前，使用命名实体识别过滤名称并用<code class="fe lz ma mb mc b">[UNK]</code>标记替换它们。</li><li id="7623" class="nx ny iq kh b ki og kl oh ko oi ks oj kw ok la oc od oe of bi translated">用命名实体的特殊标记训练BERT模型，并将它们从原始文本中屏蔽掉。</li></ol><p id="bcb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我认为，如果想要使用预先训练好的模型建立一个公平的网络，理解BERT中的偏差是很重要的。我希望这个故事有助于实现这一目标。实验中使用的所有代码都在这里<a class="ae lb" href="https://colab.research.google.com/drive/1gGrSU9l2ShVk0-aSzohE9Amumh-0n-ge?usp=sharing" rel="noopener ugc nofollow" target="_blank"/>可用，我在下面附上了完整的名称表。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mi mj l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">《伯特》中的黑白男女姓名表征</p></figure><h1 id="7588" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考</h1><p id="9c94" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">[1] Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018年)。Bert:用于语言理解的深度双向转换器的预训练。<em class="mo"> arXiv预印本arXiv:1810.04805 </em>。</p><p id="76db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]朱、虞琨；基罗斯，瑞安；泽梅尔，有钱；萨拉赫胡季诺夫、鲁斯兰；乌尔塔松、拉克尔；安东尼奥·托雷巴；桑亚·菲德勒(2015年)。“对齐书籍和电影:通过观看电影和阅读书籍实现故事般的视觉解释”。第19-27页。<a class="ae lb" href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" rel="noopener ugc nofollow" target="_blank">arXiv</a>:<a class="ae lb" href="https://arxiv.org/abs/1506.06724" rel="noopener ugc nofollow" target="_blank">1506.06724</a>[<a class="ae lb" href="https://arxiv.org/archive/cs.CV" rel="noopener ugc nofollow" target="_blank">cs。CV </a>。</p><p id="24b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]斯威尼，L. (2013年)。在线广告投放中的歧视。<em class="mo">队列</em>，<em class="mo"> 11 </em> (3)，10–29。<a class="ae lb" href="https://arxiv.org/abs/1301.6822" rel="noopener ugc nofollow" target="_blank"> arXiv:1301.6822 </a></p><p id="f47c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] Bertrand M和Mullainathan s . Emily和Greg比Lakisha和Jamal更有就业能力吗？劳动力市场歧视的现场实验。NBER工作底稿№9873。2003年7月。<a class="ae lb" href="http://www.nber.org/papers/w9873" rel="noopener ugc nofollow" target="_blank">http://www.nber.org/papers/w9873</a>(截至2013年1月9日)。</p><p id="115e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] Fryer R和Levitt S .独特黑人姓名的原因和后果。经济学季刊。2004年8月第59卷第3期。<a class="ae lb" href="http://pricetheory.uchicago.edu/levitt/Papers/FryerLevitt2004.pdf" rel="noopener ugc nofollow" target="_blank">http://price theory . uchicago . edu/Levitt/Papers/fryerlevit 2004 . pdf</a>(截至2013年1月9日)。</p><p id="24f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6]吴，m .舒斯特，陈，z .乐，Q. V .，m .马切里，w .，… &amp;克林纳，J. (2016)。<a class="ae lb" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">谷歌的神经机器翻译系统:弥合人类和机器翻译之间的鸿沟。</a> <em class="mo"> arXiv预印本arXiv:1609.08144 </em>。</p></div></div>    
</body>
</html>