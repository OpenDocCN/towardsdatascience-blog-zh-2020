<html>
<head>
<title>A Guide to Deep Learning Layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习层指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-four-deep-learning-layers-225c93646e61?source=collection_archive---------34-----------------------#2020-11-16">https://towardsdatascience.com/a-guide-to-four-deep-learning-layers-225c93646e61?source=collection_archive---------34-----------------------#2020-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bea5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">完全连接，卷积，LSTM和注意力都说明和解释。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/91a18c2dfd75da30a6c29361c11e3a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*27ZogOll20Vb1b6g1jr3ew.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者照片-尼泊尔。</p></figure><p id="5dde" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这篇帖子是关于四个重要的神经网络层架构</strong>——机器学习工程师用来构建深度学习模型的构建模块:</p><ol class=""><li id="1275" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">全连接层，</li><li id="40cc" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">2D卷积层，</li><li id="bb23" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">LSTM层，</li><li id="5ae5" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">注意力层。</li></ol><p id="320f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于每一层，我们将了解:</p><ul class=""><li id="3fcf" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated"><strong class="kx ir">每层如何工作</strong>，</li><li id="cef0" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">每一层背后的<strong class="kx ir">直觉</strong>，</li><li id="762c" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">每层的<strong class="kx ir">感应偏置</strong>，</li><li id="e89d" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">每层的<strong class="kx ir">重要超参数</strong>是什么，</li><li id="8d56" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir">何时使用</strong>每层、</li><li id="8417" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir">tensor flow 2.0中各层如何编码</strong>。</li></ul><p id="5c10" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有代码示例都是使用Keras Functional API使用<code class="fe mg mh mi mj b">tensorflow==2.2.0</code>构建的。</p><h1 id="2fd4" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">背景——什么是感性偏差？</h1><p id="6c22" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">我在这篇文章中经常使用的一个术语是<strong class="kx ir">归纳偏差</strong>——一个在晚宴上听起来很聪明并给你的朋友留下深刻印象的有用术语。</p><p id="9302" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">归纳偏差是将假设硬编码到学习算法的结构中</strong>。这些假设使得该方法更专用，灵活性更差，但通常更有用。通过对数据结构的假设进行硬编码，我们可以在实践中学习用其他方法学不到的功能。</p><p id="b1f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">机器学习中的归纳偏差的例子包括最大限度(类别应由尽可能大的边界分开，在支持向量机中使用)和最近邻(在特征空间中靠近在一起的样本在同一类别中，在k-最近邻算法中使用)。</p><p id="86a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这是机器学习中常见的一课——一点点偏差是可以的</strong>(如果你用偏差来交换的话)。这也适用于强化学习，在强化学习中，通过蒙特卡罗返回的无偏逼近比自举时间差分方法表现更差。</p><h1 id="1076" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">1.全连接层</h1><p id="7494" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">也被称为密集或前馈层，<strong class="kx ir">全连接层是最通用的深度学习层</strong>。</p><p id="6384" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一层施加了我们层中最少的结构量。几乎在所有神经网络中都可以找到它——通常用于控制输出层的大小&amp;形状。</p><h1 id="9c5c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">全连接层是如何工作的？</h1><p id="3f12" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">全连接层的核心是人工神经元——这是1943年麦卡洛克&amp;皮特的阈值逻辑单元的远祖。</p><p id="5df3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人造神经元的灵感来自于我们大脑中的生物神经元，然而，人造神经元只是生物神经元复杂性的粗略近似。</p><p id="bf2f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人工神经元由三个连续的步骤组成:</p><h2 id="ff26" class="ni ml iq bd mm nj nk dn mq nl nm dp mu le nn no mw li np nq my lm nr ns na nt bi translated">1.输入的加权线性组合</h2><p id="36c8" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">不同层中节点之间的连接强度由权重控制，这些权重的形状取决于每一侧的节点层数。每个节点都有一个称为偏置的附加参数，可以用来独立于节点的输入来改变节点的输出。</p><p id="d837" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习权重和偏差——通常在现代机器学习中，反向传播用于找到这些权重的好值——好值是那些导致网络对看不见的数据有好的预测准确性的值。</p><h2 id="71d6" class="ni ml iq bd mm nj nk dn mq nl nm dp mu le nn no mw li np nq my lm nr ns na nt bi translated">2.所有加权输入的总和</h2><p id="0c78" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">在应用了权重和偏差之后，所有输入到神经元的输入被加在一起成为一个单一的数字。</p><h2 id="6883" class="ni ml iq bd mm nj nk dn mq nl nm dp mu le nn no mw li np nq my lm nr ns na nt bi translated">3.激活功能</h2><p id="4942" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">然后通过激活函数传递。最重要的激活功能有:</p><ul class=""><li id="1d76" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated"><strong class="kx ir">线性</strong> —输出不变，</li><li id="88c0" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir"> ReLu </strong> — $0$如果输入为负，否则输入不变</li><li id="428e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir"> Sigmoid </strong>将输入压缩到$(0，1)$</li><li id="4f8e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir"> Tanh </strong>将输入压缩到$(-1，1)$</li></ul><p id="7442" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">激活函数的输出被输入到下一层中的所有神经元(也称为节点或单元)。</p><p id="843f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这就是全连接层的名称来源——每个节点都完全连接到其</strong>之后的&amp;之前的层中的节点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/552f65024f634d7bd39367e5688ce8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HLtbwa8vU7qq7eer.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">具有ReLu激活功能的单个神经元——图片由作者提供。</em></p></figure><p id="b45f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于第一层，节点从送入网络的数据中获取输入(每个数据点都连接到每个节点)。对于最后一层，输出是网络的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/b240b71997a3b00fbbb075caad543f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hayii4RaRTzyGLjZ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">全连接层—图片作者。</em></p></figure><h1 id="2ca5" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">全连接层的直觉和归纳偏差是什么？</h1><p id="8b4c" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">全连接层中所有连接背后的直觉是对信息如何在网络中流动没有任何限制。是没有直觉的直觉。</p><p id="4705" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">全连接层不强加任何结构，也不对网络将执行的数据或任务做出任何假设。<strong class="kx ir">由完全连接的层构建的神经网络可以被认为是一块空白的画布</strong>。直觉告诉我们，不要强加任何结构，让网络解决所有问题。</p><p id="55eb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">正是这种结构的缺乏赋予了完全连接层(具有足够的深度&amp;宽度)的神经网络逼近任何函数的能力</strong>——这被称为通用逼近定理。</p><p id="d435" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一开始，逼近任何函数的能力听起来都很吸引人。如果一个完全连接的层可以学习任何东西，我们为什么需要任何其他架构？</p><p id="eeb4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">能够在理论上学习并不意味着我们可以在实践中学习。实际上，使用我们可用的数据和学习算法(如反向传播)来找到正确的权重可能是不切实际和不可实现的。</p><p id="35be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些实际挑战的解决方案是使用不太专业的层——这些层对预期要执行的数据和任务有所假设。<strong class="kx ir">这种专业化就是归纳偏差</strong>。</p><h1 id="7278" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么时候应该使用全连接层？</h1><p id="902b" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">全连接层是最通用的深度学习架构——除了深度之外，它对连接性没有任何限制。<strong class="kx ir">当您的数据没有可以利用</strong>的结构时使用它——如果您的数据是平面数组(常见于表格数据问题)。</p><p id="1c4b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当从平坦的环境观察中学习时，全连接层在强化学习中是常见的。例如，在2015年的信任区域策略优化(TRPO)论文中使用了具有单个全连接层的网络:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/fc9188b17aee16365afb8845f83753b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KZnUNsb-IzzkwGsJ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">一个全连接层被用来为强化学习算法TRPO提供动力— </em> <a class="ae ny" href="https://arxiv.org/abs/1502.05477" rel="noopener ugc nofollow" target="_blank"> <em class="nv">舒尔曼等人2015 </em> </a></p></figure><p id="6bec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">大多数神经网络在某处会有完全连接的层。在执行分类的卷积神经网络中，通常将它们作为倒数第二层和最后一层进行完全连接。</p><p id="9d1d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完全连接的输出层中的单元数量将等于类的数量，使用softmax激活函数创建类的分布。</p><h1 id="349a" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么超参数对全连接层很重要？</h1><p id="6b12" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">您通常会在全连接图层中设置的两个超参数是:</p><ol class=""><li id="4d38" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">单位数量，</li><li id="b471" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">激活功能。</li></ol><p id="2179" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">全连接层由多个节点(也称为单元)定义，每个节点都具有激活功能。虽然在不同的节点上可以有一个具有不同激活功能的层，但是大多数情况下，层中的每个节点都具有相同的激活功能。</p><p id="c870" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于隐藏层，激活函数最常见的选择是校正线性单元(ReLu)。对于输出层，正确的激活函数取决于网络预测的内容:</p><ul class=""><li id="d7ca" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated">回归，目标可以是正的或负的-&gt;线性(无激活)</li><li id="327f" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">回归，目标只能是正的-&gt; ReLu</li><li id="940e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">分类-&gt; Softmax</li><li id="f321" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">控制动作，限制在-1和1 -&gt; Tanh之间</li></ul><h1 id="878f" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">通过Keras Functional API使用完全连接的层</h1><p id="e238" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">下面是一个如何使用Keras functional API的全连接层的示例。</p><p id="7c95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们使用形状像图像的输入数据来显示完全连接层的灵活性。这要求我们在网络中使用一个<code class="fe mg mh mi mj b">Flatten</code>层。</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="2aa2" class="ni ml iq mj b gy od oe l of og">import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras import Input, Model<br/>from tensorflow.keras.layers import Dense, Flatten</span><span id="3ab1" class="ni ml iq mj b gy oh oe l of og">#  the least random of all random seeds<br/>np.random.seed(42)<br/>tf.random.set_seed(42)</span><span id="5a66" class="ni ml iq mj b gy oh oe l of og">#  dataset of 4 samples, 32x32 with 3 channels<br/>x = np.random.rand(4, 32, 32, 3)</span><span id="7783" class="ni ml iq mj b gy oh oe l of og">inp = Input(shape=x.shape[1:])<br/>hidden = Dense(8, activation='relu')(inp)<br/>flat = Flatten()(hidden)<br/>out = Dense(2)(flat)<br/>mdl = Model(inputs=inp, outputs=out)</span><span id="1f93" class="ni ml iq mj b gy oh oe l of og">mdl(x)<br/>"""<br/>&lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy=<br/>array([[ 0.23494382, -0.40392348],<br/>       [ 0.10658629, -0.31808627],<br/>       [ 0.42371386, -0.46299127],<br/>       [ 0.34416917, -0.11493915]], dtype=float32)&gt;<br/>"""</span></pre><h1 id="76b3" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">2.2D卷积层</h1><p id="04ad" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated"><strong class="kx ir">如果你必须选择一个架构作为深度学习中最重要的，很难忽略卷积</strong>(我在那里做了什么？).</p><p id="f536" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2012年ImageNet竞赛的获胜者AlexNet被许多人视为现代深度学习的开端。Alexnet是一个深度卷积神经网络，在GPU上训练。</p><p id="9672" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">卷积的另一个里程碑式的应用是1998年的Le-Net-5，这是一个由Yann LeCun开发的7层卷积神经网络，用于对手写数字进行分类。</p><p id="26fb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">卷积神经网络是现代深度学习的主力</strong>——它可以用于文本、音频、视频和图像。卷积神经网络可以用来对图像内容进行分类，识别人脸并为图像创建字幕。它们也很容易在GPU上并行化，这使得它们可以快速训练。</p><h1 id="5c3d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">卷积层的直觉和归纳偏差是什么？</h1><p id="5ee4" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">2D卷积层的灵感来自我们自己的视觉皮层。</p><p id="072e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在人工神经网络中使用卷积的历史可以追溯到几十年前的neocognitron，这是由Kunihiko Fukushima在1980年推出的一种架构，其灵感来自于Hubel &amp; Wiesel的工作，Hubel &amp; Wiesel在20世纪50年代表明，哺乳动物视觉皮层中的单个神经元是由视觉的小区域激活的。</p><p id="1bce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">卷积本身就是一种数学运算，常用于信号处理。<strong class="kx ir">卷积的一个很好的心理模型是在信号上滑动滤波器的过程，在每一点检查滤波器与信号的匹配程度</strong>。</p><p id="094f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个检查过程就是模式识别，是卷积背后的直觉——在更大的空间中的任何地方寻找小的空间模式。这正是我们视觉皮层的工作方式。</p><p id="2a0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这是卷积层的感应偏差——用于识别局部空间模式</strong>。</p><h1 id="7ba4" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">2D卷积层是如何工作的？</h1><p id="1dd2" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">2D卷积层由两个组件之间的交互来定义:</p><ol class=""><li id="e646" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">具有形状(高度、宽度、颜色通道)的3D图像，</li><li id="bb70" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">一个2D滤波器，有形状(高，宽)。</li></ol><p id="cc34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面我们定义了卷积的直觉是在更大的空间中寻找模式。<strong class="kx ir">在2D卷积层中，我们寻找的模式是过滤器，更大的空间是图像</strong>。</p><h1 id="0b6c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">过滤</h1><p id="6d4c" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated"><strong class="kx ir">卷积层由其滤波器定义</strong>。这些过滤器是经过学习的——它们相当于一个完全连接的层的权重。卷积神经网络的第一层中的过滤器检测简单的特征，例如线或边。在网络的更深处，过滤器可以检测更复杂的功能，帮助网络执行其任务。</p><p id="9b95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了进一步理解这些滤镜是如何工作的，让我们用一个小图像和两个滤镜来工作。卷积神经网络的基本操作是使用这些滤波器来检测图像中的模式，方法是执行逐元素乘法并将结果相加:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/537f546d6d58993728e0e8034f9e1b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JKjT50tGNp6q6Qqy.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">对一个小图像应用不同的滤镜——按作者分类的图像。</em></p></figure><p id="0f1f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">在整个图像上重复使用相同的过滤器允许在图像的任何部分检测到特征——这种特性被称为平移不变性</strong>。该属性非常适合于分类-无论猫出现在图像中的什么位置，您都希望检测到它。</p><p id="2116" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于较大的图像(通常是<code class="fe mg mh mi mj b">32x32</code>或更大)，执行相同的基本操作，过滤器通过整个图像。该操作的输出充当网络已经学习的过滤器的特征检测，产生2D特征图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/fab8033a70801ef010a2647c779eb57c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l7_7jdPIzgvoRSXO.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">通过对图像进行卷积生成过滤图的过滤器——作者的图像。</em></p></figure><p id="2014" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由每个过滤器产生的特征图被连接，产生3D体积(第三维的长度是过滤器的数量)。</p><p id="95b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，下一层使用一组新的学习滤波器对这个新的体积执行卷积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/67c564cba94290933d4c3c02ffdde7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KV54MsHbHUapo-yp.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">多个过滤器的特征映射被连接以产生体积，该体积被传递到下一层——由作者生成的图像。</em></p></figure><h1 id="641b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">使用Keras函数API构建的2D卷积神经网络</h1><p id="a057" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">以下是如何将2D卷积图层与Keras functional API结合使用的示例:</p><ul class=""><li id="5709" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated">密集层之前的<code class="fe mg mh mi mj b">Flatten</code>层，用来展平我们2D卷积层产生的体积，</li><li id="d4f4" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><code class="fe mg mh mi mj b">8</code>的<code class="fe mg mh mi mj b">Dense</code>层大小——这控制了我们的网络可以预测多少类。</li></ul><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="0ae0" class="ni ml iq mj b gy od oe l of og">import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras import Input, Model<br/>from tensorflow.keras.layers import Dense, Flatten, Conv2D</span><span id="2dfe" class="ni ml iq mj b gy oh oe l of og">np.random.seed(42)<br/>tf.random.set_seed(42)</span><span id="a1cc" class="ni ml iq mj b gy oh oe l of og">#  dataset of 4 images, 32x32 with 3 color channels<br/>x = np.random.rand(4, 32, 32, 3)</span><span id="2d88" class="ni ml iq mj b gy oh oe l of og">inp = Input(shape=x.shape[1:])<br/>conv = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(inp)<br/>flat = Flatten()(conv)<br/>feature_map = Dense(8, activation='relu')(flat)<br/>out = Dense(2, activation='softmax')(flat)<br/>mdl = Model(inputs=inp, outputs=out)</span><span id="58ba" class="ni ml iq mj b gy oh oe l of og">mdl(x)<br/>"""<br/>&lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy=<br/>array([[-0.39803684, -0.08939186],<br/>       [-0.48165476, -0.28876644],<br/>       [-0.32680377, -0.24380796],<br/>       [-0.45394567, -0.28233868]], dtype=float32)&gt;<br/>"""</span></pre><h1 id="7bc1" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">对于卷积层，哪些超参数是重要的？</h1><p id="2de5" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">卷积层中的重要超参数有:</p><ul class=""><li id="7bad" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated">过滤器的数量，</li><li id="d86c" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">过滤器尺寸，</li><li id="e4ff" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">激活功能，</li><li id="7dae" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">大步流星，</li><li id="1efa" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">填充，</li><li id="926e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">扩张速度。</li></ul><p id="0096" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">过滤器的数量决定了每层可以学习多少个模式。过滤器的数量随着网络的深度而增加是很常见的。滤镜大小一般设置为<code class="fe mg mh mi mj b">(3, 3)</code>，用一个ReLu作为激活函数。</p><p id="ed70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">步幅可用于跳过卷积中的步骤，从而生成更小的特征地图。填充可用于使图像边缘的像素看起来像是在图像的中间。膨胀允许过滤器在图像的更大区域上操作，同时仍然产生相同大小的特征图。</p><h1 id="1d13" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么时候应该使用卷积层？</h1><p id="a792" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">当数据具有空间结构时，卷积起作用，例如，图像具有高度和宽度的空间结构。您也可以使用傅里叶变换等技术从1D信号中获得这种结构，然后在频域中执行卷积。</p><p id="8abf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">如果你在处理图像，卷积是王道</strong>。虽然有工作将基于注意力的模型应用于计算机视觉，但由于它与我们自己的视觉皮层相似，卷积很可能在未来许多年都是相关的。</p><p id="4339" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，除了空间结构，数据还能有其他什么结构呢？许多类型的数据都有顺序结构，这推动了我们下一个两层架构的发展。</p><p id="f5f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的第三层是LSTM，或长短期记忆层。LSTM是递归的— <strong class="kx ir">它按顺序处理数据</strong>。</p><p id="01dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">递归允许网络体验数据的时间结构，例如句子中的单词或一天中的时间。</p><p id="ae5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正常的神经网络接收单个输入张量$x$并生成单个输出张量$y$。递归结构在两个方面不同于非递归神经网络:</p><ol class=""><li id="73b9" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">输入$x$和输出$y$数据都被作为一系列时间步长进行<strong class="kx ir">处理，</strong></li><li id="58d4" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">网络具有<strong class="kx ir">记忆</strong>信息并将其传递到下一时间步的能力。</li></ol><p id="b52c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">递归架构的内存被称为<strong class="kx ir">隐藏状态</strong> $h$。网络选择在隐藏状态下转发什么由网络学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/e16151f07e52a54896eaa3fb837172e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0lMRDPco0DXyO7p4.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">一个递归神经网络——图片作者。</em></p></figure><h1 id="c3ab" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">输入时间步长维度</h1><p id="9fcc" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">使用递归架构需要熟悉时间步长的概念——知道如何正确地形成数据是使用递归的一半。</p><p id="e38a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们有输入数据‘x’，这是一个整数序列<code class="fe mg mh mi mj b">[0, 0] -&gt; [2, 20] -&gt; [4, 40]</code>。如果我们使用全连接层，我们可以将此数据以平面阵列的形式呈现给网络:</p><p id="1f80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然序列对我们来说是显而易见的，但对完全连接的层来说却不明显。</p><p id="f446" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">一个完全连接的层所看到的只是一个数字列表——网络需要学习其顺序结构</strong>。</p><p id="e5e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过添加一个时间步长维度来重构我们的数据“x ”,以明确地模拟这种顺序结构。</p><p id="d7ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">我们数据中的值不会改变，只有形状会改变</strong>:</p><p id="8d34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">我们的数据‘x’现在由三个维度构成</strong> — <code class="fe mg mh mi mj b">(batch, timesteps, features)</code>。递归神经网络将一次一个时间步长地处理特征，体验数据的顺序结构。</p><p id="f5b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们已经了解了如何构建用于递归神经网络的数据，我们就可以从更高的层面来了解LSTM层是如何工作的。</p><h1 id="07df" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">LSTM图层是如何工作的？</h1><p id="6ac8" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">LSTM于1997年首次推出，并形成了基于现代序列的深度学习模型的主干，擅长于机器翻译等具有挑战性的任务。多年来，机器翻译的最新技术是seq2seq模型，它由LSTM提供支持。</p><p id="89ed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">LSTM是一种特殊的a型递归神经网络。<strong class="kx ir">LSTM解决了普通递归神经网络难以应对的挑战——长期思考的能力</strong>。</p><p id="8b2f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在递归神经网络中，传递到下一个时间步骤的所有信息必须适合单个通道，即隐藏状态“h”。</p><p id="619a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">LSTM通过使用两种隐藏状态</strong>来解决长期记忆问题，这两种隐藏状态被称为隐藏状态“h”和单元状态“c”。拥有两个通道可以让LSTM人同时进行长期和短期记忆。</p><p id="d184" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在内部，LSTM使用三个关口来控制信息流:</p><ol class=""><li id="8c94" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">忘记gate来确定删除什么信息，</li><li id="2700" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">输入门来决定要记住什么，</li><li id="90f8" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">输出门来决定预测什么。</li></ol><p id="eed7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用LSTMs的一个重要架构是seq2seq。通过编码器LSTM馈送源句子，以生成固定长度的上下文向量。第二解码器LSTM获取该上下文向量并生成目标句子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/152a1c9e05e94144752192a93d1fe0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dKC87Cd-wOWSMbAC.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">seq 2 seq型号—图片作者。</em></p></figure><p id="18fb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了更深入地了解LSTM的内部，看看colah博客中对LSTM网络的精彩理解。</p><h1 id="1947" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">LSTM的直觉和归纳偏差是什么？</h1><p id="0db7" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">对于LSTM层来说，一个很好的初始模型是将其视为一个数据库。<strong class="kx ir">输出、输入和删除门允许LSTM像数据库</strong>一样工作——匹配REST API的<code class="fe mg mh mi mj b">GET</code>、<code class="fe mg mh mi mj b">POST</code>、&amp;、<code class="fe mg mh mi mj b">DELETE</code>，或者CRUD应用的<code class="fe mg mh mi mj b">read-update-delete</code>操作。</p><p id="5b8b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">遗忘门就像一个<code class="fe mg mh mi mj b">DELETE</code>，允许LSTM删除无用的信息。输入门的作用就像一个<code class="fe mg mh mi mj b">POST</code>，LSTM可以选择要记忆的信息。输出门的作用类似于一个<code class="fe mg mh mi mj b">GET</code>，其中LSTM选择向用户的信息请求返回什么。</p><p id="429f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">递归神经网络具有将数据作为序列处理和存储记忆的归纳偏向。LSTM在这种偏向的基础上又增加了一个长期和一个短期记忆通道。</p><h1 id="5f3d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">通过Keras函数式API使用LSTM图层</h1><p id="3920" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">以下是如何将LSTM图层与Keras功能API结合使用的示例:</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="cada" class="ni ml iq mj b gy od oe l of og">import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras import Input, Model<br/>from tensorflow.keras.layers import Dense, LSTM, Flatten</span><span id="df57" class="ni ml iq mj b gy oh oe l of og">np.random.seed(42)<br/>tf.random.set_seed(42)</span><span id="7276" class="ni ml iq mj b gy oh oe l of og">#  dataset of 4 samples, 3 timesteps, 32 features<br/>x = np.random.rand(4, 3, 32)</span><span id="0258" class="ni ml iq mj b gy oh oe l of og">inp = Input(shape=x.shape[1:])<br/>lstm = LSTM(8)(inp)<br/>out = Dense(2)(lstm)<br/>mdl = Model(inputs=inp, outputs=out)<br/>mdl(x)</span><span id="3569" class="ni ml iq mj b gy oh oe l of og">"""<br/>&lt;tf.Tensor: shape=(4, 2), dtype=float32, numpy=<br/>array([[-0.06428523,  0.3131591 ],<br/>       [-0.04120642,  0.3528567 ],<br/>       [-0.04273851,  0.37192333],<br/>       [ 0.03797218,  0.33612275]], dtype=float32)&gt;<br/>"""</span></pre><p id="90ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您会注意到，四个样本中的每一个都只有一个输出——其他两个时间步长在哪里？为了得到这些，我们需要使用<code class="fe mg mh mi mj b">return_sequences=True</code>:</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="de01" class="ni ml iq mj b gy od oe l of og">tf.random.set_seed(42)<br/>inp = Input(shape=x.shape[1:])<br/>lstm = LSTM(8, return_sequences=True)(inp)<br/>out = Dense(2)(lstm)<br/>mdl = Model(inputs=inp, outputs=out)<br/>mdl(x)</span><span id="3681" class="ni ml iq mj b gy oh oe l of og">"""<br/>&lt;tf.Tensor: shape=(4, 3, 2), dtype=float32, numpy=<br/>array([[[-0.08234972,  0.12292314],<br/>        [-0.05217044,  0.19100665],<br/>        [-0.06428523,  0.3131591 ]],</span><span id="3063" class="ni ml iq mj b gy oh oe l of og">[[ 0.0381453 ,  0.26402596],<br/>        [ 0.04725918,  0.34620702],<br/>        [-0.04120642,  0.3528567 ]],</span><span id="f814" class="ni ml iq mj b gy oh oe l of og">[[-0.21114576,  0.08922277],<br/>        [-0.02972354,  0.24037611],<br/>        [-0.04273851,  0.37192333]],</span><span id="d3e7" class="ni ml iq mj b gy oh oe l of og">[[-0.06888272, -0.01702049],<br/>        [ 0.0117887 ,  0.10608622],<br/>        [ 0.03797218,  0.33612275]]], dtype=float32)&gt;<br/>"""</span></pre><p id="2047" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">想要访问LSTM的隐藏状态也很常见——这可以使用参数<code class="fe mg mh mi mj b">return_state=True</code>来完成。</p><p id="4822" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在得到了三个张量——网络的输出，LSTM隐藏态和LSTM晶胞态。隐藏状态的形状等于LSTM中的单位数:</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="98b9" class="ni ml iq mj b gy od oe l of og">tf.random.set_seed(42)<br/>inp = Input(shape=x.shape[1:])<br/>lstm, hstate, cstate = LSTM(8, return_sequences=False, return_state=True)(inp)<br/>out = Dense(2)(lstm)<br/>mdl = Model(inputs=inp, outputs=[out, hstate, cstate])<br/>out, hstate, cstate = mdl(x)</span><span id="338d" class="ni ml iq mj b gy oh oe l of og">print(hstate.shape)<br/># (4, 8)</span><span id="9d46" class="ni ml iq mj b gy oh oe l of og">print(cstate.shape)<br/># (4, 8)</span></pre><p id="66e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您想在每个时间步访问隐藏状态，那么您可以将这两个结合起来，同时使用<code class="fe mg mh mi mj b">return_sequences=True</code>和<code class="fe mg mh mi mj b">return_state=True</code>。</p><h1 id="b13d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">哪些超参数对LSTM图层很重要？</h1><p id="8ed3" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">对于LSTM图层，主要的超参数是单位数。单元的数量将决定层的容量和隐藏状态的大小。</p><p id="9d48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然不是超参数，但在处理LSTMs时包含梯度裁剪会很有用，以处理可能随时间反向传播而发生的爆炸梯度。使用较低的学习速率来帮助管理梯度也是常见的。</p><h1 id="e737" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么时候我应该使用LSTM层？</h1><p id="9a5f" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">处理序列数据时，LSTM(或其近亲GRU)是一种常见的选择。LSTM的一个主要缺点是他们训练缓慢。</p><p id="d94c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是因为处理序列不容易并行化，因为<strong class="kx ir">误差信号必须通过时间</strong>反向传播。</p><p id="a899" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">LSTM的另一个有用特征是学习隐藏状态。这可以被其他模型用作未来的压缩表示——例如在2017年世界模型论文中。</p><p id="a7ee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">注意力是我们四层中最年轻的一层</strong>——目前深度学习阶段开发的唯一一层架构。</p><p id="fc7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">自2015年推出以来，注意力已经彻底改变了自然语言处理</strong>。</p><p id="b98d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先与基于LSTM的seq2seq模型结合使用，还关注为变压器提供动力——这是一种神经网络架构，形成了Open AI的GPT系列语言模型的主干。</p><p id="9ed4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">注意力很重要，因为它是一种有效的无递归序列模型</strong>——避免了随时间反向传播的需要，使并行化更容易，训练更快。</p><h1 id="2e5c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">注意层的直觉和归纳偏差是什么？</h1><p id="58d0" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">注意力是一个简单而强大的想法——当处理一个序列时，我们应该选择从序列的哪一部分获取信息。直觉很简单——序列的某些部分比其他部分更重要。</p><p id="5ac0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以机器翻译为例，将德语句子<code class="fe mg mh mi mj b">Ich bin eine Maschine</code>翻译成英语<code class="fe mg mh mi mj b">I am a machine</code>。</p><p id="4e8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在预测译文中的最后一个单词<code class="fe mg mh mi mj b">machine</code>时，我们所有的注意力都应该放在源句的最后一个单词<code class="fe mg mh mi mj b">Maschine</code>上。翻译这个单词时，查看源序列中较早的单词是没有意义的。</p><p id="d764" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们举一个更复杂的例子，把德语<code class="fe mg mh mi mj b">Ich habe ein bisschen Deutsch gelernt</code>翻译成英语<code class="fe mg mh mi mj b">I have learnt a little German</code>。在预测我们英语句子的第三个标记(<code class="fe mg mh mi mj b">learnt</code>)时，要注意德语句子的最后一个标记(<code class="fe mg mh mi mj b">gelernt</code>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/91684a1ac23710300b62ee8854de72ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8_p4-upkXHIepNELL7xrw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将德语翻译成英语时要注意——作者图片。</p></figure><p id="872b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么我们的注意层给了我们什么样的归纳偏差呢？<strong class="kx ir">注意力的一个归纳偏差是基于相似性的对齐</strong> —注意力层根据事物的相似程度选择看哪里。</p><p id="dc84" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">注意力的另一个诱导偏差是限制&amp;优先化信息流</strong>。正如我们将在下面看到的，softmax的使用迫使注意力层对信息流进行权衡——一个地方的权重越大，另一个地方的权重就越小。</p><p id="fff3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在完全连接的层中没有这样的限制，其中增加一个权重不会影响另一个。完全连接的层可以允许信息在后续层中的所有节点之间流动，并且理论上可以学习与关注层相似的模式。然而，我们现在知道，理论上不注意意味着它将在实践中发生。</p><h1 id="fb27" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">注意力层是如何工作的？</h1><p id="a7ec" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">注意层接收<strong class="kx ir">三个输入</strong>:</p><ol class=""><li id="1e90" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">查询</strong> =我们要找的东西，</li><li id="8157" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir"> key </strong> =我们比较查询的对象，</li><li id="8250" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">价值=我们所重视的东西。</li></ol><p id="f5dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意力层可以被认为是按顺序排列的<strong class="kx ir">三个机制</strong>:</p><ol class=""><li id="3199" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">查询和关键字的对齐</strong>(或相似度)</li><li id="e6dc" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir"> softmax </strong>将排列转换成概率分布</li><li id="2df1" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">根据对准选择按键</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/9b77c89f43696f0ada321e357f5f8db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XvRd8ggTiNdP13Fg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">关注层的三个步骤——对齐、softmax &amp;键选择——作者图片。</em></p></figure><p id="4598" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不同的注意层(如加性注意或点积注意)在对齐步骤中使用不同的机制。softmax &amp; key选择步骤对于所有关注层都是通用的。</p><h1 id="2cfd" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">查询、键和值</h1><p id="fb3c" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">同样，理解时间步长是理解递归神经网络的关键步骤，理解查询、键和值的含义是注意力的基础。</p><p id="08a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个很好的类比是Python字典。让我们从一个简单的例子开始，我们:</p><ul class=""><li id="10e6" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated"><strong class="kx ir">查询<code class="fe mg mh mi mj b">dog</code>的</strong></li><li id="eb89" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated"><strong class="kx ir">分别与<code class="fe mg mh mi mj b">dog</code>或<code class="fe mg mh mi mj b">cat</code>的键</strong>和<code class="fe mg mh mi mj b">1</code>或<code class="fe mg mh mi mj b">2</code>相匹配</li><li id="e642" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">并且<strong class="kx ir">基于对<code class="fe mg mh mi mj b">dog</code>的查找来选择<code class="fe mg mh mi mj b">2</code>的值</strong></li></ul><p id="7791" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的例子中，我们为我们的查询<code class="fe mg mh mi mj b">'dog'</code>找到了一个精确的匹配。然而，在神经网络中，<strong class="kx ir">我们不是在与字符串打交道——我们是在与张量打交道</strong>。我们的查询、键和值都是张量:</p><p id="d33e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们的查询没有精确匹配— <strong class="kx ir">我们可以计算查询和关键字之间的相似度</strong>(即对齐)，并返回最接近的值，而不是使用精确匹配:</p><p id="8ef0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">小的技术细节——通常键被设置为与值相等。这仅仅意味着我们正在进行相似性比较的数量也是我们将关注的数量。</p><h1 id="4b96" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">注意机制</h1><p id="8376" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">到目前为止，我们知道关注层包括三个步骤:</p><ol class=""><li id="e8db" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">基于相似性的比对</strong>，</li><li id="168d" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir"> softmax </strong>创建注意力权重，</li><li id="996c" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">根据关注度选择值</strong>。</li></ol><p id="ba4f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第二和第三步是所有注意力层共有的— <strong class="kx ir">差异都发生在第一步——相似性的比对是如何完成的</strong>。</p><p id="97fb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将简要介绍两种流行的机制——加法注意力和点积注意力。要更详细地了解这些机制，请查看优秀的<a class="ae ny" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">关注？立正！</a>王莉莲。</p><h1 id="1fdb" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">附加注意力</h1><p id="37f3" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">注意力的第一次使用(称为Bahdanau或附加注意力)解决了seq2seq模型的一个限制，即使用固定长度的上下文向量。</p><p id="4114" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如LSTM部分所解释的，seq2seq模型中的基本过程是将源句子编码成固定长度的上下文向量。问题是来自编码器的所有信息都必须通过固定长度的上下文向量。来自整个源序列的信息通过编码器和解码器之间的上下文向量被压缩。</p><p id="0a99" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在Bahdanau et。al 2015，附加注意力用于学习所有编码器隐藏状态和解码器隐藏状态之间的对准。随着序列被处理，这种对齐的输出在解码器中被用于预测下一个令牌。</p><h1 id="b156" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">点积注意力</h1><p id="0436" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">第二种类型的注意力是点积注意力——变压器中使用的对齐机制。点积注意力层不使用加法，而是使用矩阵乘法来度量查询和关键字之间的相似性。</p><p id="4c0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">点积的作用类似于键和值之间的相似性——下面是一个小程序，它绘制了随机数据的点积和余弦相似性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/6c993f3c763170037c308a001230c857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wwV9xL9p0Pfm9QwW.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">余弦相似度和随机向量点积之间的关系——作者创建。</em></p></figure><h1 id="ffdf" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">用Keras功能API实现单个注意力头</h1><p id="6874" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">点产品的关注是重要的，因为它形成了变压器的一部分。正如你在下图中看到的，变压器使用了多个缩放的点积注意力。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3fe24f9f930520211f745508c1347c6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/0*KtW80lekSUm9fu0N.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="nv">变形金刚中使用的多头注意力层— </em> <a class="ae ny" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人2017 </a></p></figure><p id="46d6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的代码演示了没有缩放的单个头的机制——有关Tensorflow 2中多头注意力层&amp;转换器的完整实现，请参见<a class="ae ny" href="https://www.tensorflow.org/tutorials/text/transformer" rel="noopener ugc nofollow" target="_blank">语言理解转换器模型</a>。</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="144e" class="ni ml iq mj b gy od oe l of og">import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras import Input, Model<br/>from tensorflow.keras.layers import Dense</span><span id="c663" class="ni ml iq mj b gy oh oe l of og">qry = np.random.rand(4, 16, 32).reshape(4, -1, 32).astype('float32')<br/>key = np.random.rand(4, 32).reshape(4, 1, 32).astype('float32')<br/>values = np.random.rand(4, 32).reshape(4, 1, 32).astype('float32')</span><span id="364f" class="ni ml iq mj b gy oh oe l of og">q_in = Input(shape=(None, 32))<br/>k_in = Input(shape=(1, 32))<br/>v_in = Input(shape=(1, 32))</span><span id="4b17" class="ni ml iq mj b gy oh oe l of og">capacity = 4<br/>q = Dense(4, activation='linear')(q_in)<br/>k = Dense(4, activation='linear')(k_in)<br/>v = Dense(4, activation='linear')(v_in)</span><span id="a8d1" class="ni ml iq mj b gy oh oe l of og">score = tf.matmul(q, k, transpose_b=True)<br/>attention = tf.nn.softmax(score, axis=-1)<br/>output = tf.matmul(attention, v)</span><span id="c6a6" class="ni ml iq mj b gy oh oe l of og">mdl = Model(inputs=[q_in, k_in, v_in], outputs=[score, attention, output])<br/>sc, attn, out = mdl([qry, key, values])<br/>print(f'query shape {qry.shape}')<br/>print(f'score shape {sc.shape}')<br/>print(f'attention shape {attn.shape}')<br/>print(f'output shape {out.shape}')<br/>"""<br/>query shape (4, 16, 32)<br/>score shape (4, 16, 1)<br/>attention shape (4, 16, 1)<br/>output shape (4, 16, 4)<br/>"""</span></pre><p id="9ab4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种架构也适用于不同长度的查询(现在是长度<code class="fe mg mh mi mj b">8</code>而不是<code class="fe mg mh mi mj b">16</code>):</p><pre class="kg kh ki kj gt nz mj oa ob aw oc bi"><span id="7521" class="ni ml iq mj b gy od oe l of og">qry = np.random.rand(4, 8, 32).reshape(4, -1, 32).astype('float32')<br/>sc, attn, out = mdl([qry, key, values])<br/>print(f'query shape {qry.shape}')<br/>print(f'score shape {sc.shape}')<br/>print(f'attention shape {attn.shape}')<br/>print(f'output shape {out.shape}')<br/>"""<br/>query shape (4, 8, 32)<br/>score shape (4, 8, 1)<br/>attention shape (4, 8, 1)<br/>output shape (4, 8, 4)<br/>"""</span></pre><h1 id="d139" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">注意层中哪些超参数是重要的？</h1><p id="f2e7" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">使用如上所示的注意力头时，需要考虑的超参数有:</p><ul class=""><li id="58c4" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq mf lx ly lz bi translated">用于转换查询、值和键的线性图层的大小</li><li id="c2e1" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">注意机制的类型(如加法或点积)</li><li id="24bd" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq mf lx ly lz bi translated">如何在softmax之前缩放对齐(通常使用层长度的平方根)</li></ul><h1 id="e418" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么时候应该使用关注层？</h1><p id="49eb" class="pw-post-body-paragraph kv kw iq kx b ky nc jr la lb nd ju ld le ne lg lh li nf lk ll lm ng lo lp lq ij bi translated">任何顺序问题都应该考虑注意层。与递归神经网络不同，它们可以很容易地并行化，从而加快训练速度。快速训练意味着更便宜的训练，或者相同计算量的更多训练。</p><p id="7486" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">转换器是一个没有递归的序列模型(它不使用LSTM)，允许它被有效地训练(避免随时间反向传播)。</p><p id="a4a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关注层的另一个好处是能够使用对齐分数来提高可解释性。</p><p id="3b1a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">我希望你喜欢这篇文章，并发现它很有用。</strong>下面是总结文章的简短表格:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/bbc00274b303e8b068f94fe408cef310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QajHOzFZxF_UwdMifET6mA.png"/></div></div></figure><p id="a9ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">感谢阅读！</strong>如果你喜欢这篇文章，请随时在<a class="ae ny" href="https://medium.com/@adgefficiency" rel="noopener"> Medium </a>上关注我，或者在<a class="ae ny" href="https://www.linkedin.com/in/adgefficiency/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</p><p id="cfac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请务必查看我的其他帖子:</p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/seven-steps-to-generate-data-science-project-ideas-8fb69400634d"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd ir gy z fp pa fr fs pb fu fw ip bi translated">产生数据科学项目想法的七个步骤</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">通过这七个步骤找到数据科学组合项目创意</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj kp ov"/></div></div></a></div><div class="os ot gp gr ou ov"><a href="https://betterprogramming.pub/should-you-be-using-pathlib-6f3a0fddec7e" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd ir gy z fp pa fr fs pb fu fw ip bi translated">应该用Python pathlib还是os？</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">两个Python路径库之间的决斗</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">better编程. pub</p></div></div><div class="pe l"><div class="pk l pg ph pi pe pj kp ov"/></div></div></a></div><div class="os ot gp gr ou ov"><a href="https://betterprogramming.pub/3-uncommon-bash-tricks-that-you-should-know-c0fc988065c7" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd ir gy z fp pa fr fs pb fu fw ip bi translated">你应该知道的3个不常见的Bash技巧</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">使用这些未充分利用的Bash模式，在终端上输入更少的内容</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">better编程. pub</p></div></div><div class="pe l"><div class="pl l pg ph pi pe pj kp ov"/></div></div></a></div></div><div class="ab cl pm pn hu po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="ij ik il im in"><p id="b717" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nh">最初发表于</em><a class="ae ny" href="https://adgefficiency.com/guide-deep-learning/" rel="noopener ugc nofollow" target="_blank"><em class="nh">【https://adgefficiency.com】</em></a><em class="nh">。</em></p></div></div>    
</body>
</html>