<html>
<head>
<title>Understanding Optical Flow &amp; RAFT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解光流和RAFT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-optical-flow-raft-accb38132fba?source=collection_archive---------14-----------------------#2020-09-26">https://towardsdatascience.com/understanding-optical-flow-raft-accb38132fba?source=collection_archive---------14-----------------------#2020-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ade1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何以迭代方式使用多尺度相关性求解光流</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ed91e988b5d9923a0b32c7e552d236e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGMjO4PtR0aSJky-m8SyBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="384f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算机视觉中最重要的问题是对应学习。即给定一个物体的两幅图像，如何从两幅图像中找到该物体对应的像素？</p><p id="9bb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主要针对视频的对应学习在目标检测和跟踪中有着广泛的应用。尤其是当视频中的对象被遮挡、变色、变形时。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="9d3a" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">什么是光流？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/b8016b5e827a81cc1d47c4dc7f10b0be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ghh5AfwYioPzlG6z0C6cpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两幅图像的光流场</p></figure><p id="393e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">光流是两幅图像之间的矢量场，显示了如何移动第一幅图像中的对象的像素以在第二幅图像中形成相同的对象。它是一种对应学习，因为如果知道一个物体对应的像素，就可以计算出光流场。</p><h2 id="e124" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">光流方程和传统方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/a6873fd2f130b0cf2a8ff12e69e11fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMFTk6XGTfTH3sMqKVzabA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两幅图像H和I之间的一点流</p></figure><p id="fdd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们采取最简洁的形式:两个图像之间的一点流。假设H中(x，y)处的像素流向I中的(x+u，y+v)，则光流矢量为(u，v)。</p><p id="4a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如何求解(u，v)？我们建立一些方程有什么约束吗？</p><p id="bab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，当H(x，y) = I(x+u，y+v)时，让我们用泰勒级数分解I(x+u，y+v):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/9bb22c218e67b6cbbaab199126bd4a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Y-SIxQul3Qd241JcE2YQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">I(x+u，y+v)的泰勒级数逼近</p></figure><p id="0885" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，舍弃高阶项，结合H(x，y) = I(x+u，y+v):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/daafebd1068472c69dab2ee1c102697d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tsxO-SB8mVOjiGPCQsALzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">光流方程的推导过程</p></figure><p id="7c49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在极限情况下，当u和v变为零时，我们得到的光流方程为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/62be64d1b651b32d8bbe40dcf3266709.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*o6tX37TLITU66gFPE61myw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">光流方程</p></figure><p id="853f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">然而，在实际应用中，u和v可能大或小，跨越几个到几十个像素，而不是零限制。因此，我们只能得到真实光流的近似。然而，如果u和v更接近于零，流场会更精确。</strong></p><p id="d081" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的等式中，未知数是u和v，因为其他变量可以从x，y和时间维度的差异中计算出来。因此，一个方程中有两个未知数，无法求解。因此，在过去的40年里，许多研究者试图提供u，v的其他方程组，使其可解。其中，最著名的方法是<a class="ae ky" href="https://www2.cs.duke.edu/courses/spring19/compsci527/papers/Lucas.pdf" rel="noopener ugc nofollow" target="_blank">卢卡斯-卡纳德法</a>。</p><p id="4170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习时代，能否用深度神经网络求解光流？如果可以，网络设计的意义何在？</p><p id="41c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答案是肯定的，这几年也有这方面的工作，效果越来越好。我将介绍一个代表作品叫做RAFT，它获得了ECCV 2020最佳论文奖。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="d7b1" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">大量</h2><p id="03b6" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated">RAFT，又称递归全对场变换，是一种迭代求解光流的深度学习方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/594c71e10cc96b1556d24541d08857c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5etGtu0Xd_QoU1FFhBBBXQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">筏子的网络结构(<a class="ae ky" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="aeaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，给定两幅图像，找到它们之间相同对象的对应像素对是核心。在深度学习中，一切都是基于潜在特征图，这带来了效率和准确性。因此，可以通过相关场提取相应的像素对。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/cd6f8bfc382d30677ddaa3df3574c746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4n2K1s9s047Gz_5yTNpAHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两幅图像的多尺度相关性</p></figure><p id="31f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相关性显示两幅图像中两个像素之间的关系。对于大小为(H，W)的两幅图像，它们的相关场的大小将为(H，W，H，W)，这称为C1。如果我们把最后两个维度结合起来，我们会得到C2和C3等。对于图像1的像素，C1示出了像素相关，C2示出了与图像2的2×2像素相关，C3示出了4×4像素相关，等等。这种多尺度方式可以帮助找到从小到大的位移，这对于具有大时间步长的视频中的光流估计是重要的。</p><p id="3ad0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于图像1的每个像素，将其光流初始化为零，并在多尺度相关图中查找其周围区域，以搜索其在图像2中的对应像素。然后，估计图像1中的像素与其在图像2中搜索到的对应像素之间的光流。最后，根据估计的光流将图像1中的像素移动到新的位置，并且重复该过程直到收敛。<strong class="lb iu">在这种迭代方式下，每次只估计一个步骤的流量，最终的流量就是它们的总和。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/a4547fe6e6bb522878c811b97830a94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWMsbWg6R7P1iAaVqnxagw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失和估计光流的收敛</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2490" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">关于RAFT的实现</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d69e2095a62d4e444db66d39a51d58b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2u8b3woa9rl4JbAkkkg-oQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="bef8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RAFT模型可以分为两部分:编码器和迭代器。编码器部分类似于编解码网络中的编码器，用于提取输入图像的潜在特征图。迭代器实现为ConvGRU模块，这是一种RNN结构，可以预测一系列流程步骤，并通过共享参数进行迭代优化。</p><p id="bba8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，规范和激活层仅用于编码器。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="16ec" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">RAFT的预测结果</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9dc350a110da7d676a713af05e1c9855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*zcjGkjqgQNV5r_lX-UMwSA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测光流图</p></figure><p id="4257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我测试了作者提供的<a class="ae ky" href="https://github.com/princeton-vl/RAFT" rel="noopener ugc nofollow" target="_blank">源</a>，结果看起来不错。我用了一个10帧的视频剪辑。如上图，第一行是帧0，第二行是帧1~9，第三行分别是帧0和帧1~9之间的预测光流。结果表明，对于小时间间隔和大时间间隔的两个视频帧之间的光流可以平滑地预测。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="945a" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">参考</h2><p id="cf61" class="pw-post-body-paragraph kz la it lb b lc na ju le lf nb jx lh li nc lk ll lm nd lo lp lq ne ls lt lu im bi translated"><a class="ae ky" href="https://www2.cs.duke.edu/courses/spring19/compsci527/papers/Lucas.pdf" rel="noopener ugc nofollow" target="_blank">一种应用于立体视觉的迭代图像配准技术，1981年</a></p><p id="53e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2003.12039.pdf" rel="noopener ugc nofollow" target="_blank"> RAFT:用于光流的循环全对场变换，2020 </a></p><div class="nk nl gp gr nm nn"><a href="https://dushuchen.medium.com/membership" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">加入我的介绍链接-陈数杜媒体</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">阅读陈数·杜(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">dushuchen.medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ks nn"/></div></div></a></div></div></div>    
</body>
</html>