<html>
<head>
<title>Assessing Generalization in Reward Learning with Procedurally Generated Games (1/2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用程序生成的游戏评估奖励学习中的概括(1/2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=collection_archive---------48-----------------------#2020-09-30">https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48?source=collection_archive---------48-----------------------#2020-09-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="94a8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">奖励学习中的泛化</h2><div class=""/><div class=""><h2 id="515a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">强化学习、概括和奖励学习概述</h2></div><p id="506d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">作者:安东·马基耶夫斯基、周亮、马克斯·奇西克</strong></p><p id="2e37" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">注:这是</em> <strong class="kq ja"> <em class="lk">第一篇</em></strong><em class="lk"/><strong class="kq ja"><em class="lk">两篇</em> </strong> <em class="lk">的博文(部分</em> <a class="ae ll" href="https://chisness.medium.com/assessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e" rel="noopener"> <em class="lk">两篇</em> </a> <em class="lk">)。在这些帖子中，我们描述了一个项目，该项目旨在评估奖励学习代理的概括能力。这个项目的实现是GitHub上可用的</em><a class="ae ll" href="https://github.com/lzil/procedural-generalization" rel="noopener ugc nofollow" target="_blank"><em class="lk"/></a><em class="lk">。</em></p><p id="1618" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这第一篇文章将提供强化学习、奖励学习和概括的背景，并总结我们项目的主要目标和灵感。如果你有必要的技术背景，可以跳过前几节。</p><h1 id="3693" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">关于我们</h1><p id="5590" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">我们是参加2020 <a class="ae ll" href="https://aisafety.camp/" rel="noopener ugc nofollow" target="_blank">人工智能安全营</a> (AISC)的团队，这是一个早期职业研究人员就人工智能安全相关的研究提案进行合作的计划。简而言之，AI安全是一个旨在确保随着AI的不断发展，它不会伤害人类的领域。</p><p id="b472" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">鉴于我们团队在人工智能技术安全和强化学习方面的共同兴趣，我们很高兴能在这个项目上合作。这个想法最初是由另一位AISC参与者Sam Clarke提出的，我们在夏令营期间与他进行了富有成效的交谈。</p><h1 id="fbd5" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">强化学习</h1><p id="442d" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">在强化学习(RL)中，一个主体以获得奖励为目标与一个环境进行交互。最终，代理想要学习一种策略，以便随着时间的推移获得最大的回报。不过，首先要做的是:代理人到底是什么，报酬是什么？一个<em class="lk">智能体</em>是一个通过采取行动与某个世界互动的角色，也称为<em class="lk">环境</em> <strong class="kq ja">、</strong>。例如，代理可以是玩视频游戏的角色、自动驾驶汽车模拟中的汽车或扑克游戏中的玩家。奖励只是代表代理人目标的数字，不管代理人的遭遇是否更好。例如，捡起一枚硬币可能给予积极的奖励，而被敌人击中则给予消极的奖励。</p><p id="9d18" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在RL中，<em class="lk">状态</em>代表环境中当前情况的一切。然而，代理实际上能看到的是一个<em class="lk">观察</em>。例如，在扑克游戏中，观察可能是代理自己的牌和对手以前的动作，而状态还包括对手的牌和一副牌中的牌的顺序(即代理看不到的东西)。在一些像象棋这样没有隐藏信息的环境中，状态和观察是一样的。</p><p id="129f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">给定观察结果，代理采取<em class="lk">动作</em>。每次行动后，代理将从环境中获得以下形式的反馈:</p><ol class=""><li id="1a3a" class="mj mk iq kq b kr ks ku kv kx ml lb mm lf mn lj mo mp mq mr bi translated"><strong class="kq ja">奖励:</strong>标量值，可以是正、零、负</li><li id="8559" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><strong class="kq ja">新观察:</strong>从先前的状态采取动作的结果，这将代理移动到新的状态并产生这个新观察。(此外，新状态是否为“终止”，意味着当前交互是已结束还是仍在进行中。比如完成一个关卡或者被对手吃掉会终止很多游戏。)</li></ol><p id="651a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在RL中，我们的目标是<em class="lk">通过使用奖励作为反馈来训练</em>代理真正擅长一项任务。通过许多可能的训练算法之一，代理逐渐学习一种策略(也称为<em class="lk">策略</em>)，该策略定义了代理在任何状态下应该采取什么行动来最大化回报。目标是在整个<em class="lk">情节</em>中最大化奖励，这是一个代理从交互开始到结束状态所经历的一系列状态。</p><p id="ec3b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">巨大成功的代理人被训练成在诸如Atari和Go游戏中有超人的表现。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi mx"><img src="../Images/ec412348def87486310961746e997a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_4Ju1_Tkyb3OlQ7X"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">强化学习过程(图片由作者提供)</p></figure><p id="3f6a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以视频游戏Mario为例，让我们看看一个示例算法是如何工作的。假设马里奥右边有一个敌人，左边有一个蘑菇，上面什么都没有(见下图)。在这三个行动选项中，如果向左，他可能得到+2的奖励，如果向右，他可能得到-10的奖励，如果向上，他可能得到0的奖励。马里奥采取行动后，他将处于一个新的状态，有新的观察，并根据他的行动获得奖励。然后就是下一个动作的时候了，过程继续。回想一下，目的是最大化整个情节的回报，在这种情况下，这是从游戏开始到马里奥一生中的一系列状态。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/fd812c2707c9b0278bf8518da1a687a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/0*Jy_RtGNTlRxbznyn"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">马里奥学习吃蘑菇(图片由作者提供)</p></figure><p id="0f8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">算法第一次看到这种情况时，它可能会随机选择一个选项，因为它还不了解可用操作的后果。随着它越来越多地看到这种情况，它会从经验中了解到，在像这些情况下，向右走不好，向上走可以，向左走最好。我们不会直接教狗如何捡球，但通过给予奖励，狗会通过强化来学习。同样，马里奥的行动也因经验反馈而得到加强，即蘑菇是好的，而敌人不是。</p><p id="d2eb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">算法是如何实现回报最大化的？不同的RL算法以不同的方式工作，但人们可能会跟踪从这个位置采取每个动作的结果，并且下次Mario在这个相同的位置时，他会根据先前的结果选择预期最有回报的动作。许多算法大多数时候选择最佳行动，但有时也会随机选择，以确保它们探索所有选项。(注意，在开始时，代理通常随机行动，因为它还没有了解任何关于环境的信息。)</p><p id="da4f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重要的是要不断探索所有的选择，以确保代理不会找到合适的东西，然后永远坚持下去，可能会忽略更好的选择。在马里奥游戏中，如果马里奥第一次试着向右走，看到它是-10，然后试着向上走，看到它是0，从那一点开始总是向上走就不好了。这将错过尚未探索的左转+2奖励。</p><p id="4940" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">想象一下，你试着在家做饭，但不喜欢食物，然后去麦当劳吃了一顿美妙的饭。你发现去麦当劳是一个很好的“行动”，但如果你一直在麦当劳吃饭，而不去尝试其他可能最终会提供更好“回报”的餐厅，那将是一种耻辱(而且对健康没有好处)。</p><h1 id="7df2" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">一般化</h1><p id="e6aa" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">RL经常用在像<a class="ae ll" href="https://gym.openai.com/envs/#atari" rel="noopener ugc nofollow" target="_blank">雅达利</a>这样的游戏设定中。在Atari游戏(类似于马里奥风格的游戏)中使用RL的一个问题是这些游戏的<em class="lk">序列</em>性质。赢得一关后，你进入下一关，并继续以同样的顺序通过关卡。<strong class="kq ja">算法可能只是简单地记住每一关发生的事情，然后在面对游戏中最微小的变化时悲惨地失败。</strong>这意味着算法可能实际上并不理解游戏，而是学习记忆一系列按钮，从而为特定级别带来高额奖励。一个更好的算法，而不是学习记忆一系列按钮按压，能够“理解”游戏的结构，从而能够适应看不见的情况，或者说<em class="lk">概括</em>。</p><p id="a297" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">成功的概括意味着在以前没有见过的情况下表现出色。如果你知道2*2 = 4，2*3 = 6，2*4 = 8，然后可以算出2*6 = 12，这意味着你能够“理解”乘法，而不仅仅是记住等式。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9f5e706ad8049a52d1cef4560be3a841.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/0*dc8oncwiq0paGQd4"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">雅达利突破(<a class="ae ll" rel="noopener" target="_blank" href="/atari-reinforcement-learning-in-depth-part-1-ddqn-ceaa762a546f">来源</a>)</p></figure><p id="8335" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看一个在垃圾邮件过滤器环境中的一般化例子。这些方法通常通过收集那些将收件箱中的邮件标记为垃圾邮件的用户的数据来工作。如果一群人将“用这一招每天赚800美元”的邮件标记为垃圾邮件，那么该算法将在未来学习阻止所有电子邮件用户的所有这些邮件。但是，如果垃圾邮件发送者注意到他的电子邮件被阻止，并决定智取过滤器呢？第二天，他可能会发送一条新消息，“用这另一个技巧每天赚900美元”。仅记忆的算法将无法捕捉到这一点，因为它只是记忆要阻止的确切消息，而不是学习一般的垃圾邮件。概括算法将学习模式，并有效地理解垃圾邮件的构成。</p><h1 id="f707" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">奖励学习</h1><p id="bcd3" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">游戏通常有非常明确的内置奖励。在像<a class="ae ll" href="https://gym.openai.com/envs/Blackjack-v0/" rel="noopener ugc nofollow" target="_blank">二十一点</a>这样的纸牌游戏中，奖励对应于代理每手牌赢或输的多少。在<a class="ae ll" href="https://gym.openai.com/envs/#atari" rel="noopener ugc nofollow" target="_blank"> </a>雅达利中，奖励依赖于游戏，但有明确的规定，比如击败敌人或完成关卡可以获得积分，被击中或死亡可以失去积分。</p><p id="e649" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图来自一个名为<a class="ae ll" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank"> CartPole </a>的经典强化学习环境，目标是让杆子在轨道上保持直立，杆子保持直立的每一秒提供+1的奖励。代理人将车向左或向右移动，试图保持杆子平衡，保持平衡的时间越长，获得的+1奖励就越多。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e8db2ed0c809283903d13de6af627d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*sxuQv-3kidRBEKbNXQz_tA.gif"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">扁担(<a class="ae ll" href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947" rel="noopener">来源</a>)</p></figure><p id="f0bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">然而，现实世界中的许多任务并没有如此明确定义的奖励，这导致了强化学习的可能应用受到限制。</strong>这个问题因这样一个事实而变得更加复杂:即使不是不可能，也很难详细说明明确定义的奖励。人类可以在训练期间向RL代理提供直接反馈，但是这将需要太多的人类时间。</p><p id="a56b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一种被称为逆向强化学习的方法包括从演示中“逆向工程”出一个奖励函数。对于复杂的任务，从演示中找出奖励函数是很难做好的。</p><p id="e72c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> <em class="lk">奖励学习</em>涉及学习奖励函数，该函数描述了在环境中的每种情况下获得多少奖励，即当前状态和动作到所接收奖励的映射。</strong>目标是学习鼓励期望行为的奖励功能。为了训练算法来学习奖励函数，我们需要另一个数据源，例如成功执行任务的演示。奖励函数输出每个状态的奖励预测，之后标准RL算法可用于通过简单地用这些近似奖励代替通常已知的奖励来学习策略。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nq"><img src="../Images/6619f7d1c3f32c5a63287f73a23bd490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Digbiy6ukz3tle9G"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">用奖励函数代替已知奖励的强化学习过程(图片由作者提供)</p></figure><p id="d216" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">先前的工作(下文描述为Christiano等人2017年)提供了一个例子，说明学习奖励函数有多困难。想象一下教一个机器人做后空翻。如果你不是一个认真的体操运动员，自己演示如何成功完成这项任务将是一个挑战。<strong class="kq ja">人们可以尝试设计一个代理可以学习的奖励函数，但是这种方法经常成为非理想奖励设计和奖励黑客攻击的牺牲品。</strong>打赏黑客是指代理商可以在打赏规范中找到“漏洞”。例如，如果我们给后空翻正确的初始姿势分配了太多的奖励，那么代理人可能会学会永远重复地进入那个弯腰姿势。它会根据我们给它的奖励函数最大化奖励，但实际上不会做我们想要做的！</p><p id="b405" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">人类可以通过在每一步人工输入奖励函数来监督代理学习的每一步，但这将非常耗时和乏味。</p><p id="1c91" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">指定奖励的困难指向了更大的人-人工智能协调问题，即人类希望人工智能系统符合他们的意图和价值观，但指定我们实际上想要的东西可能会令人惊讶地困难(回想一下每个精灵故事的结局！).</p><h1 id="84a3" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">相关文件</h1><p id="fe1c" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">我们想看看最近的几种奖励学习算法，以评估它们学习奖励的能力。我们特别感兴趣的是，当面对以前看不见的环境或游戏关卡时，算法有多成功，这测试了它们的概括能力。</p><p id="81b1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为此，我们利用了大量先前的工作:</p><ol class=""><li id="db2e" class="mj mk iq kq b kr ks ku kv kx ml lb mm lf mn lj mo mp mq mr bi translated"><a class="ae ll" href="https://arxiv.org/abs/1706.03741" rel="noopener ugc nofollow" target="_blank"> <em class="lk">基于人类偏好的深度强化学习</em></a>—Christiano等人2017。</li><li id="2ee9" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><a class="ae ll" href="https://arxiv.org/abs/1811.06521" rel="noopener ugc nofollow" target="_blank"> <em class="lk">奖励从人类偏好中学习，在雅达利</em> </a> — 2018由Ibarz等人提出。</li><li id="ff6a" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><a class="ae ll" href="https://arxiv.org/abs/1912.01588" rel="noopener ugc nofollow" target="_blank"> <em class="lk">利用过程化生成对强化学习进行基准测试</em></a>—Cobbe等人2019。</li><li id="f0a9" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><a class="ae ll" href="https://arxiv.org/abs/1904.06387" rel="noopener ugc nofollow" target="_blank"> <em class="lk">从Brown，Goo等人的观察值</em> </a> — 2019，通过逆向强化学习外推超过次优演示。</li></ol><p id="1959" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">前两篇论文在利用奖励学习和深度强化学习方面很有影响力，第三篇介绍了OpenAI Procgen基准测试，这是一组用于测试算法泛化的有用游戏。第四篇论文提出了前两篇论文方法的有效替代方案。</p><h2 id="8bdd" class="nr ln iq bd lo ns nt dn ls nu nv dp lw kx nw nx ly lb ny nz ma lf oa ob mc iw bi translated">从人类偏好进行深度强化学习(Christiano等人，2017年)</h2><p id="1ee7" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">这篇文章的中心思想是<strong class="kq ja">识别一个好的后空翻比做一个好的后空翻要容易得多。</strong>该论文表明，对于我们只能识别出想要的行为的任务，即使我们无法演示，也可以学习预测的奖励函数。</p><p id="a14e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">提议的算法如下所示。它在通过人类偏好学习奖励函数和学习策略之间交替，这两者最初都是随机的。</p><p id="8d30" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">重复直到代理牛逼:</em></p><blockquote class="oc od oe"><p id="f856" class="ko kp lk kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><em class="iq"> 1。展示两个简短的视频剪辑，展示代理在其当前策略的环境中行动</em></p><p id="b8bf" class="ko kp lk kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><em class="iq"> 2。问一个人在哪个视频片段中代理更好</em></p><p id="c2a0" class="ko kp lk kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><em class="iq"> 3。根据人类的反馈更新奖励函数</em></p><p id="b461" class="ko kp lk kq b kr ks ka kt ku kv kd kw of ky kz la og lc ld le oh lg lh li lj ij bi translated"><em class="iq"> 4。基于新的奖励函数更新策略</em></p></blockquote><p id="d3a1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">模拟机器人(如下图所示)被训练在一小时内完成900个查询的后空翻，这是一项很难演示或手动创建奖励的任务。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi np"><img src="../Images/fd5fed2746c2cf4f268f7e41bead4e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*KH4FSpUxk3yTt_wG"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">根据人类喜好训练后空翻动作(<a class="ae ll" href="https://github.com/nottombrown/rl-teacher" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="af26" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验在名为MuJoCo的物理模拟器和Atari游戏中进行。既然我们已经知道雅达利游戏的真正回报，为什么还要在雅达利进行这些实验呢？这提供了自动分配偏好的机会，而不是让人手动给出关于两个视频剪辑演示的反馈。我们可以通过简单地将具有更高真实奖励的剪辑排列为更好的剪辑来获得自动(合成)反馈。这使我们能够非常快速地进行实验，因为不需要人工。此外，在这种情况下，我们可以通过比较学习到的奖励函数和游戏中给出的真实奖励来评估算法的性能。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a4af26699519e32e16f55c5b01f131ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*05V2X5T7AuGB2AM1"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">运动中的后空翻(<a class="ae ll" href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="a2f7" class="nr ln iq bd lo ns nt dn ls nu nv dp lw kx nw nx ly lb ny nz ma lf oa ob mc iw bi translated">奖励从人类偏好和Atari演示中学习(Ibarz等人，2018年)</h2><p id="39c1" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">本文建立在前一篇论文的基础上，在Atari域中使用不同的设置和不同的RL算法进行了额外的实验。他们的主要创新是在开始时利用人类演示，以便开始一个体面的策略，而原始算法将不得不从一个完全随机行动的代理开始，因为在开始时不知道奖励。相对于Christiano使用的无演示方法，这些人类演示的加入在九个测试的雅达利游戏中的三个中显著提高了学习。</p><h2 id="fbe0" class="nr ln iq bd lo ns nt dn ls nu nv dp lw kx nw nx ly lb ny nz ma lf oa ob mc iw bi translated">利用程序生成对强化学习进行基准测试(Cobbe等人，2019年)</h2><p id="e2c1" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated"><a class="ae ll" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank">人工智能研究实验室OpenAI </a>开发了名为<a class="ae ll" href="https://openai.com/blog/procgen-benchmark/" rel="noopener ugc nofollow" target="_blank"> Procgen Benchmark </a>的强化学习测试床游戏环境，其中包括16款独特的游戏。在每个游戏中，所有关卡都是相似的，有着相同的目标，但是实际的组成部分，如敌人和危险的位置是随机生成的，因此在每个关卡中会有所不同。</p><p id="3d27" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这意味着我们可以<strong class="kq ja">在许多随机水平上训练我们的代理，然后在全新的水平上测试它</strong>，允许我们了解代理是否能够概括它的学习。请注意与雅达利游戏的对比，在雅达利游戏中，训练是在连续的游戏级别上进行的，敌人、奖励和游戏对象总是在同一个地方。此外，当在连续的和非随机生成的游戏中测试代理的能力时，它们以相同的顺序在相同的水平上被测试。一个重要的机器学习原理是，用一组数据训练，用另一组数据测试，真正评估智能体的学习/概括能力。</p><p id="b397" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们在工作中主要考察了Procgen的四种环境:</p><ol class=""><li id="3cd9" class="mj mk iq kq b kr ks ku kv kx ml lb mm lf mn lj mo mp mq mr bi translated">躲避敌人的同时，在关卡结束时收集硬币</li><li id="677c" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated">水果机器人:吃水果，避免非水果食物，如鸡蛋和冰淇淋</li><li id="ec49" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><strong class="kq ja">星际飞行员:</strong>侧面滚动射击游戏</li><li id="e36f" class="mj mk iq kq b kr ms ku mt kx mu lb mv lf mw lj mo mp mq mr bi translated"><strong class="kq ja">大鱼:</strong>从小鱼开始，吃其他更小的鱼，变得更大</li></ol><p id="5ade" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面是每个游戏的截图。代理视图使用较低的分辨率来优化算法，以减少计算量。人类视角是指如果人类在玩游戏，游戏会是什么样子。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oj"><img src="../Images/2394b4f1068eb25c3d4bf38042219920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iQmK6AEeli53av1A"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">带有代理视图的CoinRun、FruitBot、StarPilot和BigFish(图片由作者提供)</p></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ok"><img src="../Images/78c6706b02309161cb39b8a4b1ae5889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*63ua7FKAE9rK4Ecf"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">人类视角的CoinRun、FruitBot、StarPilot和BigFish(图片由作者提供)</p></figure><p id="508f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">论文中的主要实验涉及在所有16个独特的游戏中训练代理，每个游戏的训练水平在100到100，000的范围内，同时保持训练时间固定。然后，这些代理在他们从未玩过的关卡上接受测试(这是可能的，因为每个关卡都是自动生成的)。他们发现，代理人需要在多达10，000个游戏级别(培训级别)上接受培训，才能在测试级别上表现良好。</p><p id="97d3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面的StarPilot游戏图用蓝色显示训练性能，用红色显示测试性能。y轴是奖励，x轴是用于训练的等级数。请注意，x轴是对数刻度。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a4ec5611db99552332b1dff97881615c.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/0*mhd6cUSXc8UIxIC2"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">星际飞行员训练(蓝色)和测试(红色)(<a class="ae ll" href="https://openai.com/blog/procgen-benchmark/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="f512" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们看到代理在培训期间表现非常好，然后培训绩效下降，然后又略有回升。为什么代理训练得越多越差？由于训练时间是固定的，通过只训练100个级别，代理将一遍又一遍地重复相同的级别，并且可以很容易地记住所有内容(但是在测试时在看不见的级别上表现很差)。有了1，000个级别，代理将不得不将其时间分散到更多的级别，因此也无法学习这些级别。当我们达到10，000或更多的级别时，代理能够看到如此多样的级别，它能够很好地执行，因为它已经开始概括它的理解。我们还看到，测试性能很快提高到接近训练性能的水平，这表明代理能够很好地概括到看不见的水平。</p><h2 id="40c7" class="nr ln iq bd lo ns nt dn ls nu nv dp lw kx nw nx ly lb ny nz ma lf oa ob mc iw bi translated">通过从观察中进行逆向强化学习来推断次优演示之外的情况(Brown，Goo等人，2019年)</h2><p id="6688" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">本文提出的算法名为<em class="lk"> T-REX </em>，与之前提到的奖励学习方法不同，它<strong class="kq ja">在学习过程</strong>中不需要持续的人类反馈。虽然与监督每个代理动作相比，其他算法需要相对较少的人工时间，但它们仍然需要一个人来回答数千个偏好查询。T-REX的一个关键思想是，通过在开始时完成所有偏好反馈，而不是在整个学习过程中持续完成，可以显著减少人类的时间投入。</p><p id="f81a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一步是生成正在学习的游戏或任务的演示。演示可以来自标准的强化学习代理，也可以来自人。</p><p id="d9e5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主要的想法是，我们可以通过从这些演示中提取短视频剪辑来获得大量偏好数据，并仅根据它们来自的演示的排名来给它们分配偏好。例如，有20个演示，每个演示将得到从1到20的等级。大量的短视频剪辑将从这些演示中选取，每个剪辑将被分配给它所来自的演示的等级，因此当它们面对彼此时，偏好将会转到来自更好的演示的剪辑。奖励函数将基于这些偏好。</p><p id="18dd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这与以前的方法形成对比，以前的方法需要对每一对1-2秒的剪辑输入人类偏好。这里，我们只需要人类偏好输入来对初始演示进行排序。然而，霸王龙的缺点是它使用了一个近似值。并非来自较高等级的演示的所有剪辑都应该比来自较低等级的演示的剪辑更优选，但是该想法是平均而言，偏好将会很好地工作，并且该过程将足以学习奖励模型。</p><p id="fd2b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">提供对演示的排序相当于在每一对演示之间给出偏好。例如，如果我们有三个演示，并将它们排序为3&gt;1&gt;2，这意味着我们将生成3&gt;1、3&gt;2和1&gt;2的排序。则根据剪辑来自哪个演示，从演示中随机生成的剪辑将被给予相同的偏好等级。</p><p id="7ee2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">霸王龙的论文表明，只要有12次示范就足以学习一个有用的奖励模型。任何12个对象都有12 * 11 / 2 = 66个不同的对，因此从1到12对12个演示进行排名相当于回答多达66个关于哪个演示更好的查询，这比Christiano等人的算法所需的查询少约100倍。同样，尽管T-REX排名演示方法更有效，但由于简化了较好演示的所有剪辑都比较差演示的所有剪辑好的假设，它牺牲了精确度。</p><p id="e4c4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Brown和Goo等人基于Atari的实验表明，霸王龙与之前描述的Ibarz等人的方法相比具有竞争力。它能够仅使用12个示范及其相应的偏好(等级)标签来学习优于示范的质量代理。</p><p id="e222" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图显示了五款Atari游戏中人类演示的得分与T-REX算法的得分之间的比较。虽然在蒙特祖马的复仇游戏中没有获得任何分数，但霸王龙的表现远远超过了人类5分中的3分。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2d211b9239c846079c7e39db64da84fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*7KnH9Ovx5FO5kanE"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">霸王龙算法对人类(<a class="ae ll" href="https://arxiv.org/abs/1904.06387" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6bee" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">T-REX还在8场比赛中的7场比赛中超过了最先进的行为克隆算法(BCO)和模仿学习算法(盖尔)，如下图所示，同时在8场比赛中的7场比赛中击败了最佳演示。(行为克隆算法试图尽可能接近演示，而反向强化学习算法试图从专家演示中恢复奖励函数。)</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi on"><img src="../Images/15ba9f1266aa6b931b347c9744348d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xFO8ioZlDaFDqtmKPc2eQ.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">T-REX算法与其他先进方法的对比(<a class="ae ll" href="https://arxiv.org/abs/1904.06387" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="5a3e" class="lm ln iq bd lo lp lq lr ls lt lu lv lw kf lx kg ly ki lz kj ma kl mb km mc md bi translated">接下来:实现和实验</h1><p id="cc6b" class="pw-post-body-paragraph ko kp iq kq b kr me ka kt ku mf kd kw kx mg kz la lb mh ld le lf mi lh li lj ij bi translated">基于T-REX的强大结果和简单的想法，我们决定将我们的初始实验基于将该算法与Procgen游戏环境相结合，这将为我们提供一个高效的奖励学习算法和各种基准游戏来测试泛化能力。我们将在本系列的<a class="ae ll" href="https://chisness.medium.com/assessing-generalization-in-reward-learning-implementations-and-experiments-de02e1d08c0e" rel="noopener">第二篇博文</a>中解释我们实现的细节以及实验结果和我们面临的问题。</p></div></div>    
</body>
</html>