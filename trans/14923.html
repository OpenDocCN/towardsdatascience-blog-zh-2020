<html>
<head>
<title>Loading large datasets in Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Pandas中加载大型数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/loading-large-datasets-in-pandas-11bdddd36f7b?source=collection_archive---------1-----------------------#2020-10-14">https://towardsdatascience.com/loading-large-datasets-in-pandas-11bdddd36f7b?source=collection_archive---------1-----------------------#2020-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eba8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有效地使用分块和SQL来读取pandas中的大数据集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/93fee92c409afba55f80efe979d76adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkdcNXudFAEXPtnFrvxqrQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.freepik.com/vectors/business" rel="noopener ugc nofollow" target="_blank">free pik创建的业务向量</a></p></figure><p id="83e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">熊猫的</a>图书馆是数据科学生态系统的重要成员。然而，它无法分析大于内存的数据集，这使得它对于大数据来说有点棘手。考虑这样一种情况，当我们想只使用熊猫来分析一个大数据集时。我们会遇到什么样的问题？例如，我们来看一个包含3GB数据的文件，该文件总结了2016年3月的<a class="ae ky" href="https://www.kaggle.com/bharath150/taxi-data?select=yellow_tripdata_2016-03.csv" rel="noopener ugc nofollow" target="_blank">黄色出租车出行数据。为了进行任何种类的分析，我们必须将它输入到内存中。我们很容易使用pandas的<code class="fe lv lw lx ly b">read_csv()</code>函数来执行如下读取操作:</a></p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="1e82" class="md me it ly b gy mf mg l mh mi">import pandas as pd<br/>df = pd.read_csv('yellow_tripdata_2016-03.csv')</span></pre><p id="996f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我运行单元/文件时，我的系统抛出以下<strong class="lb iu">内存错误。</strong>(内存错误取决于您使用的系统容量)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/bdead6571a2dafdc1106d899973131e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrM-5OXc1OlCGXzI6P33Ig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="050f" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">有其他选择吗？</h2><p id="429b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在批评熊猫之前，重要的是要明白熊猫并不总是每项任务的合适工具。熊猫缺乏多处理支持，其他库更擅长处理大数据。一个这样的选择是Dask，它提供了一个类似熊猫的API来处理比内存大的数据集。即使是<a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html" rel="noopener ugc nofollow" target="_blank"> pandas的文档</a>也明确提到了大数据:</p><p id="3cef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ng">不用熊猫值得考虑。熊猫并不是所有情况下的合适工具。</em></p><p id="7fe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在本文中，我们将研究一种叫做分块的方法，通过这种方法，你可以在pandas中加载内存不足的数据集。这种方法有时可以提供一种健康的方式来管理熊猫的内存不足问题，但可能并不总是有效，我们将在本章的后面看到这一点。本质上，我们将研究在python中导入大型数据集的两种方法:</p><ul class=""><li id="0b44" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated">使用带有chunksize的<code class="fe lv lw lx ly b">pd.read_csv() </code></li><li id="8e10" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">使用SQL和pandas</li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="7367" class="oc me it bd mk od oe of mn og oh oi mq jz oj ka mt kc ok kd mw kf ol kg mz om bi translated">💡分块:将数据集细分成更小的部分</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/53542bb4aa63801d92600ad0aa52ff3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*T2ReKQ5XR5bt1kbghm949g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b497" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在使用示例之前，让我们试着理解我们所说的工作分块是什么意思。根据<a class="ae ky" href="https://en.wikipedia.org/wiki/Main_Page" rel="noopener ugc nofollow" target="_blank">维基百科</a>，</p><p id="7ae4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Chunking_(computing)#:~:text=Typical%20modern%20software%20systems%20allocate%20memory%20dynamically%20from%20structures%20known%20as%20heaps.&amp;text=Heap%20management%20involves%20some%20computation,aggregate%20related%20memory%2Dallocation%20requests." rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="ng">分块</em> </strong> </a> <em class="ng">指的是通过使用特定情况的知识来聚合相关的内存分配请求，从而提高性能的策略。</em></p><p id="6020" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，我们可以分成更小的部分或块，而不是一次读取内存中的所有数据。在CSV文件的情况下，这意味着在给定的时间点只将几行加载到内存中。</p><p id="27b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas的<code class="fe lv lw lx ly b">read_csv()</code>函数带有一个<strong class="lb iu">块大小</strong> <a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">参数</strong> </a>来控制块的大小。让我们看看它的实际效果。我们将使用本文前面使用的确切数据集，但不是一次性加载所有数据集，而是将它分成几个部分进行加载。</p><h1 id="08e3" class="oc me it bd mk od oo of mn og op oi mq jz oq ka mt kc or kd mw kf os kg mz om bi translated">✴️使用pd.read_csv()和chunksize</h1><p id="1287" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">为了启用分块，我们将在开始时声明块的大小。然后使用带有chunksize参数的<code class="fe lv lw lx ly b">read_csv()</code>,返回一个我们可以迭代的对象。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="3b1c" class="md me it ly b gy mf mg l mh mi">chunk_size=50000<br/>batch_no=1</span><span id="2777" class="md me it ly b gy ot mg l mh mi">for chunk in pd.read_csv('yellow_tripdata_2016-02.csv',chunksize=chunk_size):<br/>    chunk.to_csv('chunk'+str(batch_no)+'.csv',index=False)<br/>    batch_no+=1</span></pre><p id="25b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们选择50，000的块大小，这意味着一次只能导入50，000行数据。下面是一个视频，展示了主CSV文件如何拆分成多个文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou ov l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者提供的视频</p></figure><h2 id="4dd5" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">将单个块文件导入pandas数据帧:</h2><p id="eee7" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们现在有多个数据块，每个数据块都可以作为熊猫数据帧轻松加载。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="49da" class="md me it ly b gy mf mg l mh mi">df1 = pd.read_csv('chunk1.csv')<br/>df1.head()</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ow ov l"/></div></figure><p id="3238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它就像一个魔咒！。不再有内存错误。让我们快速查看一下这个块的内存使用情况:</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="e068" class="md me it ly b gy mf mg l mh mi">df1.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/d18832e7bd66683055e91630f4a2f04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*NIQOvvCJEUM-ldKle15DvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="0bbc" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">🔴提醒一句</h2><p id="311a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">分块创建数据的各种子集。因此，当您正在执行的<a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html" rel="noopener ugc nofollow" target="_blank">操作不需要或只需要块</a>之间的最小协调时，它会工作得很好。<em class="ng"> </em>这是一个重要的考虑。<em class="ng"> </em>使用组块的另一个缺点是<a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html" rel="noopener ugc nofollow" target="_blank">有些操作像</a> <code class="fe lv lw lx ly b"><a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html" rel="noopener ugc nofollow" target="_blank">groupby</a></code>做组块要难得多。在这种情况下，最好使用替代库。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="40df" class="oc me it bd mk od oe of mn og oh oi mq jz oj ka mt kc ok kd mw kf ol kg mz om bi translated">✴️Using SQL和熊猫读取大数据文件</h1><p id="e332" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">(参见参考文献)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/8559fd55ca31f740cbd3775564756df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YbsD_GziUr-eouD8.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="2eb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种方法是从块中构建一个<a class="ae ky" href="https://www.sqlite.org/index.html" rel="noopener ugc nofollow" target="_blank"> SQLite数据库</a>，然后使用SQL查询提取所需的数据。SQLite是一个基于SQL语言的关系数据库管理系统，但是针对小型环境进行了优化。它可以使用名为<a class="ae ky" href="https://docs.python.org/3/library/sqlite3.html" rel="noopener ugc nofollow" target="_blank"> sqlite3 </a>的Python模块与Python集成。如果您想了解更多关于在python中使用Sqlite的信息，可以参考我写的一篇关于这个主题的文章:</p><div class="oz pa gp gr pb pc"><a href="https://medium.com/analytics-vidhya/programming-with-databases-in-python-using-sqlite-4cecbef51ab9" rel="noopener follow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd iu gy z fp ph fr fs pi fu fw is bi translated">使用SQLite在Python中进行数据库编程</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">如果你渴望成为一名数据科学家，你将会处理大量的数据。大部分数据驻留在…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">medium.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq ks pc"/></div></div></a></div><p id="efbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.sqlalchemy.org/" rel="noopener ugc nofollow" target="_blank"> SQLAlchemy </a>是Python SQL工具包和对象关系映射器，为应用程序开发人员提供了SQL的全部功能和灵活性。它用于构建一个从原始数据创建数据库的引擎，在我们的例子中，原始数据是一个很大的CSV文件。</p><p id="9f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本文，我们将遵循以下步骤:</p><h2 id="2168" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">导入必要的库</h2><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="d79d" class="md me it ly b gy mf mg l mh mi">import sqlite3<br/>from sqlalchemy import create_engine</span></pre><h2 id="1b75" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">创建到数据库的连接器</h2><p id="572d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们将要创建的数据库命名为<code class="fe lv lw lx ly b">csv_database.</code></p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="2e5e" class="md me it ly b gy mf mg l mh mi">csv_database = create_engine('sqlite:///csv_database.db')</span></pre><h2 id="563c" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">通过分块从CSV文件创建数据库</h2><p id="192d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这个过程类似于我们在本文前面看到的。该循环读取由chunksize指定的成组数据集。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="fe3d" class="md me it ly b gy mf mg l mh mi">chunk_size=50000<br/>batch_no=1</span><span id="cc68" class="md me it ly b gy ot mg l mh mi">for chunk in pd.read_csv('yellow_tripdata_2016-02.csv',chunksize=chunk_size,iterator=True):<br/>    chunk.to_sql('chunk_sql',csv_database, if_exists='append')<br/>    batch_no+=1<br/>    print('index: {}'.format(batch_no))</span></pre><p id="ac5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们使用了函数。<code class="fe lv lw lx ly b">chunk.to_sql instead of chunk.to_csv </code>因为我们正在将数据写入数据库，也就是说<code class="fe lv lw lx ly b">csv_database.</code>，<code class="fe lv lw lx ly b">chunk_sql</code>是给块的任意名称。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/8a7e864826f31d7e371c246f7cafe043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbyKgtZ-6ocf78LukBUFuA.png"/></div></div></figure><h2 id="1259" class="md me it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">通过查询SQL数据库构建熊猫数据框架</h2><p id="b259" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">数据库已创建。我们现在可以很容易地查询它，只提取我们需要的那些列；例如，我们可以只提取乘客数量少于<code class="fe lv lw lx ly b">5</code>并且出行距离大于<code class="fe lv lw lx ly b">10</code>的那些行。<code class="fe lv lw lx ly b">pandas.read_sql_query</code>将SQL查询读入数据帧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ow ov l"/></div></figure><p id="a706" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在有了一个数据框架，它非常适合我们的记忆，可以用于进一步的分析。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="e43f" class="oc me it bd mk od oe of mn og oh oi mq jz oj ka mt kc ok kd mw kf ol kg mz om bi translated">结论</h1><p id="888c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">就数据分析而言，Pandas是一个方便且功能多样的库。然而，在处理大数据时，它遇到了几个瓶颈。在本文中，我们看到了分块和SQL如何为分析大于系统内存的数据集提供一些安慰。然而，这种替代方案不是“一刀切”的解决方案，使用为处理大数据而创建的库将是更好的选择。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h1 id="5a55" class="oc me it bd mk od oe of mn og oh oi mq jz oj ka mt kc ok kd mw kf ol kg mz om bi translated">参考</h1><ol class=""><li id="38ee" class="nh ni it lb b lc nb lf nc li ps lm pt lq pu lu pv nn no np bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=xKMyk4wDHnQ" rel="noopener ugc nofollow" target="_blank">如何用Python中的SQL和Pandas读取非常大的文件</a>作者<br/><a class="ae ky" href="https://www.linkedin.com/in/bielinskas/" rel="noopener ugc nofollow" target="_blank">Vytautas Bielinskas博士</a></li></ol><p id="cd7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html" rel="noopener ugc nofollow" target="_blank"> 2。扩展到大型数据集</a></p></div></div>    
</body>
</html>