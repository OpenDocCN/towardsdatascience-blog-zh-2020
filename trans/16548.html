<html>
<head>
<title>Drawing the Transformer Network from Scratch (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始绘制变压器网络(第1部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e?source=collection_archive---------14-----------------------#2020-11-15">https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e?source=collection_archive---------14-----------------------#2020-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="917a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">以有趣的方式获得变形金刚的心智模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/08ee2fed9d017e45a7d8547636426fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBd5d5ysZ2myU7Nxxh4yTg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="91eb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">变形金刚神经网络——通常被称为“变形金刚”——是由谷歌领导的团队在2017年发表的一篇题为“注意力是你所需要的一切”的论文中介绍的。在接下来的工作中被很多人提炼推广。</p><p id="fe52" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像在它之前发明的许多模型一样，变压器有一个编码器-解码器架构。在这篇文章中，我们把重点放在编码器部分。我们将以自下而上的方式连续绘制它的所有部分。这样做有望让读者轻松地开发出变压器的“心智模型”。</p><p id="1cfd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的动画以快动作展示了我们将在本文中涉及的内容:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lr"><img src="../Images/dd17b7d7357a2a6397f36203ff9a691d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UH5SSuMy9y-BcBtvAUhTmQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="f84d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">投入</h1><p id="6900" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">转换器将单词序列作为输入，作为向量呈现给网络。在NLP任务中，通常使用一个词汇表(也称为字典)，其中每个单词被分配一个唯一的索引。该索引可以被表示为所谓的独热向量，其主要由0组成，在正确的位置具有单个“1”值。下图显示了一个简单的十个单词的词汇编码:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/723a26b1eea794feb8dc94f1b84890c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*y5d-7nqdE-QIZJCngreBog.png"/></div></figure><p id="b140" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，独热编码向量的大小与词汇表中的字数相同，在实际应用中至少为10.000。此外，所有独热编码彼此之间具有相同的欧几里德距离√2。</p><h1 id="17dd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">单词嵌入</h1><p id="8c0a" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">接下来，我们通过将独热编码向量乘以所谓的“嵌入矩阵”来降低它们的维数。由此产生的向量被称为单词嵌入。原始论文中单词嵌入的大小是512。</p><p id="f70d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">单词嵌入的巨大好处是意思相似的单词被放在一起，例如单词“cat”和“kitty”最终具有相似的嵌入向量。</p><p id="0927" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，“嵌入矩阵”是一个普通的矩阵，只是有一个花哨的名字。</p><h1 id="f728" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">位置编码</h1><p id="9c0b" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">所有的单词同时呈现给变压器。这与递归神经网络(例如LSTMs)有很大的不同，在lst ms中，单词是连续输入的。然而，这意味着单词在输入序列中出现的顺序丢失了。为了解决这个问题，转换器向每个输入嵌入添加一个向量，从而注入一些关于相对或绝对位置的信息。</p><h1 id="671c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">键和查询</h1><p id="845c" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">最后，我们将单词嵌入乘以矩阵WQ和WK，以获得“查询向量”和“关键向量”，每个向量的大小为64。</p><p id="179c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到目前为止提到的所有组件都绘制在下面的动画中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/e9e7f8c54b97804413cf4f300bc4cf8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GDC-85VrLe-J8p-G3kEZCA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输入序列、单词嵌入、位置编码、键和查询(作者图片)</p></figure><p id="9392" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，我们绘制单个元素的顺序与计算元素的顺序无关。</p><h1 id="d440" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">并行化</h1><p id="7e5c" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在我们继续之前，有一点需要强调，那就是Transformer适合并行化的方式。请注意，所有的单词嵌入都可以并行计算。一旦我们得到了嵌入，我们还可以同时计算所有嵌入的查询向量和关键向量。这种模式将贯穿整个架构。请注意。</p><h1 id="6e72" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">点积</h1><p id="2812" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们计算“查询向量”和“关键向量”的所有可能组合的点积。点积的结果是一个单一的数字，在后面的步骤中将用作权重因子。权重因子告诉我们，在输入句子的不同位置的两个单词相互依赖的程度。这在原论文里叫自我关注。自我关注的机制允许变压器学习困难的依赖性，即使在遥远的位置之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/b3e4c13f4165dfa87401180220fcf6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GzF4v8w0rOFAczA-sI6RCQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“查询向量”和“关键向量”的点积(图片由作者提供)</p></figure><h1 id="cf81" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">缩放比例</h1><p id="3f58" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">随后，所有权重因子除以8(关键向量64的维数的平方根)。作者假设在训练期间，点积的大小可以变大，从而将softmax函数推到梯度极小的区域。除以8会产生更稳定的梯度。</p><h1 id="57fb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">Softmax</h1><p id="9502" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">缩放后的因子通过softmax函数进行归一化，因此它们都是正的，总和为1。</p><p id="c4c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的动画中，我们对句子中第一个单词“the”的权重因子进行了缩放。请记住，属于第一个词的权重因子是点积:q1*k1，q1*k2，q1*k3，q1*k4。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/1e6bbb104d1e7b1d12c4acbeda6c3baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ctrN__xt86dvW7NX06yCaQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">属于第一个单词“the”的权重因子的缩放和softmax(图片由作者提供)</p></figure><p id="e830" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">类似地，对于输入序列中的其他单词“car”、“is”和“blue ”,我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/de9b68f673c4020df124e808fac2af00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ttQAXZnlrcq4QPfWIkVSlA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">属于剩余单词“汽车”、“是”和“蓝色”的权重的缩放和softmax(图片由作者提供)</p></figure><p id="8ed6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就完成了权重因子的计算。</p><h1 id="74db" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">价值观念</h1><p id="3842" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">与“关键向量”和“查询向量”的计算相同，我们通过将单词嵌入乘以矩阵WV来获得“值向量”。值向量的大小也是64。</p><h1 id="7e5a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">额外的</h1><p id="8f75" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">现在，我们将每个“价值向量”乘以其对应的“权重因子”。如前所述，这样我们只保留我们想要关注的单词，而不相关的单词通过用像0.001这样的小数字加权来抑制</p><h1 id="d8b6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">总和</h1><p id="e54d" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">现在我们将属于一个单词的所有加权“值向量”相加。这就在这个位置产生了自我关注层的输出。</p><p id="bc76" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下一个动画中，我们将描述“值向量”的计算以及随后对输入序列中的第一个单词进行的加权和求和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/c963d1e3350496d36401ed90adb8e464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*x3NcwCBExL-_5UVPQraizg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一个单词“the”的值、权重和总和(图片由作者提供)</p></figure><p id="6ce3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">类似地，对于我们输入序列中的其他单词“汽车”、“是”、“蓝色”，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/27d250ce8ddbd2233aa34b1325044e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*I3yHUigmcykbl8EEdpJopw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">剩余单词“汽车”、“是”和“蓝色”的值、权重和总和(图片由作者提供)</p></figure><p id="d83d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自我关注的计算到此结束。自我关注层的输出可以被认为是上下文丰富的单词嵌入。根据上下文，一个词可能有不同的含义:</p><ul class=""><li id="70c7" class="mw mx iq kx b ky kz lb lc le my li mz lm na lq nb nc nd ne bi translated">我喜欢清爽的秋天天气。</li><li id="32f9" class="mw mx iq kx b ky nf lb ng le nh li ni lm nj lq nb nc nd ne bi translated">不要在去电车的路上<strong class="kx ir">摔倒</strong>。</li></ul><p id="79b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，底部的嵌入矩阵只对单个单词进行操作。因此，对于两个句子，我们会错误地获得相同的嵌入向量。自我关注层考虑到了这一点。</p><h1 id="145b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">较短的句子</h1><p id="5b83" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">输入序列的长度应该是固定的，基本上是训练数据集中最长句子的长度。因此，参数定义了转换器可以接受的序列的最大长度。长度更长的序列会被截断。较短的序列用零填充。然而，填充词不应该有助于自我注意的计算。这可以通过在自我关注计算中的softmax步骤之前屏蔽相应的单词(将它们设置为-inf)来避免。这实际上将它们的权重因子设置为零。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/040aba0d1223af3a5e9220e7c1a8b7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uAn_WIagYnYYL-q3rfqhHQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">遮蔽未使用的位置(作者图片)</p></figure><h1 id="6d47" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">多头自我关注</h1><p id="65c8" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">不是执行单一的自我注意功能，作者使用了多个自我注意头，每个都有不同的权重矩阵。多头注意力允许模型在不同位置共同注意来自不同表征子空间的信息。原论文中的变压器使用了八个平行的注意头。注意头的输出被连接并再次乘以附加的权重矩阵WO。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/83d576b58f81eb0ee309e3d29d9a8dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9SGL9LbtH1CWTkkHgV-Xpw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多头自我关注用3个头，原创论文用8个头(图片由作者提供)</p></figure><h1 id="4dbd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">添加并正常化</h1><p id="85e4" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">刚刚讲到的多头自关注机制是编码器的第一个子模块。它周围有一个剩余连接，然后是图层规范化步骤。图层标准化只是减去每个矢量的平均值，然后除以其标准偏差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/27cc536b6e0b4348769ededa90d14d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FMRS9vf5B4aBf2c_e_F8gg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">剩余连接，图层归一化(作者提供的图片)</p></figure><h1 id="2be9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">正向输送</h1><p id="033e" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">自我关注层的输出被馈送到完全连接的前馈网络。这由两个线性转换组成，中间有一个ReLU激活。输入和输出的维数是512，内层的维数是2048。完全相同的前馈网络独立地应用于每个位置，即输入序列中的每个单词。</p><p id="542d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们再次在完全连接的前馈层周围采用剩余连接，然后进行层归一化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/9f54924f03c5a80b1dd7842ad2df7354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2KZQBhtv2l2Z5or5IFUELg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">完全连接的前馈网络、剩余连接、层归一化(图片由作者提供)</p></figure><h1 id="4adf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">编码器堆栈</h1><p id="74e8" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">整个编码组件是六个编码器的堆栈。这些编码器在结构上都是一样的，但它们不共享重量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/d15d9b470c128f8ed01572e135250e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hTuCAGdXQT-DV_--RW3iYg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">六个编码器的堆栈(图片由作者提供)</p></figure><p id="995e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下一篇文章中，我们将讨论变压器的解码器部分。这应该是非常直接的，因为大多数需要的概念已经在这篇文章中介绍过了。</p><h1 id="e073" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="1caf" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><a class="ae np" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a> <br/> <a class="ae np" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图文并茂的变形金刚</a> <br/> <a class="ae np" rel="noopener" target="_blank" href="/transformers-explained-65454c0f3fa7">变形金刚解说</a> <br/> <a class="ae np" href="https://www.r-craft.org/r-news/get-busy-with-word-embeddings-an-introduction/" rel="noopener ugc nofollow" target="_blank">忙于文字嵌入</a></p></div></div>    
</body>
</html>