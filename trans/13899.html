<html>
<head>
<title>Tune the hyperparameters of your deep learning networks in Python using Keras and Talos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Keras和Talos在Python中调整深度学习网络的超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tune-the-hyperparameters-of-your-deep-learning-networks-in-python-using-keras-and-talos-2a2a38c5ac31?source=collection_archive---------21-----------------------#2020-09-24">https://towardsdatascience.com/tune-the-hyperparameters-of-your-deep-learning-networks-in-python-using-keras-and-talos-2a2a38c5ac31?source=collection_archive---------21-----------------------#2020-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5556" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Talos在CNN中网格搜索超参数，例如狗-猫CNN分类器</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79fdae5492c137d4ba1ea9dc18cb9465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTd1Fe40h2garDRJelg2sQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马里奥·高在Unsplash上拍摄的照片</p></figure><p id="6f8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着深度学习框架的发展，许多人设计人工神经网络的架构变得更加方便和容易。Tensorflow、Keras和Pytorch这3个最流行的框架使用得更频繁。为了改善我们的神经网络的性能，有许多方法，例如，使用数据扩充来改善数据质量。然而，数据质量是数据科学的源泉。为了获得更好的数据质量，通常需要额外的费用、时间和人力资源。因此，我们更喜欢处理模型的超参数🏄🏼。</p><p id="e442" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开始吧！</p><h1 id="86bc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.参数或超参数</h1><p id="b26a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">模型参数是模型内部的配置变量。它依赖于模型的训练数据。模型的参数可以通过给定数据与模型拟合来估计。</p><p id="5d3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型超参数是模型外部的配置。超参数是为了帮助我们找到不依赖于训练数据的模型参数。</p><p id="b096" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，我们通过适当设置一系列超参数来寻找模型的参数，优化了训练过程，充分利用了数据。这意味着我们可以手动给定超参数值，并且它们不能在训练期间更新。但是参数是模型的本能，在训练过程中不断更新。</p><p id="00b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个不恰当的比喻，如果我们把一个学生看作一个模型，他的知识、性格、技能更像是模型的参数。我们训练他获得这些能力和特征的方式可以被视为超参数。</p><p id="2df3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于超参数是模型参数的关键，我们应该对它们给予足够的重视。如何选择模型的超参数？处理这个问题需要足够的知识和耐心。</p><h1 id="02f8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.调整超参数的策略</h1><p id="2307" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">通常有5种不同的优化技术:</strong></p><ol class=""><li id="58a6" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir">手动搜索</strong>:我们根据自己的判断/经验选择一些模型超参数。然后，我们训练该模型，评估其准确性，并再次开始该过程。重复这一循环，直到达到令人满意的精确度。</li><li id="b61d" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">网格搜索</strong>:超参数网格，在训练算法的超参数空间的给定子集上，在每个可能的组合上训练/测试我们的模型。这是超参数优化的传统方法。</li><li id="f369" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">随机搜索</strong>:通过随机选择覆盖所有组合的全部选择。因此，它可以通过选择这些超参数的一些随机组合来减少搜索迭代的次数。</li><li id="19d4" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">贝叶斯优化:</strong>是一种针对黑盒功能全局优化的顺序设计策略。它通过牢记过去的结果来选择输入值，从而减少了搜索迭代的次数。</li><li id="0302" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">进化算法</strong>:用一些预定义的超参数创建N个机器学习模型的群体。它生成一些子代，这些子代具有与最佳模型相似的超参数，从而再次得到N个模型的群体。通过使用类似于生物进化的机制顺序地选择、组合和改变参数，只有最好的模型会在过程结束时存活下来。它模拟了自然选择的过程，这意味着那些能够适应环境变化的物种能够生存和繁殖，并延续到下一代。</li></ol><h1 id="aa76" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.我们自己的方法:网格搜索</h1><p id="6dd1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在我们的工作中，我们经常使用网格搜索。网格搜索受到高维空间的影响，但通常可以很容易地并行化，因为算法使用的超参数值通常是相互独立的。此外，我们在Colab平台上编写代码，这允许我们在您的浏览器中编写和执行Python:</p><ul class=""><li id="c99d" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr nd mv mw mx bi translated">不需要配置</li><li id="917c" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr nd mv mw mx bi translated">免费访问GPU</li><li id="8aa9" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr nd mv mw mx bi translated">轻松分享</li></ul><h1 id="4a2d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.喀拉斯和塔罗斯</h1><p id="0ce0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你想用最少的代码快速构建和测试一个神经网络，那么Keras就是你需要的。Keras是一个用Python编写的开源神经网络库，是一个为人类而不是机器设计的API。由于Tensorflow 2提供了Keras和直观的高级API tf的紧密集成。keras，有2种方式使用Keras，要么直接导入Keras，要么从tf导入Keras。</p><p id="5e64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Talos于2018年5月11日发布，此后进行了七次升级。当在scan-command中使用Talos运行代码时，所有可能的组合都会在实验中进行测试。</p><blockquote class="ne nf ng"><p id="320f" class="kw kx nh ky b kz la jr lb lc ld ju le ni lg lh li nj lk ll lm nk lo lp lq lr ij bi translated">重要提示:<a class="ae kv" href="https://pypi.org/project/talos/" rel="noopener ugc nofollow" target="_blank"> Talos </a>通过完全自动化超参数调整和模型评估，彻底改变了普通的Keras工作流程。Talos完全公开了Keras功能，不需要学习新的语法或模板。</p></blockquote><p id="54f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用一行命令行安装talos:</p><p id="7781" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nl nm nn no b">pip install talos</code></p><h1 id="ebe6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.CNN的狗与猫分类器</h1><p id="fa82" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了使我们的结果可见和直观，我们用一个简单的例子来分类图像是包含一只狗还是一只猫，这是计算机视觉中的一个古老问题😆。我从<a class="ae kv" href="https://www.kaggle.com/tongpython/cat-and-dog" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载了图像数据集。从链接下载数据集后，它是ZIP文件格式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/66883543dabb2824e6b0291c65ad5f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pq3g2NdawevfJvJII7XdQA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@hanialistek?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">汉娜·利斯塔克</a>在<a class="ae kv" href="https://unsplash.com/s/photos/cat-and-dog?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4d6d8a92297075d66df4fcfd01931c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*eoMlqPzau1G-Bu90heCSOg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Colab中解压缩的数据集</p></figure><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="453e" class="nv lt iq no b gy nw nx l ny nz">from google.colab import drive</span><span id="bdb4" class="nv lt iq no b gy oa nx l ny nz">drive.mount('/content/gdrive/')</span><span id="17a3" class="nv lt iq no b gy oa nx l ny nz">!mkdir -p dataset</span><span id="0c76" class="nv lt iq no b gy oa nx l ny nz">!unzip /content/gdrive/My\ Drive/Colab\ Notebooks/blogs_medium/cat_dog.zip -d dataset/</span></pre><p id="b702" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用几行代码直接在Google Colab中解压文件。</p><p id="b91d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们使用LeNet-5，这是一个22岁的神经网络，通常作为教学样本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/325b9a16bb5a873d8e823f39774903c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4KqpO5GDx-QZa3e49wSjQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank"> LeNet-5架构</a></p></figure><p id="54b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们开始用Keras构建LeNet-5的代码。要在Keras中获得可重复的结果，设置随机种子是必要的。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="4b1f" class="nv lt iq no b gy nw nx l ny nz">import os</span><span id="6d53" class="nv lt iq no b gy oa nx l ny nz">import tensorflow as tf</span><span id="8aaa" class="nv lt iq no b gy oa nx l ny nz">import numpy as np</span><span id="b107" class="nv lt iq no b gy oa nx l ny nz">import random as python_random</span><span id="84fa" class="nv lt iq no b gy oa nx l ny nz">np.random.seed(42)</span><span id="cd56" class="nv lt iq no b gy oa nx l ny nz">python_random.seed(42)</span><span id="4609" class="nv lt iq no b gy oa nx l ny nz">tf.random.set_random_seed(42)</span></pre><p id="8d67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以专注于图像数据。我们需要用<strong class="ky ir">keras . preparation . image</strong>将它们读入训练和验证数组，稍后流入CNN进行训练和验证。所有图片必须有统一的大小，例如(100，100，3)。虽然数据集中的狗或猫的图像大小不同，有些大，有些小，但我们可以通过调整大小使它们大小相等。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="c896" class="nv lt iq no b gy nw nx l ny nz">import keras</span><span id="650f" class="nv lt iq no b gy oa nx l ny nz">import glob</span><span id="49a6" class="nv lt iq no b gy oa nx l ny nz">import os</span><span id="cbc3" class="nv lt iq no b gy oa nx l ny nz">from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,  array_to_img</span><span id="3e29" class="nv lt iq no b gy oa nx l ny nz">from keras.layers import Dense, Conv2D, MaxPool2D, Flatten,Dropout</span><span id="fbc4" class="nv lt iq no b gy oa nx l ny nz">from keras import optimizers</span><span id="37e6" class="nv lt iq no b gy oa nx l ny nz">from keras.models import Sequential</span><span id="12cd" class="nv lt iq no b gy oa nx l ny nz">import numpy as np</span><span id="4c84" class="nv lt iq no b gy oa nx l ny nz">image_size=(100,100)</span><span id="6664" class="nv lt iq no b gy oa nx l ny nz">train_cats = glob.glob('dataset/training_set/training_set/cats/*.jpg')</span><span id="5ca4" class="nv lt iq no b gy oa nx l ny nz">train_dogs = glob.glob('dataset/training_set/training_set/dogs/*.jpg')</span><span id="a6b8" class="nv lt iq no b gy oa nx l ny nz">train_files = [fn for fn in train_cats]+[fn for fn in train_dogs]</span><span id="474c" class="nv lt iq no b gy oa nx l ny nz">print(len(train_files))</span><span id="37ce" class="nv lt iq no b gy oa nx l ny nz">train_imgs = [img_to_array(load_img(img, target_size=image_size)) for img in train_files]</span><span id="ffa3" class="nv lt iq no b gy oa nx l ny nz">train_imgs = np.array(train_imgs)</span><span id="bd9b" class="nv lt iq no b gy oa nx l ny nz">print(train_imgs.shape)</span><span id="e671" class="nv lt iq no b gy oa nx l ny nz">train_labels= [0 for i in range(len(train_cats))]+[1 for i in range(len(train_dogs))]</span><span id="ed65" class="nv lt iq no b gy oa nx l ny nz">val_cats = glob.glob('dataset/test_set/test_set/cats/*.jpg')</span><span id="70bd" class="nv lt iq no b gy oa nx l ny nz">val_dogs = glob.glob('dataset/test_set/test_set/dogs/*.jpg')</span><span id="eb97" class="nv lt iq no b gy oa nx l ny nz">val_files = [fn for fn in val_cats]+[fn for fn in val_dogs]</span><span id="44be" class="nv lt iq no b gy oa nx l ny nz">val_imgs = [img_to_array(load_img(img, target_size=image_size)) for img in val_files]</span><span id="aec8" class="nv lt iq no b gy oa nx l ny nz">val_imgs = np.array(val_imgs)</span><span id="baf5" class="nv lt iq no b gy oa nx l ny nz">print(val_imgs.shape)</span><span id="29a3" class="nv lt iq no b gy oa nx l ny nz">val_labels= [0 for i in range(len(val_cats))]+[1 for i in range(len(val_dogs))]</span></pre><p id="a5a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，所有的“狗”和“猫”都在数组中，要么是训练集，要么是验证集。此外，我们用数字1标记狗，用数字0标记猫。</p><p id="a7a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们使用一次热编码对分类整数特征0和1进行编码。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="5799" class="nv lt iq no b gy nw nx l ny nz">num_classes = 2</span><span id="0b05" class="nv lt iq no b gy oa nx l ny nz">epochs = 10</span><span id="19b9" class="nv lt iq no b gy oa nx l ny nz">input_shape = (100,100,3)</span><span id="d381" class="nv lt iq no b gy oa nx l ny nz"># encode text category labels</span><span id="b0c3" class="nv lt iq no b gy oa nx l ny nz">from sklearn.preprocessing import OneHotEncoder, LabelEncoder</span><span id="fee0" class="nv lt iq no b gy oa nx l ny nz">train_labels_array = np.array(train_labels)</span><span id="1645" class="nv lt iq no b gy oa nx l ny nz">le = LabelEncoder()</span><span id="6197" class="nv lt iq no b gy oa nx l ny nz">train_integer_encoded = le.fit_transform(train_labels_array)</span><span id="9e95" class="nv lt iq no b gy oa nx l ny nz">ohe = OneHotEncoder(sparse=False)</span><span id="b631" class="nv lt iq no b gy oa nx l ny nz">train_integer_encoded = train_integer_encoded.reshape(len(train_integer_encoded), 1)</span><span id="d5d6" class="nv lt iq no b gy oa nx l ny nz">train_labels_ohe = ohe.fit_transform(train_integer_encoded)</span><span id="6668" class="nv lt iq no b gy oa nx l ny nz">validation_labels_array = np.array(val_labels)</span><span id="eed7" class="nv lt iq no b gy oa nx l ny nz">validation_integer_encoded = le.fit_transform(validation_labels_array)</span><span id="5e84" class="nv lt iq no b gy oa nx l ny nz">ohe = OneHotEncoder(sparse=False)</span><span id="1107" class="nv lt iq no b gy oa nx l ny nz">validation_integer_encoded = validation_integer_encoded.reshape(len(validation_integer_encoded), 1)</span><span id="85a2" class="nv lt iq no b gy oa nx l ny nz">validation_labels_ohe = ohe.fit_transform(validation_integer_encoded)</span></pre><p id="f02b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据必须标准化，以便模型可以更快地收敛。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="a100" class="nv lt iq no b gy nw nx l ny nz">train_imgs_scaled = train_imgs.astype('float32')</span><span id="82e3" class="nv lt iq no b gy oa nx l ny nz">validation_imgs_scaled  = val_imgs.astype('float32')</span><span id="9f0f" class="nv lt iq no b gy oa nx l ny nz">train_imgs_scaled /= 255</span><span id="857d" class="nv lt iq no b gy oa nx l ny nz">validation_imgs_scaled /= 255</span></pre><p id="a7ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后建立一个模型结构</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="d516" class="nv lt iq no b gy nw nx l ny nz">from keras import layers</span><span id="d01d" class="nv lt iq no b gy oa nx l ny nz">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout</span><span id="90b3" class="nv lt iq no b gy oa nx l ny nz">from keras.models import Model</span><span id="5754" class="nv lt iq no b gy oa nx l ny nz">from keras import optimizers</span><span id="cbf9" class="nv lt iq no b gy oa nx l ny nz">def lenet_5(in_shape=(100,100,3), n_classes=2):</span><span id="eba5" class="nv lt iq no b gy oa nx l ny nz">in_layer = layers.Input(in_shape)</span><span id="172a" class="nv lt iq no b gy oa nx l ny nz">conv1 = layers.Conv2D(filters=20, kernel_size=5,</span><span id="77b1" class="nv lt iq no b gy oa nx l ny nz">padding='same', activation='relu')(in_layer)</span><span id="b0c6" class="nv lt iq no b gy oa nx l ny nz">pool1 = layers.MaxPool2D()(conv1)</span><span id="76e0" class="nv lt iq no b gy oa nx l ny nz">conv2 = layers.Conv2D(filters=50, kernel_size=5,</span><span id="37db" class="nv lt iq no b gy oa nx l ny nz">padding='same', activation='relu')(pool1)</span><span id="7fc4" class="nv lt iq no b gy oa nx l ny nz">pool2 = layers.MaxPool2D()(conv2)</span><span id="ca6c" class="nv lt iq no b gy oa nx l ny nz">flatten = layers.Flatten()(pool2)</span><span id="3867" class="nv lt iq no b gy oa nx l ny nz">dense1 = layers.Dense(500, activation='relu',kernel_initializer='glorot_uniform')(flatten)</span><span id="2c37" class="nv lt iq no b gy oa nx l ny nz">preds = layers.Dense(2, activation='softmax',kernel_initializer='glorot_uniform')(dense1)</span><span id="6811" class="nv lt iq no b gy oa nx l ny nz">opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)</span><span id="5d1c" class="nv lt iq no b gy oa nx l ny nz">model = Model(in_layer, preds)</span><span id="d386" class="nv lt iq no b gy oa nx l ny nz">model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])</span><span id="858f" class="nv lt iq no b gy oa nx l ny nz">return model</span><span id="d5a5" class="nv lt iq no b gy oa nx l ny nz">if __name__ == '__main__':</span><span id="bf32" class="nv lt iq no b gy oa nx l ny nz">model = lenet_5()</span><span id="2512" class="nv lt iq no b gy oa nx l ny nz">print(model.summary())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b764eee420c1950bb686a1ff043ee6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*cyAMxRrty4FzGsWYJRzZ2g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型摘要</p></figure><p id="379a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们为10个时期训练模型，并将batch_size定义为200。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="cfd2" class="nv lt iq no b gy nw nx l ny nz">from keras.callbacks import ModelCheckpoint</span><span id="9f97" class="nv lt iq no b gy oa nx l ny nz">checkpoint = ModelCheckpoint("lenet.h5",monitor='val_acc',verbose=1,save_best_only=True, save_weights_only= False, mode ='auto',period=1)</span><span id="1921" class="nv lt iq no b gy oa nx l ny nz">history = model.fit(x=train_imgs_scaled, y=train_labels_ohe, validation_data=(validation_imgs_scaled, validation_labels_ohe), batch_size=200, epochs=10, callbacks=[checkpoint], shuffle=True)</span></pre><p id="c52f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过长时间的等待，我们可以得到一个训练/验证图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/f5a2c7c1756d0d6b10525e0ad1bfbf8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2a0TlWzY678oXgk3TnJi6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">10个时期后，val_acc为0.7207，val_loss为0.5841</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3e9c9a3e6ca293d61dd8f48e90343e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*aL1pESZVvrqTT6mbu8ePpA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型acc和loss</p></figure><p id="0674" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图显示，经过5个时期后，模型改善不多。但它并没有过度适应。因此，我们仍然可以使用获得的模型。</p><p id="d702" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们想做更多的努力来为我们的狗-猫分类器训练一个更好的LeNet-5模型，所以我们专注于模型的超参数来改进模型🎑。</p><h1 id="291d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">6.LetNet-5中的Talos，带代码</h1><p id="2181" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这里我们定义了一个新的函数，它具有与LeNet-5相同的结构，但是模型中的一些超参数是可变的。我们将这些可变超参数保存在字典“p”中。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="c420" class="nv lt iq no b gy nw nx l ny nz">p = {'first_hidden_layer': [500],</span><span id="cbc2" class="nv lt iq no b gy oa nx l ny nz">'opt': [Adam, sgd],</span><span id="ca21" class="nv lt iq no b gy oa nx l ny nz">'dropout': [0,0.5],</span><span id="5e7c" class="nv lt iq no b gy oa nx l ny nz">'weight_regulizer':[None],</span><span id="aee6" class="nv lt iq no b gy oa nx l ny nz">'lr': [1],</span><span id="ec55" class="nv lt iq no b gy oa nx l ny nz">'emb_output_dims': [None],</span><span id="0ed3" class="nv lt iq no b gy oa nx l ny nz">'kernel_initializer':["glorot_uniform"]}</span></pre><p id="239f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了减少计算机计算和程序运行时间，在字典中我们只设置了' opt '和' dropout '变量，optimizer有两个选项(Adam或SGD ), dropout有两个可能的值。总共有4种组合。</p><pre class="kg kh ki kj gt nr no ns nt aw nu bi"><span id="ae47" class="nv lt iq no b gy nw nx l ny nz">from keras.optimizers import Adam,sgd<br/>from keras.models import load_model</span><span id="5939" class="nv lt iq no b gy oa nx l ny nz">from keras.utils import CustomObjectScope</span><span id="27b6" class="nv lt iq no b gy oa nx l ny nz">from keras.initializers import glorot_uniform</span><span id="000c" class="nv lt iq no b gy oa nx l ny nz">import talos</span><span id="f4ed" class="nv lt iq no b gy oa nx l ny nz">from talos.model.normalizers import lr_normalizer</span><span id="1433" class="nv lt iq no b gy oa nx l ny nz">def lenet_model(x_train, y_train,x_val, y_val, params):</span><span id="aa21" class="nv lt iq no b gy oa nx l ny nz">in_layer = layers.Input((100,100,3))</span><span id="8bc3" class="nv lt iq no b gy oa nx l ny nz">conv1 = layers.Conv2D(filters=20, kernel_size=5,</span><span id="4b4d" class="nv lt iq no b gy oa nx l ny nz">padding='same', activation='relu')(in_layer)</span><span id="a06a" class="nv lt iq no b gy oa nx l ny nz">pool1 = layers.MaxPool2D()(conv1)</span><span id="c36a" class="nv lt iq no b gy oa nx l ny nz">conv2 = layers.Conv2D(filters=50, kernel_size=5,</span><span id="92a8" class="nv lt iq no b gy oa nx l ny nz">padding='same', activation='relu')(pool1)</span><span id="44ff" class="nv lt iq no b gy oa nx l ny nz">pool2 = layers.MaxPool2D()(conv2)</span><span id="6515" class="nv lt iq no b gy oa nx l ny nz">flatten = layers.Flatten()(pool2)</span><span id="d6a2" class="nv lt iq no b gy oa nx l ny nz">dense1 = layers.Dense(params['first_hidden_layer'], activation='relu')(flatten)</span><span id="f36f" class="nv lt iq no b gy oa nx l ny nz">dropout1 = layers.Dropout(params['dropout'])(dense1)</span><span id="c07b" class="nv lt iq no b gy oa nx l ny nz">preds = layers.Dense(2, activation='softmax')(dropout1)</span><span id="5bc8" class="nv lt iq no b gy oa nx l ny nz">model = Model(in_layer, preds)</span><span id="cf06" class="nv lt iq no b gy oa nx l ny nz">model.compile(loss="categorical_crossentropy", optimizer=params['opt'](lr=lr_normalizer(params['lr'],params['opt'])), metrics=["acc"])</span><span id="c927" class="nv lt iq no b gy oa nx l ny nz">steps_per_epoch = int(np.ceil(train_imgs.shape[0] / 20)) - 1</span><span id="c18c" class="nv lt iq no b gy oa nx l ny nz">history = model.fit(x=train_imgs_scaled, y=train_labels_ohe, validation_data=(validation_imgs_scaled, validation_labels_ohe), batch_size=200, epochs=10, callbacks=[talos.utils.ExperimentLogCallback('kgt', params)], verbose=1)</span><span id="4b6a" class="nv lt iq no b gy oa nx l ny nz">return history, model</span><span id="1143" class="nv lt iq no b gy oa nx l ny nz">t = talos.Scan(x=train_imgs_scaled, y=train_labels_ohe, model=lenet_model, experiment_name= 'kgt', params=p)</span></pre><p id="eb3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">借助于扫描命令(talos。扫描)，我们开始配置实验。它将比训练最后一个基本LeNet-5模型持续更长的时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/3febb09bdc03e605fc07dded4af3a9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5r3bw9ltcz6ahDldl7bW0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练实验过程中的进度条</p></figure><p id="6d6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实验报告以csv格式保存。文件格式。我们可以阅读csv。文件在表中显示结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/dfaa43b71c17510dbf14c2f878db2b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTyhB_4hZNoBeKvr7nRx9Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表中的实验报告</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2cc65e766fc6dd501481278ce14f06d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*9Cd6u_E1MQMFQv4Wt_LAaQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上图:val_acc图。下图:val_loss图</p></figure><p id="dcaa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过绘制validation_accuracy(上图)和validation_loss(下图)，我们可以得出结论，第0次和第3次实验的训练模型比第2次和第4次实验的模型好得多。对比实验参数信息，我们发现adam模型具有更好的性能。辍学方法在培训LeNet-5中发挥了一点作用。</p><p id="4d0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">综合考虑，模型0的性能最好，它使用了Adam，但没有辍学。</p><h1 id="1bc6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">7.结论</h1><p id="49e3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这个故事中，我们介绍了如何使用talos通过Keras构建的CNN来调整a的超参数。首先，有一些参数和超参数的基础知识，并回顾了优化超参数的常用方法。在故事的其余部分，我们构建了一个基于LeNet-5的猫狗分类器，并扫描了所有感兴趣的超参数组合。通过观察验证的度量，我们可以知道哪个超参数影响最大，哪个组合给出了最好的结果🏁。</p><p id="6c79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">代码在我的GitHub里</strong>😬</p><p id="987f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/Kopfgeldjaeger/Medium_blogs_code/tree/master/2_talos_grid_search" rel="noopener ugc nofollow" target="_blank">https://github . com/Kopfgeldjaeger/Medium _ blogs _ code/tree/master/2 _ talos _ grid _ search</a></p><h1 id="f83f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">8.参考</h1><p id="a3ee" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Liashchynskyi，p .，&amp; Liashchynskyi，P. (2019)。网格搜索、随机搜索、遗传算法:NAS的一大对比。<em class="nh"> arXiv预印本arXiv:1912.06059 </em></p><p id="2616" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">范里恩，J. N .，&amp;胡特，F. (2018，7月)。跨数据集的超参数重要性。在<em class="nh">第24届ACM SIGKDD知识发现国际会议论文集&amp;数据挖掘</em>(第2367–2376页)。</p><p id="dfaa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f .哈特、j .吕克和l .施密特-蒂梅(2015年)。超越超参数的手动调整。<em class="nh"> KI-Künstliche Intelligenz </em>，<em class="nh"> 29 </em> (4)，329–337。</p><div class="oi oj gp gr ok ol"><a href="https://autonomio.github.io/docs_talos" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">Talos用户手册</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">欢迎来到塔罗斯！您可以使用Talos对Keras模型进行超参数优化。Talos允许您使用Keras…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">autonomio.github.io</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz kp ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://keras.io/" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ir gy z fp oq fr fs or fu fw ip bi translated">keras:Python深度学习API</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">Keras是为人类设计的API，不是为机器设计的。Keras遵循减少认知负荷的最佳实践:it…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">keras.io</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz kp ol"/></div></div></a></div></div></div>    
</body>
</html>