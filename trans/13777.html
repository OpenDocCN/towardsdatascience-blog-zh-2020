<html>
<head>
<title>Math for Machine Learning: Probability and Statistics Prerequisite</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的数学:概率和统计的先决条件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-for-machine-learning-prerequisites-7eb1e3faeb3e?source=collection_archive---------67-----------------------#2020-09-21">https://towardsdatascience.com/math-for-machine-learning-prerequisites-7eb1e3faeb3e?source=collection_archive---------67-----------------------#2020-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d756" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">复习一些问题集，包括机器学习的基本概率和统计(ProbStat)</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="cbdf" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">大家好，欢迎来到我的第二篇帖子！这将是我第一篇文章的延续。这里我们要讲技术，或者我应该说有点“数学”。因此，对于已经远离动手数学一段时间的你，我建议你复习概率、统计和微积分方面的内容。你可以在阅读这个故事的过程中这样做，这样你就可以轻松地理解我的解释。</p><p id="eb9e" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这个故事是受我在哥伦比亚大学所学课程的启发。课程的教学大纲和高级细节可以通过此链接访问:<a class="ae li" href="https://www.cs.columbia.edu/~djhsu/coms4771-f19/" rel="noopener ugc nofollow" target="_blank"><em class="lj">【COMS w 4771机器学习2019秋季由Daniel Hsu </em> </a> <em class="lj">。</em></p><p id="fb56" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">正如你可能意识到的，你不能访问作业文件(这是教授想要的)。因此，我要讲的习题集也会有些不同，我会说它会是我在课程中所做的一些变化。这不会完全不同，我会尽我最大的努力，这样我就能给出同样水平的东西。</p><p id="c44b" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我将在这个故事中给你三个不同的问题，我觉得它们涵盖了你开始学习机器学习(特别是贝叶斯ML)所需了解的所有基本概念。由于大多数解决方案都不是微不足道的，我要警告你，这个故事将会很长，所以请耐心等待！</p><p id="5242" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，在我们开始之前，我想让你知道这个帖子包含了乳胶配方。为了正确地渲染它们，在阅读这篇文章时，安装<a class="ae li" href="https://chrome.google.com/webstore/detail/math-anywhere/gebhifiddmaaeecbaiemfpejghjdjmhc/" rel="noopener ugc nofollow" target="_blank"><em class="lj">Math Anywhere</em></a><em class="lj"/>插件，并在你的浏览器(我建议使用谷歌Chrome)上激活它。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="1530" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题1:描述</h2><p id="4f25" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">首先，掌握你的基本概率！</p><p id="ad88" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们将使用k-最近邻(kNN)算法作为我们的背景故事。如果你不熟悉它，或者如果你需要一些新鲜的东西，我建议你看一看这个惊人的<a class="ae li" rel="noopener" target="_blank" href="/a-simple-introduction-to-k-nearest-neighbors-algorithm-b3519ed98e"> <em class="lj">故事</em> </a> <em class="lj"> </em>来自fellow toward Data Science(TDS)作者。</p><p id="9c1c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">让我们将范围限制为$k=1$,即我们将只找到一个最近的邻居。假设我们有$n$个训练示例，那么这个算法的复杂度是$O(n)$因为我们只需迭代$n$个示例，每个示例计算$L_2$个差异，然后最后挑选具有最小$L_2$个距离的邻居。</p><p id="f400" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">假设你在一家公司工作，你的老板要求你加快这个1-NN过程的计算速度。他不在乎结果是不是寸寸完美，只要落到最少10%的邻居身上。例如，如果有100万个数据，您挑选的邻居必须在100k个数据内，并且具有最小的$L_2$距离。</p><p id="3192" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">让我们称这个问题为“近似最近邻”，我们应该有下面的数学定义:</p><blockquote class="mi mj mk"><p id="1bc4" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">给定\mathbb{R}^d$的测试点$x，我们说10%近似的最近邻点是$ x _ { I(x)} s . t . | | x _ { I(x)} | | _ 2 $是到训练点$ | | x x1 | | _ 2、…，| | x xn | | _ 2 $的最小10%距离之一，其中$n$是训练样本的数量</p></blockquote><p id="4c88" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">当然，从训练示例中抽取样本$\lceil{0.9n}\rceil + 1$就可以简单地解决上述问题。换句话说，如果您有一百万个数据，只需采样900k + 1个数据，无需替换，就可以了。然而，这并没有加快计算的速度，这是相当无用的。</p><p id="fc00" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">所以，让我提出一些相当出乎意料的东西，至少这是我在处理与这个问题类似的问题时的感受。如果我说，通过精确地采样50个例子，称之为$T=50$,我们可以确保，从数学上讲，我们得到具有最小$ L2 $距离的10%例子之外的邻居的概率非常小，最多0.5%。这与数据量($n$)无关。你能证明吗？</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="b430" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题1:解决方案</h2><p id="10e7" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">问题1的解决方案实际上非常简单，尽管问题的描述有点让人不知所措。看到这基本上只是一个没有更换问题的抽样。我处理这个问题的方法是:既然我们在寻找所有采样数据不在前10%的概率，那么这与计算所有采样数据(其中50个)在其余90%的数据中的概率是一样的。</p><p id="25af" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">因此，我们可以把概率，比如说$p$写成:</p><p id="7d72" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$ $ p = \ frac { \ text {从0.9n个数据中选取50个样本的组合数} } { \ text {从n个数据中选取50个样本的组合数}}$$</p><p id="0baa" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$ $ p = \ frac { \ binom { \ lce il { 0.9n } \ rce il { 50 } } { \ binom { n } { 50 } } = \ frac { \ frac { \ lce il { 0.9n } \ rce il！}{(\lceil{0.9n}\rceil-50)！\:50!} } { n！}{(n-50)！\:50!} } = \ frac { \ lce il { 0.9n } \ rce il \:(n-50)！}{n！\:(\lceil{0.9n}\rceil-50)！}=(\frac{(n-50)！}{n！})(\frac{\lceil{0.9n}\rceil！}{(\lceil{0.9n}\rceil-50)！})$ $<br/>$$p=\frac{\lceil{0.9n}\rceil(\lceil{0.9n}\rceil-1)(\lceil{0.9n}\rceil-2)…(\lceil{0.9n}\rceil-49)}{n(n-1)(n-2)…(n-49)}=\prod_{i=0}^{49}\frac{\lceil{0.9n}\rceil-i}{n-i}$$</p><p id="05d9" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">不管数据大小如何，我们都有兴趣看看上述概率的值。因此，我的方法是我们取$n \to \infty$。由此得出的概率值将是最高的(试着解释为什么会这样)。因此，我们可以证明这种可能性至多是0.5%。</p><p id="a688" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$$p_{n\to\infty}=\lim_{n\to\infty}\prod_{i=0}^{49}\frac{\lceil{0.9n}\rceil-i}{n-i}$$</p><p id="2c46" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">使用乘法法则求极限:</p><p id="9749" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$$p_{n\to\infty}=\prod_{i=0}^{49}\lim_{n\to\infty}\frac{\lceil{0.9n}\rceil-i}{n-i}$$</p><p id="ac64" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这很容易解决，如果不是天花板函数的话。我将把如何解决如果$0.9n$不是整数的问题留给读者作为练习。在这里，我将只讨论更简单的情况，即当$0.9n$是整数时:</p><p id="d905" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$$p_{n\to\infty}=\prod_{i=0}^{49}\lim_{n\to\infty}\frac{\lceil{0.9n}\rceil-i}{n-i}=\prod_{i=0}^{49}\lim_{n\to\infty}\frac{0.9n-i}{n-i}=\prod_{i=0}^{49}\lim_{n\to\infty}\frac{0.9-i/n}{1-i/n}$$<br/>$$p_{n\to\infty}=\prod_{i=0}^{49}\frac{0.9–0}{1–0}=\prod_{i=0}^{49}0.9={0.9}^{50}= 0.000美元。20121.868686866667</p><p id="4766" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们可以从上面看到，我们没有从前10%的例子中得到哪怕一个样本的机会(不管数据大小)最多是0.5%左右。</p><p id="292d" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这相当了不起，我们可以将复杂度从$O(n)$降低到$O(50)$这是常数！然而，在现实中，情况并非总是如此。在下面的评论部分，让我知道你是否知道为什么会这样！</p><p id="699a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">PS:您可能会发现另一个问题，请注意，$n \to\infty$的无替换问题抽样可能会被视为(或被视为)有替换问题的抽样！</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="d49f" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题2:描述</h2><p id="6535" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">这将是一个标准的教科书问题，当你学习统计学时，你可能会发现到处都是，这是为了推导最大似然估计(MLE)的公式。但是知道这一点意味着你有一个很好的基础去钻研ML的一些子领域，特别是贝叶斯推理。</p><p id="a267" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">如果你不熟悉这个话题，可以考虑看看这篇很酷的文章，也是TDS的作者之一。你可能也想熟悉我下面要用到的术语(独立同分布，随机变量，概率密度函数，可能性，等等。).所以，我们开始吧！</p><blockquote class="mi mj mk"><p id="90fd" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">考虑一个独立同分布随机变量$X_1，X_2，…，X_n$的统计模型，由\ theta \ in \ Theta $参数化。参数空间是R:\ Theta &gt; 0 \ }＄中的正实数\ Theta = \ { \ theta。</p><p id="6ad2" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">$X_1$的分布$P_\theta$如下:</p><p id="8e32" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">$ X _ 1 \ ~ P _ \有一个概率密度函数(P . d . f .)$ f _ \θ$满足:</p><p id="6f23" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">$ $ f _ \ theta \ propto 1 \ text { for all } 0 \ leq x \ leq \ theta $ $</p><p id="28e2" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">$ $ f _ \ theta = 0 \ text { for } x&lt; 0 \text{ and } x &gt;\ theta $ $</p><p id="cf05" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">给定数据$(X_1，X_2，…，X_n) = (x_1，x_2，…，x_n)$，推导出一个简单的最大似然估计公式！</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="c236" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题2:解决方案</h2><p id="3d42" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">我们将考虑p.d.f的两种定义:</p><ol class=""><li id="a7bc" class="mo mp iq ko b kp kq ks kt kv mq kz mr ld ms lh mt mu mv mw bi translated">对于$x &lt;0$ and $x&gt; \theta$，f_{\theta}(x)=0$。因此，在这种情况下，可能性总是为零，MLE也将为零。</li><li id="19b1" class="mo mp iq ko b kp mx ks my kv mz kz na ld nb lh mt mu mv mw bi translated">对于$0\leq{x}\leq{\theta}美元，$f_{\theta}(x)\propto1美元。记住$\propto$意味着p.d.f .与1成比例，但它并不完全是1，而是某个常数。这意味着$f_{\theta}(x)=c$对于某个常数c: $0 <c/>使用p.d.f应该积分为1的性质，我们可以确定c:<br/>$$c\int_{0}^{\theta}dx=1$$<br/>$$cx\big|_0^{\theta}=1$$<br/>$ $ c[\ theta-0]= 1 $ $<br/>$ $ c = 1/\ theta $ $<br/>因此，$\{x_1，x_2，x_3，…，x_n\}$的可能性将是:<br/>$$l(\theta|x)=\prod_{i=1}^{n} f _ { \ theta }(x _ I因为我们知道$0\leq{x}\leq{\theta}$，所以最小的$\theta$可能是集合中最大的$x$。因此，MLE公式将是:<br/> $$\boldsymbol{\max(\{x_1，x_2，x_3，…，x_n\})}$$由于$0\leq{x}\leq{\theta}$和$\theta &gt; 0$，此MLE公式将总是大于或等于情况1的MLE($ \ geq { 0 })$。因此，我们可以选择它作为我们的MLE公式。</li></ol></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="17ed" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题3:描述</h2><p id="fe20" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">在这个问题中，你将看到一些模型来收集数据，确保某种程度的隐私保证，这是一个你可能会感兴趣的话题。</p><p id="865b" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">由于一些国家的选举日即将到来，我认为在这里把它作为背景故事会很有趣。假设其中一个国家有两个总统候选人，称之为国家$U$，即Murpt先生和Nebid先生。将Murpt先生称为$M$而将Nebid先生称为$N$。</p><p id="c583" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">想象一下，你是一名独立的数据科学家，一些团体雇用你来估计该国选择其中一名候选人的人口比例。你当然可以很容易地直接问人们他们会投谁的票，但有些人可能会觉得与你直接分享他们的信息很不舒服，因为你基本上什么都不是。</p><p id="3307" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在这里，我给大家介绍一个概念，叫做“随机化回答”。基本上，它试图通过不直接询问他们的选择来解决隐私问题。相反，您需要对每个被调查的个人进行以下操作:</p><blockquote class="mi mj mk"><p id="7a28" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">让他/她扔两次硬币，告诉他/她不要让你知道结果</p><p id="be81" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">如果至少有一次投掷是正面，告诉他/她如实回答(例如，如果他/她投了$M$那么他/她会告诉你$M$)</p><p id="9344" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">如果两次投掷都是反面，告诉他/她给出相反的回答(例如，如果他/她投了$M$，那么他/她会告诉你$N$)</p></blockquote><p id="9f91" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我认为上面的方案保证了高度的隐私保证，当然，除非硬币被操纵了。</p><p id="773f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">现在我们来看主要问题。</p><blockquote class="mi mj mk"><p id="c76e" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">考虑使用上述方案收集的$n$响应的统计模型，其中响应被视为iid {0，1}值的随机变量$Y_1，Y_2，…，Y_n$，并且所有投掷硬币都是独立的。设[0，1]$中的$ \ theta \表示模型的参数，该参数等于人口中投票给$M$的个体比例。</p></blockquote><p id="880f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">然后，回答这三个问题:</p><blockquote class="mi mj mk"><p id="4dd1" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">$Y_1=1$的概率是多少？答案应该以$\theta$的形式给出。</p><p id="f5cc" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">给定数据$y_1，y_2，…，y_n \in {0，1}$的对数似然是多少？答案应该以$\theta$和$y_1，y_2，…，y_n$的形式给出</p><p id="19fd" class="km kn lj ko b kp kq jr kr ks kt ju ku ml kw kx ky mm la lb lc mn le lf lg lh ij bi translated">假设$n=100$，也就是说，我们只从国家的每个州抽取几个人，等于1的$y_i$的数量是40，也就是$\sum_{i=1}^n{y_i}=40$.用[0，1]$绘制对数似然作为$ \ theta \的函数。最有可能出现的$\theta$是什么？</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="7b4f" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题3.1:解决方案</h2><p id="1cfd" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">设X是随机变量，表示在两次掷硬币中出现的正面数量($X=[0，1，2]$)。那么，我们可以在下面定义两个概率:<br/> $$P(X=0)=P(尾)。P(tails)=(1/2)(1/2)= 1/4 $ $<br/>$ $ P(X \ ge Q1)= 1-P(X = 0)= 1-(1/2)(1/2)= 3/4 $ $</p><p id="ac84" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">使用这些概率，$Y_1=1$发生在当一个人选择$M$得到至少一个人头$P(X\geq1)$或当一个人选择$N$没有人头$P(X=0)$时。另外，请记住，选择$M$的概率为$\theta$，选择$N$的概率为$1-\theta$。因此，$P(Y_1)=1$是:</p><p id="04a4" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$ $ P(Y _ 1 = 1)= P(X = 0)(1-\ theta)+P(X \ ge Q1)\ theta $<br/>$ $ P(Y _ 1 = 1)=(1/4)(1-\ theta)+(3/4)\ theta $<br/>$ \ bold symbol { P(Y _ 1 = 1)=(1/2)\ theta+(1/4)} $ $</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="944f" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题3.2:解决方案</h2><p id="accb" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">因为这个问题涉及到离散的随机变量，我们可以用与建模概率相同的函数来建模可能性。为了计算概率，我们需要定义两个部分:</p><ol class=""><li id="6812" class="mo mp iq ko b kp kq ks kt kv mq kz mr ld ms lh mt mu mv mw bi translated">$\binom{n}{\sum_{i=1}^{n}y_i}$.说，事件(值为y_i$)有多少种组合比如$n=2$有四种可能:[0，0]，[0，1]，[1，0]，[1，1]。</li><li id="227f" class="mo mp iq ko b kp mx ks my kv mz kz na ld nb lh mt mu mv mw bi translated">对于每一个事件，如果$y_i=1$，概率将是$P(y_i=1)$，否则如果$y_i=0$，概率将是$P(y_i=0)$。这可以表述为$p(y_i=1)^{y_i}p(y_i=0)^{1-y_i}$，它基本上是一个二项分布。</li></ol><p id="468c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">因此，可能性是:</p><p id="0774" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$$l(\theta|n,\sum_{i=1}^{n}y_i)=p(\sum_{i=1}^{n}y_i)|n,\theta)$$<br/>$$l(\theta|n,\sum_{i=1}^{n}y_i)=\binom{n}{\sum_{i=1}^{n}y_i}\prod_{i=1}^{n}p(y_i=1)^{y_i}p(y_i=0)^{1-y_i}$$<br/>$$l(\theta|n,\sum_{i=1}^{n}y_i)=\binom{n}{\sum_{i=1}^{n}y_i}\prod_{i=1}^{n}p(y_i=1)^{y_i}(1-p(y_i=1))^{1-y_i}$$<br/>$$l(\theta|\sum_{i=1}^{n}y_i)=\binom{n}{\sum_{i=1}^{n}y_i}\prod_{i=1}^{n}({\frac{1}{2}\theta+\frac{1}{4}})^{y_i}({-\frac{1}{2}\theta+\frac{3}{4}})^{1-y_i}$$<br/>那么，对数似然将是:<br/>$$\ln{l(\theta|\sum_{i=1}^{n}y_i)}=\ln{\binom{n}{\sum_{i=1}^{n}y_i}\prod_{i=1}^{n}({\frac{1}{2}\theta+\frac{1}{4}})^{y_i}({-\frac{1}{2}\theta+\frac{3}{4}})^{1-y_i}}$$<br/>$$\ln{l(\theta|\sum_{i=1}^{n}y_i)}=\ln{\binom{n}{\sum_{i=1}^{n}y_i}}+\ln{\prod_{i=1}^{n}({\frac{1}{2}\theta+\frac{1}{4}})^{y_i}({-\frac{1}{2}\theta+\frac{3}{4}})^{1-y_i}}$$<br/>$$\ln{l(\theta|\sum_{i=1}^{n}y_i)}=\ln{\binom{n}{\sum_{i=1}^{n}y_i}}+\sum_{i=1}^{n}\ln({\frac{1}{2}\theta+\frac{1}{4}})^{y_i}+\sum_{i=1}^n\ln({-\frac{1}{2}\theta+\frac{3}{4}})^{1-y_i}$$<br/>$$\ln{l(\theta|\sum_{i=1}^{n}y_i)}=\ln{\binom{n}{\sum_{i=1}^{n}y_i}}+\sum_{i=1}^{n}y_i\ln({\frac{1}{2}\theta+\frac{1}{4}})+\sum_{i=1}^n(1-y_i)\ln({-\frac{1}{2}\theta+\frac{3}{4}})$$<br/>$ $ \ bold symbol { \ ln { l(\ theta | \ sum _ { I = 1 } { n } y _ I)} = \ ln { \ binom { n } { \ sum _ { I = 1 } { n } y _ I } }+\ sum _ { I = 1 } { n } y _ I \ ln({ \ frac { 1 } { 2 })</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="5c82" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">问题3.3:解决方案</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e22d7591622e368a1e7425bcc7587b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*QM8yM36ek0f_O8V2cYF-cw.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">问题3.3的对数似然图</p></figure><p id="cae4" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">上面是给定n=100和$\sum_{i=1}^{n}y_i=40$.时，对数似然作为＄\ theta \ in[0，1]＄的函数的曲线图我用来生成这样的图的代码片段在这个GitHub Gist中:<a class="ae li" href="https://gist.github.com/geraldzakwan/ec67ca4d36c124c80002eb383e4d7aa8" rel="noopener ugc nofollow" target="_blank"> log likelihood plot.ipynb </a>。</p><p id="ae4f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们可以看到，可能性最大的$\theta$似乎在$0.2$和$0.4$左右。让我们通过推导以下对数似然的最优$\theta$来验证，即$\hat{\theta}_{MLE}$。</p><p id="295a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">对应于该图的函数通过代入给定值而导出如下:</p><p id="d7a3" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">$$l(\theta|n=100,\sum_{i=1}^{n}y_i=40 )=\ln{\binom{100}{\sum_{i=1}^{100}y_i}}+\sum_{i=1}^{100}y_i\ln({\frac{1}{2}\theta+\frac{1}{4}})+(100-\sum_{i=1}^{100}(y_i))\ln({-\frac{1}{2}\theta+\frac{3}{4}})$$<br/>$$l(\theta|n=100,\sum_{i=1}^{n}y_i=40)=\ln{\binom{100}{40}}+40\ln({\frac{1}{2}\theta+\frac{1}{4}})+(100–40)\ln({-\frac{1}{2}\theta+\frac{3}{4}})$$<br/>$$l(\theta|n=100,\sum_{i=1}^{n}y_i=40)=\ln({\frac{100！}{60!40!} })+40 \ ln({ \ frac { 1 } { 2 } \ theta+\ frac { 1 } { 4 } })+60 \ ln({-\ frac { 1 } { 2 } \ theta+\ frac { 3 } { 4 } })$<br/>$$\boldsymbol{l(\theta|n=100,\sum_{i=1}^{n}y_i=40)=64.7906+40\ln({\frac{1}{2}\theta+\frac{1}{4}})+60\ln({-\frac{1}{2}\theta+\frac{3}{4}}})$$<br/>为了确定可能性最大的$\theta$，我们可以对$L(\theta)$求一阶导数，然后求出$\theta$使其等于零。<br/>$ $ \ frac { d } { d \ theta } L(\ theta)= 0 $ $<br/>$ $ \ frac { d } { d \ theta } \ ln({ \ frac { 100！}{60!40!} })+\ frac { d } { d \ theta } 40 \ ln({ \ frac { 1 } { 2 } \ theta+\ frac { 1 } { 4 } })+\ frac { d } { d \ theta } 60 \ ln({-\ frac { 1 } { 2 } \ theta+\ frac { 3 } { 4 } })= 0 $ $<br/>$ \ frac {(40)(\ frac { 1 } { 2 })} { 1 } { 2 } \ theta</p><p id="16ad" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们可以看到，这与我们在图中看到的一致。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="ab24" class="lk ll iq bd lm ln lo dn lp lq lr dp ls kv lt lu lv kz lw lx ly ld lz ma mb mc bi translated">结束语</h2><p id="1d6c" class="pw-post-body-paragraph km kn iq ko b kp md jr kr ks me ju ku kv mf kx ky kz mg lb lc ld mh lf lg lh ij bi translated">我想这就是这个故事的全部内容。很抱歉，我要求你安装<a class="ae li" href="https://chrome.google.com/webstore/detail/math-anywhere/gebhifiddmaaeecbaiemfpejghjdjmhc/" rel="noopener ugc nofollow" target="_blank"><em class="lj">Math Anywhere</em></a><em class="lj"/>附件来阅读这个故事，因为我找不到任何更方便的方法来在介质上编写LaTeX。</p><p id="3aa7" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我不喜欢上传数学符号的图像，而且<a class="ae li" href="https://chrome.google.com/webstore/detail/tex-to-unicode" rel="noopener ugc nofollow" target="_blank"> <em class="lj"> Tex到Unicode </em> </a>对我来说不太好。在下面的评论区，如果你知道任何更好的在媒体上展示数学符号的方法，请告诉我，我会非常感激。</p><p id="2b14" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">此外，请让我知道你是否对这个话题感兴趣，解释是否清晰简洁，因为我觉得我在这里有点罗嗦。</p><p id="823a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">干杯，</p><p id="8adf" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">热拉尔</p></div></div>    
</body>
</html>