<html>
<head>
<title>The importance of problem framing for supervised predictive maintenance solutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督预测性维护解决方案中问题框架的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-importance-of-problem-framing-for-supervised-predictive-maintenance-solutions-cc8646826093?source=collection_archive---------10-----------------------#2020-09-20">https://towardsdatascience.com/the-importance-of-problem-framing-for-supervised-predictive-maintenance-solutions-cc8646826093?source=collection_archive---------10-----------------------#2020-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2b92e9c32ba7deb05b18003f29879175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vk2VnRDH_X4Kx85W"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">杰西卡·鲁斯切洛在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="f8ae" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/exploring-nasa-turbofan" rel="noopener" target="_blank">探索美国宇航局的涡轮风扇数据集</a></h2><div class=""/><div class=""><h2 id="8a01" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">重温我们的剩余使用寿命假设&amp;支持向量回归</h2></div><p id="65c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">&lt;<em class="md">免责声明:我的目的是展示模型开发过程中不同方法和选择的效果。这些影响经常使用测试集来显示，这被认为是(非常)不好的做法，但有助于教育目的。</em> &gt;</p><p id="274a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我的上一篇文章中，我们探索了NASA的FD001涡轮风扇退化数据集。简单概括一下，传感器1、5、6、10、16、18和19没有与剩余使用寿命相关的信息(RUL)。从数据中去除这些因素后，我们拟合了一个RMSE为31.95的基线线性回归模型。今天，我们将重新检查我们的RUL假设，以提高我们的准确性，并拟合支持向量回归机(SVR)，以进一步改善我们的结果。我们开始吧！</p><h1 id="3191" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">加载数据</h1><p id="d608" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">首先，我们将加载数据并检查前几行，以确认数据加载正确。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0cbf4944894aae0e92422412d588123b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*k2BzjECfoR7PSEhFlaqs4A.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">train.head()的结果</p></figure><p id="50f7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">加载的数据看起来不错，让我们像以前一样计算线性下降RUL。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">RUL在击穿时可以认为是0，我们天真地假设RUL是一个线性函数。意味着在击穿前的10个周期时RUL是10，在击穿前的50个周期时是50，等等。</p></figure><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/159b4fffe4598dded3faae24347847ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*HSXK7dzPl2eL6FkeYkUj_g.png"/></div></figure><p id="d174" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在重新检查我们计算RUL的方法之前，我们将通过删除不包含有用信息的列来准备数据。这将允许我们直接测试我们所做的任何更改，因为我们的数据已经准备好了。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="9e20" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还定义了评估模型性能的函数。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="2e41" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">改装我们的基线模型。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="b9ee" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:44.66819159545453, R2:0.5794486527796716<br/># test set RMSE:31.952633027741815, R2:0.40877368076574083</span></pre><h1 id="24e0" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">重新审视RUL</h1><p id="cdfb" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">之前我假设RUL随着时间线性下降。然而，在上一篇文章中，我们看到这可能会影响我们的整体模型性能。有一种方法可以改进我们的假设，我将在下面解释[1]。</p><p id="432f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">观察传感器信号(参见下面的<em class="md">一个</em>示例)，许多传感器在开始时似乎相当稳定。这是因为随着时间的推移，发动机只会出现故障<em class="md">。信号曲线的弯曲是提供给我们的第一个信息，即发动机正在退化，并且第一次有理由假设RUL线性下降。在那之前，我们真的不能说任何关于RUL的事情，因为我们没有关于最初磨损的信息。</em></p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ff0af9dcee2eb5b477dfb870120e5a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*bdcUDO-NlpAalglFGCimFQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">描述S12、线性和削波RUL的图形。</p></figure><p id="ba88" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以更新我们的假设来反映这个逻辑。我们没有让RUL线性下降，而是将RUL定义为一个常数，一段时间后才线性下降(见上面的例子)。通过这样做，我们实现了两件事:</p><ol class=""><li id="1c90" class="nu nv jj lj b lk ll ln lo lq nw lu nx ly ny mc nz oa ob oc bi translated">初始恒定的RUL与初始恒定的平均传感器信号更好地相关</li><li id="c1c0" class="nu nv jj lj b lk od ln oe lq of lu og ly oh mc nz oa ob oc bi translated">RUL的较低峰值导致我们的目标变量的较低分布，从而更容易拟合直线</li></ol><p id="7237" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，这种变化允许我们的回归模型更准确地预测低RUL值，这通常是更有趣/更关键的正确预测。</p><p id="6ab2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用pandas，您可以简单地将之前计算的线性下降RUL限制在所需的上限值。测试多个上限值表明在125处剪切RUL对模型产生了最大的改进。当我们更新我们对列车组的RUL的假设时，我们应该在评估中包括这一变化。测试集的真实RUL保持不变。让我们来看看这一变化的影响。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="93ba" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:21.491018701515415, R2:0.7340432868050447<br/># test set RMSE:21.900213406890515, R2:0.7222608196546241</span></pre><p id="46d7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">火车RMSE已经减半多了。当然，我们已经自己设定了这些目标，但是它显示了之前的RUL假设对整体模型性能的影响有多大。更重要的是测试集的改进。测试RMSE从31.95降低到21.90，提高了31%！这告诉我们更新的假设有利于模拟真实的RUL。让我们看看使用另一种技术是否能做得更好。</p><h1 id="a6cf" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">支持向量回归</h1><p id="7240" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">线性SVR与线性回归的主要区别在于，它将边界设置在距离参考数据ε(ɛ)的位置(见下图)。当在模型拟合期间最小化损失函数时，落在边界内的点被忽略。在这些边界之外的点上拟合你的模型减少了计算量，并允许你捕捉更复杂的行为，<strong class="lj jt">但是</strong>这种技术对异常值也更敏感！</p><figure class="nb nc nd ne gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fa3a367fe45a506c2f4607fa6f6f001e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*sV3ctDOvNAUSIRmUnwKknA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来源:[2]。黑色实线代表目标，虚线是距离ε(ɛ).)处的边界只有边界外的点有助于模型拟合和最小化损失函数。损失函数类似于岭回归和套索回归的损失函数。</p></figure><p id="80fb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实例化一个SVR就像建立一个线性回归一样简单，一定要确保将内核设置为“线性”。拟合模型后，我们再次对训练集和测试集进行评估。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="872b" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:29.57783070266026, R2:0.49623314435506494<br/># test set RMSE:29.675150117440094, R2:0.49005151605390174</span></pre><p id="60cc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，均方根误差比我们的RUL线性回归差得多。让我们通过缩放我们的特征来尝试改进我们的模型。</p><h1 id="024d" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">缩放比例</h1><p id="8658" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">支持向量回归机通过比较特征向量之间的距离来工作。但是当特征在范围上变化时，计算的距离由具有更大范围的一个来支配。假设一个特性的范围在10-11之间，另一个在1000-1100之间。两者相差10%，但后者的绝对差异要大得多。SVR将更重视其范围的变化。<br/>为了解决这个问题，您可以缩放您的要素，使它们都在同一范围内。这使得您的SVR可以比较相对距离，并对差异进行大致相同的加权。[3, 4].<br/> Sklearns的MinMaxScaler可用于创建适合我们训练数据的缩放器。默认设置创建一个缩放器，在0-1之间缩放我们的训练特征。然后，缩放器被应用于我们的X_train和X_test集。我们用缩放后的数据拟合和评估一个新的SVR模型。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="790f" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:21.578263975067888, R2:0.7318795396979632<br/># test set RMSE:21.580480163289597, R2:0.730311354095216</span></pre><p id="804f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，21.58的测试RMSE已经是对我们的线性回归的改进，该线性回归的RMSE为21.90。接下来，我们将应用一些特征工程来尝试进一步改进我们的预测。</p><h1 id="19e2" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">特征工程</h1><p id="01ea" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">一种非常有用的要素工程技术是创建要素的多项式组合，这可能会揭示数据中与原始要素不明显的模式。假设我们想要创建二次多项式特征<code class="fe oi oj ok nk b">s_2</code>和<code class="fe oi oj ok nk b">s_3</code>，结果将是<code class="fe oi oj ok nk b">[1, s_2, s_3, s_2², s_3², s_2*s_3]</code>。<br/>将该技术应用于我们当前数据集中的所有传感器，可将特征空间从14个增加到120个。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="728b" class="no mf jj nk b gy np nq l nr ns"># returns<br/># (20631, 14)<br/># (20631, 120)</span></pre><p id="3cfd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在设计了新的特征之后，我们安装了新的模型。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="e85d" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:19.716789731130874, R2:0.7761436785704136<br/># test set RMSE:20.585402508370592, R2:0.75460868821153</span></pre><p id="b5ed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，测试集RMSE和方差再次得到改善，表明通过添加多项式特征可以获得更多信息。我也考虑过对数变换，但是传感器值范围不够大，不足以证明这些变换的合理性。然而，多项式特征确实扩大了我们的特征空间，使我们的模型有点“臃肿”,并增加了训练时间。让我们看看我们是否可以在保持分数的同时，通过保留最具信息性的特征来降低音量。</p><h1 id="6c7c" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">特征选择</h1><p id="f6de" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">使用包含工程特征的模型，我们可以计算哪些特征对模型性能的贡献最大。为此，我们使用SelectFromModel，在其中传递我们训练好的模型并将prefit设置为True。我们将选择“重要”特征的阈值设置为“平均”，表示所选特征的特征重要性将大于整个集合的平均特征重要性。获取支持将返回一个布尔数组，指示哪些要素的重要性高于平均值。我们将用它来划分特征子集，只保留“特征重要性&gt;平均特征重要性”等于真的特征。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="53c0" class="no mf jj nk b gy np nq l nr ns"># returns<br/># Original features:<br/> Index(['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_13',<br/>       's_14', 's_15', 's_17', 's_20', 's_21'],<br/>      dtype='object')<br/># Best features:<br/> ['x0' 'x1' 'x2' 'x3' 'x5' 'x6' 'x7' 'x9' 'x10' 'x11' 'x12' 'x13' 'x2 x5'<br/> 'x2 x8' 'x2 x9' 'x3 x5' 'x3 x8' 'x3 x9' 'x4^2' 'x4 x6' 'x4 x7' 'x4 x8'<br/> 'x5^2' 'x5 x6' 'x5 x7' 'x5 x9' 'x5 x12' 'x5 x13' 'x6^2' 'x6 x8' 'x6 x9'<br/> 'x7 x8' 'x7 x9' 'x8^2' 'x9^2' 'x9 x12' 'x9 x13']<br/># shape: (37,)</span></pre><p id="40cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">用所选择的特征来拟合和评估新的SVR模型。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="b4a7" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:19.746789101481127, R2:0.775461959316527<br/># test set RMSE:20.55613819605483, R2:0.7553058913450649</span></pre><p id="d74a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，测试RMSE和方差略有改善，而模型使用的特征数量从120个减少到37个！这种改善可能是由于模型在列车组上过度拟合得稍微少一些。我们现在有了所有的构建模块来训练和选择我们的最终模型。</p><h1 id="411c" class="me mf jj bd mg mh mi mj mk ml mm mn mo ky mp kz mq lb mr lc ms le mt lf mu mv bi translated">选择我们的最终型号</h1><p id="ef3a" class="pw-post-body-paragraph lh li jj lj b lk mw kt lm ln mx kw lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">对于最终模型，我们将在训练集上使用简单的超参数调整来调整ε的值。正如本文前面所解释的，ε表示最小化损失函数时要考虑的数据点的边界。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="7970" class="no mf jj nk b gy np nq l nr ns"># returns<br/># epsilon: 0.4 RMSE: 19.74772556660336 R2: 0.7754406619776462<br/># epsilon: 0.3 RMSE: 19.747580761069848 R2: 0.7754439552496148<br/># epsilon: 0.2 RMSE: 19.74660007817171 R2: 0.7754662580123992<br/># epsilon: 0.1 RMSE: 19.746789101481127 R2: 0.775461959316527<br/># epsilon: 0.05 RMSE: 19.746532456984006 R2: 0.7754677958176168</span></pre><p id="c246" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">ε为0.2似乎在训练集上产生最佳性能。让我们重新训练我们的模型，并检查最终结果。</p><figure class="nb nc nd ne gt iv"><div class="bz fp l di"><div class="nf ng l"/></div></figure><pre class="nb nc nd ne gt nj nk nl nm aw nn bi"><span id="9b12" class="no mf jj nk b gy np nq l nr ns"># returns<br/># train set RMSE:19.74660007817171, R2:0.7754662580123992<br/># test set RMSE:20.54412482077374, R2:0.7555918150093489</span></pre><p id="c087" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">具有剪切RUL的线性模型的RMSE为21.90，比我们的基线回归提高了31%，基线回归的RMSE为31.95。我们的最终模型利用边界调谐的SVR、用于训练的限幅RUL、特征缩放和贡献最大的二阶多项式特征来达到20.54的测试RMSE。这比我们的RUL限幅线性模型提高了6%，比基线模型总体提高了35.7%。</p><p id="b5cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，这篇文章展示了正确构建数据科学问题的重要性。虽然SVR肯定是对线性回归的改进，但与我们更新的RUL假设相比，它的改进就相形见绌了。完整的笔记本你可以点击这里查看我的github repo。</p><p id="6545" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我要感谢Maikel Grobbe和Wisse Smit对我的文章的评论。<a class="ae jg" rel="noopener" target="_blank" href="/time-series-analysis-for-predictive-maintenance-of-turbofan-engines-1b3864991da4?source=friends_link&amp;sk=a62dbeb8230f6b29123b692ac08dad59">下一次</a>我们将深入研究时间序列分析，20.54的RMSE将是要打破的分数。如果你有任何建议、问题或评论，请在下面的评论中留下！</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="3e1a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">参考文献:<br/>【1】f . o . Heimes，“用于剩余有用寿命估计的递归神经网络”，<em class="md"> 2008年国际预测和健康管理会议</em>，科罗拉多州丹佛，2008年，第1–6页，doi:10.1109/PHM . 2008.4711422 .<br/>【2】Kleynhans，Tania &amp; Montanaro，Matthew &amp; Gerace，Aaron &amp; Kanan，Christopher。(2017).使用深度学习的MERRA-2大气数据预测大气顶部热辐射。遥感。9.1133.doi: 10.3390/rs9111133。<br/>【3】<a class="ae jg" href="https://en.wikipedia.org/wiki/Feature_scaling" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Feature_scaling</a><br/>【4】<a class="ae jg" href="https://stats.stackexchange.com/questions/154224/when-using-svms-why-do-i-need-to-scale-the-features" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/154224/when-using-SVMs-why-do-I-need-scale-the-features</a></p></div></div>    
</body>
</html>