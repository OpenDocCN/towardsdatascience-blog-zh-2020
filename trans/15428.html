<html>
<head>
<title>Logistic Regression Step by Step Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步实施逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-step-by-step-implementation-f032a89936ca?source=collection_archive---------35-----------------------#2020-10-23">https://towardsdatascience.com/logistic-regression-step-by-step-implementation-f032a89936ca?source=collection_archive---------35-----------------------#2020-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e2ef" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从理论到实践</h2></div><p id="53da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们正在执行一个经典的预测任务，其中给定一个包含$n$个变量的输入向量:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/9567975a461bc68fcebc82aa2a4f0266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KH5OFsoUANwsGrKLgTfvfA.png"/></div></div></figure><p id="490c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并预测1个响应变量$y$(可能是明年的销售额、房价等。)，最简单的形式是使用线性回归进行预测，公式如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/2145a31ebc899ce2940f36cab79f3972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*OJjpDS5e8W24tIv8ljrK9Q.png"/></div></figure><p id="7b80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中$W$是维数为$n$的列向量。比如现在我们的问题变了一点，我们希望预测一个概率，比如明天下雨的概率是多少？在这种意义上，这种线性回归可能有点不合适，因为线性表达式可以是无限的，但我们的概率范围是$[0，1]$。</p><h1 id="92eb" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">Sigmoid函数</h1><p id="9f72" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">为了将我们的预测限制在$[0，1]$，广泛使用的技术是应用一个<code class="fe mo mp mq mr b">sigmoid</code>函数:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ms"><img src="../Images/6619bd929623fa7ac0a40f2af4e29bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBSdWjGTiI5kQJBQ_5-LIg.png"/></div></div></figure><p id="dd02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<code class="fe mo mp mq mr b">numpy</code>,我们可以很容易地将功能可视化。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d19c13438ed0d2121bf80ee23436d471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*ZZTiCrjtMYtl9zGHXcCfCg.png"/></div></figure><h1 id="b410" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">损失函数</h1><p id="d6b1" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">逻辑回归的损失函数的定义是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mu"><img src="../Images/7a72a315d2978a9df72bf5b0bcb7f184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4vOk1r6TxQm1TJ8Vm6E7wg.png"/></div></div></figure><p id="c223" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中,<code class="fe mo mp mq mr b">y_hat</code>是我们的预测值，范围是＄[ 0，1]＄而<code class="fe mo mp mq mr b">y</code>是真实值。当实际值为<code class="fe mo mp mq mr b">y = 1</code>时，等式变为:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/586a0825ad9348b3151dd81c75a25af1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*9g8cxN5GXJHdn7TlvJSb2Q.png"/></div></figure><p id="c9bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mo mp mq mr b">y_hat</code>越接近1，我们的损失就越小。而<code class="fe mo mp mq mr b">y = 0</code>也是如此。</p><h1 id="9bdb" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">梯度下降</h1><p id="6077" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">给定这个实际值<code class="fe mo mp mq mr b">y</code>，我们希望将损失<code class="fe mo mp mq mr b">L</code>最小化，我们这里要应用的技术是梯度下降(细节已经在<a class="ae mw" rel="noopener" target="_blank" href="/gradient-descent-explanation-implementation-c74005ff7dd1">这里</a>说明)，基本上我们需要做的是对我们的变量应用导数，并将它们稍微向下移动到最优值。</p><p id="2b65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里我们有两个变量，<code class="fe mo mp mq mr b">W</code>和<code class="fe mo mp mq mr b">b</code>，对于这个例子，它们的更新公式是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mx"><img src="../Images/661f339ab49e9c4031cc6a58bac5e0b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQXrcFKxYyyIuyllrU-t2g.png"/></div></div></figure><p id="5e92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mo mp mq mr b">W</code>是一个列向量，其<code class="fe mo mp mq mr b">n</code>权重对应于<code class="fe mo mp mq mr b">x^(i)</code>的<code class="fe mo mp mq mr b">n</code>维度。为了得到我们目标的导数，将应用链式法则:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi my"><img src="../Images/e2db9d392b03ae3b68f9ed8894edc528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mJzqC5_Dg-zvCZqUkzGaDQ.png"/></div></div></figure><p id="fc83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以自己试着推导，唯一棘手的部分是<code class="fe mo mp mq mr b">sigmoid</code>函数的导数，要得到一个好的解释你可以参考<a class="ae mw" href="https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><h1 id="831e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">批量训练</h1><p id="31a5" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">上面给出了向前和向后更新的过程，如果我们一次一个地输入我们的训练模型，这足以实现逻辑回归。然而，在大多数培训案例中，我们不会这样做。取而代之的是，训练样本被分批馈送，并且反向传播用该批次的平均损失来更新。</p><p id="daeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着对于一个一次输入<code class="fe mo mp mq mr b">m</code>个样本的模型，损失函数将是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi mz"><img src="../Images/fd9fe74e119a7885f73b0c799d83bfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyNY5bTDvw6nIXi09XauDQ.png"/></div></div></figure><p id="049c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<code class="fe mo mp mq mr b">i</code>表示<code class="fe mo mp mq mr b">ith</code>训练样本。</p><h1 id="e9cd" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">批量训练的正向传播</h1><p id="b385" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">现在，代替使用单个向量<code class="fe mo mp mq mr b">x</code>作为我们的输入，我们指定大小为<code class="fe mo mp mq mr b">n x m</code>的矩阵<code class="fe mo mp mq mr b">X</code>，其中如上所述，<code class="fe mo mp mq mr b">n</code>是特征的数量，<code class="fe mo mp mq mr b">m</code>是训练样本的数量(基本上，我们在矩阵中排列<code class="fe mo mp mq mr b">m</code>个训练样本)。现在公式变成了:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi na"><img src="../Images/90e896e5db574a1e83467c932b375bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4g_IaLRvSFimKAonnSf_RQ.png"/></div></div></figure><p id="bbe4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，这里我们使用*大写字母*来表示我们的矩阵和向量(注意这里的<code class="fe mo mp mq mr b">b</code>仍然是单个值，更正式的方式是将<code class="fe mo mp mq mr b">b</code>表示为向量，但是在python中，将单个值添加到矩阵会自动广播)。</p><p id="1089" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们逐个分解矩阵的大小。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nb"><img src="../Images/a40353d14880980ccc396fb337d164d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXx4iVvV6Ks_ZqGlH3Eedw.png"/></div></div></figure><h1 id="84b8" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">生成分类任务</h1><p id="07bc" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们的公式到此结束，让我们实现我们的算法，在此之前需要生成一些数据来完成分类任务(整个实现也在我的<a class="ae mw" href="https://github.com/MJeremy2017/deep-learning/tree/main/logistic-regression" rel="noopener ugc nofollow" target="_blank"> git repo </a>中)。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="1196" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">履行</h1><p id="d9b8" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">现在一切都准备好了，让我们开始实现吧。</p><h2 id="2803" class="ne ls it bd lt nf ng dn lx nh ni dp mb kr nj nk md kv nl nm mf kz nn no mh np bi translated">助手功能</h2><ol class=""><li id="21a7" class="nq nr it kk b kl mj ko mk kr ns kv nt kz nu ld nv nw nx ny bi translated">接受数组的sigmoid函数</li><li id="e9df" class="nq nr it kk b kl nz ko oa kr ob kv oc kz od ld nv nw nx ny bi translated">将<code class="fe mo mp mq mr b">W</code>和<code class="fe mo mp mq mr b">b</code>初始化为零的权重函数</li><li id="c263" class="nq nr it kk b kl nz ko oa kr ob kv oc kz od ld nv nw nx ny bi translated">精度函数来衡量我们的二进制预测的准确性</li></ol><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h2 id="0db1" class="ne ls it bd lt nf ng dn lx nh ni dp mb kr nj nk md kv nl nm mf kz nn no mh np bi translated">预言；预测；预告</h2><p id="ff59" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">我们的<code class="fe mo mp mq mr b">predict</code>函数将简单地通过给定训练权重的正向过程</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h2 id="2c4c" class="ne ls it bd lt nf ng dn lx nh ni dp mb kr nj nk md kv nl nm mf kz nn no mh np bi translated">培养</h2><p id="837f" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">注意，对于<code class="fe mo mp mq mr b">train</code>函数，<code class="fe mo mp mq mr b">X</code>的输入形状需要具有<code class="fe mo mp mq mr b">n x m</code>的形状，<code class="fe mo mp mq mr b">Y</code>需要具有<code class="fe mo mp mq mr b">1 x m</code>的形状，其中<code class="fe mo mp mq mr b">m</code>是批量大小。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0f74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="oe">输入需要转置，以适应我们的培训要求。</em></p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h2 id="7358" class="ne ls it bd lt nf ng dn lx nh ni dp mb kr nj nk md kv nl nm mf kz nn no mh np bi translated">班级中的合奏</h2><p id="5abe" class="pw-post-body-paragraph ki kj it kk b kl mj ju kn ko mk jx kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">现在让我们把所有的东西集成到一个类中，看起来更有结构。为完整起见，还将实施分批培训</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="c559" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">完整的培训详情可点击查看<a class="ae mw" href="https://github.com/MJeremy2017/deep-learning/tree/main/logistic-regression" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>