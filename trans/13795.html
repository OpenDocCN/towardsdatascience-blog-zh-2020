<html>
<head>
<title>Hierarchical topic modeling with BigARTM library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BigARTM库的层次主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hierarchical-topic-modeling-with-bigartm-library-6f2ff730689f?source=collection_archive---------15-----------------------#2020-09-22">https://towardsdatascience.com/hierarchical-topic-modeling-with-bigartm-library-6f2ff730689f?source=collection_archive---------15-----------------------#2020-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由Unsplash上的Alina Grubnyak 拍摄</p></figure><p id="1925" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主题建模是一种统计建模，用于发现文档集合中的抽象“主题”。LDA(潜在狄利克雷分配)是最流行和最广泛使用的工具之一。然而，我将展示一个替代工具BigARTM，它为主题建模提供了大量的机会(例如特殊的度量和正则化)。</p><p id="f612" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们制定我们的任务。最初，我们有文档中单词的分布，但是我们需要得到主题-单词分布和主题-文档分布。所以，这只是一个随机矩阵分解的任务。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/31769dec00960f47f71f358529d7db65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zdOeSkjj6_NiMUWW9KYQg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者的米罗形象</p></figure><p id="25ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将使用NIPS文件来说明图书馆的原则。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="beb4" class="lo lp it lk b gy lq lr l ls lt">df = pd.read_csv(‘./papers.csv’)<br/>all_texts = df.PaperText<br/>all_texts[111]</span></pre><p id="00b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="lu"> ' </em>差分私有子空间聚类\n宁王、于和阿提辛格\ n机器学习部… <em class="lu"> ' </em> </strong></p><p id="9cee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从用流水线预处理数据<strong class="ki iu">开始:</strong></p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lv"><img src="../Images/954a6c945b58c0e6a34cf91f60a7805a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbtblCEFZXlSbjo6LE7LRg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者的米罗形象</p></figure><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">预处理流水线</p></figure><p id="9ab3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="lu"> ' </em>差分私有子空间聚类王艺凝王玉祥阿尔提辛格机器学习部……<em class="lu">'</em></strong></p><p id="468f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们已经准备好对句子进行分词，获得一个单词包，并进行主题建模。顺便说一句，n-grams有时对于这个目的非常有用。它们有助于提取成熟的表达方式，更好地理解每个主题。我决定只得到二元模型，但是，你可以选择任何数字。我们将选择文档中出现频率最高的文档。</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">创造二元模型</p></figure><p id="5177" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 10349 </strong></p><p id="2855" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> ['机器学习'，'神经网络'，'下界'，'国际会议'，'上界'] </strong></p><p id="e013" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">二元模型似乎很有用，它将帮助我们区分不同的主题。所有的预处理都已经完成，所以我们可以移动到我们的模型。要做到这一点，我们必须创建一个包含文档单词的矩阵，模型将它作为输入。</p><h2 id="fc55" class="lo lp it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">ARTM模型</h2><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">创建矩阵“文档上的单词”</p></figure><p id="ac47" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ARTM图书馆为你提供了影响学习过程的巨大功能。例如，它允许添加各种正则化器来控制学习过程，并将phi和theta矩阵变得更加稀疏。在顶层模型中，我添加了一个用于theta矩阵的稀疏正则化器和一个去相关器，它可以激发phi稀疏性。</p><p id="be79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，我们可以指定我们想要用于评估的度量(这里有Perxplexity和matrices sparstities)。我们添加这些正则项是为了让主题更容易理解，但是我们必须小心翼翼地做，这样只会稍微减少困惑。</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="770f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看主要措施:</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/bc0fec4afbcb50bebbc845c2bf4738e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVwdI7VAo5gdkj1V3lt1nw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="8a41" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在还可以观看我们已经获得的主题</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="5ec1" class="lo lp it lk b gy lq lr l ls lt">for topic_name in model_artm.topic_names: <br/>    print(topic_name + ': ' +  model_artm.score_tracker['TopTokensScore'].last_tokens[topic_name])</span></pre><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/e31e112863f66692a091250611cdb632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmLO47emW_zLMpEYJQqXAA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图像</p></figure><p id="59fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个文档的主题矩阵相当稀疏，所以我们得到了我们所需要的。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/a2fa8bdc9de1ae249f12a1d1343d23f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycvaj3lo7D6W6bKq9ubRwg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="ead4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">阅读与特定主题相关的文章会很方便。所以在这里我们可以获得一个按主题概率排序的文章列表。</p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ms"><img src="../Images/6f20ab57ab83263e219ddd3cd24c06c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTlKAYB3Wwtlz85vOMW6hQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><h2 id="2e69" class="lo lp it bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated">建筑层次结构</h2><p id="0cec" class="pw-post-body-paragraph kg kh it ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">我们得到的主题似乎相当模糊，尽管我们可以看到它们之间的差异。如果我们对某个特定的主题感兴趣，我们可能想看看这个主题的副主题，并缩小搜索范围。出于这样的目的，我们可以构建一个看起来像树的模型层次结构。我们将只使用一个额外的50个主题的水平</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/e9f58d9cd913201d11f9897d395de54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLR6xOFxTBwGsBK7DyEvzg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者的米罗形象</p></figure><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/96137c041c77e97e200e995b4b01ac2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bE1Av2Xofx_aBDVEs1UNQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者图片</p></figure><p id="26f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">副标题中的一些单词示例:</p><p id="8f33" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> ['风险'，'经验'，'度量'，'类'，'推广'，'假设'，'距离'，'估计量'，'性质'，'证明'，'有界'，'预期']，</strong></p><p id="22cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> ['活动'，'试验'，'神经元'，'尖峰'，'刺激'，'放电'，'神经科学'，'情境'，'潜在'，'反应'，'亚单位'，'放电率']，</strong></p><p id="b0b3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> ['训练'，'特征'，'标签'，'对象'，'损失'，'输出'，'分类'，'地图'，'建议'，'数据集'，'输入'，'区域'] </strong></p><p id="be9b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来更好！话题开始更加具体。因此，我们可以查看与我们感兴趣的主题最相关的副主题。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="d932" class="lo lp it lk b gy lq lr l ls lt">def subtopics_wrt_topic(topic_number, matrix_dist):<br/>   return matrix_dist.iloc[:, topic_number].sort_values(ascending = False)[:5]</span><span id="9255" class="lo lp it lk b gy na lr l ls lt">subtopics_wrt_topic(0, subt)</span></pre><p id="b76e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="lu">subtopic _ 7 0.403652</em></strong></p><p id="1913" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="lu">subtopic _ 58 0.182160</em></strong></p><p id="e599" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="lu">subtopic _ 56 0.156272</em></strong></p><p id="3c10" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="lu">副主题_13 0.118234 </em> </strong></p><p id="b941" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="lu">subtopic _ 47 0.015440</em></strong></p><p id="5b26" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以像以前一样选择副主题文档。</p><p id="797c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读。我希望我简要地介绍了这个库的功能，但是如果你想更详细地介绍，可以参考文档，其中有很多附加信息和有用的技巧(模态、正则化、输入格式等等)。).</p><p id="fd8b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">期待听到任何问题。</p></div></div>    
</body>
</html>