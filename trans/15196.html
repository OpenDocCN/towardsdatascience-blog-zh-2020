<html>
<head>
<title>Understanding Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-logistic-regression-81779525d5c6?source=collection_archive---------26-----------------------#2020-10-19">https://towardsdatascience.com/understanding-logistic-regression-81779525d5c6?source=collection_archive---------26-----------------------#2020-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="64d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这种方法的数学详细解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c3d3fcc2bfb19c8d0cedef695b53e5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zFM1ajUSh2r_sG8dQGkkeA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8e9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">什么是逻辑回归？逻辑回归只是将线性回归应用于只有2个输出的特殊情况:0或1。这个东西最常用于分类问题，其中0和1代表两个不同的类，我们想要区分它们。</p><p id="a2ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归输出一个范围从-∞到+∞的实数。我们甚至可以在0/1分类问题中使用它:如果我们得到一个&gt; = 0.5的值，将其报告为类标签1，如果输出&lt; 0.5，将其报告为0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/0ce531b2c5d9fc72438a8be9ebb5c496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vfphQ8LtrOP-YvgerKMsg.png"/></div></div></figure><p id="b88b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中x是一个观察值的特征向量(加上偏差为常数1的分量), w是权重向量。</p><p id="51d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，如果我们将回归线压缩成介于0和1之间的“S”形曲线，我们可以在准确性和可解释性方面获得稍好的结果。我们通过将sigmoid函数应用于线性回归模型的输出值来压缩回归线。</p><p id="346c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是sigmoid函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/5a1d163075879c9675991fda7c866a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyKaWYeICuDN8pcW9FTT7w.png"/></div></div></figure><p id="3bec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更确切地说，我们按如下方式计算输出:取输入的加权和，然后将这个结果数传递给sigmoid函数，并将sigmoid的输出报告为我们的逻辑回归模型的输出。</p><p id="8447" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一过程有助于我们获得稍好的准确性和输出的可解释性。如果我们的模型输出任何实数，如-5或7，这些数字实际上意味着什么？关于我们的两个类:0和1，我们能知道什么？</p><p id="b03f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，当我们有介于0和1之间的输出时，我们可以把它们解释为概率。逻辑回归模型的输出是我们的输入属于标有1的类别的概率。我们模型输出的补充是我们输入属于标记为0的类别的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/2ee8aaec4694e6352a97c7c4327a1dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngC-rj7aabn8qo_Eqik2SA.png"/></div></div></figure><p id="c323" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中y是输入x的真实类标签。</p><p id="59e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好的。至此，我们已经看到了给定输入，逻辑回归模型如何获得输出。但是它的重量呢？它应该有什么样的权重才能做出好的预测？</p><p id="c538" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的模型需要学习这些权重，它学习的方式是给我们的模型一个目标函数，然后它找到最小化或最大化这个目标的权重。</p><p id="2d36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多方法可以得到一个目标函数，特别是当我们考虑在目标中加入正则项的时候。</p><p id="e251" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将只探讨两个这样的目标函数。</p><p id="9f91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们将逻辑回归模型写成如下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/19c9e81ee0041c1386b8d3dd9c64a685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LiPgSQzeoWl6GyQ1XbrVtw.png"/></div></div></figure><p id="a30d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中X是一个矩阵，以行的形式包含我们所有的观察结果，列代表特征。这是我们模型的输出，它是一个向量，包含对每个观察的预测。</p><p id="b90b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们以下面的方式重写我们的逻辑回归方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/e6158868364ec1a07fdfd6525d4a509f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4e5WaMqmBnY27IcFWDBsQg.png"/></div></div></figure><p id="0b09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一行右侧的操作是基于元素的。</p><p id="789b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你在上面最后一行观察到了什么？如果我们将最后一个等式右侧的函数应用于逻辑回归的标签，并将该函数应用的输出视为新标签，则我们获得线性回归。因此，我们可以使用误差平方和作为损失函数，并找出使其最小的权重。我们可以通过使用封闭形式的公式或SGD(随机梯度下降)来找到权重，您可以在以下关于线性回归的文章中了解更多信息:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/understanding-linear-regression-eaaaed2d983e"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">了解线性回归</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">线性回归背后的数学详细解释</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="7f36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是线性回归的闭合解和损失梯度(我们可以在SGD算法中使用):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/aceb351ac6d6e9fb2c301d91020ac068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoJtjCtCUHZ4ifL2rhq07w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/de417cb49cc07a185a752354d2bc2f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVH9bDdh5uJUN3b_n6t1bQ.png"/></div></div></figure><p id="a142" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于逻辑回归，我们只需将上述两个方程中的y替换为前一个方程的右侧:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/ccd5b9c34fcbf6cf76c95392534b3df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CwI-3hAg7wFyrkFbnuiTQA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/a7084bf4880712aaefccae3d6b970116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XRLTiXG44Vj0R-LnKYpqaA.png"/></div></div></figure><p id="f61a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们应用这些公式时，我们为y hat提供了真正的标签。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="dfeb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们可以将逻辑回归视为线性回归的一种形式，并使用线性回归的工具来解决逻辑回归。好的。除此之外我们还能做什么？</p><p id="4f64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以利用逻辑回归的特性来提出一个稍微好一点的方法。逻辑回归有什么类型的输出？一种可能性。</p><p id="3e77" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当涉及概率时，一种方便的方法是最大似然估计。我们将找到在给定输入的情况下使标签的可能性最大化的模型的权重。</p><p id="7ddc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从写似然函数开始。可能性只是给定输入的标签的联合概率，如果我们假设观察是独立的，则可以写成每个观察的概率的乘积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/78fea259bb3886bad7bd16c6b9fcbeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVDZDU0cJdqfPDZD6uLLfg.png"/></div></div></figure><p id="909c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中m是观察次数。</p><p id="2d75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">似然性是一切的函数:输入x、真实标签y和权重w。但出于我们的目的(相对于w最大化它)，我们将进一步将其视为w的函数。x和y被视为我们无法更改的给定常数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/f60d8d6f7c0a9f9816edfae1c901332b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UudY8Qt8c3mGhkTDrkJm0A.png"/></div></div></figure><p id="4ac5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据yi为0或1，每个个体概率具有下列值之一:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/b9f82e99ae7564a96657e57d00a44bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_9fdIFxHQh_8W1wkTsfxQ.png"/></div></div></figure><p id="7047" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更简洁的写法是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/7059d5b6e8eef6e2100a9d8879081311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5SZqh2doPcHvLzWormG0zw.png"/></div></div></figure><p id="36bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们在似然函数中替换这个量，简化它的argmax，并过渡到矩阵符号:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/be15bd8a84c51b961e057fe02fa4bea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMmz28GvdDxWkHGTkmRrXg.png"/></div></div></figure><p id="20cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你在上面看到的，最大化权重的可能性和最小化最后一行的数量是一样的。这次找到一个封闭形式的解更加困难(如果可能的话)，所以我们能做的最好的事情是计算这个量的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lu"><img src="../Images/be98a777fccf59edab2d754a9e494a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YEDeahI52mcscIt7ImjPQ.png"/></div></div></figure><p id="c85d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中:上述部分中涉及的操作是基于元素的。X前的点表示“将左边的列向量与矩阵X的每一列逐元素相乘”。上面的1是与y形状相同的列向量，用值1填充。</p><p id="b822" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，上述梯度可以与基于梯度的优化算法(如SGD)一起使用，以找到最佳权重。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="d09c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在结束之前，让我们回顾一下我们在这篇文章中看到的一些东西:</p><ul class=""><li id="16b5" class="mu mv it la b lb lc le lf lh mw ll mx lp my lt mz na nb nc bi translated">什么时候可以用逻辑回归？答:当我们遇到二元分类问题时。</li><li id="2e16" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">逻辑回归模型如何获得其输出？答:它计算其输入的加权和，然后将其传递给sigmoid函数。输出可以解释为概率。</li><li id="ac12" class="mu mv it la b lb nd le ne lh nf ll ng lp nh lt mz na nb nc bi translated">我们如何找到模型的权重？答:我们可以摆弄标签，这样我们仍然可以使用线性回归，或者我们可以使用更适合它的东西，如MLE。MLE倾向于给出稍微好一点的结果。</li></ul><p id="2916" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是本文的全部内容。希望你觉得有用。</p><p id="6d86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在接下来的两篇文章中，我将展示如何在NumPy、TensorFlow和PyTorch中实现逻辑回归。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/how-to-code-logistic-regression-from-scratch-with-numpy-d33c46d08b7f"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">如何用NumPy从头开始编写逻辑回归代码</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">学习逻辑回归的同时提高你的数字技能</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="ni l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://medium.com/nabla-squared/how-to-implement-logistic-regression-with-tensorflow-f5bf18416da1" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">如何用TensorFlow实现Logistic回归</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">…没有你想象的那么难</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="nj l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://medium.com/nabla-squared/how-to-implement-logistic-regression-with-pytorch-fe60ea3d7ad" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">如何用PyTorch实现逻辑回归</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">了解逻辑回归并提高您的PyTorch技能</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="nk l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="eed3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这些信息对你有用，感谢你的阅读！</p><p id="6079" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章也贴在我自己的网站<a class="ae nm" href="https://www.nablasquared.com/understanding-logistic-regression/" rel="noopener ugc nofollow" target="_blank">这里</a>。随便看看吧！</p></div></div>    
</body>
</html>