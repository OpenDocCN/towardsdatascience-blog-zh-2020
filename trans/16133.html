<html>
<head>
<title>Spark Fundamentals for Python Programmers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python程序员的Spark基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-jargon-for-babies-and-python-programmers-5ccba2c60f68?source=collection_archive---------32-----------------------#2020-11-06">https://towardsdatascience.com/spark-jargon-for-babies-and-python-programmers-5ccba2c60f68?source=collection_archive---------32-----------------------#2020-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4cbe" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">入门指南</h2><div class=""/><div class=""><h2 id="4183" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">你的第一个星火计划需要的一切</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/7a27ace46ddfa519e1e614e69b179146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*iUK1orO7py_ACTva00p4IA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片:火花蜜蜂</p></figure><p id="f4e0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我是一名python程序员，几个月前开始使用PySpark，并意识到这是一件相当“大”的事情。为了用spark编写我的第一个程序，我真的必须努力收集和理解一些基本术语。因此，我决定为我的同龄人编写这个指南。</p><p id="03c2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果您要从Python开始，这是一篇从PySpark开始的浓缩文章。它旨在回答初学者的问题，并清除一些行话。</p><h1 id="41e2" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">Spark基础知识—什么是Spark？</h1><p id="1b45" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated"><strong class="lf jd"> <em class="mw"> Spark是一款软件应用，通过允许您从多台计算机访问计算资源，使您能够处理大数据。</em>T3】</strong></p><p id="c9d1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">关键词:</strong> <em class="mw">大数据</em> <strong class="lf jd"> <em class="mw">，</em> </strong> <em class="mw">并行计算，集群计算，分布式计算，容错。</em></p><p id="dd00" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可以使用本地计算机和python(或excel)来计划你的假期。但是如果你要管理和处理你要去度假的整个国家的火车时刻表，你需要更大的东西。为了做更大的任务，你还需要更大的计算能力。Spark正好可以做到这一点。它<strong class="lf jd">是一个完整的框架</strong>(不仅仅是一种语言或服务器)，允许你在多台计算机上分布你的数据和处理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/aca9f39318b36fcd9a129adea2c384ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sj7CRC2iB7odRkrBdnelig.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片:火花涂鸦</p></figure><p id="f555" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有了数据、查询语言和一台计算机，作为一名数据科学家，您不得不有效地对数据进行采样，使其在您的处理能力范围内。然后，您会得到“尽可能代表”整个数据集的解决方案。借助基于Spark的软件，您可以使用整个有意义的数据集进行分析。这基本上就是为什么整个事情被发明出来——因为<em class="mw">数据，无处不在的数据，而不是一台计算机来处理！</em></p><p id="ef76" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">可供使用的整个计算机网络称为<strong class="lf jd">集群</strong>，网络中的每台计算机称为<strong class="lf jd">节点</strong>。有了Spark，您不必费心决定哪个节点处理您的处理需求的哪一部分。Spark隐式地这样做，因此您不必为线程化您的流程流而烦恼。</p><p id="cc19" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在Spark之前，一个流行的开源库叫做<strong class="lf jd"> Hadoop </strong>是大数据处理的核心库。由于Hadoop的MapReduce并行计算编程模型不具备容错能力，并且在并行处理方面有其自身的局限性，因此Spark被开发出来，并一直是首选解决方案。</p><p id="7770" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Spark是一个引擎，可以让你用很多语言编程，比如R，Python，Scala，Java，SQL等等。这就是pySpark的用武之地。PySpark使您能够使用python语言的Spark框架。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nc"><img src="../Images/fd49fb9d5e36d0a31b5c1d8bc87a4a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z5CDWZYIGLGh1P7-vaNJ2A.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图解:如果火花是一辆汽车</p></figure><h1 id="0cf1" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">火花行话</h1><p id="c64e" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated"><strong class="lf jd">关键词:</strong> <em class="mw"> RDD，转换，动作，沿袭，懒惰执行，容错，数据帧，Hadoop文件系统，Spark会话，Spark上下文。</em></p><h2 id="a44c" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">RDD:数据框架的构建模块</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/7a27ace46ddfa519e1e614e69b179146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*iUK1orO7py_ACTva00p4IA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图解:又是火花蜜蜂</p></figure><p id="eb16" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">rdd是Spark所基于的数据的核心<strong class="lf jd">元素。RDD的架构赋予spark跨多台机器处理它们的能力。</strong></p><p id="62b1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">RDD代表弹性分布式数据集。它们没有模式(也就是说，不要把它们想象成行列表)。RDD是分布在不同节点上的一组数据元素。RDD的名字就是它的定义:</p><ol class=""><li id="f548" class="no np it lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated"><strong class="lf jd">有弹性:</strong>一旦产生就能抵抗<strong class="lf jd"> </strong>外部变化。这意味着您可以在RDD上执行转换来创建新的RDD，但是原始的RDD保持不变。如果您有数据元素RDD 1和RDD 2，并使用两者执行转换，您将得到结果元素RDD3，而没有更改1和2。注意这与你的熊猫的执行风格有什么不同。在pandas中，你可以修改现有的数据帧，然后继续生活。pySpark不支持这一点。</li><li id="a189" class="no np it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><strong class="lf jd">分布式:</strong>rdd被设计成能够分布到多个节点上进行并行数据处理。</li><li id="4ac9" class="no np it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated"><strong class="lf jd">数据集:</strong>它们以列表、元组等形式保存数据。</li></ol><h2 id="aa19" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">创建RDD</h2><p id="c056" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">您可以通过在Spark驱动程序中并行化原始数据，或者从外部存储(如)加载数据来创建RDD。镶木地板、Avro等。<em class="mw">现在，不要担心示例中的spark上下文语句。我们将在以后讨论它。</em></p><p id="2b4f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mw">通过并行化原始数据创建RDD:示例</em></p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="9657" class="nd ma it od b gy oh oi l oj ok">from pyspark.context import SparkContext</span><span id="855e" class="nd ma it od b gy ol oi l oj ok">sc = sparkContext.getOrCreate( )</span><span id="87c0" class="nd ma it od b gy ol oi l oj ok">parallelData = sc.parallelize([1,2,3,4])</span></pre><p id="5ae4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="mw">从现有文件系统创建RDD:示例</em></p><p id="2f38" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">您可以从现有文件格式加载rdd，如Parquet、Avro、序列文件或纯文本存储(例如。txt，。csv)</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="7b40" class="nd ma it od b gy oh oi l oj ok">parallelData = sc.read.parquet(sourcepath+ "filename.parquet")</span></pre><h2 id="92ab" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">惰性执行、转换和沿袭</h2><p id="219a" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">正如前面所讨论的，rdd在创建后是不可改变的。但是为了使用它们，您显然需要对数据元素本身进行一些控制。这是通过转换实现的。转换作用于现有的rdd并生成新的rdd，而不改变原来的rdd。</p><p id="577d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">动作</strong>是计算后返回一些输出给驱动程序或导出数据存储的点。在spark中，当发现<strong class="lf jd">转换</strong>时，不会对其进行评估，但是spark会等待，直到对其调用<strong class="lf jd">动作</strong>。</p><p id="2a2a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">不是执行所有的转换，而是制作一个<strong class="lf jd">沿袭</strong>图，它记录了没有实际数据的rdd之间依赖关系的信息。当一个动作被调用时，这个图被计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/909fc5a003436b7cda52464969943050.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*4QMXnW1dVTh768Edr6RnvQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图解。谱系图和延迟执行:当在RDD F上调用一个动作时，为了到达RDD F，只执行虚线转换。不相关的转换不被执行。</p></figure><p id="e722" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这就是所谓的懒惰执行。当一个<strong class="lf jd">动作</strong>被发现时，所有且仅相关的转换被立即执行。这节省了时间和内存，否则这些时间和内存将用于计算不相关的变换。</p><p id="6ca8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">延迟执行的示例:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="9d54" class="nd ma it od b gy oh oi l oj ok">#This is a transformation, it is not performed at this point<br/>parallelData_10 = parallelData.map(lambda x : x*10 )</span><span id="761d" class="nd ma it od b gy ol oi l oj ok">#This is an action, the above transformation is executed only when this line is executed</span><span id="8cee" class="nd ma it od b gy ol oi l oj ok">parallelData_10.collect()</span></pre><h2 id="d27a" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">容错</h2><p id="0ed1" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">容错是指RDD在集群中的某个节点发生故障的情况下保持正常工作的能力。这是RDD开发的关键特性，也是Spark如此强大的原因。沿袭图跟踪所执行的操作，并在RDD丢失的情况下帮助数据恢复。要么将数据复制到不同的节点上，要么使用沿袭图从源节点恢复数据，然后由不同的节点继续执行操作，具体取决于“管理器节点”在集群中的设置方式。</p><h1 id="a0fc" class="lz ma it bd mb mc md me mf mg mh mi mj ki mk kj ml kl mm km mn ko mo kp mp mq bi translated">火花数据帧</h1><p id="f916" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">rdd没有模式。但是出于许多实际目的，数据以半结构化或完全结构化的形式出现。在spark中，可以使用数据帧来处理结构化数据。您可以将spark数据帧想象成类似于python中的data frame或RDBMS中的表，具有带标题的行列结构。您可以从RDD、现有CSV创建spark数据框。txt文件或分布式文件系统，如Avro或Parquet。通常，在处理大量数据时，将文件转换为拼花格式比使用文本或csv格式更有意义。</p><p id="8f4c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">它和熊猫数据框有什么不同？</strong></p><p id="2273" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Spark数据帧分布在集群中，允许隐式并行处理。在spark中，您对熊猫使用的一些功能有不同的含义。SQL库比熊猫库中(计数，描述等。).所以，作为初学者，我会注意语法。</p><h2 id="a0b1" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">创建火花数据帧</h2><p id="0f19" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">从现有文件中读取<em class="mw">(你可能会经常这样做。请注意与pandas类型执行的差异。)</em>:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="02c3" class="nd ma it od b gy oh oi l oj ok">#Feel free to replace csv in this example with parquet, avro, json, </span><span id="a08c" class="nd ma it od b gy ol oi l oj ok">myCsv = spark.read.csv(“where/my/file/lives/filename.csv”, inferSchema = True, header = True)</span><span id="1336" class="nd ma it od b gy ol oi l oj ok">#see how your file schema looks, header and datatype<br/>myCSV.printSchema()</span><span id="6ca7" class="nd ma it od b gy ol oi l oj ok">#see the size</span><span id="b4ae" class="nd ma it od b gy ol oi l oj ok">myCSV.columns #shows column's names</span><span id="b438" class="nd ma it od b gy ol oi l oj ok">len(myCSV.columns) #shows number columns</span><span id="e90d" class="nd ma it od b gy ol oi l oj ok">myCSV.count() #shows rows</span></pre><p id="fe6b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">从头开始创建数据框架<em class="mw">(你可能只有在学习或实验时才会这么做)</em>:</p><pre class="ks kt ku kv gt oc od oe of aw og bi"><span id="fbad" class="nd ma it od b gy oh oi l oj ok">parallelData = sc.parallelize([(1,2,3,4),(5,6,7,8),(9,10,11,12)])</span><span id="8c65" class="nd ma it od b gy ol oi l oj ok">df = parallelData.toDF(["header1","header2","header3","header4"])</span></pre><h2 id="01b3" class="nd ma it bd mb ne nf dn mf ng nh dp mj lm ni nj ml lq nk nl mn lu nm nn mp iz bi translated">Spark会话和Spark上下文</h2><p id="b118" class="pw-post-body-paragraph ld le it lf b lg mr kd li lj ms kg ll lm mt lo lp lq mu ls lt lu mv lw lx ly im bi translated">SparkContext连接运行其执行器或节点(网络上的单个计算机)的Spark驱动程序。它建立了使用rdd和分布式数据处理所需的基础设施。这是火花发动机的主要入口。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi on"><img src="../Images/ccd5a8d6c91e0e263a89a66d2b43f515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A1mxyegrIPFm01mE4mLt6w.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者创作的图形。火花语境。</p></figure><p id="df6e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果有多个用户想要使用相同的集群，并且拥有相同名称的表，但是想要将他们的工作彼此分开，例如，他们的输出、转换、动作等的属性。他们需要自己定制的环境。一种方法是为每个用户创建一个spark上下文。它们保持独立于彼此的火花环境。由于这是花费资源的次优且昂贵的方式，Spark 2.0开始提供不同的解决方案，这就是Spark会话。每个sparksession保持相互独立，并有自己的属性，对其他用户不可见。</p><p id="982e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">综上所述，<strong class="lf jd"> spark context是一个应用</strong>。这是驱动你的spark软件所需要的。它允许您的Spark应用程序在资源管理器的帮助下访问Spark集群。</p><p id="9caf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于它的特性，有多个火花会议是可能的。<strong class="lf jd"> SparkSession是SparkContext的包装器。</strong>上下文由构建器隐式创建，无需任何额外的配置选项。</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="a560" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是对spark中主要概念的基本介绍。当您刚从小数据处理转移到大数据时，它应该会让您知道您正在处理什么。</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="4ffc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我希望在不久的将来能看到更多spark特有的编程技巧！</p><p id="12d5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">查看我的<a class="ae ov" href="https://sruthi-korlakunta.medium.com/" rel="noopener">媒体简介</a>获取更多数据科学文章！</p><p id="1e74" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如果您对数据科学、技术或社交能力方面的文章感兴趣，请联系我。这里是我的<a class="ae ov" href="https://sruthi-korlakunta.medium.com/" rel="noopener">中</a>简介，这里是我的<a class="ae ov" href="https://www.linkedin.com/in/sruthi-korlakunta-7a5b80121" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>。</p></div></div>    
</body>
</html>