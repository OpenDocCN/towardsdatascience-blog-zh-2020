<html>
<head>
<title>Introduction to MCMC</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MCMC简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-mcmc-1c8e3ea88cc9?source=collection_archive---------22-----------------------#2020-10-26">https://towardsdatascience.com/introduction-to-mcmc-1c8e3ea88cc9?source=collection_archive---------22-----------------------#2020-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4d9d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是蒙特卡洛近似法，Metropolis算法是如何工作的？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ab104a3e5e0b0bee0eeefc8ba6eeb6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdYUe-nZYZnOfj_Oqt9CHw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Metropolis算法的图示。作者图片</p></figure><p id="095e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">概率建模如今风靡一时，但当我第一次了解它时，总有一件事困扰着我。许多贝叶斯建模方法需要计算积分，我看到的任何成功的例子似乎都使用高斯或伯努利分布，原因很简单，如果你试图使用比这更复杂的东西，这将成为一个分析噩梦(甚至是棘手的)。将贝叶斯建模限制在“表现良好的”分布的小子集会极大地阻碍您对问题进行良好建模的能力，因此我们想出克服这种限制的方法是很重要的。</p><h2 id="7064" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">蒙特卡洛近似法</h2><p id="d9f1" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">如果我不想解析地计算一些讨厌的积分，我该怎么做？输入蒙特卡罗近似值。</p><p id="fb22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们知道，我们可以通过使用目标分布的样本并计算它们的样本均值来计算期望值。为什么这很重要？那么，除了积分，还有什么是期望呢…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/33f027054637cd8533b20cb83ca27886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQQHFYCsvz7A-qpgdDf0Jw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">连续随机变量的期望。同样的过程也适用于离散，通过改变求和的积分。作者图片</p></figure><p id="45d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于中心极限定理，这种估计积分的方法有一些很好的保证。首先，这是期望的无偏估计，其次，我们可以计算估计的方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fa6b452568203edb44b1cfdc5a85255d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4Gzz99NJAcrWTJpsVl_CA.jpeg"/></div></div></figure><p id="65a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用蒙特卡罗样本计算积分当然很好，但是我们如何从目标分布中抽取样本呢？绘制高斯或均匀样本很容易，但是任何更难和np.random的东西都会让你不及格。</p><p id="bacd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">抽取样本的最简单方法是使用逆CDF方法，但这依赖于对逆CDF函数的访问，该函数通常没有很好的分析形式，只对一维随机变量有意义。</p><p id="2bd5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Metropolis算法是许多马尔可夫链蒙特卡罗(MCMC)采样方法的构建模块之一。当您只能访问目标发行版的pdf时，它允许我们提取样本。</p><p id="2150" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MCMC方法附带了一个警告，即我们不再获取独立的样本，因此我们不能保证我们的估计方差随着我们获取的样本数量的增加而减少。如果独立抽取样本，中心极限定理告诉我们，我们估计的方差将与样本数(N)成反比地减少。对于MCMC，我们可以通过将样本数从N调整到N_eff来忽略这一点。N_eff(几乎)总是小于N，并且与链中样本的相关程度有关。</p><h2 id="8629" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">大都市抽样</h2><p id="1a4a" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Metropolis算法的步骤如下:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f517" class="lu lv it mt b gy mx my l mz na">1. Sample a starting point uniformly from the domain of the target distribution or from the prior distribution.</span><span id="8be3" class="lu lv it mt b gy nb my l mz na">2. Calculate the pdf at that point.</span><span id="331a" class="lu lv it mt b gy nb my l mz na">3. Make a proposal for the new sample by taking a step from the current position in accordance with some state transition function.</span><span id="50db" class="lu lv it mt b gy nb my l mz na">4. Calculate the new pdf value.</span><span id="1033" class="lu lv it mt b gy nb my l mz na">5. Calculate the value of new pdf / old pdf.</span><span id="0f0d" class="lu lv it mt b gy nb my l mz na">6. If the ratio is greater than 1, accept the step.</span><span id="c6eb" class="lu lv it mt b gy nb my l mz na">7. If the ratio is less than 1:<br/>    1. Sample a uniform random variable.<br/>    2. If the ratio greater than the random sample, accept the step.</span><span id="76b4" class="lu lv it mt b gy nb my l mz na">8. Else reject the proposal, add the current position as a sample and take a new step.</span></pre><p id="3264" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，5–8中描述的过程相当于根据概率为min(1，p(新)/p(旧))的伯努利概率接受样本，请记住这一点，以便以后使用…</p><h2 id="cf92" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">为什么大都会抽样有效？</h2><p id="7382" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">对于任何MCMC方法，我们希望确保一个称为详细平衡或可逆性的属性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bd4c3228fd8b545fcf67e1d4f7dc9198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3oLy119LqXIwW8S8N7L3g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">明细余额。作者图片</p></figure><p id="7422" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果π满足这个条件，那么π就是马尔可夫链<a class="ae nc" href="http://www.robots.ox.ac.uk/~fwood/teaching/C19_hilary_2015_2016/mcmc.pdf" rel="noopener ugc nofollow" target="_blank"> (1) </a>的平稳分布。我们可以通过对等式两边求和来证明这一点。如果我们能保证详细的平衡，那么我们也知道我们是从马尔可夫链的平稳分布中取样，我们将把它指定为我们的目标分布。</p><p id="4ef5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种对详细平衡的直觉是，由于从状态<code class="fe nd ne nf mt b">i</code>到状态<code class="fe nd ne nf mt b">j</code>的概率‘质量’转移与从状态<code class="fe nd ne nf mt b">j</code>到状态<code class="fe nd ne nf mt b">i</code>的转移相同，所以在链的每次转移之后，我们保持在稳定分布。</p><p id="bbc6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么现在让我们展示Metropolis算法如何满足这个条件…</p><p id="62a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了找到p(i，j)使得它满足详细平衡，我们首先提出任意的“跳转”概率q(i，j)，然后通过只接受概率为α(i，j)的“跳转”来获得p(i，j)。当一个“跳转”被拒绝时，状态保持j=i。这种“接受”思想不是Metropolis算法所独有的，并且存在于MCMC的大多数变体中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e254483c003c1a347b217936403e14e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAYY-z6wb95gWn5wYqYCeg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">跳跃概率推导。作者图片</p></figure><p id="9619" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这取决于α是有效的概率分布。所以α的有效形式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/298d6845d3673a115aea2d2d08d4f24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODLKFeEZP4C0gAS7NHmmHA@2x.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">大都会-黑斯廷斯台阶。作者图片</p></figure><p id="c178" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果跳动概率是对称的，那么这可以简化为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/4a4b2dc8ba9a1f811d1aaeba2c734f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlfHffYcij21T6XVas3ehA@2x.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">大都会台阶。作者图片</p></figure><p id="bf4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">否则，它可以保留其完整形式，称为Metropolis-Hasting MCMC。</p><p id="479a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以保证详细的平衡，我们可以让马尔可夫链机器接管。如果马尔可夫链是遍历的(所有状态都是不可约的)，那么在某个点上，该链将到达平稳分布，并且我们能够从目标分布中提取样本。</p><p id="0531" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能还注意到，由于α是π(j)/π(i)的函数。这意味着目标分布不需要标准化。这在使用Metropolis进行贝叶斯后验估计时特别有用，因为证据项很难计算。</p><h2 id="8232" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">实施说明</h2><p id="107c" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Metropolis算法的常见版本称为“随机行走Metropolis ”,其中建议的状态是当前状态加上具有零均值和协方差矩阵σ I的多元高斯。σ应选择为足够大，以便剔除足够多的样本。这是为了确保目标分布的良好探索。</p><p id="4aae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二个需要注意的是老化的概念。在马尔可夫链达到稳定分布之前采集的样本应该被移除，因为在马尔可夫链收敛之前，它们不代表目标分布。确定应该移除多少样本有些困难，但一般来说，会移除10–20%的样本(或10–100个有效样本)。</p><h2 id="4717" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">让我们在Numpy建造它</h2><p id="19b0" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">为了简单起见，这里我们实现随机漫步大都会。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="50aa" class="lu lv it mt b gy mx my l mz na">def metropolis(pi, dims, n_samples, burn_in=0.1, var=1):<br/>    theta_ = np.random.randn(dims)*var</span><span id="48d2" class="lu lv it mt b gy nb my l mz na">    samples = []<br/>    while len(samples) &lt; n_samples:<br/>        theta = theta_ + np.random.randn(dims)*var</span><span id="0ded" class="lu lv it mt b gy nb my l mz na">ratio = pi(theta)/pi(theta_)<br/>        if np.random.rand(1) &lt; ratio:<br/>            sample = theta<br/>            theta_ = theta<br/>            samples.append(sample)<br/>        else:<br/>            sample = theta_<br/>            samples.append(sample)</span><span id="57fe" class="lu lv it mt b gy nb my l mz na">    samples = np.array(samples)<br/>    return samples[int(samples*burn_in):,:]</span></pre><p id="a44f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到这在两个高斯分布的和上的表现(注意这是一个非标准化的分布)。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="83ad" class="lu lv it mt b gy mx my l mz na">from scipy.stats import multivariate_normal</span><span id="0bff" class="lu lv it mt b gy nb my l mz na">def make_pdf(mean1, mean2, cov1, cov2):</span><span id="c3aa" class="lu lv it mt b gy nb my l mz na">    pdf1 = multivariate_normal(mean1, cov1)<br/>    pdf2 = multivariate_normal(mean2, cov2)</span><span id="e970" class="lu lv it mt b gy nb my l mz na">    def pdf(x):<br/>        return pdf1.pdf(x) * pdf2.pdf(x)</span><span id="4a49" class="lu lv it mt b gy nb my l mz na">    return pdf</span><span id="5c09" class="lu lv it mt b gy nb my l mz na">pdf = make_pdf(<br/>    [3, 3],<br/>    [-1, -1],<br/>    np.array([[1,0.1],[0.1,1]], dtype=float),<br/>    np.array([[1,0.1],[0.1,1]], dtype=float),<br/>)</span><span id="af61" class="lu lv it mt b gy nb my l mz na">samples = metropolis(pdf, 2, 1_000, 0.1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/5df5f008ea4070ad27c9f07af090ea38.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*OV6xi8Fp4fjnrT02G-mLuA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个多元高斯和的大都会样本。作者图片</p></figure><p id="2afc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的gif显示了算法如何遍历分布，偶尔在分布的两种不同模式之间跳跃。注意，这也突出了Metropolis算法的一个弱点，它处理多模型分布相对较差。这是因为，为了探索一种新的模式，步长必须足够大，以便从一种模式转换到另一种模式。这要么需要一个大的步长，要么需要模式接近的分布。诸如哈密尔顿MCMC的修改可以对此有所帮助，但是一般来说，这是大多数MCMC方法的问题。</p><h2 id="340f" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">附加“阅读”</h2><ul class=""><li id="baf3" class="ni nj it la b lb mn le mo lh nk ll nl lp nm lt nn no np nq bi translated"><a class="ae nc" href="https://www.youtube.com/watch?v=xxDkdwQdGvs&amp;t=0s" rel="noopener ugc nofollow" target="_blank">关于详细平衡的视频</a></li><li id="0f4b" class="ni nj it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated"><a class="ae nc" href="https://arxiv.org/pdf/1504.01896.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>关于Metropolis-Hastings算法</li><li id="302f" class="ni nj it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated"><a class="ae nc" href="https://arxiv.org/pdf/1206.1901.pdf):" rel="noopener ugc nofollow" target="_blank">论文</a>关于哈密顿动力学(stan和其他概率语言使用的MCMC方法)</li></ul></div></div>    
</body>
</html>