<html>
<head>
<title>Gibbs Sampling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吉布斯采样</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gibbs-sampling-8e4844560ae5?source=collection_archive---------6-----------------------#2020-10-02">https://towardsdatascience.com/gibbs-sampling-8e4844560ae5?source=collection_archive---------6-----------------------#2020-10-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ef6e0830a5ac95ba6798c31c113699df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DX1EITY19sJQC-Tk"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jg" href="https://unsplash.com/@kmuza?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Carlos Muza </a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="2252" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">又一种MCMC方法</h2></div><p id="6b80" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与其他MCMC方法一样，Gibbs抽样器构建了一个马尔可夫链，其值向目标分布收敛。Gibbs抽样实际上是<a class="ae jg" rel="noopener" target="_blank" href="/monte-carlo-markov-chain-89cb7e844c75"> Metropolis-Hastings </a>算法的一个特例，其中建议总是被接受。</p><p id="e6f6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">具体来说，假设您想要对一个多元<strong class="la jk"> </strong>概率分布进行采样。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/775218dfaa04b6300ad544a98e5ba431.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*oJRqiZNHOJ4SWTqVSZwHdA.png"/></div></figure><p id="21ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lz">注:</em> </strong> <em class="lz">一个</em> <strong class="la jk">多元</strong>概率分布是多个变量的函数(即二维正态分布)。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ma"><img src="../Images/ad781d8ca95de0cf456021110d07103b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZU1QB0SO--NTNwNF.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://commons.wikimedia.org/wiki/File:MultivariateNormal.png" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:multivariatenormal . png</a></p></figure><p id="a524" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不知道如何直接从后者取样。然而，由于一些数学上的便利，或者仅仅是运气，我们碰巧知道条件概率。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/098294b071db4a80147399822b074bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*20UwRnA9OGvwAFLNvDU9vQ.png"/></div></figure><p id="a027" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是吉布斯采样的用武之地。吉布斯抽样适用于联合分布未知或难以直接抽样的情况，但每个变量的条件分布已知且易于抽样。</p><h1 id="6647" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">吉布斯采样算法</h1><p id="8bbe" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们从选择随机变量X<strong class="la jk"><em class="lz"/></strong>&amp;<strong class="la jk"><em class="lz">Y</em></strong>的初始值开始。然后，我们从x给定Y = Y⁰的条件概率分布中取样，表示为p(X|Y⁰).在下一步中，我们采样一个以X为条件的Y的新值，这是我们刚刚计算的。我们重复该过程进行额外的<strong class="la jk"> <em class="lz"> n - 1 </em> </strong>次迭代，在给定另一个随机变量的当前值的情况下，在从X的条件概率分布和Y的条件概率分布中抽取新样本之间交替进行。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/37064ea9d84f1c47e53b88a59de5cf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*uenaoaVdM2V7Cf-PUv-UdQ.png"/></div></figure><p id="3600" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看一个例子。假设我们有下面的后验和条件概率分布。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/9e458e73f68eb4f7dbc828fec4b7ae3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*hFk3xKCJTOezfSmcTOhh2w.png"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/3859521fbb4305abda9af32b2aec11f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*sKcqOnnegTb3e1DJmNIqiw.png"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/653aeabed5ff689feb703913d1b3feb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*pvujtPCFYF2XcgJcafyhxw.png"/></div></figure><p id="32d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<em class="lz"> </em> g(y)包含不包含x的项，g(x)包含不依赖于y的项。</p><p id="6444" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不知道C(归一化常数)的值。然而，我们知道条件概率分布。因此，我们可以用吉布斯抽样来近似后验分布。</p><p id="bc4a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lz">注:</em> </strong>条件概率实际上是正态分布，可以改写如下。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7ff0fefa68d768cb3c806b623ec33f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*X487uBuk2y--JDF76xLKZQ.png"/></div></figure><p id="719d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定前面的等式，我们继续用Python实现Gibbs采样算法。首先，我们导入以下库。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="efc7" class="nj md jj nf b gy nk nl l nm nn">import numpy as np<br/>import scipy as sp<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="1e72" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们定义后验分布的函数(假设C=1)。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="f4b8" class="nj md jj nf b gy nk nl l nm nn">f = lambda x, y: np.exp(-(x*x*y*y+x*x+y*y-8*x-8*y)/2.)</span></pre><p id="08ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们画出概率分布。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="f174" class="nj md jj nf b gy nk nl l nm nn">xx = np.linspace(-1, 8, 100)<br/>yy = np.linspace(-1, 8, 100)<br/>xg,yg = np.meshgrid(xx, yy)<br/>z = f(xg.ravel(), yg.ravel())<br/>z2 = z.reshape(xg.shape)<br/>plt.contourf(xg, yg, z2, cmap='BrBG')</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/507634219d12b29d5bc06ba26ead8663.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*qqyEtJDKs8QWqcjWFYqnWg.png"/></div></figure><p id="61f7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将尝试使用吉布斯抽样来估计概率分布。正如我们之前提到的，条件概率是正态分布。因此，我们可以用mu和sigma来表示它们。</p><p id="04ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下面的代码块中，我们为mu和sigma定义函数，初始化我们的随机变量<em class="lz"> X </em> &amp; <em class="lz"> Y </em>，并设置<em class="lz"> N </em>(迭代次数)。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="6f92" class="nj md jj nf b gy nk nl l nm nn">N = 50000<br/>x = np.zeros(N+1)<br/>y = np.zeros(N+1)<br/>x[0] = 1.<br/>y[0] = 6.<br/>sig = lambda z, i: np.sqrt(1./(1.+z[i]*z[i]))<br/>mu = lambda z, i: 4./(1.+z[i]*z[i])</span></pre><p id="3a20" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们逐步执行吉布斯采样算法。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="b449" class="nj md jj nf b gy nk nl l nm nn">for i in range(1, N, 2):<br/>    sig_x = sig(y, i-1)<br/>    mu_x = mu(y, i-1)<br/>    x[i] = np.random.normal(mu_x, sig_x)<br/>    y[i] = y[i-1]<br/>    <br/>    sig_y = sig(x, i)<br/>    mu_y = mu(x, i)<br/>    y[i+1] = np.random.normal(mu_y, sig_y)<br/>    x[i+1] = x[i]</span></pre><p id="05ca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们绘制结果。</p><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="495b" class="nj md jj nf b gy nk nl l nm nn">plt.hist(x, bins=50);<br/>plt.hist(y, bins=50);</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/71935ec908dbef3bc9f0165b44451643.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*cmZbKNsTg0t8PUtzV7Jj7g.png"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/dfda5fa3345b7716c8bb4b9edb0bb730.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*Sm6ZmX1fIpAeSXh4WHny9w.png"/></div></figure><pre class="lv lw lx ly gt ne nf ng nh aw ni bi"><span id="33a1" class="nj md jj nf b gy nk nl l nm nn">plt.contourf(xg, yg, z2, alpha=0.8, cmap='BrBG')<br/>plt.plot(x[::10],y[::10], '.', alpha=0.1)<br/>plt.plot(x[:300],y[:300], c='r', alpha=0.3, lw=1)</span></pre><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5e18588508442d25487e378259df17fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*KAUTBE39TJ8kSzTr0_pBbw.png"/></div></figure><p id="63e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所看到的，使用Gibbs抽样算法获得的概率分布很好地逼近了目标分布。</p><h1 id="06c7" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">结论</h1><p id="e323" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Gibbs抽样是一种蒙特卡罗马尔可夫链方法，它根据其他变量的当前值从每个变量的分布中迭代地抽取一个实例，以便估计复杂的联合分布。与Metropolis-Hastings算法相反，我们总是接受建议。因此，吉布斯采样可以更有效。</p></div></div>    
</body>
</html>