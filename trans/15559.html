<html>
<head>
<title>NLP Easy explanation of common terms with python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python中常见术语的NLP简易解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-easy-explanation-of-common-terms-with-python-dc7c323a4691?source=collection_archive---------49-----------------------#2020-10-26">https://towardsdatascience.com/nlp-easy-explanation-of-common-terms-with-python-dc7c323a4691?source=collection_archive---------49-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c643" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">自然语言处理</em> </strong> ( <strong class="jp ir"> <em class="kl"> NLP </em> </strong>)是数据科学处理语言学的领域，人工智能涉及计算机系统与人类<strong class="jp ir"><em class="kl"/></strong>语言的交互，以解释和分析系统中的自然语言，这是数据科学中一个不断扩展的领域，其中各种技术被应用于分析大量的<strong class="jp ir"><em class="kl"/></strong>自然语言数据。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/81376b60b77f18fa5e522fca8a26e4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-Az6X_4IdW_1mcPrzsNyQ.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">NLP[作者图片]</p></figure><p id="9ada" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于这项工作的最流行的库是nltk，它可以通过下面的代码行导入。</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="9e0e" class="lh li iq ld b gy lj lk l ll lm">import nltk</span></pre><p id="f915" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">下面给出了我们在讨论NLP模型时使用的几个常用术语及其含义和实现。</strong></p><ol class=""><li id="6273" class="ln lo iq jp b jq jr ju jv jy lp kc lq kg lr kk ls lt lu lv bi translated"><strong class="jp ir">标记化:</strong>将文本分解成更小的部分称为标记化。它可以把段落分成句子，把句子分成单词。</li></ol><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="2250" class="lh li iq ld b gy lj lk l ll lm"># Tokenizing sentences<br/>sentences = nltk.sent_tokenize(paragraph)<br/> <br/># Tokenizing words<br/>word = nltk.word_tokenize(paragraph)</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lw"><img src="../Images/7b902aaf8a79e93fd9f866bab6cbf4c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLSABYEK8GJLxkQ93LTdTw.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">段落[图片由作者提供]</p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lx"><img src="../Images/2d90b2475dfe499d6a12aaa06e36bcd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1e_W0C732eniA7KWRTVhcg.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">段落被分割成句子[图片由作者提供]</p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ly"><img src="../Images/2384901685fdcdb554638e7a8ae771d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ol0aqLGTgc05LpaJzsAkGg.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">段落被拆分成单词[图片由作者提供]</p></figure><p id="497f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">2)</strong>T22】词干化:将许多相似的词化简为一个词干或基词，称为词干化。</p><p id="e918" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">英语中最流行的词干分析器叫做波特斯特梅尔，它是nltk的一个库。</p><p id="371a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">斯特梅尔函数的例子如下:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lz"><img src="../Images/5216f5a43a088f5215cbd2d1e86c1e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A3aucY_ePr0Q_Nrzng-LeA.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">词干[作者图片]</p></figure><p id="5f44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">优点:</em> </strong>这是快速算法，可以用在需要速度的模型中。</p><p id="462b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">缺点:</em> </strong>没有考虑到stem函数的意义，只是把它简化为stem。</p><p id="2abb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它是这样完成的:</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="d98e" class="lh li iq ld b gy lj lk l ll lm">#import<br/>from nltk.stem.porter import PorterStemmer<br/>ps = PorterStemmer()<br/>review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]</span></pre><p id="bf6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"/></p><p id="12b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">示例如下:</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lz"><img src="../Images/7cc38aef2b68fd0e202f837667346142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brdEZ_wvDfxO9lmd5DNptw.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">词汇化[图片由作者提供]</p></figure><p id="8681" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">优点:</em> </strong>它主要用于聊天机器人，因为给出有意义的回应是它的主要思想。</p><p id="cf35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">缺点:</em> </strong>比炮泥慢，时间是主要考虑的地方就是时间。</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="dd52" class="lh li iq ld b gy lj lk l ll lm"><em class="kl">#import</em><br/>from nltk.stem import WordNetLemmatizer<br/>wordnet=WordNetLemmatizer()<br/>review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]</span></pre><p id="7264" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl"> 4) </em> </strong> <strong class="jp ir"> <em class="kl">停用词:</em> </strong>在语言处理中不是很重要的词，可以在对其应用任何模型之前，或者在对其进行情感处理之前去除。像is，an，you，the这些词可以称为停用词，可以从nltk.corpus导入为' nltk.corpus导入停用词'。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ma"><img src="../Images/6faf6fc88228fef19a2247dd5fd3371f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEV_VJ2AIVCgkgZRMe89FQ.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">停用词[图片由作者提供]</p></figure><p id="7a2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中:</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="4e70" class="lh li iq ld b gy lj lk l ll lm">#import<br/>from nltk.corpus import stopwords<br/>review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]</span></pre><p id="b7eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5) </strong> <strong class="jp ir">单词包</strong>这是机器学习算法中一种表示和处理单词的方式。它代表单词的出现和频率。它是在<strong class="jp ir">自然语言处理</strong>中使用的模型，表示为其<strong class="jp ir">单词</strong>的<strong class="jp ir">包</strong> (multiset)，不考虑语法甚至<strong class="jp ir">单词</strong>的顺序，而是保持其出现的次数。</p><p id="0d5a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">优点:</em> </strong></p><p id="1d0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑了它的速度和频率。容易实现。</p><p id="4ad4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">缺点:</em> </strong></p><p id="beac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它没有将数据表示成信息，也就是说，在这样做的时候，单词的含义丢失了。它假设所有的<strong class="jp ir">单词</strong>都是相互独立的。</p><p id="b435" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仅适用于小数据。</p><p id="bbd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">例如:</strong></p><blockquote class="mb mc md"><p id="98ab" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated">她是一个非常好和体面的女人，她也是一个很好的艺术家。</p><p id="4743" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated">他是个坏人，但却是个好司机。</p><p id="a46c" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated"><strong class="jp ir">第三句:体面社会男女平等。</strong></p></blockquote><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mh"><img src="../Images/d62607cb863611d0ec880110dff7f2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qlsNkD58nw1PhSltiIAXCA.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">单词包[图片由作者提供]</p></figure><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="0f72" class="lh li iq ld b gy lj lk l ll lm"># Creating the Bag of Words model<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer(max_features = 1500)<br/>X = cv.fit_transform(paragraph).toarray()</span></pre><p id="39fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 6) </strong> TFIDF: <strong class="jp ir">词频-逆文档频率，</strong>是一个统计公式，在该公式中评估一个词在文本集合中的文本中的相关程度。它是通过两个指标的乘积获得的，即术语频率和逆文档频率。</p><p id="c61d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">词频</strong> =一句话中的重复字数/一句话中的字数。</p><p id="172a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">逆文档频率</strong> =log(句子数量/包含单词的句子数量)</p><p id="23ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">例如:</strong></p><p id="73d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">她是一个非常好和体面的女人，她也是一个很好的艺术家。</p><p id="95e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第二句:他是个坏人，但却是个好司机。</strong></p><p id="30c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第三句:体面社会男女平等。</strong></p><p id="6082" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">把停用词去掉，句子就可以变得更简洁。</p><blockquote class="mb mc md"><p id="ff6e" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated"><strong class="jp ir">第一句:很好很正派的女人好艺术家。</strong></p><p id="727e" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated"><strong class="jp ir">第二句:坏人好司机。</strong></p><p id="6484" class="jn jo kl jp b jq jr js jt ju jv jw jx me jz ka kb mf kd ke kf mg kh ki kj kk ij bi translated"><strong class="jp ir">第三句:男女平等体面社会。</strong></p></blockquote><p id="eb41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其计算方法如下:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mi"><img src="../Images/72a8ce11b7f5f1e1b976fc1493dc27e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2XUJNDBOdbuYZmm-Ot1HQ.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">TF[作者图片]</p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mj"><img src="../Images/00798c2eff7383b5baea7228166a6deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQsvERb3hDV96gExn6aHBw.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">IDF[图片由作者提供]</p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mk"><img src="../Images/52f9ab2a9ee0039ea7995a28353925cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ua_pCQhPgYZwopOvdYMWw.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">tfi df[作者图片]</p></figure><p id="f499" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">优点:</em> </strong>易于计算和实现。它可以给出一些基本的度量来提取文本中最具描述性的术语。使用它可以很容易地计算两个文本之间的相似度。搜索引擎可以使用它。</p><p id="de96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">缺点:</em> </strong> TF-IDF不能捕获文本中单词的语义或出现位置。</p><p id="7312" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中，这可以通过以下方式实现:</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="83ff" class="lh li iq ld b gy lj lk l ll lm"><strong class="ld ir"># Creating the TF-IDF <br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>cv=TfidfVectorizer()<br/>X=cv.fit_transform(paragraph).toarray()</strong></span></pre><p id="c508" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 7) </strong> <strong class="jp ir"> Word2Vec </strong>是一种自然语言处理(NLP)的技术。word2vec算法使用神经网络模型从大型文本语料库中学习单词语义及其关联。一旦经过训练，这种模型可以检测相似的单词，或者让我们知道部分句子的附加单词。顾名思义，word2vec用称为向量的特定数字列表来表示每个不同的单词。</p><p id="ae97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仔细选择向量，使得向量之间的简单数学余弦表示由这些向量表示的单词之间的语义相似度。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/7b479d94a5f03d0613b9762409f73cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NpT1jB5iiXKgUuG7BBLOmA.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">word 2 vec[图片由作者提供]</p></figure><p id="48fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">优点:</em> </strong></p><p id="3f7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这通过将目标词映射到与其有上下文关系的词来将未标记的原始语料库转换成标记的数据，从而学习词在分类模型中的表示。</p><p id="9926" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目标词与其上下文关系词之间的映射将次线性关系嵌入到词的向量空间中，从而可以通过词向量来推断类似于“<em class="kl">国王:男人</em>作为<em class="kl">王后:女人</em>的关系。</p><p id="3332" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">易于理解和实施。</p><p id="13a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">缺点:</em> </strong></p><p id="5691" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单词序列丢失，因此子线性关系没有被很好地定义。</p><p id="5a5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据必须在线提供给模型，并且可能需要预处理，这需要存储空间。</p><p id="ecfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果类别的数量太大，即语料库太大并且词汇太多，则该模型可能非常难以训练。</p><p id="f58d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中，它可以实现为:</p><pre class="kn ko kp kq gt lc ld le lf aw lg bi"><span id="3182" class="lh li iq ld b gy lj lk l ll lm">from gensim.models import Word2Vec<br/>from gensim.models import KeyedVectors</span><span id="d930" class="lh li iq ld b gy mm lk l ll lm">sentences = nltk.sent_tokenize(paragraph)<br/>sentences = [nltk.word_tokenize(sentence) for sentence in sentences]</span><span id="c2af" class="lh li iq ld b gy mm lk l ll lm"># Training the Word2Vec model<br/>model=Word2Vec(sentences, min_count=1)<br/>words=model.wv.vocab<br/><br/># Most similar words<br/>similar=model.wv.most_similar('woman')</span><span id="33b9" class="lh li iq ld b gy mm lk l ll lm">#Output</span><span id="9df3" class="lh li iq ld b gy mm lk l ll lm">[ ('driver', 0.15176300704479218),<br/> ('artist', 0.06272515654563904),<br/> ('good', 0.0425836481153965),<br/> ('man', -0.0059792473912239075)]</span></pre><p id="7906" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论:</strong></p><p id="7bb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">值得注意的是，每种技术都是在发现它的缺点之后发展起来的，比如词汇化比词干化更好，因为它可以将相似的单词转换成一些有意义的单词。TFIDF比单词袋更好，因为它可以包含比单词袋更好的信息。Word2Vec比TFIDF更好，因为它可以预测单词之间的上下文关系。</p><p id="eb10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，在所有这些技术中，语义或单词顺序丢失，为此RNN或递归神经网络出现，其中按照单词顺序的所有重要信息保持完整，并且情感分析以惊人的方式完成。</p><p id="993f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望通过阅读这篇博客，许多术语必须清楚，NLP可能看起来更容易。</p><p id="dd17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p></div><div class="ab cl mn mo hu mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="ij ik il im in"><p id="96af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">原载于2020年10月26日https://www.numpyninja.com</em><em class="kl">T21</em><a class="ae mu" href="https://www.numpyninja.com/post/nlp-easy-explanation-of-common-terms-with-python" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>