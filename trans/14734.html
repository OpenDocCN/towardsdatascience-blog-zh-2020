<html>
<head>
<title>Weakly Supervised Learning: Classification with limited annotation capacity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">弱监督学习:具有有限标注能力的分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-supervised-learning-getting-started-with-unstructured-data-123354dad7c1?source=collection_archive---------10-----------------------#2020-10-11">https://towardsdatascience.com/weekly-supervised-learning-getting-started-with-unstructured-data-123354dad7c1?source=collection_archive---------10-----------------------#2020-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6ffd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着大量基于海量数据训练的预训练模型的引入，机器学习在工业应用中的成功呈指数级增长。虽然我们可以通过迁移学习(作为特征提取器或通过对我们的特定数据集进行微调)来轻松利用这些模型，但通常情况下，如果模式与这些模型最初训练的数据明显不同，性能不会很好。</p><p id="0a8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特别是在自然语言处理的情况下，有许多特定于数据的模式，这通常会降低预训练模型的适用性。与此同时，获取特定于应用程序的注释所涉及的挑战不仅降低了数据的可用性，还降低了建模的监督(转移)学习的可用性。</p><p id="a191" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于未标注的文本数据通常大量存在，无监督/弱监督方法成为数据探索甚至建模的第一手选择。因为建立一个具有领域专业知识的注释者团队不仅昂贵而且耗时，所以最近，从业者已经转向了较弱形式的监督，这也是这篇博客的主要焦点。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/ac6c76e32c516e84864d0818818f3004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzSe2tWFtIrnR7Vk857law.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图片(按作者):结构化非结构化信息的各种方法</p></figure><h2 id="e359" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">什么是监管不力？</h2><p id="4cc6" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">正如在<a class="ae lz" href="https://en.wikipedia.org/wiki/Weak_supervision" rel="noopener ugc nofollow" target="_blank">维基百科</a>中提到的，<strong class="jp ir">弱监督</strong>是机器学习的一个分支，其中嘈杂、有限或不精确的来源被用来提供监督信号，以便在监督学习设置中标记大量训练数据。这种方法减轻了获取手工标记的数据集的负担，这可能是昂贵的或不切实际的。取而代之的是，使用廉价的弱标签，因为它们是不完美的，但是仍然可以用来创建强预测模型。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d1a4" class="mh lc iq bd ld mi mj mk lg ml mm mn lj mo mp mq lm mr ms mt lp mu mv mw ls mx bi translated"><strong class="ak"> <em class="my">弱监督学习在亚马逊商品评论上的应用</em> </strong></h1><p id="5917" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">对于大规模数据集，例如<a class="ae lz" href="https://www.kaggle.com/bittlingmayer/amazonreviews" rel="noopener ugc nofollow" target="_blank">亚马逊情感评论</a>，目标是识别关于用户在图书的<strong class="jp ir">负面评论</strong> <strong class="jp ir">中提及的内容的广泛类别，并进一步构建可用于向卖家提供<strong class="jp ir">分类反馈</strong>的预测模型。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mz"><img src="../Images/836b75bde62a4e1692b5c6dde28e3b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPBsnINXHDmD9-17-cIV7Q.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表:亚马逊评论数据示例(总共360万行)</p></figure><h2 id="da20" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">通过集群上的批量注释建立训练数据集</h2><ol class=""><li id="4d1a" class="na nb iq jp b jq lu ju lv jy nc kc nd kg ne kk nf ng nh ni bi translated"><em class="nj">启发式提取评分较低的评论</em></li></ol><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="ba05" class="lb lc iq nl b gy np nq l nr ns">import pandas as pd</span><span id="b661" class="lb lc iq nl b gy nt nq l nr ns"># Load dataset<br/>df = pd.read_csv("amazon_product_reviews.csv")</span><span id="1cb3" class="lb lc iq nl b gy nt nq l nr ns"># Negative Reviews (With rating 1-2)<br/>df = df[df['Rating'] == '1-2']</span><span id="bad0" class="lb lc iq nl b gy nt nq l nr ns"># Sample set of negative Review that mentions book<br/>df = df[df['Amazon Review'].str.contains(<br/>    'book|publication|novel|paperback|hardcover')].sample(50000)</span></pre><p id="4d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nj"> 2。文本数据的标准预处理</em></p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="6608" class="lb lc iq nl b gy np nq l nr ns">import re<br/>import nltk<br/>import itertools<br/>import contractions<br/></span><span id="38f5" class="lb lc iq nl b gy nt nq l nr ns"># Load dataset<br/>df = pd.read_csv("amazon_product_reviews.csv")</span><span id="0f49" class="lb lc iq nl b gy nt nq l nr ns">column = 'Amazon Review'</span><span id="ff09" class="lb lc iq nl b gy nt nq l nr ns"># Lowercase text<br/>df[column] = df[column].apply(<br/>    lambda x: x.lower())</span><span id="6ce4" class="lb lc iq nl b gy nt nq l nr ns"># Fix contraction in the words<br/>df[column] = df[column].apply(<br/>    lambda x: contractions.fix(str(x)))</span><span id="1381" class="lb lc iq nl b gy nt nq l nr ns"># Standardise words<br/>df[column] = df[column].apply(<br/>    lambda x: ''.join(''.join(c)[:2] for _, c in itertools.groupby(x)))</span><span id="902d" class="lb lc iq nl b gy nt nq l nr ns"># Remove punctuations<br/>punctuations_regex="!#%&amp;\'()\*+,-/:;&lt;=&gt;?@\[\]^_`{|}~.1234567890"<br/>df[column] = df[column].apply(<br/>    lambda x: re.sub('[%s]'%re.escape(punctuations_regex), ' ', x))</span><span id="8edf" class="lb lc iq nl b gy nt nq l nr ns"># Tokenize words<br/>df[column] = df[column].apply(<br/>    lambda x: ' '.join(nltk.word_tokenize(x)))</span></pre><p id="3eb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nj"> 3。特征提取和聚类</em></p><ul class=""><li id="6cef" class="na nb iq jp b jq jr ju jv jy nu kc nv kg nw kk nx ng nh ni bi translated">可以进一步试验ngram_range和min_df，以优化对重要短语的更多覆盖和对不重要短语的更少覆盖。</li><li id="9cd7" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated">n_clusters基于Elbow方法+后续步骤中注释集群的专家的可用性进行了优化。</li></ul><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="fe70" class="lb lc iq nl b gy np nq l nr ns">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.cluster import KMeans</span><span id="faa4" class="lb lc iq nl b gy nt nq l nr ns"># Extract TFIDF features with 1-4 grams<br/>TFIDF_PARAMS = {<br/>    'strip_accents': 'ascii',<br/>    'stop_words': 'english',<br/>    'sublinear_tf': True,<br/>    'ngram_range': (1, 4),<br/>    'min_df': 0.003,<br/>}<br/>vectorizer = TfidfVectorizer(**TFIDF_PARAMS)<br/>tfidf_model = vectorizer.fit(df[column])<br/>train_features = tfidf_model.transform(df[column])</span><span id="10d7" class="lb lc iq nl b gy nt nq l nr ns"># Identify clusters on the model<br/>cluster_model = KMeans(n_clusters=200).fit(train_features)<br/>df['cluster_label'] = cluster_model.labels_</span></pre><p id="e246" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nj"> 4。分析和注释集群</em></p><ul class=""><li id="f293" class="na nb iq jp b jq jr ju jv jy nu kc nv kg nw kk nx ng nh ni bi translated">所有的聚类由领域专家标记，并且聚类标签被分配给所有的聚类样本。这极大地减少了标注时间和成本，因为只有<strong class="jp ir"> <em class="nj"> n个聚类被标记为</em> </strong> <em class="nj">，而不是所有360万个样本。</em></li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi od"><img src="../Images/74ca8cafd3f524c242208f8165272c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6h7MfDmULlN1m_0Zh_BlA.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:分析基于TFIDF分数的聚类，并为其添加标签。</p></figure><ul class=""><li id="65b5" class="na nb iq jp b jq jr ju jv jy nu kc nv kg nw kk nx ng nh ni bi translated"><em class="nj">最后，对提取的数据集上的“最终类别”的采样结果表明，它的</em> <strong class="jp ir"> <em class="nj"> 85 ( </em> 5 <em class="nj"> ) %准确。</em>T15】</strong></li></ul><h2 id="2e67" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">训练预测模型以分配最终类别进行审查</h2><p id="df51" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">最后，通过对整个非结构化数据运行聚类模型来管理大型训练数据集，并基于聚类标识符来分配标签。最终的预测模型以受监督的方式在该数据集上训练，这不仅处理数据集中的错误，而且以更高的置信度(基于置信度阈值)提供高质量的预测。</p><pre class="km kn ko kp gt nk nl nm nn aw no bi"><span id="c4e3" class="lb lc iq nl b gy np nq l nr ns"># Training XGBoost (Weak supervision)<br/>from xgboost import XGBClassifier<br/>from sklearn.metrics import classification_report</span><span id="63bd" class="lb lc iq nl b gy nt nq l nr ns">clf = XGBClassifier(n_jobs= -1, objective='multi:softmax')<br/>clf = clf.fit(X_train, y_train)<br/>scores = clf.predict_proba(X_test)</span><span id="ece7" class="lb lc iq nl b gy nt nq l nr ns">y_true = pd.DataFrame(scores,columns=clf.classes_).idxmax(axis=1)<br/>print (classification_report(y_test, y_true, digits=4))<br/></span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oe"><img src="../Images/8f3a22e295583f8f213d3eff8cacf9df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ExTIakcMOqi5NQDL5MufpQ.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:对XGBoost模型的评估，该模型根据聚类定义的类别进行训练</p></figure><p id="7400" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型在弱监管下表现异常出色，但根据类别和属于该类别的簇的纯度，<strong class="jp ir">准确性可能会在现场设置中下降5–15%。</strong></p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="4ed7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种方法无疑帮助我们<strong class="jp ir">快速解决问题</strong>，而无需花费大量的时间和金钱来构建大量的训练数据。此外，它可以是基于启发法、分层聚类和建模的学习算法的迭代过程，直到我们获得高质量的基础事实标签。</p><p id="66ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过对更高质量的数据集进行重新训练，预测模型的性能会随着时间的推移而提高，该数据集是由领域专家对模型预测进行重复采样而收集的。</p><p id="4fe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nj">参考文献:</em> </strong></p><ul class=""><li id="6c2a" class="na nb iq jp b jq jr ju jv jy nu kc nv kg nw kk nx ng nh ni bi translated"><a class="ae lz" href="https://medium.com/@datasciencemilan/weakly-supervised-learning-introduction-and-best-practices-c65f490d4a0a" rel="noopener">https://medium . com/@ datasciencemilan/weak-supervised-learning-introduction-and-best-practices-c 65 f 490d 4a 0 a</a></li><li id="afcf" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated"><a class="ae lz" href="https://academic.oup.com/nsr/article-pdf/5/1/44/31567770/nwx106.pdf" rel="noopener ugc nofollow" target="_blank">https://academic . oup . com/nsr/article-pdf/5/1/44/31567770/nwx 106 . pdf</a></li><li id="d918" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Weak_supervision" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Weak_supervision</a></li><li id="d6f4" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated"><a class="ae lz" href="https://pdfs.semanticscholar.org/3adc/fd254b271bcc2fb7e2a62d750db17e6c2c08.pdf" rel="noopener ugc nofollow" target="_blank">https://pdfs . semantic scholar . org/3 ADC/FD 254 b 271 BC C2 FB 7 e 2 a 62d 750 db 17 e 6 C2 c 08 . pdf</a></li><li id="076b" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated"><a class="ae lz" href="https://egc2020.sciencesconf.org/data/pages/e_EGC_2020_VLemaire.pdf" rel="noopener ugc nofollow" target="_blank">https://EGC 2020 . science conf . org/data/pages/e _ EGC _ 2020 _ vle maire . pdf</a></li><li id="07ae" class="na nb iq jp b jq ny ju nz jy oa kc ob kg oc kk nx ng nh ni bi translated">【http://ai.stanford.edu/blog/weak-supervision/ T4】</li></ul></div></div>    
</body>
</html>