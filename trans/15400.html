<html>
<head>
<title>Spelling Correction: How to make an accurate and fast corrector</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拼写校正:如何做一个准确快速的校正器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spelling-correction-how-to-make-an-accurate-and-fast-corrector-dc6d0bcbba5f?source=collection_archive---------7-----------------------#2020-10-23">https://towardsdatascience.com/spelling-correction-how-to-make-an-accurate-and-fast-corrector-dc6d0bcbba5f?source=collection_archive---------7-----------------------#2020-10-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e3272b4ff4ff4ffa990354db8ee22c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_--dOl7YCHzmftYnr5wiA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:https://unsplash.com/photos/wRgNwR9CZDA</p></figure><p id="b592" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">脏数据导致模型质量差。在现实世界的NLP问题中，我们经常会遇到有很多错别字的文本。结果，我们无法达到最好的分数。尽管这可能很痛苦，但在拟合之前应该清理数据。</p><p id="932c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要一个自动拼写校正器，它可以修复错别字，同时不破坏正确的拼写。</p><p id="0ad6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是如何才能实现这一点呢？</p><p id="94ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从一个<a class="ae kc" href="https://norvig.com/spell-correct.html" rel="noopener ugc nofollow" target="_blank"> Norvig的拼写校正器</a>开始，反复增加它的功能。</p><h1 id="fb5d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">诺尔维格方法</h1><p id="e179" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">Peter Norvig(谷歌研究部主任)描述了以下拼写纠正方法。</p><blockquote class="me mf mg"><p id="3eb4" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">我们就拿一个词，蛮力的进行所有可能的编辑，比如删除，插入，转置，替换，拆分。例如，单词<strong class="kf ir"> abc </strong>可能的候选单词有:<strong class="kf ir">ab AC BC BAC CBA ACB a _ BC ab _ c aabc abbc acbc adbc aebc</strong>等。</p><p id="dbee" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">每个单词都被添加到候选列表中。我们第二次对每个单词重复这个过程，以获得具有更大编辑距离的候选词(对于有两个错误的情况)。</p><p id="d933" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">每个候选人都用一元语言模型进行评估。对于每个词汇，词频是基于一些大的文本集合预先计算的。将出现频率最高的候选词作为答案。</p></blockquote><h1 id="24d8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">添加一些上下文</h1><p id="9a4e" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">第一个改进—添加了n元语言模型(3元)。让我们不仅预计算单个单词，而且预计算单词和一个小上下文(3个最近的单词)。让我们估计某个片段作为所有n-grams的n-size的乘积的概率:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/81291c68c012f89a08eebeb3e9707ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*jRSZIFj4qC9Wbr5FLulk7w.jpeg"/></div></figure><p id="29de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简单起见，让我们将大小为n的n-gram的概率计算为所有低阶gram的概率的乘积(实际上<a class="ae kc" href="https://jon.dehdari.org/teaching/uds/lt1/ngram_lms.pdf" rel="noopener ugc nofollow" target="_blank">有一些平滑技术</a>，如Kneser–Ney-它们提高了模型的准确性，但让我们稍后再讨论它，参见下面的“提高准确性”段落):</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/1e5cd4b40165b263ea783ef9ef69974f.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*UrEV29J9l7qPd4JtTFs0EQ.jpeg"/></div></figure><p id="7997" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了从出现频率中获得n-gram的概率，我们需要归一化频率(例如，将n-gram的数量除以n-gram的数量，等等。):</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3001e3cd8bbaa6c9e6aeebbe12e74de2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*25amlvEsZfwqwv8S0a2HoA.jpeg"/></div></figure><p id="c5c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以使用我们的扩展语言模型来估计候选人的上下文。</p><p id="6849" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">句子概率可以这样计算:</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="298c" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir">def </strong>predict(self, sentence):<br/>    result = 0<br/>    <strong class="mt ir">for </strong>i <strong class="mt ir">in </strong>range(0, len(sentence) - 2):<br/>        p2 = self.getGram3Prob(sentence[i], sentence[i + 1], sentence[i + 2])<br/>        p3 = self.getGram2Prob(sentence[i], sentence[i + 1])<br/>        p4 = self.getGram1Prob(sentence[i])<br/>        result += math.log(p2) + math.log(p3) + math.log(p4)<br/>    <strong class="mt ir">return </strong>result</span></pre><p id="f0d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">n元概率是这样的:</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="07e9" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir">def </strong>getGram1Prob(self, wordID):<br/>    wordCounts = self.gram1.get(wordID, 0) + SimpleLangModel.K<br/>    vocabSize = len(self.gram1)<br/>    <strong class="mt ir">return </strong>float(wordCounts) / (self.totalWords + vocabSize)<br/><br/><strong class="mt ir">def </strong>getGram2Prob(self, wordID1, wordID2):<br/>    countsWord1 = self.gram1.get(wordID1, 0) + self.totalWords<br/>    countsBigram = self.gram2.get((wordID1, wordID2), 0) + SimpleLangModel.K<br/>    <strong class="mt ir">return </strong>float(countsBigram) / countsWord1<br/><br/><strong class="mt ir">def </strong>getGram3Prob(self, wordID1, wordID2, wordID3):<br/>    countsGram2 = self.gram2.get((wordID1, wordID2), 0) + self.totalWords<br/>    countsGram3 = self.gram3.get((wordID1, wordID2, wordID3), 0) + SimpleLangModel.K<br/>    <strong class="mt ir">return </strong>float(countsGram3) / countsGram2</span></pre><p id="f0c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了更高的精确度。然而，模型变得非常庞大，一切都变得非常缓慢。对于600 Mb的训练文本，我们得到:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/61fd80e2379c968d516ba39463817f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAb-fatnmRyLSf7d1cxVuA.png"/></div></div></figure><h1 id="aa8f" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">提高速度——对称方法</h1><p id="c9e5" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了提高速度，让我们使用SymSpell的一个想法。想法相当优雅。我们可以预先计算所有的删除错别字(以及由删除产生的其他错别字)，而不是每次遇到不正确的单词时都生成所有可能的编辑。更多详情可以在<a class="ae kc" href="https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f" rel="noopener">原文</a>中阅读。</p><p id="1d30" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，我们无法达到与原始版本一样高的速度(因为我们使用语言模型并查看上下文，而不仅仅是单个单词)，但我们可以显著提高性能。代价是额外的内存消耗:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/b03aed8d49f60c58468715a65b2d6ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eU9RbHFLNy9Kwn_AWg2SwQ.png"/></div></div></figure><h1 id="4644" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">改善内存消耗</h1><p id="7410" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了获得尽可能高的精度，我们需要一个大的数据集(至少几千兆字节)。在600 mb文件上训练n-gram模型会导致大量内存消耗(25 Gb)。一半大小由语言模型使用，另一半由符号拼写索引使用。</p><p id="0e44" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如此高的内存使用率的一个原因是我们不存储纯文本，而是存储频率。例如，对于以下5个单词的文本:<br/><em class="mh">a b c a b</em>，我们存储以下频率:</p><blockquote class="me mf mg"><p id="013a" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated"><strong class="kf ir">a</strong>=&gt;2<br/><strong class="kf ir">b</strong>=&gt;2<br/><strong class="kf ir">c</strong>=&gt;1<br/><strong class="kf ir">a b</strong>=&gt;2<br/>2<strong class="kf ir">b</strong>=&gt;1<br/><strong class="kf ir">c</strong>=&gt;1<br/><strong class="kf ir">a b</strong>=&gt;1</p></blockquote><p id="b91c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个原因——哈希表数据结构内存开销大(哈希表用在python <em class="mh"> dict </em>或者c++ <em class="mh"> unordered_map </em>内部)。</p><p id="e7c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了压缩我们的n-gram模型，让我们使用在<a class="ae kc" href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/860_Paper.pdf" rel="noopener ugc nofollow" target="_blank">高效最小完美散列语言模型</a>论文中描述的方法。让我们使用一个<a class="ae kc" href="https://en.wikipedia.org/wiki/Perfect_hash_function" rel="noopener ugc nofollow" target="_blank">完美散列</a> ( <a class="ae kc" href="http://cmph.sourceforge.net/chd.html" rel="noopener ugc nofollow" target="_blank">压缩、散列和置换</a>)来存储n元文法计数。完美散列是保证没有冲突的散列。如果没有碰撞，就可能只存储值(计数频率)而不是原始的n元文法。为了确保未知单词哈希不匹配现有的哈希，我们将使用一个已知单词的布隆过滤器。我们还可以使用非线性量化将32位长的计数频率打包成16位值。这不会影响最终指标，但会减少内存使用。</p><p id="1049" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">量化:</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="931c" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir">static const </strong>uint32_t MAX_REAL_NUM = 268435456;<br/><strong class="mt ir">static const </strong>uint32_t MAX_AVAILABLE_NUM = 65536;<br/><br/>uint16_t PackInt32(uint32_t num) {<br/>    <strong class="mt ir">double </strong>r = <strong class="mt ir">double</strong>(num) / <strong class="mt ir">double</strong>(MAX_REAL_NUM);<br/>    assert(r &gt;= 0.0 &amp;&amp; r &lt;= 1.0);<br/>    r = pow(r, 0.2);<br/>    r *= MAX_AVAILABLE_NUM;<br/>    <strong class="mt ir">return </strong>uint16_t(r);<br/>}<br/><br/>uint32_t UnpackInt32(uint16_t num) {<br/>    <strong class="mt ir">double </strong>r = <strong class="mt ir">double</strong>(num) / <strong class="mt ir">double</strong>(MAX_AVAILABLE_NUM);<br/>    r = pow(r, 5.0);<br/>    r *= MAX_REAL_NUM;<br/>    <strong class="mt ir">return </strong>uint32_t(ceil(r));<br/>}</span></pre><p id="a397" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计数频率提取:</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="9497" class="mx lc iq mt b gy my mz l na nb"><strong class="mt ir">template</strong>&lt;<strong class="mt ir">typename </strong>T&gt;<br/>TCount GetGramHashCount(T key,<br/>                        <strong class="mt ir">const </strong>TPerfectHash&amp; ph,<br/>                        <strong class="mt ir">const </strong>std::vector&lt;uint16_t&gt;&amp; buckets,<br/>                        TBloomFilter&amp; filter)<br/>{<br/>    <strong class="mt ir">constexpr int </strong>TMP_BUF_SIZE = 128;<br/>    <strong class="mt ir">static char </strong>tmpBuff[TMP_BUF_SIZE];<br/>    <strong class="mt ir">static </strong>MemStream tmpBuffStream(tmpBuff, TMP_BUF_SIZE - 1);<br/>    <strong class="mt ir">static </strong>std::ostream out(&amp;tmpBuffStream);<br/><br/>    tmpBuffStream.Reset();<br/><br/>    NHandyPack::Dump(out, key);<br/>    <strong class="mt ir">if </strong>(!filter.Contains(tmpBuff, tmpBuffStream.Size())) {<br/>        <strong class="mt ir">return </strong>TCount();<br/>    }<br/><br/>    uint32_t bucket = ph.Hash(tmpBuff, tmpBuffStream.Size());<br/><br/>    assert(bucket &lt; ph.BucketsNumber());<br/><br/>    <strong class="mt ir">return </strong>UnpackInt32(buckets[bucket]);<br/>}</span></pre><p id="bf7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mh">首先，我们检查密钥是否存在于布隆过滤器中。然后我们得到基于完美哈希桶数的计数。</em></p><p id="19fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了压缩符号索引，让我们使用一个<a class="ae kc" href="https://en.wikipedia.org/wiki/Bloom_filter" rel="noopener ugc nofollow" target="_blank">布隆过滤器</a>。布隆过滤器是一种节省空间的概率数据结构，用于测试元素是否是集合的成员。让我们将所有删除散列放入一个bloom过滤器，并使用这个索引来跳过不存在的候选项。</p><p id="1c16" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是符号拼写算法的第二步。这里我们取候选单词，这些单词是通过从原始单词中去掉一个或多个字母而生成的，并检查每个单词是否包含在索引中。<em class="mh">删除1 </em>和<em class="mh">删除2 </em>是布隆过滤器。</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="dd7b" class="mx lc iq mt b gy my mz l na nb">TWords CheckCandidate(const std::wstring&amp; s)<br/>{<br/>    TWords results;<strong class="mt ir"><br/>    if </strong>(Deletes1-&gt;Contains(w)) {<br/>        Inserts(w, results);<br/>    }<br/>    <strong class="mt ir">if </strong>(Deletes2-&gt;Contains(w)) {<br/>        Inserts2(w, results);<br/>    }<br/>}</span><span id="7b13" class="mx lc iq mt b gy nd mz l na nb"><strong class="mt ir">void </strong>TSpellCorrector::Inserts(<strong class="mt ir">const </strong>std::wstring&amp; w, TWords&amp; result) <strong class="mt ir">const <br/></strong>{<br/>    <strong class="mt ir">for </strong>(size_t i = 0; i &lt; w.size() + 1; ++i) {<br/>        <strong class="mt ir">for </strong>(<strong class="mt ir">auto</strong>&amp;&amp; ch: LangModel.GetAlphabet()) {<br/>            std::wstring s = w.substr(0, i) + ch + w.substr(i);<br/>            TWord c = LangModel.GetWord(s);<br/>            <strong class="mt ir">if </strong>(!c.Empty()) {<br/>                result.push_back(c);<br/>            }<br/>        }<br/>    }<br/>}<br/><br/><strong class="mt ir">void </strong>TSpellCorrector::Inserts2(<strong class="mt ir">const </strong>std::wstring&amp; w, TWords&amp; result) <strong class="mt ir">const <br/></strong>{<br/>    <strong class="mt ir">for </strong>(size_t i = 0; i &lt; w.size() + 1; ++i) {<br/>        <strong class="mt ir">for </strong>(<strong class="mt ir">auto</strong>&amp;&amp; ch: LangModel.GetAlphabet()) {<br/>            std::wstring s = w.substr(0, i) + ch + w.substr(i);<br/>            <strong class="mt ir">if </strong>(Deletes1-&gt;Contains(WideToUTF8(s))) {<br/>                Inserts(s, result);<br/>            }<br/>        }<br/>    }<br/>}</span></pre><p id="3010" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">经过优化后，模型大小显著减小，降至800 Mb:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/548efc8a88ad40e7e505c4b9d51f3f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFCbu5sa8qkzCvRv_pTnxg.png"/></div></div></figure><h1 id="66aa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">提高准确性</h1><p id="a73b" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了提高准确性，让我们添加几个机器学习分类器。第一个将被用来决定单词是否有错误。第二个是回归变量，将用于候选人排名。这个分类器部分地起到了语言模型平滑的作用(它将所有的gram作为单独的输入，并且分类器决定每个gram有多大的影响)。</p><p id="2ee2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于候选人排名，我们将训练一个具有以下特征的<a class="ae kc" href="https://catboost.ai/docs/concepts/loss-functions-ranking.html" rel="noopener ugc nofollow" target="_blank"> catboost </a>(梯度提升决策树)排名模型:</p><ul class=""><li id="eca8" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">字频率</li><li id="f645" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">n克频率，每克独立(2，3)</li><li id="636f" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">距离为3，4的邻近单词的频率</li><li id="bee1" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">n元模型预测</li><li id="7ff2" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">编辑候选词和源词之间的距离</li><li id="8882" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">编辑距离更远的候选人数量</li><li id="1279" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">单词长度</li><li id="3c7b" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">一个干净的静态字典中的单词存在</li></ul><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="1b25" class="mx lc iq mt b gy my mz l na nb">from catboost import CatBoost</span><span id="ee4b" class="mx lc iq mt b gy nd mz l na nb">params = {<br/>        'loss_function': 'PairLogit',<br/>        'iterations': 400,<br/>        'learning_rate': 0.1,<br/>        'depth': 8,<br/>        'verbose': False,<br/>        'random_seed': 42,<br/>        'early_stopping_rounds': 50,<br/>        'border_count': 64,<br/>        'leaf_estimation_backtracking': 'AnyImprovement',<br/>        'leaf_estimation_iterations': 2,<br/>        'leaf_estimation_method': 'Newton',<br/>        'task_type': 'CPU'<br/>    }</span><span id="fddc" class="mx lc iq mt b gy nd mz l na nb">model = CatBoost(params, )<br/>    model.fit(trainX, trainY, pairs=trainPairs, group_id=groupIDs, eval_set=evalPool, verbose=1)</span></pre><p id="b6b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于误差预测，我们将训练一个二元分类器。让我们使用为原始单词的每个单词计算的相同特征，并让分类器决定该单词是否有错误。这将提供一种根据上下文检测错误的能力，即使是字典中的单词。</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="9406" class="mx lc iq mt b gy my mz l na nb">from catboost import CatBoostClassifier</span><span id="7901" class="mx lc iq mt b gy nd mz l na nb">model = CatBoostClassifier(<br/>    iterations=400,<br/>    learning_rate=0.3,<br/>    depth=8<br/>)<br/><br/>model.fit(trainX, trainY, sample_weight=trainWeight, verbose=<strong class="mt ir">False</strong>)</span></pre><p id="9874" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这进一步提高了准确性，但是，这不是免费的，我们会降低性能。尽管如此，对于大多数应用来说，这种性能已经足够了，精度通常更重要。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/bf3f3a69761b97a22d0f39e3af133982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnKlGO6MSOMzA2BQvdsSXg.png"/></div></div></figure><h1 id="7b64" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">评价</h1><p id="ff28" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了评估一个模型，我们需要一些数据集。我们可以基于干净的文本生成人为的错误。我们也可以使用公共数据集——其中之一是<a class="ae kc" href="http://www.dialog-21.ru/en/evaluation/2016/spelling_correction/" rel="noopener ugc nofollow" target="_blank"> SpellRuEval </a>数据集。让我们检查两个人工数据集和一个真实数据集的准确性。我们将使用一些来自大型IT公司的替代拼写检查器进行比较。这里，jamspell是我们的拼写检查器，我们得到了以下指标:</p><h2 id="c628" class="mx lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">茹，人工，文学</h2><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/6f46ad81d1b63962b94b07a7c6cdb492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vvrlk1ibRD5Wh-4dn82cMA.png"/></div></div></figure><h2 id="8205" class="mx lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">RU，real，互联网帖子</h2><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/3e77f85ecb8aeaf4316bbddb688432cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4wJLXOoc6oZgDQ59GV5wg.png"/></div></div></figure><h2 id="411a" class="mx lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">人工新闻</h2><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/fc8e776a51f5e76490413fa294238d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPBqKVgh1jAwxLHO7DFlXA.png"/></div></div></figure><ul class=""><li id="138a" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">errRate — <em class="mh">执行自动更正后文本中剩余的错误数</em></li><li id="5b9a" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">fixRate — <em class="mh">已修复错误的数量</em></li><li id="7553" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">破损— <em class="mh">破损的正确单词数</em></li><li id="f105" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">pr，re，f1 — <em class="mh">精度，召回，f1分数</em></li></ul><h1 id="f7dc" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">进一步的步骤</h1><p id="2bea" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">提高准确性的后续步骤—收集大量带有错误和已更正文本的并行文本语料库(分别用于移动和桌面平台),并训练专用的错误模型。</p><p id="5297" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种提高准确性的可能方法是添加动态学习选项。我们可以边修正边在飞行中学习，或者我们可以进行两遍修正。在第一次通过时，模型将学习一些统计数据，并在第二次通过时进行实际校正。</p><p id="7315" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，神经网络语言模型(双向LSTM或BERT)可以提供一些额外的准确性提升。它们在直接方法(LSTM误差分类器、序列-2-序列LSTM模型、使用BERT输出作为候选排序权重)中工作得不好，但是它们的预测可能作为排序模型/误差检测器中的特征是有用的。</p><p id="4253" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是一些我们无法实现的方法的细节(这并不意味着它们无法实现——这只是我们的经验)。</p><h2 id="7713" class="mx lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">用BERT法选择最佳候选人</h2><p id="ba54" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们训练伯特，并试图用它来预测最佳候选人。</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="bf29" class="mx lc iq mt b gy my mz l na nb">from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling</span><span id="fb39" class="mx lc iq mt b gy nd mz l na nb">config = RobertaConfig(<br/>    vocab_size=52_000,<br/>    max_position_embeddings=514,<br/>    num_attention_heads=12,<br/>    num_hidden_layers=6,<br/>    type_vocab_size=1,<br/>)</span><span id="3a43" class="mx lc iq mt b gy nd mz l na nb">dataset = LineByLineTextDataset(<br/>    tokenizer=tokenizer,<br/>    file_path=TRAIN_TEXT_FILE,<br/>    block_size=128,<br/>)</span><span id="8ff0" class="mx lc iq mt b gy nd mz l na nb">from transformers import</span><span id="13a0" class="mx lc iq mt b gy nd mz l na nb">data_collator = DataCollatorForLanguageModeling(<br/>    tokenizer=tokenizer, mlm=True, mlm_probability=0.15<br/>)</span><span id="c7a6" class="mx lc iq mt b gy nd mz l na nb">from transformers import Trainer, TrainingArguments</span><span id="caf5" class="mx lc iq mt b gy nd mz l na nb">training_args = TrainingArguments(<br/>    output_dir="~/transformers",<br/>    overwrite_output_dir=True,<br/>    num_train_epochs=3,<br/>    per_gpu_train_batch_size=32,<br/>    save_steps=10_000,<br/>    save_total_limit=2,<br/>)</span><span id="05b9" class="mx lc iq mt b gy nd mz l na nb">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    data_collator=data_collator,<br/>    train_dataset=dataset,<br/>    prediction_loss_only=False,<br/>)</span><span id="c539" class="mx lc iq mt b gy nd mz l na nb">trainer.train()</span></pre><p id="41bd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于屏蔽词预测任务，它比n-gram语言模型工作得更好(BERT模型的准确率为30 %, n-gram模型的准确率为20%),但是在从候选词列表中选择最佳词时，它的表现更差。我们的假设是，这是由于伯特不知道编辑距离或与原词匹配的事实。我们相信添加BERT预测作为catboost排名模型的一个特征可以提高准确性。</p><h2 id="bfd6" class="mx lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">使用LSTM作为错误分类器</h2><p id="1634" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们试图使用LSTM作为错误检测器(来预测word是否有错误)，但我们最好的结果是与常规n元语言模型错误预测器+手动试探法相同。而且训练时间要长得多，所以我们决定暂时不用。这是一个得分最高的模型。输入是单词级的<strong class="kf ir">手套</strong>嵌入，在同一个文件上训练。</p><pre class="mm mn mo mp gt ms mt mu mv aw mw bi"><span id="990f" class="mx lc iq mt b gy my mz l na nb">Model: "functional_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 9, 200)]          0         <br/>_________________________________________________________________<br/>bidirectional (Bidirectional (None, 9, 1800)           7927200   <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 9, 1800)           0         <br/>_________________________________________________________________<br/>attention (Attention)        (None, 1800)              1809      <br/>_________________________________________________________________<br/>dense (Dense)                (None, 724)               1303924   <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 724)               0         <br/>_________________________________________________________________<br/>batch_normalization (BatchNo (None, 724)               2896      <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 2)                 1450      <br/>=================================================================<br/>Total params: 9,237,279<br/>Trainable params: 9,235,831<br/>Non-trainable params: 1,448<br/>_________________________________________________________________<br/>None</span></pre><h1 id="0c9d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="093f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们从非常简单的模型开始，迭代地增加它的功能，最终我们得到了一个强大的、产品级的拼写检查器。然而，这并不是终点，在通往我们的目标——做世界上最好的拼写检查器——的漫长道路上还有很多步骤。</p><h1 id="9f9b" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">链接</h1><ul class=""><li id="baab" class="nf ng iq kf b kg lz kk ma ko of ks og kw oh la nk nl nm nn bi translated"><a class="ae kc" href="https://jamspell.com/" rel="noopener ugc nofollow" target="_blank">项目网站</a></li><li id="756b" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><a class="ae kc" href="https://github.com/bakwc/JamSpell" rel="noopener ugc nofollow" target="_blank"> Github回购</a></li></ul></div></div>    
</body>
</html>