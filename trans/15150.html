<html>
<head>
<title>Improve Your Neural Network’s Generalization Performance By Adding an Unsupervised Auxiliary Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过添加无监督的辅助损失来提高神经网络的泛化性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improve-your-neural-networks-generalization-performance-by-adding-an-unsupervised-auxiliary-loss-4d58b2dead54?source=collection_archive---------29-----------------------#2020-10-18">https://towardsdatascience.com/improve-your-neural-networks-generalization-performance-by-adding-an-unsupervised-auxiliary-loss-4d58b2dead54?source=collection_archive---------29-----------------------#2020-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8780" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">作为一种正则化形式的无监督重建损失。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/864fc6a16ab09bfcbe2d0e5373f6da82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgC5TIfjapbzD7R9w53kdQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Sebastien Gabriel 在<a class="ae kv" href="https://unsplash.com/s/photos/valley?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c0e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度神经网络有一个很大的过拟合问题，特别是在应用于少量标记数据时。研究人员设计了多种方法来解决这个问题，如L1/L2权重正则化、辍学、迁移学习和多任务学习。</p><p id="9f6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个项目中，我们将重点关注使用多任务学习作为一种提高神经网络泛化性能的方法。这里实现的想法受到了三篇非常有趣的论文的启发:</p><ul class=""><li id="09d0" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="http://www.cs.cornell.edu/~caruana/mlj97.pdf" rel="noopener ugc nofollow" target="_blank">多任务学习。自主代理和多代理系统</a></li><li id="1018" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://ruder.io/multi-task/" rel="noopener ugc nofollow" target="_blank">深度神经网络中多任务学习概述</a></li><li id="85fd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://papers.nips.cc/paper/7296-supervised-autoencoders-improving-generalization-performance-with-unsupervised-regularizers.pdf" rel="noopener ugc nofollow" target="_blank">有监督的自动编码器:用无监督的正则化器提高泛化性能</a></li></ul><p id="3715" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">前两篇论文试图解释为什么多任务学习可以提高单个任务的表现，它们提供的一些可能的解释是:</p><p id="73ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">表征偏差:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/d752977ee75ed0b92a2cb1eb71901b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*mPNY_0kq0Mj4KH39Qbt_XQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">—作者图片</p></figure><p id="6f01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们同时在任务T和T '上训练一个网络，这个网络就会偏向服务于这两个任务的表征。这使得网络最有可能推广到新的任务。</p><p id="3380" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">正规化:</strong></p><p id="c870" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用多任务学习使得网络不太可能过度适应来自训练数据的噪声，因为它减少了可能的解决方案的数量，因为MTL的解决方案需要同时对所有任务起作用。</p><p id="1390" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意力聚焦:</strong></p><p id="5f0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对多个相关任务的训练可以在什么是相关特征以及什么只是噪声上给模型更强的信号。</p><p id="f8f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三篇论文考虑了只有一个监督任务T的情况，因此作者增加了一个新的人工和非监督的重建输入的任务。他们在一个简化的设置中证明，增加重建损失提高了监督任务的泛化性能，并显示了一些支持他们假设的经验结果。</p><h1 id="ad95" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">数据:</h1><p id="35f7" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在接下来的实验中，我们使用免费音乐档案(FMA)小版本。这是一个包含8000首歌曲片段的数据集，分为8种类型:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="5e37" class="nj mi iq nf b gy nk nl l nm nn">{<br/>    "International": 0,<br/>    "Pop": 1,<br/>    "Instrumental": 2,<br/>    "Hip-Hop": 3,<br/>    "Electronic": 4,<br/>    "Experimental": 5,<br/>    "Folk": 6,<br/>    "Rock": 7<br/>}</span></pre><p id="ff37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们以70%-10%-20%的比例将数据集分为Train-Val-Test，并将原始音频波形转换为Mel频谱图，然后将它们馈送到网络。关于预处理的更多细节，你可以看看我以前的一个项目:</p><p id="2a1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/music-genre-classification-transformers-vs-recurrent-neural-networks-631751a71c58"> <strong class="ky ir">音乐流派分类:变形金刚vs递归神经网络</strong> </a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ae45a3cf7d17fa3b2fa13d38144e1015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*9sKRBtLy7na60czoPdCC7g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Mel光谱图示例—作者提供的图像</p></figure><h1 id="85ce" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">型号:</h1><p id="141d" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们沿着时间轴应用基于LSTM的神经网络来分类音乐流派。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1ee9f8d2754877565e7b1f942589ac68.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*IeoMkPQiRa3SYHPp-Fg11w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">音乐流派模型-作者图像</p></figure><p id="d49f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">添加脱落图层作为额外的正则化，并使重建任务对模型来说更具挑战性。<br/>我们使用Pytorch Lightning来实现这个模型，转发函数看起来是这样的:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="3a69" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir">def </strong>forward(self, x):<br/>    x = self.do(x)<br/><br/>    x, _ = self.lstm1(x)<br/>    x_seq, _ = self.lstm2(x)<br/><br/>    x, _ = torch.max(self.do(x_seq), dim=1)<br/><br/>    x = F.relu(self.do(self.fc1(x)))<br/>    y_hat = self.fy(x)<br/><br/>    x_reconstruction = torch.clamp(self.fc2(self.do(x_seq)), -1.0, 1.0)<br/><br/>    <strong class="nf ir">return </strong>y_hat, x_reconstruction</span></pre><p id="99f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们将损失定义为分类损失和重建损失之间的加权和，如下所示:</p><blockquote class="nq"><p id="1785" class="nr ns iq bd nt nu nv nw nx ny nz lr dk translated">损失=损失_分类+ λ *损失_重建</p></blockquote><p id="1e72" class="pw-post-body-paragraph kw kx iq ky b kz oa jr lb lc ob ju le lf oc lh li lj od ll lm ln oe lp lq lr ij bi translated">其中，λ是一个超参数，它有助于缓解两个损失的比例不同这一事实，同时还能更好地控制我们希望赋予辅助任务的重要性。损失定义如下:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="1f80" class="nj mi iq nf b gy nk nl l nm nn"><strong class="nf ir">def </strong>training_step(self, batch, batch_idx):<br/>    x, y = batch<br/><br/>    y_hat, x_reconstruction = self(x)<br/><br/>    loss_y = F.cross_entropy(y_hat, y)<br/>    loss_x = F.l1_loss(x, x_reconstruction)<br/><br/>    <strong class="nf ir">return </strong>loss_y + self.reconstruction_weight * loss_x</span></pre><h1 id="4f47" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结果:</h1><p id="310f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在实验中，我们尝试了多个λ值，以查看哪一个效果更好，基线为λ= 0，这意味着辅助损耗被忽略。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/7488d09d29b59302f31a36f7c3177387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*awoDGHwr_E1jW-WeSmJgtQ.png"/></div></figure><p id="a3ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，与基线相比，增加重建损失(λ = 10和λ = 2)会产生更好的性能。</p><p id="7d98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就分类准确度而言，我们有:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/185e3d0769122e4c2728c17c3d6cf1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*BmjbFW4ELWqTAGayRyauzw.png"/></div></figure><ul class=""><li id="2ea7" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">随机猜测:</strong> 12.5%</li><li id="0bbf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">基线:</strong>准确率= 47.5%</li><li id="e500" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> λ =10 </strong>:精度= <strong class="ky ir"> 51% </strong></li></ul><p id="e703" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当使用相同的分类体系结构时，增加重建损失给出了超过基线约3%的精度改进。</p><h1 id="0587" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结论:</h1><p id="80e9" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在这个项目中，我们证明了向神经网络添加辅助的无监督任务可以通过作为正则化的附加形式来提高其泛化性能。添加重建损失的方法在Pytorch Lightning中很容易实现，但代价是我们需要优化新的超参数λ。</p><p id="5659" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">复制结果的代码在这里分享:<a class="ae kv" href="https://github.com/CVxTz/ReconstructionAuxLoss" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/ReconstructionAuxLoss</a></p></div></div>    
</body>
</html>