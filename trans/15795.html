<html>
<head>
<title>Neural Networks: its internal functioning and uses</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:其内部功能和用途</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8?source=collection_archive---------40-----------------------#2020-10-30">https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8?source=collection_archive---------40-----------------------#2020-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="31ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络试图复制人类大脑及其神经元网络。ANN人工神经网络由人工神经元或节点组成。人工神经网络主要用于解决人工智能(AI)问题。</p><p id="6931" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当人脑从提供给它的信息中学习时，神经网络也做同样的事情。</p><p id="0a93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经元之间的连接被建模为权重。正值被称为令人兴奋的连接，而负值意味着避免连接。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/3770a9deb884b21208ee0f187498c44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NOue_j8ffzOfPAp5rVhzA.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">一个神经元<a class="ae lb" href="https://unsplash.com/photos/OH5BRdggi2w" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d7c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在神经网络中，工作分两步完成:</p><p id="44d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1)所有输入乘以一个权重并求和。此活动类似于线性方程，并添加一个偏差作为b，y=∑xw+b。</p><p id="d135" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2)激活函数被应用于输出，该输出决定该神经元在最终决策中是否将是激活的。例如，可接受的输出范围通常介于0和1之间，也可能是1和1或0或1，具体取决于所用的激活函数，可能是Sigmoid、tanh或ReLu。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lc"><img src="../Images/881706e601bb64003f32cc44963cfb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LVSyEEXCsYuyg_uBoEmmg.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="71a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些人工网络可用于预测建模，在数据集上进行训练。从经验中产生的自我学习可以发生在网络中，这有助于在数据集中看不到的重要问题上得出结论。</p><p id="e546" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">神经网络的应用</strong></p><p id="3458" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络用于人工智能的各个领域，例如:</p><p id="8507" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1)用时间序列对金融数据进行预测建模。</p><p id="8cbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2)模式和序列识别中的分类。</p><p id="db49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3)聚类和过滤。</p><p id="100c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个简单的神经网络有一个输入层，隐藏层和输出层。更复杂的神经网络或深层多层网络可以具有大量的隐藏层，在这些隐藏层中，计算具有权重和偏差的输入的乘积的总和，并且在第二步中，关于所使用的函数执行激活。</p><p id="2608" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我要解释一个3隐藏分层网络，前向传播和反向传播。</p><p id="9b6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">神经网络中的前向传播</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ld"><img src="../Images/bfc757c940f2a28dc94e79b6fd7c95d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOCASjTsoZeypBkhnGsDng.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="850b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在前向传播中，来自每个输入单元的输入被加载一些权重。在每个隐藏神经元层中，完成2个步骤:</p><p id="29a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1) x1 *w1 +b1，取权重和输入的乘积并加到偏差b</p><p id="65af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2)激活功能应用于步骤1中的输出。o1=Act(x1 *w1 +b1)。</p><p id="4101" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在隐藏的第2层中，输入将从第1层输出，即o1，并且对于上层神经元，它将乘以w2，对于下层神经元，它将乘以w3，并且将偏置加到每一个上。</p><p id="5bfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">O2 = Act(O1 * w2+B2 ), O3 = Act(O1 * w3+B3)</p><p id="97c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在隐藏层3中，2个输入将作为o2和o3到来，它们将分别乘以权重w5和w4并相加。</p><p id="24d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">o5= Act((o2* w5 + o3*w4) +b4)</p><p id="d9cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">y=Act(o5*w6+b5)</p><p id="ed09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑到预测值和实际值的简单相减，损失计算为y-y^。</p><p id="72c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">神经网络中的反向传播</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ld"><img src="../Images/c1206ccb0a093365c4fbd5c96da00fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NO1Y1QYdeDrBLqKjZMdQIg.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="ffbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在神经网络中进行反向传播，以找到更新的权重值，这反过来有助于最小化损失。</p><p id="2e1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这也是一种将反馈传播到神经网络中的方式，以了解每个节点对多少损失负责，并依次更新权重，从而通过给节点更高或更低的权重来最小化损失。</p><p id="6ce5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">梯度</strong>是一种数值计算，它帮助我们知道如何调整<strong class="jp ir">网络</strong>的参数，以使其输出偏差最小化并达到全局最小值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi le"><img src="../Images/19d41a1e4690720683841da532522c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EtT53COOSaY6E98bdX3rfQ.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/eb6e8873562b080d5ebe17b500197d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*b0wKv_UenqDHAaHWwQm3jw.jpeg"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">原始图像<a class="ae lb" href="https://unsplash.com/photos/XF0m9EL4AXE?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="c28e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于有效训练一个<strong class="jp ir">神经网络</strong>的算法是通过<strong class="jp ir">链规则</strong>。在每次向前通过神经网络<strong class="jp ir">之后，反向传播会执行一次向后传递，同时调整模型的权重和偏差。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lg"><img src="../Images/5d3cddca23e2ab38b9aba4d5f5141768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SB-goBvhRk-Iy2X1MBC6yw.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="b1aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">激活功能</strong></p><p id="1704" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在人工神经网络中，节点的输出取决于激活函数，激活函数又根据所用函数的类型使节点开或关、不太活跃或更活跃。</p><p id="b5c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我们将讨论几个最常用的激活函数，如Sigmoid、tanh和ReLu。</p><p id="e434" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">乙状结肠激活功能</strong></p><p id="ab7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它是一个数学函数，具有典型的“S”形曲线或<strong class="jp ir">S形曲线</strong>。sigmoid函数的一个常见示例是逻辑函数。</p><p id="8338" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">公式如图所示。</p><p id="cce0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该函数确保将神经元的输出值保持在0到1之间。</p><p id="81f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，它的导数范围在0到0.25之间。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lh"><img src="../Images/5b530d57fed114d84f54bb5b38a1b770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WeH23vlXhCvNBu68A-q53Q.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="2b9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该函数可以用作不是很深的神经网络中的激活函数，即其中隐藏层的数量不是很大，因为如果层数更多，将导致消失梯度问题。</p><p id="fc94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">消失梯度问题</strong>是当等式w1 new = w1 old-ƞƌl/("ƌw1old中的(ƞ“ƌl”/("ƌ“w1old))的值很小时，得到的新权重几乎与旧权重相似，因此不进行权重更新。</p><p id="db0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这导致梯度下降中的值处于相同的位置，不会向全局最小值前进。</p><p id="9936" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的等式显示了消失梯度是一个怎样的问题。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi li"><img src="../Images/3b77c4021f1beebfe56c29b5d9a9a785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phOM4ZB3_ofYD1fcgVkCRg.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lj"><img src="../Images/054afcc25b104f2675e62d9bce49cac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_odSOq1iVoHTTy_ItPKIw.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="f18f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Tanh激活功能</strong></p><p id="9229" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该激活函数具有与sigmoid相似的功能，其公式如下。在此函数中，输出值介于-1到1之间，其导数范围为0到1。</p><p id="9ea1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，它也有消失梯度问题，但它比Sigmoid好。</p><p id="40f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，在与非常深的神经网络一起工作时，它也开始出现问题。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lk"><img src="../Images/36792c435897cac8b98b3726da3d103a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxvBxzAIYCmmfUTDXFrgQw.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="bd8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ReLu激活功能</strong></p><p id="396d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个函数中，输出的值是0或1，这意味着它或者去激活神经元或者激活它。</p><p id="08ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它的公式很简单，如下所示。</p><p id="efdb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这没有消失梯度的问题，因为数字是0或1，但它可能会导致死亡的神经元。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ll"><img src="../Images/eacfe48231c83dfc7b1f492816d1017e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tC2NpmIlbIOEs9O4bocsg.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="eff4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论</strong></p><p id="ae36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望在读完这篇文章后，关于神经网络，它的工作以及为什么需要这些，很多事情一定是清楚的。</p><p id="e6a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有最常用的激活函数以及它们的优缺点。</p><p id="b90e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是为了让学习者简单起见的一个概述。</p><p id="10af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="6017" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lt">原载于2020年10月30日https://www.numpyninja.com</em><a class="ae lb" href="https://www.numpyninja.com/post/neural-network-and-its-functionality" rel="noopener ugc nofollow" target="_blank"><em class="lt"/></a><em class="lt">。</em></p></div></div>    
</body>
</html>