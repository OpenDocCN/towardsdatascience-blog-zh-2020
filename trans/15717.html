<html>
<head>
<title>Random forests — An ensemble of decision trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林——决策树的集合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forests-an-ensemble-of-decision-trees-37a003084c6c?source=collection_archive---------10-----------------------#2020-10-29">https://towardsdatascience.com/random-forests-an-ensemble-of-decision-trees-37a003084c6c?source=collection_archive---------10-----------------------#2020-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b9d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这就是决策树组合成随机森林的方式</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ed32cc3f438f80f785cccb42db06e8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VljYAbqLqm-K26BFM3HrQQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">菲利普·兹恩泽维奇在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="dfd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">随机森林</em> </strong>是当今最强大的机器学习算法之一。它是一种<em class="lv">监督的</em>机器学习算法，可用于<strong class="lb iu">分类</strong>(预测离散值输出，即一个类)和<strong class="lb iu">回归</strong>(预测连续值输出)任务。在本文中，我描述了如何使用流行的Iris数据集将它用于分类任务。</p><h1 id="0c74" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">随机森林的动机</h1><p id="df4c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">首先，我们讨论决策树算法的一些缺点。这将激励你使用随机森林。</p><ul class=""><li id="37d0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">对训练数据的微小更改会导致树结构的显著变化。</li><li id="2f29" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">它可能会有过度拟合的问题(模型非常适合训练数据，但无法对新的输入数据进行推广)，除非您调整<strong class="lb iu"> <em class="lv"> max_depth </em> </strong>的模型超参数。</li></ul><p id="278f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，与其训练单个决策树，不如训练一组决策树，它们一起构成一个随机森林。</p><h1 id="8c03" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">随机森林如何在幕后工作</h1><p id="f4b4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">随机森林背后的两个主要概念是:</p><ul class=""><li id="d7b1" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">群体的智慧——一大群人集体起来比单个专家更聪明</li><li id="98f8" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">多元化</strong>——一组<em class="lv">不相关的</em>树</li></ul><p id="51f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林由一组单独的决策树组成。因此，该技术被称为<em class="lv">集成学习</em>。一大群<em class="lv">不相关的</em>决策树可以产生比任何单个决策树更准确和稳定的结果。</p><p id="ef37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您为分类任务训练随机森林时，您实际上训练了一组决策树。然后，您获得所有单个树的预测，并预测获得最多投票的类。尽管一些单独的树产生错误的预测，但是许多树可以产生准确的预测。作为一个群体，他们可以走向准确的预测。这就叫群众的智慧。下图显示了幕后实际发生的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/ff11fa33d538422274e37b4488999395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hGKhKI2s2DCnguBqUFwWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6701" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持单棵树之间的低相关性(高多样化)，算法会自动考虑以下事项。</p><ul class=""><li id="e9d0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">特征随机性</li><li id="a3fb" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">打包(引导汇总)</li></ul><h2 id="f5a0" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">特征随机性</h2><p id="62c8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在一个普通的决策树中，当它想要分割一个节点时，该算法从所有特征中的<em class="lv">中搜索最佳特征。相比之下，随机森林中的每棵树都从<em class="lv">随机特征子集</em>中搜索最佳特征。当在随机森林中种植树木时，这会产生额外的随机性。由于特征的随机性，随机森林中的决策树是不相关的。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/9b275414e1fd5ce31ef2d21fd289abda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h59804V5gNRmQFKdzmkGGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="34b7" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">打包(引导汇总)</h2><p id="3f99" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在随机森林中，每个决策树都在训练集的不同随机样本上进行训练。当用替换完成采样时，该方法被称为<strong class="lb iu">打包</strong>(自举聚集)。在统计学中，带替换的重采样被称为<em class="lv">自举</em>。bootstrap方法减少了决策树之间的相关性。在决策树中，对训练数据的微小更改会导致树结构的显著变化。bootstrap方法利用这一点来生成不相关的树。我们可以用下面的简单例子来演示自举方法。同样的事情也适用于随机森林。</p><p id="b183" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一个由10个观察值组成的训练集，从1到10进行编号。在这些观察结果中，我们使用bootstrap方法进行采样。我们想考虑:</p><ul class=""><li id="75f4" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">样本大小</strong> —在机器学习中，通常使用与训练集相同的样本大小。在本例中，样本量为10。</li><li id="488c" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">样本数量</strong> —等于随机森林中决策树的数量。</li></ul><p id="0d3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了创建第一个样本，我们从训练集中随机选择一个观察值。假设这是第五次观察。这个观察结果被返回到训练数据集中，我们重复这个过程，直到我们得到整个样本。在整个过程之后，想象我们用下面的观察制作第一个样本。</p><p id="90aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">样本_1 = [5，4，6，6，5，1，3，2，10，9] </strong></p><p id="4468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们用这个样本训练一个决策树。由于替换，一些观察值可能在样本中出现更多次。另外，请注意，有些观察值在样本中至少不会出现一次。这些观察结果被称为<strong class="lb iu"><em class="lv">【OOB】</em></strong>观察结果。第一个样本的<strong class="lb iu"> <em class="lv"> oob </em> </strong>观测值为:</p><p id="8b5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> oob_1 = [7，8] </strong></p><p id="0af3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">样本1对应的决策树在训练过程中从来看不到那些<strong class="lb iu"> <em class="lv"> oob </em> </strong>的观测值。因此，这组<strong class="lb iu"> <em class="lv"> oob </em> </strong>观察值可以用作该决策树的验证集。我们可以通过平均每个决策树的<strong class="lb iu"> <em class="lv"> oob </em> </strong>评估来评估整个集合。这被称为<strong class="lb iu"> <em class="lv">非现金评估</em> </strong>，它是<em class="lv">交叉验证</em>的替代方案。</p><p id="0f46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们创建另一个示例。</p><p id="3819" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Sample_2 = [5，4，4，5，5，1，3，2，10，9] </strong></p><p id="5e28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">OOB _ 2 =【6，7，8】</strong></p><p id="e36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们创建的样本数量等于随机森林中决策树的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/a0d0c76641791370393a97cb81b2c926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BtU2gT8Og5RE-BybDA9iQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="98f9" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">随机森林中的要素重要性</h2><p id="6d8d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">随机森林的另一个巨大优势是，它允许您根据在训练阶段计算的分数，了解每个特性的相对重要性。为此，he Scikit-learn<strong class="lb iu">RandomForestClassifier</strong>提供了一个名为<strong class="lb iu"><em class="lv">feature _ importances _</em></strong>的属性。这将返回总和为1的值的数组。分数越高，特性越重要。分数是根据基尼系数计算的，基尼系数衡量的是分割的质量(基尼系数越低，分割越好)。基尼系数平均下降幅度较大的分割特征被认为更重要。</p><p id="3d9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看要素的重要性，您可以决定删除哪些要素，因为它们对构建模型的贡献不足。这很重要，原因如下。</p><ul class=""><li id="b3d2" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">移除最不重要的特征将增加模型的准确性。这是因为我们通过移除不必要的特征来移除噪声</li><li id="c567" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">通过删除不必要的功能，您将避免过度拟合的问题。</li><li id="e92b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">较少的功能也减少了训练时间。</li></ul><p id="0965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论够了！让我们动手编写一些Python代码，为Iris数据集训练一个随机森林。</p><h1 id="84be" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">关于虹膜数据集</h1><p id="85f7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Iris数据集(<a class="ae ky" href="https://drive.google.com/file/d/1Q6wDxtB7ylxGhWNRvjcB3Y4K0B4E69xk/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处下载</a>)有150个观察值和4个数字属性。目标列(物种)由每个观察的类组成。有3类(0-setosa，1-versicolor，2-virginica)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/54ae14d8a5a1221320ad1f9e5501af2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*FI2rbljMAQXWy0C8HaAiSw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虹膜数据集的前5行(图片由作者提供)</p></figure><p id="7aff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集没有缺失值，所有要素都是数值型的。这意味着数据集无需任何预处理即可使用！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/077dd7dfdb692d736730645cc3c9168f.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*L6s5aQjCvrfJfyL82P3x1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虹膜数据集信息(图片由作者提供)</p></figure><h1 id="cbff" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">为Iris数据集训练随机森林分类器</h1><p id="1c59" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">运行以下代码后，您将获得0.97的模型准确性分数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f79e9f8ebf22a5752f659df52fb0b071.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*OIv2Frx4BWvpWcRbUb2CsQ.png"/></div></figure><p id="e9e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的随机森林里有100棵树。这是因为我们设置了<strong class="lb iu"> n_estimators=100 </strong>。因此，自举样本的数量也是100。</p><h1 id="f06a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">开箱(oob)评估</h1><p id="e3a8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在随机森林中，每个决策树都是使用观察值的自举子集来训练的。因此，每棵树都有一个单独的子集<strong class="lb iu"><em class="lv">【OOB】</em></strong>观察值。我们可以使用<strong class="lb iu"> <em class="lv"> oob </em> </strong>观察值作为验证集来评估我们的随机森林的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/015e85ac1b329be32306d18e1e4da313.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*ac6SsuCMUDT6Iv3n1zPNWQ.png"/></div></figure><p id="fa44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该值接近模型准确度分数0.97。</p><h1 id="15ff" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">可视化特征重要性</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4fee763d78d06b09fb9f09faac9c9c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*vjzkHMEBwT20oxMeJU8ffA.png"/></div></figure><p id="4979" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看特征重要性，我们可以决定删除<strong class="lb iu"> <em class="lv">【萼片宽度(cm)】</em></strong>特征，因为它对制作模型的贡献不足。</p><h1 id="b94e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">最后的想法</h1><p id="9880" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">决策树分类器和RandomForestClassifier等基于树的模型是分类任务中最常用的机器学习算法。如果要将模型解释为“为什么模型预测的是那种类”，最好使用普通的决策树算法，而不是随机森林。这是因为单个决策树很容易解释。但是请记住，正如我们前面讨论的，决策树算法有一些缺点。</p><p id="54a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用随机森林算法时，请按照指定的顺序执行下列操作。</p><ol class=""><li id="94f7" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu od mz na nb bi translated">首先，通过处理缺失值并将分类值转换为数值来预处理数据。</li><li id="7530" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">然后，将数据集分为训练测试和测试测试，不要将相同的数据用于训练和测试。这样做可以让模型记住数据，而不是学习任何模式。</li><li id="dd2b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">如下所述，在RandomForestClassifier中设置模型超参数。永远要考虑算法的性能和训练速度之间的平衡。例如，如果您在森林中包含更多的树，则性能较高，速度较慢。</li><li id="d07c" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">然后训练您的模型并可视化特征重要性。</li><li id="dab5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">移除不太重要的特征(如果有)，并使用所选特征重新训练模型。</li><li id="dc25" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">使用测试集测试您的模型，并获得准确性分数。</li></ol><h2 id="3d82" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">选择模型超参数</h2><ul class=""><li id="41e1" class="mt mu it lb b lc mo lf mp li oe lm of lq og lu my mz na nb bi translated"><strong class="lb iu"> n_estimators: </strong>森林中树木的数量。<strong class="lb iu"> </strong>默认为100。您可以使用与训练集中的观察次数相等的数字。</li><li id="c458" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> max_depth: </strong>树的最大深度。默认值为无。您可以首先训练一个决策树分类器，并为<strong class="lb iu"> <em class="lv"> max_depth </em> </strong>执行超参数调整。通过交叉验证和网格搜索获得最佳值后(我已经这样做了，并获得了值3)，可以将该值用于RandomForestClassifier的<strong class="lb iu"> <em class="lv"> </em> </strong>中的<strong class="lb iu"> <em class="lv"> max_depth </em> </strong>。</li><li id="1a8d" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> bootstrap: </strong>默认为真。使用此默认值执行引导抽样以获得不相关的树。</li><li id="1ca3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> oob_score: </strong>默认为False。如果您要执行袋外(oob)评估(交叉验证的替代方法),请将此项设置为True。</li></ul><p id="1219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要访问RandomForestClassifier的Scikit-learn官方文档，只需在从sklearn . ensemble import RandomForestClassifier导入类<strong class="lb iu"> <em class="lv">后执行<strong class="lb iu"><em class="lv">help(RandomForestClassifier)</em></strong>。</em></strong></p><p id="191f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="b32c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程由<a class="ae ky" href="https://www.linkedin.com/in/rukshan-manorathna-700a3916b/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Rukshan Pramoditha</em></a><em class="lv">，</em>数据科学365博客作者设计创作。</p><h2 id="7f2a" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">本教程中使用的技术</h2><ul class=""><li id="f8c3" class="mt mu it lb b lc mo lf mp li oe lm of lq og lu my mz na nb bi translated"><strong class="lb iu"> Python </strong>(高级编程语言)</li><li id="dd17" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">熊猫</strong> (Python数据分析与操纵库)</li><li id="7967" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> matplotlib </strong> (Python数据可视化库)</li><li id="662a" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> seaborn </strong> (Python高级数据可视化库)</li><li id="d2c3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> Scikit-learn </strong> (Python机器学习库)</li><li id="8c62" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> Jupyter笔记本</strong>(集成开发环境)</li></ul><h2 id="5d82" class="ni lx it bd ly nj nk dn mc nl nm dp mg li nn no mi lm np nq mk lq nr ns mm nt bi translated">本教程中使用的机器学习</h2><ul class=""><li id="3060" class="mt mu it lb b lc mo lf mp li oe lm of lq og lu my mz na nb bi translated"><strong class="lb iu">随机森林分类器</strong></li></ul><p id="f82e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">2020–10–29</p></div></div>    
</body>
</html>