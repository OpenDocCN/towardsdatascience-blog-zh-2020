<html>
<head>
<title>How ‘Copy-and-Paste’ is embedded in CNNs for Image Inpainting — Review: Shift-Net: Image Inpainting via Deep Feature Rearrangement</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“复制粘贴”是如何嵌入到细胞神经网络中用于图像修复的——综述:移位网络:通过深度特征重排的图像修复</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963?source=collection_archive---------27-----------------------#2020-10-16">https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963?source=collection_archive---------27-----------------------#2020-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4759" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大家好:)欢迎回来！！今天，我们将深入研究一种更具体的深度图像修复技术，<strong class="jp ir">深度特征重排</strong>。该技术结合了现代数据驱动细胞神经网络和传统复制粘贴修复方法的优点。让我们一起学习和享受吧！</p><h1 id="757b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">回忆</h1><p id="e333" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这是我的第五篇关于深度图像修复的文章。在<a class="ae lo" href="https://medium.com/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244" rel="noopener">我的第一篇帖子</a>中，我介绍了图像修复的目的和第一个基于GAN的图像修复方法。在<a class="ae lo" href="https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">我的第二个帖子</a>中，我们经历了第一个基于GAN的图像修复方法的改进版本，其中使用了纹理网络来增强局部纹理细节。在<a class="ae lo" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">我的第三篇文章</a>中，我们深入探讨了深层图像修复中的一个里程碑，其中提出的网络架构可以被视为图像修复的标准网络设计。在我的第四篇博文<a class="ae lo" rel="noopener" target="_blank" href="/revision-for-deep-image-inpainting-and-review-patch-based-image-inpainting-with-generative-4197d29c5468">中，我们修改并浏览了一个标准修复网络的变体/改进版本。如果你是这个话题的新手，我强烈建议你先看看之前的帖子。我希望你能对最近的深度图像修复的进展有一个全面的了解。我已经尽力讲好故事了:)</a></p><h1 id="b6d3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">动机</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/e683cc8f68496ecbe69c85b4375ed170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35F21D72bzCrMqsCmzkAeA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图一。不同方法修复效果的定性比较。(a)输入(b)常规方法(基于复制粘贴)(c)首先基于GAN的方法，上下文编码器(d)提出的方法。图片来自Yan等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="8231" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我在以前的帖子中提到的，填充图像中缺失部分的传统方法是搜索最相似的图像补丁，然后直接将这些补丁复制粘贴到缺失部分(即复制粘贴方法)。这种方法提供了良好的局部细节，因为我们直接粘贴其他图像补丁在丢失的部分。然而，补丁可能不完全适合整个图像的上下文，这可能导致较差的全局一致性。请看图1(b)为例，可以看到填充区域的局部纹理细节良好但与非缺失部分(即有效像素)不一致。</p><p id="b2be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，基于深度学习的方法关注整个图像的上下文。全连接层或扩展卷积层用于捕捉整个图像的上下文。使用L1损失来训练深度学习模型，以确保逐像素重建精度。因此，深度学习方法提供的填充图像具有更好的全局一致性。然而，L1损失导致模糊的修补结果，即使对抗损失(GAN损失)可用于增强填充像素的锐度。请参见图1(c)作为示例，您可以看到填充区域与非缺失区域更加一致，但是填充区域是模糊的。</p><p id="7c6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，本文的作者希望利用传统的“复制粘贴”方法(良好的局部细节)和现代深度学习方法(良好的全局一致性)的优势。</p><h1 id="527c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1><p id="7388" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在图像修复中，我们希望得到具有良好视觉质量的完整图像。因此，我们既需要<strong class="jp ir">正确的全局语义结构</strong>，也需要<strong class="jp ir">精细的细节纹理</strong>。正确的全局语义结构意味着生成的像素和有效的像素应该是一致的。换句话说，我们必须填充一个图像，并且必须保持它的上下文。精细的纹理意味着生成的像素应该看起来逼真，并且尽可能清晰。</p><p id="efbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上一节中，我们提到传统的“复制粘贴”方法可以提供精细的细节纹理，而最近的深度学习方法可以提供更好的正确的全局语义结构。因此，本文引入了一个移位连接层，通过在网络内部进行“复制粘贴”的概念来实现深度功能重组。图1(d)显示了他们提出的方法提供的修复结果。</p><h1 id="5788" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">解决方案(简而言之)</h1><p id="dcc8" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">提出一个<strong class="jp ir">引导损失</strong>来鼓励他们的网络(<strong class="jp ir">移位网</strong>)在解码过程中学习填补缺失部分。除此之外，建议使用<strong class="jp ir">移位连接层将缺失区域内的解码特征与缺失区域外的编码特征进行匹配</strong>，然后将缺失区域外的编码特征的每个匹配位置移位到缺失区域内的相应位置。这捕获了关于在丢失区域之外找到的最相似的局部图像补片的信息，并且该信息被连接到解码的特征，用于进一步的重建。</p><h1 id="e4a0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">贡献</h1><p id="a236" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">如前所述，提出了一个<strong class="jp ir">转换连接层，以在现代CNN</strong>中嵌入复制粘贴的概念，从而他们提出的模型可以提供具有正确的全局语义结构和精细细节纹理的修复结果。</p><p id="e707" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了标准L1和对抗性损失，他们还建议<strong class="jp ir">制导损失</strong>以端到端数据驱动的方式训练他们的移位网。</p><h1 id="0fbd" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">方法</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mf"><img src="../Images/8c091f2ea493994f5cd413167fe0bd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rbgYNfHM35WRfL_QoQt6bg.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图二。移动网的网络体系结构。以32×32的分辨率添加移位连接层。图片来自Yan等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="43d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图2显示了移位网络的网络架构。没有移位连接层，这是一个非常<strong class="jp ir">标准的U型网络结构，具有跳跃连接</strong>。请注意，编码要素会连接到解码要素的相应图层。就更好的局部视觉细节和重建精度而言，这种跳过连接对于包括图像修补的低级视觉任务是有用的。</p><h1 id="7914" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">导向损失</h1><p id="c864" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">提出用制导损耗来训练它们的移位网。简单来说，这个损失计算的是丢失区域内的输入屏蔽图像的解码特征和丢失区域内的基本事实的编码特征之间的<strong class="jp ir">差异</strong>。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mg"><img src="../Images/9eefd45eea42b10993474fa0841358bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nAUzQALWe5Q3E-iEMN1YcQ.png"/></div></div></figure><p id="98a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">先定义一下问题。设ω为缺失区域，ω(bar)为有效区域(即非缺失区域)。对于具有<em class="mh"> L </em>层的u网，<strong class="jp ir"> ϕ </strong> _ <em class="mh"> l </em> ( <em class="mh"> I </em>)表示第<em class="mh">l</em>-层的编码特征，<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)表示第<em class="mh">l</em>-<em class="mh">l</em>-层的解码特征我们的最终目标是恢复<em class="mh"> I^gt </em>(地面真相)，因此我们可以预计<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)包含了<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt)中几乎所有的信息如果我们考虑<strong class="jp ir">y</strong><strong class="jp ir">∈</strong>ω，(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>应该是0(即<strong class="jp ir">一幅输入的蒙版图像中第<em class="mh"> l </em>层缺失区域的编码特征为零</strong>)。所以，(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>应该包含了(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">y</strong>(即【t77这意味着解码过程应该填充丢失的区域。</em></p><p id="3294" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式1显示了(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>与(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">y</strong>之间的关系。注意，对于<strong class="jp ir">x</strong><strong class="jp ir">∈</strong>ω(条形)(即非缺失区域)，他们假设(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">x</strong>与(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">x【几乎相同因此，导向损失仅定义在缺失区域。如图2所示将<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)串联起来，几乎可以得到<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)中的所有信息。</strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mi"><img src="../Images/18d8efe2c612c06f9e33114708496a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nu5tEQPUSW0OeR6uK-Vjvw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图3。通过移位网络学习的特征的可视化。(a)输入(浅色区域表示缺失区域)(b)可视化(<strong class="bd mj">ϕ</strong>_<em class="mk">l</em>(<em class="mk">i^gt</em>)_<strong class="bd mj">y</strong>)(c)可视化(<strong class="bd mj">ϕ</strong>_<em class="mk">l</em>-<em class="mk">l</em>(<em class="mk">I</em>)_【t77</p></figure><p id="b62b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了进一步显示(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>)和(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">y</strong>之间的关系，作者将他们的移位网学习到的特征可视化，如图所示对比图3(b)和(c)可以看出(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>可以合理估计(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">y<strong class="jp ir">这导致模糊的修复结果，没有精细的纹理细节。这个问题通过他们提出的移位连接层得到解决，结果如图(d)所示。那么，我们来说说移位操作。</strong></strong></p><p id="240d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于对他们的观想方法感兴趣的读者，请参考他们的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>或者他们的<a class="ae lo" href="https://github.com/Zhaoyi-Yan/Shift-Net_pytorch" rel="noopener ugc nofollow" target="_blank"> github页面</a>。观想方法只是用来展示学到的特征，因此我不会在这里涵盖它。</p><h1 id="9fe4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">移位连接层</h1><p id="13df" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我个人认为这是本文的核心思想。回想一下<strong class="jp ir"> ϕ </strong> _ <em class="mh"> l </em> ( <em class="mh"> I </em>)和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)假设拥有<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)中几乎所有的信息。从上一节我们可以看到(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>可以合理估计(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt</em>)_<strong class="jp ir">y</strong>但是不够尖锐。让我们看看作者如何利用缺失区域外的特征来进一步增强缺失区域内的模糊估计。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ml"><img src="../Images/9a1be9272bfb04d2ac99e89a4485ad1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIv3kMBYaN5sLLSkmdMAXg.png"/></div></div></figure><p id="b873" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单来说，上面的等式4是为了<strong class="jp ir">找到丢失区域外与丢失区域内的每个解码特征最相似的编码特征</strong>。这是一个余弦相似运算。对于每个(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>与<strong class="jp ir">y</strong><strong class="jp ir">∈</strong>ω，我们在(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">中找到它最近的邻居输出<strong class="jp ir"> x </strong> *( <strong class="jp ir"> y </strong>)表示匹配特征位置的坐标，我们可以得到一个移位向量<strong class="jp ir">u _ y</strong>=<strong class="jp ir">x</strong>*(<strong class="jp ir">y)</strong>-<strong class="jp ir">y</strong>。注意，该移位操作可以被公式化为<a class="ae lo" href="https://arxiv.org/pdf/1601.04589.pdf" rel="noopener ugc nofollow" target="_blank">卷积层</a>。我将在下一篇文章中详细讨论这个问题。</strong></p><p id="548d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">得到移位向量后，我们可以重新排列(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">x</strong>)的空间位置，然后串接成<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)进一步增强(<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">x</strong>的空间重排如下，</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mm"><img src="../Images/f8b5f939295ea60886428517c55289ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clBB68hOnVG9Ou1dsDg6xw.png"/></div></div></figure><blockquote class="mn mo mp"><p id="43a3" class="jn jo mh jp b jq jr js jt ju jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj kk ij bi translated">口头上，对于丢失区域内的每个解码特征，在找到丢失区域外最相似的编码特征之后，我们基于移位向量形成另一组特征图。这组特征映射包含关于缺失区域外的编码特征与缺失区域内的解码特征最近的信息。然后，如图2所示，将所有相关信息进行组合，以便进一步重建。</p></blockquote><p id="f139" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我想强调一下关于转换连接层的几点。<strong class="jp ir"> <em class="mh"> i) </em> </strong>传统的“复制粘贴”方法在像素或图像补片域操作，而移位连接层在深度特征域操作。<strong class="jp ir"> <em class="mh"> ii) </em> </strong>深层特征是从大量训练数据中学习的，所有组件都是以端到端数据驱动的方式学习的。因此，使用“复制-粘贴”和CNN的优点都被继承了。</p><h1 id="15cd" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">损失函数</h1><p id="1b15" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">他们的损失函数非常标准。如前所述，除了我们介绍的建议<strong class="jp ir">制导损耗</strong>外，他们还采用了<strong class="jp ir"> L1损耗</strong>和<strong class="jp ir">标准对抗损耗</strong>。总损失函数如下，</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mt"><img src="../Images/dae13ce97592e8c573bb1c8d4507cb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFGIBiYwCqnsbIULZ4IYlg.png"/></div></div></figure><p id="9bf1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"><em class="mh">λg</em></strong>和<strong class="jp ir"><em class="mh">λadv</em></strong>分别用于控制制导损耗和敌方损耗的重要性。在他们的实验中，这两个超参数分别设置为0.01和0.002。</p><p id="dc3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你熟悉CNN的训练过程，你可能会注意到<strong class="jp ir">移位操作是一种手动修改特征图</strong>的操作。因此，我们必须修改关于特征的第<em class="mh"> l </em>层<em class="mh">f</em>_<em class="mh">l</em>=<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)的梯度计算。根据等式5，<strong class="jp ir">ϕ</strong>^<em class="mh">shift</em>_<em class="mh">l-l</em>(<em class="mh">I</em>)和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)之间的关系可以写成:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mu"><img src="../Images/aeff18c0efd67ff6bd557db692c31f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sIau6jr6pcCfmOW_lI5nPA.png"/></div></div></figure><p id="570e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> P </strong>为{0，1}的移位矩阵，<strong class="jp ir"> P </strong>的每一行中只有一个元素为1。元素1表示最近邻居的位置。因此，相对于<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)的梯度计算如下:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mv"><img src="../Images/1d8ed2269691bf2f560be4da72e6f100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTbgnwdpfy3tq8kZ4JjJLA.png"/></div></div></figure><p id="0651" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="mh"> F </em> ^ <em class="mh">跳过</em> _ <em class="mh"> l </em>代表跳过连接后的<em class="mh"> F </em> _ <em class="mh"> l </em>，以及<em class="mh"> F </em> ^ <em class="mh">跳过</em>_<em class="mh">l</em>=<em class="mh">f</em>_<em class="mh">l</em>。除了我们必须将移位矩阵<strong class="jp ir"> P </strong>的转置乘以最后一项以确保梯度被正确地反向传播之外，所有三项都可以被直接计算。</p><p id="ef7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也许，你会发现这部分有点难以理解，因为我们必须修改梯度的计算。对于那些对作者实际上是如何实现感兴趣的读者，我强烈推荐你访问他们的<a class="ae lo" href="https://github.com/Zhaoyi-Yan/Shift-Net_pytorch" rel="noopener ugc nofollow" target="_blank"> github页面</a>。如果你不理解这一部分，只要你能抓住移位操作的核心思想就没关系。在这里，<strong class="jp ir">他们的移位操作是一种硬赋值</strong>。这意味着<strong class="jp ir">缺失区域中的每个解码特征在缺失区域之外只能有一个最近邻</strong>。这就是为什么移位矩阵<strong class="jp ir"> P </strong>是{0，1}的形式，以及为什么我们必须修改梯度的计算。随后，提出了类似的移位操作思想，并采用了<strong class="jp ir">软赋值</strong>。在这种情况下，<strong class="jp ir">缺失区域外的所有邻居被分配权重以指示缺失区域内的每个解码特征的接近度</strong>，并且我们不需要修改梯度的计算，因为该操作是完全可微分的。我会在下一篇文章中详细讨论这个问题:)</p><h1 id="f427" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">实验</h1><p id="1208" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">作者在两个数据集上评估了他们的模型，即<strong class="jp ir">Paris street view</strong>【2】和<strong class="jp ir">six scenes from places 365-Standard</strong>【3】。Paris StreeView包含14，900幅训练图像和100幅测试图像。对于Places365，有来自365个场景的160万个训练图像。选择六个场景进行评估。每个场景有5000幅训练图像、900幅测试图像和100幅验证图像。对于两个数据集，他们调整每个图像的大小，使最小尺寸为350，然后随机裁剪一个大小为256×256的子图像作为模型的输入。</p><p id="cc9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于训练，他们使用Adam optimiser，学习率为0.0002，beta_1 = 0.5。批量大小被设置为1，并且训练时期的总数是30。注意，翻转被采用作为数据扩充。他们声称，在Nvidia Titan X Pascal GPU上训练他们的Shift-Net大约需要一天时间。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mw"><img src="../Images/a3fe6fce8e2662e8fc53cc78ffb4ff4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CfSb0lu1uZiRI6S6cLnwbA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图4。巴黎街景数据集上修复结果的可视化比较。(a)输入(b)内容感知填充(复制粘贴方法)(c)上下文编码器(d)多尺度神经补片合成(MNPS) (e)移位网。图片由严等人从他们的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【1】</p></figure><p id="db35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图4显示了在巴黎街景数据集上最先进方法的视觉比较。内容感知填充(图4(b))是利用复制粘贴概念的传统方法。你可以看到它提供了良好的局部纹理细节，但错误的全局语义结构。图4(c)和(d)分别是<a class="ae lo" href="https://medium.com/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244" rel="noopener">上下文编码器</a>和<a class="ae lo" href="https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc" rel="noopener">多尺度神经补片合成</a>的结果。我们之前已经回顾了这两种方法。你可以看到上下文编码器的结果具有正确的全局语义结构，但它们是模糊的。MNPS提供了比上下文编码器更好的结果，但是我们仍然可以很容易地观察到带有一点伪像的填充区域。相比之下，Shift-Net可以提供具有正确的全局语义结构和良好的局部纹理细节的修复结果。结果如图4(e)所示，请放大查看。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mx"><img src="../Images/617c77728966d3375e00063edb111550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MSiP60pj82SgEnQsU_l8iw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图5。Places数据集上修复结果的可视化比较。(a)输入(b)内容感知填充(复制粘贴方法)(c)上下文编码器(d)多尺度神经补片合成(MNPS) (e)移位网。图片来自Yan等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="08e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图5显示了在Places数据集上的最新方法的定性比较。进行了类似的观察，请放大以更好地查看局部纹理细节。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi my"><img src="../Images/96821286f2e6fe3f6b60ec5da3a0ae51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dNEffXeGGCqkKQr0326FRw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">表1。最新方法的定量比较。表由严等人从他们的论文[1]</p></figure><p id="fd12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">表1列出了巴黎StreeView数据集上的一些定量评估指标数字。显然，所提出的移位网络提供了最佳的PSNR、SSIM和平均l2损失。正如我在以前的帖子中提到的，这些数字与像素重建精度(客观评估)有关。它们不能反映修复结果的视觉质量。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mz"><img src="../Images/05837446838e97806d7a4e722b47aebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8XZ142YcDqdECu-iArM0w.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图6。填充随机区域的示例。从上到下:输入、内容感知填充和Shift-Net。图片由颜等人从他们的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【1】</p></figure><p id="eb76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图6显示了使用内容感知填充和提议的移位网络来填充随机区域的一些例子。Shift-Net能够以良好的视觉质量处理随机裁剪区域。请放大以更好地查看局部纹理细节。</p></div><div class="ab cl na nb hu nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ij ik il im in"><h1 id="5b23" class="kl km iq bd kn ko nh kq kr ks ni ku kv kw nj ky kz la nk lc ld le nl lg lh li bi translated">消融研究</h1><p id="9e2b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">作者还做了消融研究，以显示提出的制导损失和转移连接层的有效性。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nm"><img src="../Images/516242a674334a3024cdd5ba68977d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBkgkVKpKY8XzSJJ8w7vfA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图7。在标准U网和提议的移位网中提议的制导损失的影响。图片由严等人从他们的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【1】</p></figure><p id="6a10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图7显示了U-Net和Shift-Net在有和没有建议的引导损失的情况下的修复结果。很明显，引导损失对于减少视觉假象是有用的。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nn"><img src="../Images/a4ffa5f51bae71a5c6fbd28c4dc677f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJI1xjOrnI_wU0O2cegPWQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图8。不同λg对制导损失的影响。图片来自颜等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="4e74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图8示出了具有不同<em class="mh">λ_ g</em>的移位网的修复结果。我们可以看到当<em class="mh"> lambda_g </em> = 0.01时可以获得更好的修复效果。因此，他们根据经验为他们的实验设置<em class="mh">λ_ g</em>= 0.01。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi no"><img src="../Images/a16104b6f4417465b012899b67d610c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKwQyLexp1ekk2Il_97w-g.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图9。分层移位操作的效果图来自颜等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a> [1]</p></figure><p id="aa4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图9显示了不同层的移位操作的效果。回想一下，使用第<em class="mh"> l </em>层的特征，在第(<em class="mh"> L </em> - <em class="mh"> l </em>)层的深度特征图上执行移位操作。<strong class="jp ir">当<em class="mh"> l </em>越小时，特征图尺寸越大，因此该层的移位操作在计算上更加昂贵。当<em class="mh"> l </em>较大时，特征图尺寸较小，因此时间成本较低，但是随着空间尺寸较小，更多的空间信息丢失。这也可能导致修复效果不佳。</strong>在图9中，我们可以看到<em class="mh"> L </em> -3 (c)和<em class="mh"> L </em> -2 (d)都给出了不错的修复效果(可能是<em class="mh"> L </em> -2好一点)。请注意，<em class="mh"> L </em> -2处理一幅图像大约需要400毫秒，而<em class="mh"> L </em> -3处理一幅图像大约需要80毫秒。因此，为了平衡时间成本和性能，作者决定在第(<em class="mh"> L </em> -3)层执行移位操作。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi np"><img src="../Images/8760fe2060d53f7e6c4cbfe35e5bb0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YqZBCPKitmiK5sdOYQvB9g.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图10。零点出来的效果(b)<strong class="bd mj">ϕ</strong>_<em class="mk">l</em>-<em class="mk">l</em>(<em class="mk">I</em>)；(c)<strong class="bd mj">ϕ</strong>_<em class="mk">l</em>(<em class="mk">I</em>)；以及(d)<strong class="bd mj">ϕ</strong>^shift_<em class="mk">l</em>-<em class="mk">l</em>(<em class="mk">I</em>)(e)是Shift-Net的结果，使用所有(b)、(c)、(d)。图片来自Yan等人的论文【1】</p></figure><p id="791b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想图2，对于Shift-Net的架构，在shift-connection层之后串接了3个不同的特征图，分别是<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">I</em>)和<strong class="jp ir">ϕ</strong>^shift_<em class="mh">l</em>-<em class="mh">作者试图检查这些特征图对于最终重建的重要性，结果如图10所示。很明显，解码后的特征<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)对于最终的重建极其重要。如果我们将这个解码特征归零，重建将完全失败，如图10 (b)所示。这样，我们就可以知道解码后的特征<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)包含了缺失区域的主要结构和内容的信息。</em></p><p id="e573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图10 (c)显示了去除编码特征<strong class="jp ir"> ϕ </strong> _ <em class="mh"> l </em> ( <em class="mh"> I </em>)的结果。我们可以看到，主要结构仍然可以重建，但视觉质量比全模型差，如图10 (e)所示。这意味着，指导损失不仅有助于促进<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)_<strong class="jp ir">y</strong>和<strong class="jp ir">ϕ</strong>_<em class="mh">l</em>(<em class="mh">i^gt【t21)_<strong class="jp ir">y</strong>之间的关系，而且有助于促进<strong class="jp ir">ϕ</strong>_<em class="mh">l之间的关系</em></em></p><p id="8094" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，如果我们去掉图10 (d)所示的<strong class="jp ir">ϕ</strong>^shift_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)，在填充的缺失区域有明显的伪影。因此，我们可以知道，<strong class="jp ir">ϕ</strong>^shift_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)通过提供缺失区域外最近邻搜索的结果作为细化的参考，对于细化填充的缺失区域是有用的。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nq"><img src="../Images/7b042b0bf641334866c618bfd7741a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5RRx3VmC0NDKYlacjvBXGA.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">图11。从上到下。随机移位连接和最近邻搜索的移位网修复结果。图片来自严等人的<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>【1】</p></figure><p id="7de5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了进一步显示<strong class="jp ir">ϕ</strong>^shift_<em class="mh">l</em>-<em class="mh">l</em>(<em class="mh">I</em>)的有效性，作者比较了随机移位连接和最近邻搜索，如图11所示。我们可以看到，与最近邻搜索相比，随机移位连接对于改进修补结果以获得更好的全局语义结构一致性是无用的。因此，我们可以说，正确的移位操作对于获得视觉上良好的修复结果是重要的。</p><blockquote class="mn mo mp"><p id="8515" class="jn jo mh jp b jq jr js jt ju jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj kk ij bi translated">为了总结移位连接层的使用，我认为最重要的思想是我们提供对缺失区域内生成的特征的参考(假设生成的特征是缺失区域的良好估计)，以基于该参考进一步细化这些生成的特征，并且该参考是与从非缺失区域获得的每个生成的特征最相似的特征。因此，我们可以借用非缺失区域中的特征的结构和纹理来改进缺失区域中的特征。</p></blockquote><h1 id="2c16" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">其他需要注意的事项</h1><p id="bf34" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">有兴趣的读者，这里还有三点供你进一步研究。首先，您可能会对要素地图中的掩膜区域的定义感兴趣。实际上，我们只有输入遮罩图像。因此，必须定义特征地图的掩蔽区域。简单地说，作者使用与编码器具有相同架构的简单CNN来获得网络内部的掩蔽区域。使用简单的卷积和阈值技术来获得特征图中的掩蔽区域。如果你对此感兴趣，请阅读<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。我在这里不再赘述，因为我们将很快介绍一种可学习的方法来获得网络内部的掩码！</p><p id="eddf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二，关于生成器和鉴别器的详细架构。<a class="ae lo" rel="noopener" target="_blank" href="/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df">正如我在</a>之前提到的，我们有一个标准的图像修复网络设计，本文中的网络也非常标准。他们使用一个带有移位连接层的U-Net作为生成器，他们的鉴别器就是我们之前讨论过的<a class="ae lo" rel="noopener" target="_blank" href="/revision-for-deep-image-inpainting-and-review-patch-based-image-inpainting-with-generative-4197d29c5468"> PatchGAN鉴别器</a>。同样，感兴趣的读者可以参考<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>以获得架构的完整描述。</p><p id="d468" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第三，关于巴黎街景和地点数据集的更多比较可以在<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中找到。你可以看一看，你会对今天的深度图像修复算法的视觉质量有一个简单的想法。</p><h1 id="47fe" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结论</h1><p id="e8bd" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">本文主要有两个观点。首先，所提出的引导损失促使缺失区域的解码特征(给定输入屏蔽图像)接近缺失区域的编码特征(给定地面实况)。因此，解码过程应该能够用对地面实况中丢失区域的合理估计来填充丢失区域。第二，提出的移位连接层可以有效地借用缺失区域外最近邻的信息，进一步增强全局语义结构和局部纹理细节。</p><h1 id="bee7" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">外卖食品</h1><p id="8d12" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这篇文章对你们中的一些人来说可能有点超前。我认为本文最重要的思想是移位操作，它可以看作是一种应用于图像修复任务的注意技术。</p><p id="d251" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">口头上，移位操作允许缺失区域的生成特征借用缺失区域外最相似特征的信息。由于缺失区域外的特征具有良好的全局语义结构和精细的局部纹理细节，我们可以将它们作为参考来细化生成的特征。希望你能明白这个主旨。</p><h1 id="aa4f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">下一步是什么？</h1><p id="d363" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">在接下来的文章中，我们将介绍另一篇论文，它也利用了从非缺失区域借用信息的思想，以保持整个图像的上下文。我还将讨论如何以卷积层的形式进行最近邻搜索。真的希望你们喜欢我的帖子:)</p><h1 id="08fb" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">参考</h1><p id="6081" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">[1]，闫，，，，左，，，单，<a class="ae lo" href="https://arxiv.org/pdf/1801.09392.pdf" rel="noopener ugc nofollow" target="_blank"> Shift-Net:基于深度特征重排的图像修复</a>，<em class="mh"> Proc .2018年欧洲计算机视觉会议</em> ( <em class="mh"> ECCV </em>)。</p><p id="0e73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] C. Doersch、S. Singh、A. Gupta、J. Sivic和A. Efros。是什么让巴黎看起来像巴黎？2012年美国计算机学会图形汇刊。</p><p id="d08d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]周，Agata Lapedriza，Aditya Khosla，Aude Oliva，Antonio Torralba，“地点:用于场景识别的1000万图像数据库”，<em class="mh"> IEEE模式分析与机器智能汇刊</em>，2017。</p><p id="4871" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">再次感谢你阅读我的帖子！如果您有任何问题，请随时给我发电子邮件或在这里留言。下次见！:)</p></div></div>    
</body>
</html>