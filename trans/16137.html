<html>
<head>
<title>Understanding Fast-RCNN for Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解用于目标检测的快速RCNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-fast-rcnn-for-object-detection-7a1c3f63fc66?source=collection_archive---------36-----------------------#2020-11-06">https://towardsdatascience.com/understanding-fast-rcnn-for-object-detection-7a1c3f63fc66?source=collection_archive---------36-----------------------#2020-11-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="65d1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">快速递归神经网络综述</h2><div class=""/><div class=""><h2 id="131e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">fast-RCNN论文强调了SPPNet和RCNN的缺点，并建立了一个相对快速和准确的模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/89479451cf9146b486f41cdd00530049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nX-LomXmIT-yRrENCb685g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">瑞士伯尔尼——作者图片</p></figure><p id="74ad" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Fast-RCNN模型是通过克服<a class="ae ma" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank"> SPPNet </a>和<a class="ae ma" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank"> RCNN </a>的缺点而构建的。我已经写了关于这两个方面的文章，您应该在继续之前看一下:</p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/understanding-regions-with-cnn-features-r-cnn-ec69c15f8ea7"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">了解具有CNN功能的区域(R-CNN)</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">R-CNN的架构细节以及模型设计和论文的要点。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ky me"/></div></div></a></div><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/understanding-sppnet-for-object-detection-and-classification-682d6d2bdfb"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ja gy z fp mj fr fs mk fu fw iz bi translated">了解用于对象检测和分类的SPPNet</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">SPPNet允许可变大小的CNN输入图像，并可用于分类和对象检测</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms ky me"/></div></div></a></div><p id="4305" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该博客与上述两个博客的结构相同，即学生和教师之间的对话。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h2 id="7526" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">教师</h2><p id="5c8c" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">我们之前看过R-CNN和SPPNet。尽管这些模型表现得非常好，但它们都有一些缺点。以下是两种架构共有的<strong class="lg ja">缺点</strong>:</p><ul class=""><li id="678f" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated"><strong class="lg ja">多阶段训练:</strong>首先在ImageNet(预训练权重us)上训练分类模型，然后针对检测数据集进行微调。在微调之后，softmax被一个用于对象检测任务(在硬挖掘的数据上进行训练)的one-vs-rest分类器所取代。通过向最后一个池化图层的要素添加边界框回归器来提高性能。这是一个多阶段的过程，培训是一步一步进行的。</li><li id="6ba9" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">高空间和时间复杂度:</strong>在微调网络之后，并且在训练SVM和包围盒回归器之前，将特征缓存到磁盘以避免重复计算。生成这些特性需要大量时间，存储这些特性也需要数百GB的空间。</li></ul><p id="407c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">spp net特有的缺点:</strong></p><ul class=""><li id="2953" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated"><strong class="lg ja">低效微调卷积层:</strong>与R-CNN不同，SPP层很难更新它之前卷积层的权重。避免卷积层的微调会妨碍模型的性能。</li></ul><p id="fd4d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所有上述缺点都在Fast-RCNN论文中得到解决。顾名思义，它是RCNN的一个相对快速的版本，并且利用了SPPNet的一些架构细节。</p><h2 id="2026" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">学生</h2><p id="3d37" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">R-CNN论文中显示的深入分析和SPPNet层中引入的新颖SPP层让我非常惊讶，以至于我没有注意到任何这些缺点。<strong class="lg ja">您能解释一下Fast-RCNN中使用的模型架构吗？</strong></p><h2 id="1f2d" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">教师</h2><p id="e490" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">作者在Fast-RCNN论文中分析了三组模型:</p><ul class=""><li id="663d" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated"><strong class="lg ja">小号(S): </strong>卡芬内模型</li><li id="6dbe" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">VGG _美国有线电视新闻网_M_1024 (M): </strong>与卡芬内相似的型号，但更宽</li><li id="341b" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja"> VGG16 (L): </strong>非常深的VGG-16</li></ul><p id="2474" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将把我们的讨论限制在VGG-16(预先在ImageNet上训练过)，这是他们最深的网络。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8bf96b7e6dc274c4122658ee4eba3ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*4QaKULHkQECd-c5YHL0VFQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Fast-RCNN架构—论文</p></figure><p id="c7b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">输入图像被发送到VGG-16并被处理直到最后的卷积层(没有最后的汇集层)。并且在这之后，图像被发送到小说的<strong class="lg ja">感兴趣区域(RoI) </strong>池层。该池图层始终为最后一个卷积图层输出的每个要素地图输出一个7 x 7的地图。<strong class="lg ja">这个7 x 7的地图是通过池化产生的，其中窗口大小根据输入图像而改变。</strong>顺便说一句，来自该图的展平特征产生了与VGG-16的预训练FC-6层所期望的相同大小的特征向量。然而，最后的1000路softmax层被替换为21路Softmax(与RCNN和SPPNet情况下的SVM不同)。另外<strong class="lg ja">对于边界框回归器，分支从最后一个FC-7层</strong>开始，而不是卷积层特征图。</p><blockquote class="on oo op"><p id="dc77" class="le lf oq lg b lh li ka lj lk ll kd lm or lo lp lq os ls lt lu ot lw lx ly lz ij bi translated"><strong class="lg ja">注意:</strong>RoI pooling层只是SPP层的<strong class="lg ja">特例，其中只使用了一个金字塔等级。在这种情况下(7 x 7)。此外，每个子窗口和步距的计算来自SPPNet论文。</strong></p></blockquote><h2 id="819e" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">学生</h2><p id="462b" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">你上面提到的一个缺点是，<strong class="lg ja">SPP层不能有效地反向传播。请你解释一下这个问题是如何解决的？</strong>此外，<strong class="lg ja">这种架构如何解决空间和时间复杂性等其他问题？</strong></p><h2 id="5436" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">教师</h2><p id="c346" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated"><strong class="lg ja">当区域提议来自不同图像时，反向传播在SPP层变得无效</strong>。然而，他们提出了一种微调网络的有效方法。使用N=2个输入图像，并且对于每个图像，他们对每个图像采样R=128个RoI。此外，他们用地面真实边界框拍摄25% IoU大于0.5的前景图像。为IoU与地面真值箱之间的间隔[0.1，0.5]；这些建议被视为背景。提交人声称:</p><blockquote class="ou"><p id="66db" class="ov ow iq bd ox oy oz pa pb pc pd lz dk translated">较低的阈值0.1似乎是硬示例挖掘的启发</p></blockquote><blockquote class="on oo op"><p id="eddf" class="le lf oq lg b lh pe ka lj lk pf kd lm or pg lp lq os ph lt lu ot pi lx ly lz ij bi translated"><strong class="lg ja">注意:</strong>通过RoI合并层的反向传播具有与任何正常最大合并层类似的实现。作者已经用数学方法描述过了。更简单的理解，参考<a class="ae ma" href="https://datascience.stackexchange.com/a/11703" rel="noopener ugc nofollow" target="_blank">这个答案</a>。</p></blockquote><p id="9026" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">既然我们已经看到了反向传播在这个网络中是如何发生的，那么让我们也来看看多阶段训练的<strong class="lg ja">问题是如何处理的</strong>。作者没有单独训练，而是将包围盒回归器和softmax层一起训练。他们将这个损失函数<strong class="lg ja">命名为多任务损失函数</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pj"><img src="../Images/3737ae061f340b9e7feb340099601c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4WodmqUBEZhc6l2Bl_6yLw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">多任务损失函数—按作者分类的图像</p></figure><p id="d2ae" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在上图中:</p><ul class=""><li id="99b0" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated"><strong class="lg ja">类预测(p): </strong>每个RoI的离散概率分布[p = (p0，p1，p2 … pk)](包含k+1个类，其中k = 0为背景类)</li><li id="31b8" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">类别标签(u): </strong>是正确的类别</li><li id="6724" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">每次损失的权重(λ): </strong>该值始终等于<strong class="lg ja"> 1 </strong></li><li id="64ec" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">艾弗森括号函数[u ≥ 1]: </strong>这个赋值给<strong class="lg ja">其中一个类不是背景</strong>，否则为零。</li><li id="4827" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">预测边界框标签(t): </strong> t = (tx，ty，tw，th)给出所选RoI图像中<strong class="lg ja"> u </strong>中每个类别的预测边界框元组。</li><li id="323a" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">基本事实包围盒标签(v): </strong> v = (vx，vy，vw，vh)给出了<strong class="lg ja"> u </strong>中正确类的相应基本事实包围盒。</li></ul><p id="74a4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">口头上，<strong class="lg ja">交叉熵损失用于训练最后21路softmax层</strong>，而<strong class="lg ja"> smoothL1损失处理为84回归单元</strong>处理包围盒定位而添加的密集层的训练。这两个损失的总和用于微调剩余的网络，这与新的softmax和回归层的训练一起发生。</p><p id="753b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了证实这种新的训练方法不会妨碍表演，本文给出了以下分析。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pk"><img src="../Images/2e9a8a27b1008dd60de07e985f7b89df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7kLUnpHnTs_XUOyWz7gzA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者编辑的论文中的多任务训练分析</p></figure><p id="573c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">黄色框表示在训练或测试时没有边界框回归量的训练，而红色框表示使用多任务损失函数进行训练和微调后的结果。剩下的两列不言自明。<strong class="lg ja">需要注意的关键点是，使用这种多任务损失可以改善结果。</strong></p><h2 id="dc29" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">学生</h2><p id="ed9b" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">他们使用的损失函数确实改善了Pascal VOC 2007上的地图。<strong class="lg ja">那么，是不是由于这种多任务训练，避免了缓存这些特性，从而节省了我们将它们生成并写入磁盘的时间？</strong></p><h2 id="7ff5" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">教师</h2><p id="02b3" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">是的，这个观察是正确的。此外，作者还提供了一些关于提高速度的更多信息。据观察，将奇异值分解(SVD)应用于FC层，将运算分成两个矩阵乘法，并减少了计算时间。</p><blockquote class="on oo op"><p id="d075" class="le lf oq lg b lh li ka lj lk ll kd lm or lo lp lq os ls lt lu ot lw lx ly lz ij bi translated"><strong class="lg ja">奇异值分解:</strong>将一个矩阵分解成三个矩阵，其中一个对角矩阵夹在两个正交矩阵之间。对角矩阵表示特定轴上的方差，并且是降序排列的。因此，从对角矩阵中选择顶部的<strong class="lg ja"> t </strong>对角值，意味着选择在输出中贡献最大的值，因为它们具有高方差。<strong class="lg ja">(如果你对SVD的直觉不熟悉的话可以看看这个系列视频上的</strong> <a class="ae ma" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja">线性代数</strong> </a> <strong class="lg ja">然后再查看一下</strong> <a class="ae ma" href="https://www.youtube.com/watch?v=rYz83XPxiZo" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja">这个视频</strong> </a> <strong class="lg ja"> ) </strong></p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pl"><img src="../Images/0f5d650fb099393302cfc858aa1573ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_KNZuNS_dpoEj1jtmxHMkg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">SVD时序分析来自<a class="ae ma" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="bc1d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">虽然该模型比RCNN和SPPNet更快，但使用SVD可以在mAP下降最小的情况下缩短时间。对于上图，<strong class="lg ja">前1024个值选自FC-6层的25088 x 4096矩阵，前256个值选自4096 x 4096 FC-7层。</strong>下图显示了该型号在速度方面与其他型号相比的表现。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/1e1b9b50eb42e35de55265f74ad35564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*q8ZPcjtx6Gzs4f6UFwa-fQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">与另一个模型的时间比较— <a class="ae ma" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="b96a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">上图可以总结如下:</p><ul class=""><li id="31cf" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated"><strong class="lg ja">Fast-RCNN模型的训练速度比RCNN快9倍，预测速度比RCNN快213倍</strong></li><li id="311b" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated"><strong class="lg ja">快速RCNN也比SPPNet训练快3倍，预测快10倍，提高。</strong></li></ul><h2 id="4cae" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">学生</h2><p id="81d1" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">该文件是否提供了对他们架构的任何分析？</p><h2 id="5710" class="nb nc iq bd nd ne nf dn ng nh ni dp nj ln nk nl nm lr nn no np lv nq nr ns iw bi translated">教师</h2><p id="a4b7" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">通过实现前面描述的硬挖掘策略，反向传播变得很容易实现。然而，在深层VGG-16网络中哪些层需要微调也进行了探索，并在下面进行了描述。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7e05621c8bd16957805ab37c1438d531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*-hZnaUvcp4fz1n6RviGiqg.png"/></div></figure><p id="f3f1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作者发现，对于VGG-16，<strong class="lg ja">微调来自conv3_1的所有图层显著影响了地图。</strong>微调Conv2层时，训练速度下降，微调conv1层超过GPU内存。然而，训练Conv层后的结果显示，与未被训练的层相比，在mAP <strong class="lg ja">(从61.4%到66.9%) </strong>中有巨大的跳跃。因此，微调卷积层也变得至关重要，这是SPPNet的一个主要缺点。之前讨论的所有结果均从conv3_1开始微调。</p><p id="db72" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作者将他们的模型与当代模型进行了比较，快速RCNN的性能优于它们。我已经展示了pascal VOC10的结果。pascal VOC 2007和VOC 2012的结果可以在论文中看到。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi po"><img src="../Images/b926f91fae94a1cd59bde0fd032c4db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wh3_EIlGkkkFZN53h-iX3Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Pascal VOC 2010的结果— <a class="ae ma" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="3dda" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">本文分享的其他一些观察结果如下:</p><ul class=""><li id="df60" class="ny nz iq lg b lh li lk ll ln oa lr ob lv oc lz od oe of og bi translated">作者分析了改变输入网络的建议数量的影响。<strong class="lg ja">据观察，增加区域提案并不一定会增加地图。</strong></li><li id="3feb" class="ny nz iq lg b lh oh lk oi ln oj lr ok lv ol lz od oe of og bi translated">作者还尝试了在多尺度环境中的训练和测试，其中训练的规则保持与SPPNet(从最接近224的尺度中选择的区域建议)的规则相同。他们还使用了与SPPNet相同的一套标尺，但将最长的一边修剪到2000像素。<strong class="lg ja">据观察，虽然精确度有所提高，但单秤处理在速度和精确度之间提供了最佳平衡。</strong></li></ul><p id="ecf2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">论文中最重要的细节已经讨论过了，但是还是建议看一下论文。在阅读Fast-RCNN论文之前，请确保先阅读RCNN，然后再阅读SPPNet。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="807b" class="pp nc iq bd nd pq pr ps ng pt pu pv nj kf pw kg nm ki px kj np kl py km ns pz bi translated">参考</h1><p id="8843" class="pw-post-body-paragraph le lf iq lg b lh nt ka lj lk nu kd lm ln nv lp lq lr nw lt lu lv nx lx ly lz ij bi translated">R.Girshick，J. Donahue，T. Darrell，J. Malik，<a class="ae ma" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">用于精确对象检测和语义分割的丰富特征层次，</a>计算机视觉和模式识别，2014年</p><p id="6f67" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">K.何，X张，s任，孙军，<a class="ae ma" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络空间金字塔池</a>，，2014</p><p id="3c8c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">R.Girshick，<a class="ae ma" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a>，ICCV，2015</p></div></div>    
</body>
</html>