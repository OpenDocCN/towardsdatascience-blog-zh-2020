<html>
<head>
<title>How Transfer Learning works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习是如何工作的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-transfer-learning-works-a90bc4d93b5e?source=collection_archive---------33-----------------------#2020-10-26">https://towardsdatascience.com/how-transfer-learning-works-a90bc4d93b5e?source=collection_archive---------33-----------------------#2020-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84c1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">“迁移学习将是ML成功的下一个驱动力”——吴恩达。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4b3c19a7294c8c6b2eafef06d66e7b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iGRW7YlTmErdbjJb"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@element5digital?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Element5数码</a>拍摄</p></figure><p id="8723" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迁移学习是采用预训练的神经网络，并通过迁移或重新调整已学习的特征来使神经网络适应新的不同数据集的过程。例如，我们采用在<a class="ae kv" href="http://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上训练的模型，并使用该模型中学习到的权重来初始化全新数据集的训练和分类。例如，在Sebastrian Thrun 和他的团队发表的<a class="ae kv" href="https://www.nature.com/articles/nature21056.epdf?referrer_access_token=_snzJ5POVSgpHutcNN4lEtRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuP9jVts1q2g1KBbk3Pd3AelZ36FalmvJLxw1ypYW0UxU7iShiMp86DmQ5Sh3wOBhXDm9idRXzicpVoBBhnUsXHzVUdYCPiVV0Slqf-Q25Ntb1SX_HAv3aFVSRgPbogozIHYQE3zSkyIghcAppAjrIkw1HtSwMvZ1PXrt6fVYXt-dvwXKEtdCN8qEHg0vbfl4_m&amp;tracking_referrer=edition.cnn.com" rel="noopener ugc nofollow" target="_blank">研究工作</a>中，使用预先训练的模型对129，450张临床皮肤癌图像进行了分类，并取得了优异的结果。该方法在两个任务上与来自专家的结果达到相同水平的性能:皮肤癌识别和最致命皮肤癌任务的识别。这只是一个例子，它向我们表明，人工智能确实能够以堪比皮肤科医生的能力水平对皮肤癌进行分类。DeepMind的CEO对迁移学习有这样的看法:</p><blockquote class="ls lt lu"><p id="7fab" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">“我认为迁移学习是一般智力的关键。我认为进行迁移学习的关键是获得概念性知识，这些知识是从你从哪里学到的感性细节中抽象出来的。”——戴密斯·哈萨比斯(DeepMind首席执行官)</p></blockquote><p id="10fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迁移学习对于有限的计算资源也特别有用。在某些情况下，即使是在功能强大的GPU机器上进行训练，许多最先进的模型也需要几天甚至几周的时间来训练。因此，为了不在长时间内重复相同的过程，迁移学习允许我们利用预先训练的权重作为起点。</p><p id="aa74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从零开始学习很难做到，并且很难达到迁移学习方法中的相同性能水平。客观地说，我使用了一个AlexNe t架构来从头开始训练一个狗品种分类器，并在200个时期后达到了10%的准确率。在ImageNet上预先训练的同一个AlexNet在与冻结重量一起使用时，在50个时期内实现了69%的准确性。</p><p id="ca8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迁移学习通常包括在第一层中获取预训练的权重，这些权重通常是许多数据集的通用权重，并随机初始化最后几层，然后训练它们以用于分类目的。因此，在迁移学习方法中，学习或反向传播只发生在用随机权重初始化的最后几层。同时，有几种迁移学习的方法，我们使用的方法取决于我们想要根据预训练模型的数据集进行分类的新数据集的性质。迁移学习有四种主要情景或案例。</p><h1 id="234c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">转移学习场景</h1><ol class=""><li id="da62" class="mr ms iq ky b kz mt lc mu lf mv lj mw ln mx lr my mz na nb bi translated">与原始训练数据集相似的小型新数据集</li><li id="3d12" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">小但不同于原始训练数据集的新数据集</li><li id="831e" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">与原始训练数据集相似的大型新数据集</li><li id="4944" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">新数据集很大，但新数据不同于原始训练数据。</li></ol><h2 id="814f" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">我们说的相似数据集是什么意思？</h2><p id="1573" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">如果图像共享相似的特征，则它们是相似的。例如，猫和狗的图像会被认为是相似的，因为它们包含相似的特征，如眼睛、脸、毛发、腿等。植物的数据集将不同于动物图像的数据集，并且不会被认为是相似的。</p><p id="a083" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大型数据集的频率高达一百万次。小型数据集通常在几千个的范围内</p><h2 id="f741" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">AlexNet架构</h2><p id="37a7" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">我会用一个AlexNet来演示这个程序。AlexNet的架构如下图所示</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="58d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">案例1:小型相似数据集</strong></p><p id="ecf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果新数据集很小并且与原始训练数据相似:</p><ul class=""><li id="fa18" class="mr ms iq ky b kz la lc ld lf ny lj nz ln oa lr ob mz na nb bi translated">移除完全连接的神经网络的末端</li><li id="3a3a" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">添加一个新的全连接图层，其输出维度等于新数据集中的类数。</li><li id="a00d" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">随机化新的全连接层的权重；</li><li id="4281" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">冻结预训练网络的所有权重</li><li id="061a" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">训练网络以更新新的全连接层的权重</li></ul><p id="1693" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预训练模型的整个CNN层保持不变，即冻结，因为图像是相似的，并且它们将包含更高级别的特征。然而，这种方法有过度适应我们的小数据集的趋势。因此，原始预训练模型的权重保持不变，并且不被重新训练。</p><p id="d0ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的可视化如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/58cc230c6f7fc5933e2af7c4ae4e2ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*D3S8Cq0jtAIEVOr_BVt9zg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">小规模相似数据集的迁移学习方法</p></figure><p id="43e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的代码演示如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="86a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的第3行使我们能够冻结我们的权重，这样没有学习发生，权重保持不变。</p><p id="445d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的第6行使我们能够改变完全连接的层的输出形状。如果我们将其与原始AlexNet架构进行比较，我们会观察到输出大小为1000，但在这种情况下，我们的新数据集的输出大小为133。</p><h2 id="1187" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">案例2:小而不同的数据集</h2><p id="c9b5" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">如果新数据集很小并且不同于原始训练数据，则方法如下:</p><ul class=""><li id="f2f8" class="mr ms iq ky b kz la lc ld lf ny lj nz ln oa lr ob mz na nb bi translated">我们去掉全连接神经网络的末端和网络末端的一些CNN层。</li><li id="643d" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">添加一个新的全连接图层，其输出维度等于新数据集中的类数。</li><li id="a900" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">随机化新的全连接层的权重；</li><li id="0aab" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">冻结剩余预训练CNN网络的所有权重</li><li id="8b93" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">训练网络以更新新的全连接层的权重</li></ul><p id="d1ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，我们注意到数据集很小，但有所不同。因为我们的数据集是图像，所以我们离开网络的起点，并移除在完全连接的层之前提取更高特征的CNN层。然而，这种方法也有过度适应我们的小数据集的趋势。因此，原始预训练模型的权重保持不变，并且不被重新训练。</p><p id="4d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的可视化如下:</p><h2 id="3b16" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">案例3:大型相似数据集</h2><p id="fd3a" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">如果新数据集很大并且与原始训练数据相似，则方法如下:</p><ul class=""><li id="f0f9" class="mr ms iq ky b kz la lc ld lf ny lj nz ln oa lr ob mz na nb bi translated">移除完全连接的神经网络的末端</li><li id="7a94" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">添加一个新的全连接图层，其输出维度等于新数据集中的类数。</li><li id="2b0b" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">随机化新的全连接层的权重；</li><li id="f507" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">初始化预训练网络的权重</li><li id="55ed" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">训练网络以更新新的全连接层的权重</li></ul><p id="fd0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于新数据集与原始训练数据相似，因此不会从预训练网络中移除较高层要素。过度拟合不是一个潜在的问题；因此，我们可以重新训练所有的重量。</p><p id="3469" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是如何将这种方法形象化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c9db92387e1cd5a69464c8ab62f286ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*vtQKtZzvPPmM5XBaX0Xu_g.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">大数据集和小数据集的迁移学习方法</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e2db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的第3行允许我们将全连接层的输出权重从1000修改为133。网络用预先训练的权重初始化，并且学习发生在网络的所有层上。</p><p id="cb8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们不需要在这里指定requires_grad为False。默认值为真，因此新网络将仅使用预训练的权重作为训练过程的起点。</p><h2 id="8610" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">案例4:大型且不同的数据集</h2><p id="a2fc" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">如果新数据集很大并且不同于原始训练数据，则方法如下:</p><ul class=""><li id="d4a2" class="mr ms iq ky b kz la lc ld lf ny lj nz ln oa lr ob mz na nb bi translated">移除完全连接的神经网络的末端，并添加一个新的完全连接的层，其输出维度等于新数据集中的类的数量。</li><li id="e779" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">随机化新的全连接层的权重，并用随机权重初始化权重</li><li id="fd91" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr ob mz na nb bi translated">训练网络以更新新的全连接层的权重</li></ul><p id="1e2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，CNN层大部分是从零开始重新训练的。但是我们也可以用预先训练好的权重来初始化它。</p><p id="3ac7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是如何将这种方法形象化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/da2498135a58439a9860121cb0b4c868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*1-Gb0eua99u0OzXTha8uoQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">面向大规模不同数据集的迁移学习方法</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="60c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，当加载我们的模型时，我们简单地指定我们不需要预训练的重量，这更多的是从零开始训练。尽管如此，最好还是不要从完全随机的权重开始。从预先训练的重量开始在实践和文献中更常见。或者，我们用类似于<a class="ae kv" href="https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/" rel="noopener ugc nofollow" target="_blank"> Xavier初始化</a>、均匀分布初始化或仅仅是恒定值初始化的方法来开始或初始化我们的权重。</p><p id="fcfe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，每当你有很多图像要分类时，为什么不先尝试进行迁移学习，然后开始考虑如何调整模型，应用不同的转换技术来提高模型的性能。</p><p id="6954" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您一直以来的阅读。</p><h2 id="b3f9" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated"><strong class="ak">一些有用的资源</strong></h2><ol class=""><li id="0cb1" class="mr ms iq ky b kz mt lc mu lf mv lj mw ln mx lr my mz na nb bi translated"><a class="ae kv" href="https://modelzoo.co/" rel="noopener ugc nofollow" target="_blank">模型动物园</a>:这个网站包含了很多预先训练好的模型，你可以下载并用于你的数据集。这个<a class="ae kv" href="https://github.com/onnx/models" rel="noopener ugc nofollow" target="_blank"> github页面</a>是另一个有用的资源，它收集了预先训练好的模型。</li><li id="9b9f" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">这个<a class="ae kv" href="https://github.com/jimohafeezco/Udacity-AWS-ML/tree/master/capstone_project" rel="noopener ugc nofollow" target="_blank">库</a>包含了我用迁移学习做的狗品种分类器项目。</li><li id="d638" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">PyTorch关于迁移学习的教程可以在<a class="ae kv" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到</li><li id="2a27" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">关于迁移学习的Tensorflow教程可以在<a class="ae kv" href="https://www.tensorflow.org/tutorials/images/transfer_learning" rel="noopener ugc nofollow" target="_blank">这里</a>找到</li><li id="ee33" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">关于迁移学习的Keras教程可以在<a class="ae kv" href="https://keras.io/guides/transfer_learning/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li></ol><h2 id="1d39" class="nh ma iq bd mb ni nj dn mf nk nl dp mj lf nm nn ml lj no np mn ln nq nr mp ns bi translated">参考</h2><ol class=""><li id="b27e" class="mr ms iq ky b kz mt lc mu lf mv lj mw ln mx lr my mz na nb bi translated">Esteva、Andre、Brett Kuprel、Roberto A. Novoa、Justin Ko、Susan M. Swetter、Helen M. Blau和巴斯蒂安·特龙。"用深度神经网络对皮肤癌进行皮肤科医生级别的分类."<em class="lv">性质</em> 542，7639号(2017):115–118。</li><li id="a5ed" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">克里日夫斯基、亚历克斯、伊利亚·苏茨基弗和杰弗里·e·辛顿。"使用深度卷积神经网络的图像网络分类."在<em class="lv">神经信息处理系统进展</em>中，第1097–1105页。2012.</li><li id="6c89" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><a class="ae kv" href="https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/" rel="noopener ugc nofollow" target="_blank">https://www . deep learning wizard . com/deep _ learning/boosting _ models _ py torch/weight _ initial ization _ activation _ functions/</a></li></ol></div></div>    
</body>
</html>